body,comms_num,created,id,is_video,link_flair,num_crossposts,ops_flair,score,subreddit,subreddit_subs,thumbnail,title,url,whitelist_status,timestamp,time_up,thumbnail_size
,15,1528007219.0,8o44a3,False,,0,,13,statistics,54935,,Is statistical learning a subset of machine learning?,https://www.reddit.com/r/statistics/comments/8o44a3/is_statistical_learning_a_subset_of_machine/,all_ads,2018-06-03 02:26:59,-1 days +23:08:35.036584000,
"I understand the difference between the standard deviation of a random variable & the standard deviation of a statistic's sampling distribution, but I've encountered two subtly different definitions of ""standard error"".

The first definition is that the standard error of a statistic **is** the standard deviation of its sampling distribution, as described on these websites:

[http://www.ucl.ac.uk/ich/short\-courses\-events/about\-stats\-courses/stats\-rm/Chapter\_5\_Content/standard\_error](http://www.ucl.ac.uk/ich/short-courses-events/about-stats-courses/stats-rm/Chapter_5_Content/standard_error)

[https://www.investopedia.com/terms/s/standard\-error.asp](https://www.investopedia.com/terms/s/standard-error.asp)

[http://www.statisticssolutions.com/standard\-error/](http://www.statisticssolutions.com/standard-error/)

The second definition is that the standard error of a statistic is **an approximation of** the standard deviation of its sampling distribution, as described here:

[https://onlinecourses.science.psu.edu/stat100/node/56/](https://onlinecourses.science.psu.edu/stat100/node/56/)

and here:

[https://www.khanacademy.org/math/statistics\-probability/confidence\-intervals\-one\-sample/estimating\-population\-proportion/a/conditions\-inference\-one\-proportion](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/estimating-population-proportion/a/conditions-inference-one-proportion)

where it's specifically claimed that it's called ""standard error"" to distinguish it from the standard deviation of the sampling distribution, because it's only an approximation for it.

The Wikipedia article for [standard error](https://en.wikipedia.org/wiki/Standard_error) lists both these definitions in its first sentence, and it's talk page has some discussions debating which is the correct definition.

So, which is it?",6,1528014605.0,8o4w28,False,"I understand the difference between the standard deviation of a random variable & the standard deviation of a statistic's sampling distribution, but I've encountered two subtly different definitions of ""standard error"".

The first definition is that the standard error of a statistic **is** the standard deviation of its sampling distribution, as described on these websites:

[http://www.ucl.ac.uk/ich/short\-courses\-events/about\-stats\-courses/stats\-rm/Chapter\_5\_Content/standard\_error](http://www.ucl.ac.uk/ich/short-courses-events/about-stats-courses/stats-rm/Chapter_5_Content/standard_error)

[https://www.investopedia.com/terms/s/standard\-error.asp](https://www.investopedia.com/terms/s/standard-error.asp)

[http://www.statisticssolutions.com/standard\-error/](http://www.statisticssolutions.com/standard-error/)

The second definition is that the standard error of a statistic is **an approximation of** the standard deviation of its sampling distribution, as described here:

[https://onlinecourses.science.psu.edu/stat100/node/56/](https://onlinecourses.science.psu.edu/stat100/node/56/)

and here:

[https://www.khanacademy.org/math/statistics\-probability/confidence\-intervals\-one\-sample/estimating\-population\-proportion/a/conditions\-inference\-one\-proportion](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/estimating-population-proportion/a/conditions-inference-one-proportion)

where it's specifically claimed that it's called ""standard error"" to distinguish it from the standard deviation of the sampling distribution, because it's only an approximation for it.

The Wikipedia article for [standard error](https://en.wikipedia.org/wiki/Standard_error) lists both these definitions in its first sentence, and it's talk page has some discussions debating which is the correct definition.

So, which is it?",0,"I understand the difference between the standard deviation of a random variable & the standard deviation of a statistic's sampling distribution, but I've encountered two subtly different definitions of ""standard error"".

The first definition is that the standard error of a statistic **is** the standard deviation of its sampling distribution, as described on these websites:

[http://www.ucl.ac.uk/ich/short\-courses\-events/about\-stats\-courses/stats\-rm/Chapter\_5\_Content/standard\_error](http://www.ucl.ac.uk/ich/short-courses-events/about-stats-courses/stats-rm/Chapter_5_Content/standard_error)

[https://www.investopedia.com/terms/s/standard\-error.asp](https://www.investopedia.com/terms/s/standard-error.asp)

[http://www.statisticssolutions.com/standard\-error/](http://www.statisticssolutions.com/standard-error/)

The second definition is that the standard error of a statistic is **an approximation of** the standard deviation of its sampling distribution, as described here:

[https://onlinecourses.science.psu.edu/stat100/node/56/](https://onlinecourses.science.psu.edu/stat100/node/56/)

and here:

[https://www.khanacademy.org/math/statistics\-probability/confidence\-intervals\-one\-sample/estimating\-population\-proportion/a/conditions\-inference\-one\-proportion](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/estimating-population-proportion/a/conditions-inference-one-proportion)

where it's specifically claimed that it's called ""standard error"" to distinguish it from the standard deviation of the sampling distribution, because it's only an approximation for it.

The Wikipedia article for [standard error](https://en.wikipedia.org/wiki/Standard_error) lists both these definitions in its first sentence, and it's talk page has some discussions debating which is the correct definition.

So, which is it?",2,statistics,54935,,"Are there two competing definitions of ""standard error""?",https://www.reddit.com/r/statistics/comments/8o4w28/are_there_two_competing_definitions_of_standard/,all_ads,2018-06-03 04:30:05,-1 days +21:05:29.036584000,
"I think the answer is Mann Whiteny U statistics, but I would like confirmation.

Problem:

I have 100k data samples which are class imblanced \(10&#37; positive class, 90&#37; negative class\).  I make an algorithm A to discriminate between the two and it achieves an AUC of AUC\_A.  I make another algorithm B, which is an improvement on A, and measure its AUC called AUC\_B.  I want to determine if AUC\_B \> AUC\_A by chance or not \(statistically significant\).

What is an algorithm to determine this answer? \(say we set p\<0.001\)",34,1527965696.0,8nzt9k,False,"I think the answer is Mann Whiteny U statistics, but I would like confirmation.

Problem:

I have 100k data samples which are class imblanced \(10&#37; positive class, 90&#37; negative class\).  I make an algorithm A to discriminate between the two and it achieves an AUC of AUC\_A.  I make another algorithm B, which is an improvement on A, and measure its AUC called AUC\_B.  I want to determine if AUC\_B \> AUC\_A by chance or not \(statistically significant\).

What is an algorithm to determine this answer? \(say we set p\<0.001\)",0,"I think the answer is Mann Whiteny U statistics, but I would like confirmation.

Problem:

I have 100k data samples which are class imblanced \(10&#37; positive class, 90&#37; negative class\).  I make an algorithm A to discriminate between the two and it achieves an AUC of AUC\_A.  I make another algorithm B, which is an improvement on A, and measure its AUC called AUC\_B.  I want to determine if AUC\_B \> AUC\_A by chance or not \(statistically significant\).

What is an algorithm to determine this answer? \(say we set p\<0.001\)",21,statistics,54935,,I have 2 AUCs from the same data but 2 algorithms. How I determine if one of the AUCs is greater in a statistically signifant sense.,https://www.reddit.com/r/statistics/comments/8nzt9k/i_have_2_aucs_from_the_same_data_but_2_algorithms/,all_ads,2018-06-02 14:54:56,0 days 10:40:38.036584000,
"How would I analyze significant ratios of proportions between two specific items within multiple samples. In other words, if I had multiple samples, and within each sample, I was to take a ratio of one number of items to another, how would I overall judge the significance of the average ratio of the two items among all the samples?",3,1528021760.0,8o5kns,False,"How would I analyze significant ratios of proportions between two specific items within multiple samples. In other words, if I had multiple samples, and within each sample, I was to take a ratio of one number of items to another, how would I overall judge the significance of the average ratio of the two items among all the samples?",0,"How would I analyze significant ratios of proportions between two specific items within multiple samples. In other words, if I had multiple samples, and within each sample, I was to take a ratio of one number of items to another, how would I overall judge the significance of the average ratio of the two items among all the samples?",1,statistics,54935,,How do I analyze significance in this context (see below)?,https://www.reddit.com/r/statistics/comments/8o5kns/how_do_i_analyze_significance_in_this_context_see/,all_ads,2018-06-03 06:29:20,-1 days +19:06:14.036584000,
"First time poster here, so I'm not sure if this is an okay thing to post. If not I can remove it. Thanks in advance!

As the title says, what is Data Analysis and furthermore what does a Data Analyst do? 

So I recently graduated with my B.A. in Applied Mathematics and I'm looking for jobs in the field. I've done some statistical research with the biology department finding differences in phenotypes of certain gall wasps using excel and R. So with some guidance I can get work done, but I'm wondering if working for a business is similar or not. 

Since I graduated and am taking a gap year; I need a job. I've noticed that most in my area (Central Illinois) want a ""Business Analyst"" which prefers you have a CPA. 

I guess my question is what do each of these jobs do? Why are they different, and is working with a business similar to doing research?",8,1527995178.0,8o2sz0,False,"First time poster here, so I'm not sure if this is an okay thing to post. If not I can remove it. Thanks in advance!

As the title says, what is Data Analysis and furthermore what does a Data Analyst do? 

So I recently graduated with my B.A. in Applied Mathematics and I'm looking for jobs in the field. I've done some statistical research with the biology department finding differences in phenotypes of certain gall wasps using excel and R. So with some guidance I can get work done, but I'm wondering if working for a business is similar or not. 

Since I graduated and am taking a gap year; I need a job. I've noticed that most in my area (Central Illinois) want a ""Business Analyst"" which prefers you have a CPA. 

I guess my question is what do each of these jobs do? Why are they different, and is working with a business similar to doing research?",0,"First time poster here, so I'm not sure if this is an okay thing to post. If not I can remove it. Thanks in advance!

As the title says, what is Data Analysis and furthermore what does a Data Analyst do? 

So I recently graduated with my B.A. in Applied Mathematics and I'm looking for jobs in the field. I've done some statistical research with the biology department finding differences in phenotypes of certain gall wasps using excel and R. So with some guidance I can get work done, but I'm wondering if working for a business is similar or not. 

Since I graduated and am taking a gap year; I need a job. I've noticed that most in my area (Central Illinois) want a ""Business Analyst"" which prefers you have a CPA. 

I guess my question is what do each of these jobs do? Why are they different, and is working with a business similar to doing research?",3,statistics,54935,,"What is ""Data Analysis?""",https://www.reddit.com/r/statistics/comments/8o2sz0/what_is_data_analysis/,all_ads,2018-06-02 23:06:18,0 days 02:29:16.036584000,
"For my STATS final I'm doing a project to determine the correlation between a students grade level and their satisfaction with administration. My null hypothesis is that the two are independent. My alternative is that as grade level increases \(from 9 to 12\) satisfaction decreases. I collected data and found my observed values, but I'm not sure what to list as my expected, since the satisfaction rating was on a 1 out of 10 basis and there are 4 different grades. \(I'm using a chi\-squared test\) Any advice? Thanks! ",4,1527994512.0,8o2q54,False,"For my STATS final I'm doing a project to determine the correlation between a students grade level and their satisfaction with administration. My null hypothesis is that the two are independent. My alternative is that as grade level increases \(from 9 to 12\) satisfaction decreases. I collected data and found my observed values, but I'm not sure what to list as my expected, since the satisfaction rating was on a 1 out of 10 basis and there are 4 different grades. \(I'm using a chi\-squared test\) Any advice? Thanks! ",0,"For my STATS final I'm doing a project to determine the correlation between a students grade level and their satisfaction with administration. My null hypothesis is that the two are independent. My alternative is that as grade level increases \(from 9 to 12\) satisfaction decreases. I collected data and found my observed values, but I'm not sure what to list as my expected, since the satisfaction rating was on a 1 out of 10 basis and there are 4 different grades. \(I'm using a chi\-squared test\) Any advice? Thanks! ",2,statistics,54935,,What is my expected value?,https://www.reddit.com/r/statistics/comments/8o2q54/what_is_my_expected_value/,all_ads,2018-06-02 22:55:12,0 days 02:40:22.036584000,
"A: Mean 9.79 SD 6.039331469 N 125

B: Mean 12.38608696 SD 6.309014243 N 115

C: Mean 11.19259259 SD 6.588981981 N 81

Group data given above. Comparing means using t-testing. Can someone explain to me in words why this is the case? In my head if A is different from B and B is the same as C then A should be different from C OR A should not be different from B
",8,1528004189.0,8o3spj,False,"A: Mean 9.79 SD 6.039331469 N 125

B: Mean 12.38608696 SD 6.309014243 N 115

C: Mean 11.19259259 SD 6.588981981 N 81

Group data given above. Comparing means using t-testing. Can someone explain to me in words why this is the case? In my head if A is different from B and B is the same as C then A should be different from C OR A should not be different from B
",1,"A: Mean 9.79 SD 6.039331469 N 125

B: Mean 12.38608696 SD 6.309014243 N 115

C: Mean 11.19259259 SD 6.588981981 N 81

Group data given above. Comparing means using t-testing. Can someone explain to me in words why this is the case? In my head if A is different from B and B is the same as C then A should be different from C OR A should not be different from B
",1,statistics,54935,,"A is significantly different from B, B is NOT significantly different from C, A is NOT significantly different from C. How is this possible?",https://www.reddit.com/r/statistics/comments/8o3spj/a_is_significantly_different_from_b_b_is_not/,all_ads,2018-06-03 01:36:29,-1 days +23:59:05.036584000,
" Assume a survey plot that shows a histogram of percentages of respondent's answers, like [this](https://i.stack.imgur.com/FT7ZA.png) which shows ""*Percentages of respondents' answers for five tornado warning behaviors across six lead time and daylight scenarios*"".

Do error bars make any sense for a plot like that? As far as I know, error bars are useful only when showing averages or median values.",6,1527970601.0,8o061f,False," Assume a survey plot that shows a histogram of percentages of respondent's answers, like [this](https://i.stack.imgur.com/FT7ZA.png) which shows ""*Percentages of respondents' answers for five tornado warning behaviors across six lead time and daylight scenarios*"".

Do error bars make any sense for a plot like that? As far as I know, error bars are useful only when showing averages or median values.",0," Assume a survey plot that shows a histogram of percentages of respondent's answers, like [this](https://i.stack.imgur.com/FT7ZA.png) which shows ""*Percentages of respondents' answers for five tornado warning behaviors across six lead time and daylight scenarios*"".

Do error bars make any sense for a plot like that? As far as I know, error bars are useful only when showing averages or median values.",3,statistics,54935,,Does it make sense to add error bars in a histogram?,https://www.reddit.com/r/statistics/comments/8o061f/does_it_make_sense_to_add_error_bars_in_a/,all_ads,2018-06-02 16:16:41,0 days 09:18:53.036584000,
"Long story short I'm doing a ML and Data Mining program and I'm not too happy with my understanding of stats compared to other math fields and cs but I'd like to really understand it. A couple weeks ago I asked a question similar to this but asked if there were any good books on these topics and I got recommended around 3 books. Books are great and it's going okay but when it comes to stats, I'm starting to find that textbooks are a little rough for me in terms of following along for some reason and I heavily prefer online courses or video series that are nicely setup (such as a class would be I guess). I've searched around and found a couple of courses on edx or youtube and places alike.

I would appreciate it if these were free courses since I've managed to learn CS online using free resources as information is pretty available these days, but if it's really amazing, I wouldn't mind spending a bit of money.

These are the courses I'd be taking

* **Introduction to Numerical Algorithms for Computational Mathematics (Not really a stats course but thought I'd throw it out there)** - An introduction to computational methods for solving problems in linear algebra, non-linear equations, approximation and integration. Floating-point arithmetic; numerical algorithms; application of numerical software packages.

* **Stochastic Processes** - This course continues the development of probability theory begun in STAB52H3. Topics covered include finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* **Regression Analysis** - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models. Instruction in the use of SAS.

* **Statistical Inference** - Principles of statistical reasoning and theories of statistical analysis. Topics include: statistical models, likelihood theory, repeated sampling theories of inference, prior elicitation, Bayesian theories of inference, decision theory, asymptotic theory, model checking, and checking for prior-data conflict. Advantages and disadvantages of the different theories.

",0,1527988706.0,8o21q1,False,"Long story short I'm doing a ML and Data Mining program and I'm not too happy with my understanding of stats compared to other math fields and cs but I'd like to really understand it. A couple weeks ago I asked a question similar to this but asked if there were any good books on these topics and I got recommended around 3 books. Books are great and it's going okay but when it comes to stats, I'm starting to find that textbooks are a little rough for me in terms of following along for some reason and I heavily prefer online courses or video series that are nicely setup (such as a class would be I guess). I've searched around and found a couple of courses on edx or youtube and places alike.

I would appreciate it if these were free courses since I've managed to learn CS online using free resources as information is pretty available these days, but if it's really amazing, I wouldn't mind spending a bit of money.

These are the courses I'd be taking

* **Introduction to Numerical Algorithms for Computational Mathematics (Not really a stats course but thought I'd throw it out there)** - An introduction to computational methods for solving problems in linear algebra, non-linear equations, approximation and integration. Floating-point arithmetic; numerical algorithms; application of numerical software packages.

* **Stochastic Processes** - This course continues the development of probability theory begun in STAB52H3. Topics covered include finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* **Regression Analysis** - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models. Instruction in the use of SAS.

* **Statistical Inference** - Principles of statistical reasoning and theories of statistical analysis. Topics include: statistical models, likelihood theory, repeated sampling theories of inference, prior elicitation, Bayesian theories of inference, decision theory, asymptotic theory, model checking, and checking for prior-data conflict. Advantages and disadvantages of the different theories.

",0,"Long story short I'm doing a ML and Data Mining program and I'm not too happy with my understanding of stats compared to other math fields and cs but I'd like to really understand it. A couple weeks ago I asked a question similar to this but asked if there were any good books on these topics and I got recommended around 3 books. Books are great and it's going okay but when it comes to stats, I'm starting to find that textbooks are a little rough for me in terms of following along for some reason and I heavily prefer online courses or video series that are nicely setup (such as a class would be I guess). I've searched around and found a couple of courses on edx or youtube and places alike.

I would appreciate it if these were free courses since I've managed to learn CS online using free resources as information is pretty available these days, but if it's really amazing, I wouldn't mind spending a bit of money.

These are the courses I'd be taking

* **Introduction to Numerical Algorithms for Computational Mathematics (Not really a stats course but thought I'd throw it out there)** - An introduction to computational methods for solving problems in linear algebra, non-linear equations, approximation and integration. Floating-point arithmetic; numerical algorithms; application of numerical software packages.

* **Stochastic Processes** - This course continues the development of probability theory begun in STAB52H3. Topics covered include finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* **Regression Analysis** - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models. Instruction in the use of SAS.

* **Statistical Inference** - Principles of statistical reasoning and theories of statistical analysis. Topics include: statistical models, likelihood theory, repeated sampling theories of inference, prior elicitation, Bayesian theories of inference, decision theory, asymptotic theory, model checking, and checking for prior-data conflict. Advantages and disadvantages of the different theories.

",1,statistics,54935,,Free online course recommendation for these topics,https://www.reddit.com/r/statistics/comments/8o21q1/free_online_course_recommendation_for_these_topics/,all_ads,2018-06-02 21:18:26,0 days 04:17:08.036584000,
"Hey,

I'm analyzing some SEO data from an SEO tool. I input some keywords and it spits out search result rankings for a list of competitors.

I want to analyze average ranking based on a few categorical variables (industry, location), but there is one problem. The SEO software spits out the value "">100"" when a competitor ranks in a position greater than 100. 

This happens pretty frequently in the data so I can't average the positions.

Is there any way to measure search ranking performance when the value "">100"" pops up sometimes. If it didn't show up, I could just average the rankings for each industry, location and competitor.

I'm doing a competitive analysis of search ranking.

Any help is greatly appreciated.",0,1527987790.0,8o1xzm,False,"Hey,

I'm analyzing some SEO data from an SEO tool. I input some keywords and it spits out search result rankings for a list of competitors.

I want to analyze average ranking based on a few categorical variables (industry, location), but there is one problem. The SEO software spits out the value "">100"" when a competitor ranks in a position greater than 100. 

This happens pretty frequently in the data so I can't average the positions.

Is there any way to measure search ranking performance when the value "">100"" pops up sometimes. If it didn't show up, I could just average the rankings for each industry, location and competitor.

I'm doing a competitive analysis of search ranking.

Any help is greatly appreciated.",0,"Hey,

I'm analyzing some SEO data from an SEO tool. I input some keywords and it spits out search result rankings for a list of competitors.

I want to analyze average ranking based on a few categorical variables (industry, location), but there is one problem. The SEO software spits out the value "">100"" when a competitor ranks in a position greater than 100. 

This happens pretty frequently in the data so I can't average the positions.

Is there any way to measure search ranking performance when the value "">100"" pops up sometimes. If it didn't show up, I could just average the rankings for each industry, location and competitor.

I'm doing a competitive analysis of search ranking.

Any help is greatly appreciated.",1,statistics,54935,,Average Search Result Ranking with Cutoff Value,https://www.reddit.com/r/statistics/comments/8o1xzm/average_search_result_ranking_with_cutoff_value/,all_ads,2018-06-02 21:03:10,0 days 04:32:24.036584000,
I'm a now high school junior interested in learning statistics not only to get ahead in school but to also learn data science and machine learning. I considered reading \*an introduction to statistical learning with applications in R\* but I don't think this summer I simply don't have the time to extensively read such books. I was wondering if anyone knew of any comprehensive courses I could use to self learn statistics for data science \(and I guess school too\). ,4,1527944039.0,8nycog,False,I'm a now high school junior interested in learning statistics not only to get ahead in school but to also learn data science and machine learning. I considered reading \*an introduction to statistical learning with applications in R\* but I don't think this summer I simply don't have the time to extensively read such books. I was wondering if anyone knew of any comprehensive courses I could use to self learn statistics for data science \(and I guess school too\). ,0,I'm a now high school junior interested in learning statistics not only to get ahead in school but to also learn data science and machine learning. I considered reading \*an introduction to statistical learning with applications in R\* but I don't think this summer I simply don't have the time to extensively read such books. I was wondering if anyone knew of any comprehensive courses I could use to self learn statistics for data science \(and I guess school too\). ,5,statistics,54935,,Best way to learn statistics online?,https://www.reddit.com/r/statistics/comments/8nycog/best_way_to_learn_statistics_online/,all_ads,2018-06-02 08:53:59,0 days 16:41:35.036584000,
"Hello redditors,

I’ve struggled these couple of days trying to find a solution to my little statistical problem of mine; I was thus curious to know if somebody would have an idea as to how to deal with this. So...

**First step**: I have these data, age standardized suicide rates \(per 100,000 people\) plotted over time \(from 1979 to 2014\).

**Second step**: I later found a positive correlation between suicide rates and unemployment – around .6. No problem as of now, but…

**Third step:** Now I was wondering if somebody knew how to adjust the original suicide line, plotted over 35 years, to take into account, or ‘’control’’ the effect of unemployment. In summary, I would like to plot a new line, of suicide rates, per 100 000, but this time I would like to remove the influence of unemployment, which is positively correlated. What would the line look like?

Would anybody know where to start?

Thank you very much!

[https://i.gyazo.com/03636a4c7c6fa84ea7a9c567e5d54021.png](https://i.gyazo.com/03636a4c7c6fa84ea7a9c567e5d54021.png)",5,1527911690.0,8nuxfe,False,"Hello redditors,

I’ve struggled these couple of days trying to find a solution to my little statistical problem of mine; I was thus curious to know if somebody would have an idea as to how to deal with this. So...

**First step**: I have these data, age standardized suicide rates \(per 100,000 people\) plotted over time \(from 1979 to 2014\).

**Second step**: I later found a positive correlation between suicide rates and unemployment – around .6. No problem as of now, but…

**Third step:** Now I was wondering if somebody knew how to adjust the original suicide line, plotted over 35 years, to take into account, or ‘’control’’ the effect of unemployment. In summary, I would like to plot a new line, of suicide rates, per 100 000, but this time I would like to remove the influence of unemployment, which is positively correlated. What would the line look like?

Would anybody know where to start?

Thank you very much!

[https://i.gyazo.com/03636a4c7c6fa84ea7a9c567e5d54021.png](https://i.gyazo.com/03636a4c7c6fa84ea7a9c567e5d54021.png)",0,"Hello redditors,

I’ve struggled these couple of days trying to find a solution to my little statistical problem of mine; I was thus curious to know if somebody would have an idea as to how to deal with this. So...

**First step**: I have these data, age standardized suicide rates \(per 100,000 people\) plotted over time \(from 1979 to 2014\).

**Second step**: I later found a positive correlation between suicide rates and unemployment – around .6. No problem as of now, but…

**Third step:** Now I was wondering if somebody knew how to adjust the original suicide line, plotted over 35 years, to take into account, or ‘’control’’ the effect of unemployment. In summary, I would like to plot a new line, of suicide rates, per 100 000, but this time I would like to remove the influence of unemployment, which is positively correlated. What would the line look like?

Would anybody know where to start?

Thank you very much!

[https://i.gyazo.com/03636a4c7c6fa84ea7a9c567e5d54021.png](https://i.gyazo.com/03636a4c7c6fa84ea7a9c567e5d54021.png)",24,statistics,54935,,Time series: How to control the effect of a correlated variable?,https://www.reddit.com/r/statistics/comments/8nuxfe/time_series_how_to_control_the_effect_of_a/,all_ads,2018-06-01 23:54:50,1 days 01:40:44.036584000,
"This question is more for my own learning and is based around a side-project I am tackling to help me learn Bayesian methods. Being trained more in frequentist methods I'm having to adjust, but I recognize there is value in applying Bayesian methods for some problems.

The Bayesian bootstrap treats sampling with replacement as drawing weights for each sample from a Dirichlet distribution. This part seems intuitive but I am basing it on my understanding of the Frequentist approach using the Multinomial. It seems Bayesians treat this more as estimating a posterior rather than a number of draws per sample but I digress.

However, what I am unsure about is how you'd sample with replacement M < N samples using Bayesian bootstrap methods, where N is our number of samples, and M is some smaller number.

Specifically for my data I have a time-series for two years worth of some prices (normalized to be percent difference with lag = 1). I checked for autocorrelation and there doesn't seem to be any.

However, I am not interested in looking at two years worth of bootstrapped samples, instead I'd like to pull about a 90 days of this data out at a time. The goal is to see how a quarter might look from two years worth of data.

I tried using random draws from a Dirichlet(1,1,...,1) for the whole data set, then keeping the 90 highest numbers before renormalizing so the sum of weights still = 1. However I have no idea if this is principled or not. 

I checked a few QQ plots and this method seems to produce weights that are similarly distributed to drawing from a Dirichlet(1, 1, ..., 1) with M entries but there are usually some differences I am not sure are due to random chance or not.

Does anyone have some reading material links or advice on how to approach?",2,1527925565.0,8nwkx1,False,"This question is more for my own learning and is based around a side-project I am tackling to help me learn Bayesian methods. Being trained more in frequentist methods I'm having to adjust, but I recognize there is value in applying Bayesian methods for some problems.

The Bayesian bootstrap treats sampling with replacement as drawing weights for each sample from a Dirichlet distribution. This part seems intuitive but I am basing it on my understanding of the Frequentist approach using the Multinomial. It seems Bayesians treat this more as estimating a posterior rather than a number of draws per sample but I digress.

However, what I am unsure about is how you'd sample with replacement M < N samples using Bayesian bootstrap methods, where N is our number of samples, and M is some smaller number.

Specifically for my data I have a time-series for two years worth of some prices (normalized to be percent difference with lag = 1). I checked for autocorrelation and there doesn't seem to be any.

However, I am not interested in looking at two years worth of bootstrapped samples, instead I'd like to pull about a 90 days of this data out at a time. The goal is to see how a quarter might look from two years worth of data.

I tried using random draws from a Dirichlet(1,1,...,1) for the whole data set, then keeping the 90 highest numbers before renormalizing so the sum of weights still = 1. However I have no idea if this is principled or not. 

I checked a few QQ plots and this method seems to produce weights that are similarly distributed to drawing from a Dirichlet(1, 1, ..., 1) with M entries but there are usually some differences I am not sure are due to random chance or not.

Does anyone have some reading material links or advice on how to approach?",0,"This question is more for my own learning and is based around a side-project I am tackling to help me learn Bayesian methods. Being trained more in frequentist methods I'm having to adjust, but I recognize there is value in applying Bayesian methods for some problems.

The Bayesian bootstrap treats sampling with replacement as drawing weights for each sample from a Dirichlet distribution. This part seems intuitive but I am basing it on my understanding of the Frequentist approach using the Multinomial. It seems Bayesians treat this more as estimating a posterior rather than a number of draws per sample but I digress.

However, what I am unsure about is how you'd sample with replacement M < N samples using Bayesian bootstrap methods, where N is our number of samples, and M is some smaller number.

Specifically for my data I have a time-series for two years worth of some prices (normalized to be percent difference with lag = 1). I checked for autocorrelation and there doesn't seem to be any.

However, I am not interested in looking at two years worth of bootstrapped samples, instead I'd like to pull about a 90 days of this data out at a time. The goal is to see how a quarter might look from two years worth of data.

I tried using random draws from a Dirichlet(1,1,...,1) for the whole data set, then keeping the 90 highest numbers before renormalizing so the sum of weights still = 1. However I have no idea if this is principled or not. 

I checked a few QQ plots and this method seems to produce weights that are similarly distributed to drawing from a Dirichlet(1, 1, ..., 1) with M entries but there are usually some differences I am not sure are due to random chance or not.

Does anyone have some reading material links or advice on how to approach?",10,statistics,54935,,Bayesian Bootstrap (For M < N Samples),https://www.reddit.com/r/statistics/comments/8nwkx1/bayesian_bootstrap_for_m_n_samples/,all_ads,2018-06-02 03:46:05,0 days 21:49:29.036584000,
"Hi all, I have a project I want to start that will have to use crowdsourced data and surveys. The questions I want to explore are a bit controversial (essentially involving the role appearances play in certain outcomes), and the data I would need to collect may be personal. I can and will of course anonymize it, however, I realize that even with careful planning and best intentions I can mess up and hurt people, pissing them off and potentially expose myself to liability. I'm sure there are good resources for how to go about doing things like this correctly - any recommendations?",2,1527935443.0,8nxl7j,False,"Hi all, I have a project I want to start that will have to use crowdsourced data and surveys. The questions I want to explore are a bit controversial (essentially involving the role appearances play in certain outcomes), and the data I would need to collect may be personal. I can and will of course anonymize it, however, I realize that even with careful planning and best intentions I can mess up and hurt people, pissing them off and potentially expose myself to liability. I'm sure there are good resources for how to go about doing things like this correctly - any recommendations?",0,"Hi all, I have a project I want to start that will have to use crowdsourced data and surveys. The questions I want to explore are a bit controversial (essentially involving the role appearances play in certain outcomes), and the data I would need to collect may be personal. I can and will of course anonymize it, however, I realize that even with careful planning and best intentions I can mess up and hurt people, pissing them off and potentially expose myself to liability. I'm sure there are good resources for how to go about doing things like this correctly - any recommendations?",3,statistics,54935,,Good course/module/series on correct sourcing ethics?,https://www.reddit.com/r/statistics/comments/8nxl7j/good_coursemoduleseries_on_correct_sourcing_ethics/,all_ads,2018-06-02 06:30:43,0 days 19:04:51.036584000,
"I’ve been teaching myself stats and have hit an obstacle in Chapter 10 in the book I’m using. The book problem is provided in the link below, there was another Reddit question on this but the answer given wasn’t right according to the book.

[Book Problem](https://www.google.com/amp/s/amp.reddit.com/r/HomeworkHelp/comments/486rvc/university_business_statistics_hypothesis_testing/)

I have the answers to the book, but in this particular case I don’t understand how the hypothesis test was set up and, how the book determined how to define which score was population mean 1 and population mean 2. I

Depending on which score is u1 and u2, it affects whether the test statistic is positive or negative


",2,1527950301.0,8nyu1z,False,"I’ve been teaching myself stats and have hit an obstacle in Chapter 10 in the book I’m using. The book problem is provided in the link below, there was another Reddit question on this but the answer given wasn’t right according to the book.

[Book Problem](https://www.google.com/amp/s/amp.reddit.com/r/HomeworkHelp/comments/486rvc/university_business_statistics_hypothesis_testing/)

I have the answers to the book, but in this particular case I don’t understand how the hypothesis test was set up and, how the book determined how to define which score was population mean 1 and population mean 2. I

Depending on which score is u1 and u2, it affects whether the test statistic is positive or negative


",0,"I’ve been teaching myself stats and have hit an obstacle in Chapter 10 in the book I’m using. The book problem is provided in the link below, there was another Reddit question on this but the answer given wasn’t right according to the book.

[Book Problem](https://www.google.com/amp/s/amp.reddit.com/r/HomeworkHelp/comments/486rvc/university_business_statistics_hypothesis_testing/)

I have the answers to the book, but in this particular case I don’t understand how the hypothesis test was set up and, how the book determined how to define which score was population mean 1 and population mean 2. I

Depending on which score is u1 and u2, it affects whether the test statistic is positive or negative


",0,statistics,54935,,Inference About Means with Two Populations,https://www.reddit.com/r/statistics/comments/8nyu1z/inference_about_means_with_two_populations/,all_ads,2018-06-02 10:38:21,0 days 14:57:13.036584000,
"I want to place a prior on a value I am highly confident is in a given interval, but not sure exactly where inside it. Essentially a uniform [a,b] but it should still have some tail on the off chance we're wrong. Any ideas for a distribution that fits?",4,1527922586.0,8nw95g,False,"I want to place a prior on a value I am highly confident is in a given interval, but not sure exactly where inside it. Essentially a uniform [a,b] but it should still have some tail on the off chance we're wrong. Any ideas for a distribution that fits?",0,"I want to place a prior on a value I am highly confident is in a given interval, but not sure exactly where inside it. Essentially a uniform [a,b] but it should still have some tail on the off chance we're wrong. Any ideas for a distribution that fits?",5,statistics,54935,,Looking for a unimodal distribution with tails thinner and top flatter than a normal,https://www.reddit.com/r/statistics/comments/8nw95g/looking_for_a_unimodal_distribution_with_tails/,all_ads,2018-06-02 02:56:26,0 days 22:39:08.036584000,
"I'm taking statistics course in college right now for the first time, so forgive me if this is phrased a little weirdly or this isn't the right place to ask. I've taken calculus and I can figure out area under a curve with an equation that has an antiderivative. However, my stats prof says that the t\-table curve equation does not have an antiderivative, so what method did mathematicians actually use to figure out the area? How did they go about finding all of the t\-table areas corresponding to degrees of freedom? Did they use physical volumes with liquids or something similar? 

I'm not really sure if what I'm asking makes sense, or if my question is a dumb, but I appreciate help! Thanks! ",8,1527927202.0,8nwr6l,False,"I'm taking statistics course in college right now for the first time, so forgive me if this is phrased a little weirdly or this isn't the right place to ask. I've taken calculus and I can figure out area under a curve with an equation that has an antiderivative. However, my stats prof says that the t\-table curve equation does not have an antiderivative, so what method did mathematicians actually use to figure out the area? How did they go about finding all of the t\-table areas corresponding to degrees of freedom? Did they use physical volumes with liquids or something similar? 

I'm not really sure if what I'm asking makes sense, or if my question is a dumb, but I appreciate help! Thanks! ",0,"I'm taking statistics course in college right now for the first time, so forgive me if this is phrased a little weirdly or this isn't the right place to ask. I've taken calculus and I can figure out area under a curve with an equation that has an antiderivative. However, my stats prof says that the t\-table curve equation does not have an antiderivative, so what method did mathematicians actually use to figure out the area? How did they go about finding all of the t\-table areas corresponding to degrees of freedom? Did they use physical volumes with liquids or something similar? 

I'm not really sure if what I'm asking makes sense, or if my question is a dumb, but I appreciate help! Thanks! ",2,statistics,54935,,How did statisticians figure out the t-table areas?,https://www.reddit.com/r/statistics/comments/8nwr6l/how_did_statisticians_figure_out_the_ttable_areas/,all_ads,2018-06-02 04:13:22,0 days 21:22:12.036584000,
"Interested to hear community input on the use of ANOVA/ANCOVA 
 subtypes (i-iii) when dealing with unbalanced data. Is this widely accepted and/or restricted to certain instances/fields or does it remain pretty broadly controversial across academia? Also, when using type iii, are there preferred post-hoc tests?

Any literature or personal insights would be much appreciated. Apologies if this has been posted before - was not able to find info via the search feature, but feel free to redirect me.",2,1527917352.0,8nvn4z,False,"Interested to hear community input on the use of ANOVA/ANCOVA 
 subtypes (i-iii) when dealing with unbalanced data. Is this widely accepted and/or restricted to certain instances/fields or does it remain pretty broadly controversial across academia? Also, when using type iii, are there preferred post-hoc tests?

Any literature or personal insights would be much appreciated. Apologies if this has been posted before - was not able to find info via the search feature, but feel free to redirect me.",0,"Interested to hear community input on the use of ANOVA/ANCOVA 
 subtypes (i-iii) when dealing with unbalanced data. Is this widely accepted and/or restricted to certain instances/fields or does it remain pretty broadly controversial across academia? Also, when using type iii, are there preferred post-hoc tests?

Any literature or personal insights would be much appreciated. Apologies if this has been posted before - was not able to find info via the search feature, but feel free to redirect me.",3,statistics,54935,,Looking for perspective/literature on unbalanced ANCOVA and associated post-hoc tests,https://www.reddit.com/r/statistics/comments/8nvn4z/looking_for_perspectiveliterature_on_unbalanced/,all_ads,2018-06-02 01:29:12,1 days 00:06:22.036584000,
"I have been a recent recipient of a an Master's in Biostatistics but have been hard pressed to find a job - have been getting docked with having no experience.

I got my bachelors in Public Health and have worked in healthcare for 5-7 years as an EMT and Cardiac Tech.

I got an interview as a Clinical Research Coordinator at UCSF Medical Center. Would taking this position help on the road to becoming a Biostatistician in medical research? Does it actually give translatable experience? (note: google has been vague on this)

EDIT
======================
Thank you all the for replies and advice. Its been a great help",23,1527894008.0,8nshtv,False,"I have been a recent recipient of a an Master's in Biostatistics but have been hard pressed to find a job - have been getting docked with having no experience.

I got my bachelors in Public Health and have worked in healthcare for 5-7 years as an EMT and Cardiac Tech.

I got an interview as a Clinical Research Coordinator at UCSF Medical Center. Would taking this position help on the road to becoming a Biostatistician in medical research? Does it actually give translatable experience? (note: google has been vague on this)

EDIT
======================
Thank you all the for replies and advice. Its been a great help",0,"I have been a recent recipient of a an Master's in Biostatistics but have been hard pressed to find a job - have been getting docked with having no experience.

I got my bachelors in Public Health and have worked in healthcare for 5-7 years as an EMT and Cardiac Tech.

I got an interview as a Clinical Research Coordinator at UCSF Medical Center. Would taking this position help on the road to becoming a Biostatistician in medical research? Does it actually give translatable experience? (note: google has been vague on this)

EDIT
======================
Thank you all the for replies and advice. Its been a great help",9,statistics,54935,,A bit perplex with Biostatistician journey,https://www.reddit.com/r/statistics/comments/8nshtv/a_bit_perplex_with_biostatistician_journey/,all_ads,2018-06-01 19:00:08,1 days 06:35:26.036584000,
"For instance, using data collected at local scales to rank candidate models, then using the full dataset at regional scales (which includes the local datasets) to see if the same models are selected (i.e., the same model is likeliest to explain the data at local and regional scales).

Burnham and Anderson emphasize not to compare models generated from different datasets and I'm wondering if this falls into that realm. I interpret them to mean one should not compare AIC values generated by different datasets.",2,1527921040.0,8nw2xf,False,"For instance, using data collected at local scales to rank candidate models, then using the full dataset at regional scales (which includes the local datasets) to see if the same models are selected (i.e., the same model is likeliest to explain the data at local and regional scales).

Burnham and Anderson emphasize not to compare models generated from different datasets and I'm wondering if this falls into that realm. I interpret them to mean one should not compare AIC values generated by different datasets.",0,"For instance, using data collected at local scales to rank candidate models, then using the full dataset at regional scales (which includes the local datasets) to see if the same models are selected (i.e., the same model is likeliest to explain the data at local and regional scales).

Burnham and Anderson emphasize not to compare models generated from different datasets and I'm wondering if this falls into that realm. I interpret them to mean one should not compare AIC values generated by different datasets.",2,statistics,54935,,Is it valid to use nested datasets in model selection approaches such as AIC?,https://www.reddit.com/r/statistics/comments/8nw2xf/is_it_valid_to_use_nested_datasets_in_model/,all_ads,2018-06-02 02:30:40,0 days 23:04:54.036584000,
"MIT OCW has an excellent free course called [""Statistics for Applications""](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/index.htm), by Philippe Rigollet. Despite the name it has a heavy theoretical component. The videos are great, but the ""lecture notes"" are just very barebones slides.

I worked my way through the whole series as a supplement to my own University's theoretical stats course, and I made extensive handwritten notes with some extra exposition in places where I had trouble understanding what the prof was saying. It occurred to me that other people might find these useful if I were to transcribe them into LaTeX -- maybe not everyone has the time to go through 24+ hours of video footage. Since it's all licensed under Creative Commons sharealike, this is fine so long as I keep the attribution.

Does anyone want them? These are the topics:

- Introduction to Statistics  
- Parametric Inference  
- Maximum Likelihood Estimation  
- The Method of Moments  
- Parametric Hypothesis Testing  
- Testing Goodness of Fit  
- Regression  
- Bayesian Statistics  
- Principal Component Analysis  
- Generalized Linear Models

**edit**

oh god i didn't expect this to blow up :x

i will make a new thread when i'm done and PM the link everyone who replied itt",132,1527829020.0,8nm0gu,False,"MIT OCW has an excellent free course called [""Statistics for Applications""](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/index.htm), by Philippe Rigollet. Despite the name it has a heavy theoretical component. The videos are great, but the ""lecture notes"" are just very barebones slides.

I worked my way through the whole series as a supplement to my own University's theoretical stats course, and I made extensive handwritten notes with some extra exposition in places where I had trouble understanding what the prof was saying. It occurred to me that other people might find these useful if I were to transcribe them into LaTeX -- maybe not everyone has the time to go through 24+ hours of video footage. Since it's all licensed under Creative Commons sharealike, this is fine so long as I keep the attribution.

Does anyone want them? These are the topics:

- Introduction to Statistics  
- Parametric Inference  
- Maximum Likelihood Estimation  
- The Method of Moments  
- Parametric Hypothesis Testing  
- Testing Goodness of Fit  
- Regression  
- Bayesian Statistics  
- Principal Component Analysis  
- Generalized Linear Models

**edit**

oh god i didn't expect this to blow up :x

i will make a new thread when i'm done and PM the link everyone who replied itt",0,"MIT OCW has an excellent free course called [""Statistics for Applications""](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/index.htm), by Philippe Rigollet. Despite the name it has a heavy theoretical component. The videos are great, but the ""lecture notes"" are just very barebones slides.

I worked my way through the whole series as a supplement to my own University's theoretical stats course, and I made extensive handwritten notes with some extra exposition in places where I had trouble understanding what the prof was saying. It occurred to me that other people might find these useful if I were to transcribe them into LaTeX -- maybe not everyone has the time to go through 24+ hours of video footage. Since it's all licensed under Creative Commons sharealike, this is fine so long as I keep the attribution.

Does anyone want them? These are the topics:

- Introduction to Statistics  
- Parametric Inference  
- Maximum Likelihood Estimation  
- The Method of Moments  
- Parametric Hypothesis Testing  
- Testing Goodness of Fit  
- Regression  
- Bayesian Statistics  
- Principal Component Analysis  
- Generalized Linear Models

**edit**

oh god i didn't expect this to blow up :x

i will make a new thread when i'm done and PM the link everyone who replied itt",182,statistics,54935,,"Gauging Interest: Anyone Want Transcribed Lecture Notes from MIT OCW's ""Statistics for Applications""?",https://www.reddit.com/r/statistics/comments/8nm0gu/gauging_interest_anyone_want_transcribed_lecture/,all_ads,2018-06-01 00:57:00,2 days 00:38:34.036584000,
"In my probability class, we never learned about the Pareto distribution.  I knew about the ""80-20"" rule, but i didn't realize it was it's own distribution with a parameter until I had to do some problems involving it in a subsequent class. Anyway, by the Pareto principle, in any given job, 20% of the workers will do 80% of the work. 

Today I read a blog post about Price's law, with which I was previously unfamiliar. Price's Law states that for a job with n workers, 50% of the work will be done by the square root of the total number of workers, or sqrt(n). As the number of workers increases, the proportion doing 50% of the work will decrease - sqrt(n)/n.  There will be only one value of n at which the Pareto principle will agree with Price's Law. 

The author mentioned the Pareto principle and Price's Law as being competing models for the same set of observed data. But I was thinking that couldn't Price's Law be a special case of the Pareto distribution? My logic is as follows: we take the cumulative distribution function of the Pareto distribution, and for it to follow Price's Law, we make it equal .5 and set it equal to P(X<sqrt(n)/n).  We can then solve for the parameter, alpha, which would in theory, only depend on n. 

Am I correct?",0,1527900545.0,8ntg1f,False,"In my probability class, we never learned about the Pareto distribution.  I knew about the ""80-20"" rule, but i didn't realize it was it's own distribution with a parameter until I had to do some problems involving it in a subsequent class. Anyway, by the Pareto principle, in any given job, 20% of the workers will do 80% of the work. 

Today I read a blog post about Price's law, with which I was previously unfamiliar. Price's Law states that for a job with n workers, 50% of the work will be done by the square root of the total number of workers, or sqrt(n). As the number of workers increases, the proportion doing 50% of the work will decrease - sqrt(n)/n.  There will be only one value of n at which the Pareto principle will agree with Price's Law. 

The author mentioned the Pareto principle and Price's Law as being competing models for the same set of observed data. But I was thinking that couldn't Price's Law be a special case of the Pareto distribution? My logic is as follows: we take the cumulative distribution function of the Pareto distribution, and for it to follow Price's Law, we make it equal .5 and set it equal to P(X<sqrt(n)/n).  We can then solve for the parameter, alpha, which would in theory, only depend on n. 

Am I correct?",0,"In my probability class, we never learned about the Pareto distribution.  I knew about the ""80-20"" rule, but i didn't realize it was it's own distribution with a parameter until I had to do some problems involving it in a subsequent class. Anyway, by the Pareto principle, in any given job, 20% of the workers will do 80% of the work. 

Today I read a blog post about Price's law, with which I was previously unfamiliar. Price's Law states that for a job with n workers, 50% of the work will be done by the square root of the total number of workers, or sqrt(n). As the number of workers increases, the proportion doing 50% of the work will decrease - sqrt(n)/n.  There will be only one value of n at which the Pareto principle will agree with Price's Law. 

The author mentioned the Pareto principle and Price's Law as being competing models for the same set of observed data. But I was thinking that couldn't Price's Law be a special case of the Pareto distribution? My logic is as follows: we take the cumulative distribution function of the Pareto distribution, and for it to follow Price's Law, we make it equal .5 and set it equal to P(X<sqrt(n)/n).  We can then solve for the parameter, alpha, which would in theory, only depend on n. 

Am I correct?",4,statistics,54935,,Is Price's Law a special case of the Pareto distribution?,https://www.reddit.com/r/statistics/comments/8ntg1f/is_prices_law_a_special_case_of_the_pareto/,all_ads,2018-06-01 20:49:05,1 days 04:46:29.036584000,
"I'm confused about the error term in the population regression and it's relation to what we usually need to assume in linear regression for the Gauss\-Markov Theorem to apply. It is my understanding that for the population regression, errors are mean zero and uncorrelated with X \*by definition\*. So I am confused as to what it means to assume errors are mean zero given X \(strict exogeneity\) when running a regression in practice. Because, won't the errors in the population always satisfy this assumption by design \- and it is the population regression function parameters we estimate when we fit a linear regression model? So why do we need to assume anything about the population errors? What am I missing here?

EDIT: Here is the more precise area I am confused on. This is from class notes discussing the role of OLS as estimating the population regression function, called the linear projection here, of the CEF \(found [here](http://www.mattblackwell.org/files/teaching/gov2000/s07-what-is-regression.pdf) on page 14\):

>Define the error of the linear projection as: ui = Yi − β0 − β1Xi .  
It can be shown that this error has two properties: E\[Xiui \] = 0 and E\[ui \] = 0. This implies that the covariance \(and thus the correlation\) between ui and Xi is 0, since Cov\[Xi , ui \] = E\[Xiui \] − E\[Xi \]E\[ui \] = 0

So my confusion is \- if these parameters are what regression is estimating and the errors will be uncorrelated in the population, then why do we ever need to assume things like the mean of the population errors is zero and are uncorrelated with X?",0,1527891990.0,8ns9pg,False,"I'm confused about the error term in the population regression and it's relation to what we usually need to assume in linear regression for the Gauss\-Markov Theorem to apply. It is my understanding that for the population regression, errors are mean zero and uncorrelated with X \*by definition\*. So I am confused as to what it means to assume errors are mean zero given X \(strict exogeneity\) when running a regression in practice. Because, won't the errors in the population always satisfy this assumption by design \- and it is the population regression function parameters we estimate when we fit a linear regression model? So why do we need to assume anything about the population errors? What am I missing here?

EDIT: Here is the more precise area I am confused on. This is from class notes discussing the role of OLS as estimating the population regression function, called the linear projection here, of the CEF \(found [here](http://www.mattblackwell.org/files/teaching/gov2000/s07-what-is-regression.pdf) on page 14\):

>Define the error of the linear projection as: ui = Yi − β0 − β1Xi .  
It can be shown that this error has two properties: E\[Xiui \] = 0 and E\[ui \] = 0. This implies that the covariance \(and thus the correlation\) between ui and Xi is 0, since Cov\[Xi , ui \] = E\[Xiui \] − E\[Xi \]E\[ui \] = 0

So my confusion is \- if these parameters are what regression is estimating and the errors will be uncorrelated in the population, then why do we ever need to assume things like the mean of the population errors is zero and are uncorrelated with X?",0,"I'm confused about the error term in the population regression and it's relation to what we usually need to assume in linear regression for the Gauss\-Markov Theorem to apply. It is my understanding that for the population regression, errors are mean zero and uncorrelated with X \*by definition\*. So I am confused as to what it means to assume errors are mean zero given X \(strict exogeneity\) when running a regression in practice. Because, won't the errors in the population always satisfy this assumption by design \- and it is the population regression function parameters we estimate when we fit a linear regression model? So why do we need to assume anything about the population errors? What am I missing here?

EDIT: Here is the more precise area I am confused on. This is from class notes discussing the role of OLS as estimating the population regression function, called the linear projection here, of the CEF \(found [here](http://www.mattblackwell.org/files/teaching/gov2000/s07-what-is-regression.pdf) on page 14\):

>Define the error of the linear projection as: ui = Yi − β0 − β1Xi .  
It can be shown that this error has two properties: E\[Xiui \] = 0 and E\[ui \] = 0. This implies that the covariance \(and thus the correlation\) between ui and Xi is 0, since Cov\[Xi , ui \] = E\[Xiui \] − E\[Xi \]E\[ui \] = 0

So my confusion is \- if these parameters are what regression is estimating and the errors will be uncorrelated in the population, then why do we ever need to assume things like the mean of the population errors is zero and are uncorrelated with X?",3,statistics,54935,,Gauss-Markov Assumptions and Population Regression Error Term?,https://www.reddit.com/r/statistics/comments/8ns9pg/gaussmarkov_assumptions_and_population_regression/,all_ads,2018-06-01 18:26:30,1 days 07:09:04.036584000,
"I have so far been using this formula \(on page 10 in document below\), to calculate simple exponential moving average:

[http://www.eckner.com/papers/Algorithms&#37;20for&#37;20Unevenly&#37;20Spaced&#37;20Time&#37;20Series.pdf](http://www.eckner.com/papers/Algorithms%20for%20Unevenly%20Spaced%20Time%20Series.pdf)

time\_interval = \(now \- last\_update\) / tau

w = exp\(\-time\_interval\)

w2 = \(1 \- w\) / time\_interval

new\_ema = old\_ema \* w \+ current\_value \* \(1 \- w2\) \+ previous\_value \* \(w2 \- w\)

\(The benefit of this method is that if time interval is very long, it assumes that the value changes linearly from previous\_value to current\_value.\)

If I was to extend this method to double exponential smoothing, how would I do that?

Standard double EMA:

dema\_new = dema\_old \* alpha \+ \(current\_value \+ previous\_trend\) \* \(1\-alpha\)

current\_trend = old\_trend \* beta \+ \(dema\_new \- dema\_old\) \* \(1\-beta\)",1,1527894088.0,8nsiab,False,"I have so far been using this formula \(on page 10 in document below\), to calculate simple exponential moving average:

[http://www.eckner.com/papers/Algorithms&#37;20for&#37;20Unevenly&#37;20Spaced&#37;20Time&#37;20Series.pdf](http://www.eckner.com/papers/Algorithms%20for%20Unevenly%20Spaced%20Time%20Series.pdf)

time\_interval = \(now \- last\_update\) / tau

w = exp\(\-time\_interval\)

w2 = \(1 \- w\) / time\_interval

new\_ema = old\_ema \* w \+ current\_value \* \(1 \- w2\) \+ previous\_value \* \(w2 \- w\)

\(The benefit of this method is that if time interval is very long, it assumes that the value changes linearly from previous\_value to current\_value.\)

If I was to extend this method to double exponential smoothing, how would I do that?

Standard double EMA:

dema\_new = dema\_old \* alpha \+ \(current\_value \+ previous\_trend\) \* \(1\-alpha\)

current\_trend = old\_trend \* beta \+ \(dema\_new \- dema\_old\) \* \(1\-beta\)",0,"I have so far been using this formula \(on page 10 in document below\), to calculate simple exponential moving average:

[http://www.eckner.com/papers/Algorithms&#37;20for&#37;20Unevenly&#37;20Spaced&#37;20Time&#37;20Series.pdf](http://www.eckner.com/papers/Algorithms%20for%20Unevenly%20Spaced%20Time%20Series.pdf)

time\_interval = \(now \- last\_update\) / tau

w = exp\(\-time\_interval\)

w2 = \(1 \- w\) / time\_interval

new\_ema = old\_ema \* w \+ current\_value \* \(1 \- w2\) \+ previous\_value \* \(w2 \- w\)

\(The benefit of this method is that if time interval is very long, it assumes that the value changes linearly from previous\_value to current\_value.\)

If I was to extend this method to double exponential smoothing, how would I do that?

Standard double EMA:

dema\_new = dema\_old \* alpha \+ \(current\_value \+ previous\_trend\) \* \(1\-alpha\)

current\_trend = old\_trend \* beta \+ \(dema\_new \- dema\_old\) \* \(1\-beta\)",3,statistics,54935,,Double exponential smoothing for unevenly spaced series?,https://www.reddit.com/r/statistics/comments/8nsiab/double_exponential_smoothing_for_unevenly_spaced/,all_ads,2018-06-01 19:01:28,1 days 06:34:06.036584000,
"Hello Redditers,

I conducted a survey in which I measured eight independent variable (on a scale from 1 - 5) and one dependent variable (on a scale from 1 - 5). Sample size is N=193. The data for all the independent variable is skewed to the right. The data for the dependent variable is skewed to the left. Seeing as it is a strongly disagree - strongly agree scale, it is logical that the data is skewed to one of the ends. 

My goal is to measure if the independent variable have an effect on the dependent variable. I was taught that to measure this, linear regression is the way to go. However, I know linear regression with strongly skewed data is not going to work. I do not think transforming the data is going to work, seeing as the data is strongly skewed to either one of the sides. Also, with a sample of N=193 I do not think bootstrapping is going to be a solution either.

Which brings me to non-parametric tests, or another test for which the assumption of normality does not matter. I have little knowledge of these tests. Hence, my question is: What tests or way can you recommend to see if the independent variable have an effect on the dependent variable? 

Here are two histograms to visualize the distribution of the independent and dependent variable:
https://imgur.com/a/tvMR3LV

Thank you very much in advance if you take the time to respond.
",8,1527874681.0,8nqlxg,False,"Hello Redditers,

I conducted a survey in which I measured eight independent variable (on a scale from 1 - 5) and one dependent variable (on a scale from 1 - 5). Sample size is N=193. The data for all the independent variable is skewed to the right. The data for the dependent variable is skewed to the left. Seeing as it is a strongly disagree - strongly agree scale, it is logical that the data is skewed to one of the ends. 

My goal is to measure if the independent variable have an effect on the dependent variable. I was taught that to measure this, linear regression is the way to go. However, I know linear regression with strongly skewed data is not going to work. I do not think transforming the data is going to work, seeing as the data is strongly skewed to either one of the sides. Also, with a sample of N=193 I do not think bootstrapping is going to be a solution either.

Which brings me to non-parametric tests, or another test for which the assumption of normality does not matter. I have little knowledge of these tests. Hence, my question is: What tests or way can you recommend to see if the independent variable have an effect on the dependent variable? 

Here are two histograms to visualize the distribution of the independent and dependent variable:
https://imgur.com/a/tvMR3LV

Thank you very much in advance if you take the time to respond.
",0,"Hello Redditers,

I conducted a survey in which I measured eight independent variable (on a scale from 1 - 5) and one dependent variable (on a scale from 1 - 5). Sample size is N=193. The data for all the independent variable is skewed to the right. The data for the dependent variable is skewed to the left. Seeing as it is a strongly disagree - strongly agree scale, it is logical that the data is skewed to one of the ends. 

My goal is to measure if the independent variable have an effect on the dependent variable. I was taught that to measure this, linear regression is the way to go. However, I know linear regression with strongly skewed data is not going to work. I do not think transforming the data is going to work, seeing as the data is strongly skewed to either one of the sides. Also, with a sample of N=193 I do not think bootstrapping is going to be a solution either.

Which brings me to non-parametric tests, or another test for which the assumption of normality does not matter. I have little knowledge of these tests. Hence, my question is: What tests or way can you recommend to see if the independent variable have an effect on the dependent variable? 

Here are two histograms to visualize the distribution of the independent and dependent variable:
https://imgur.com/a/tvMR3LV

Thank you very much in advance if you take the time to respond.
",5,statistics,54935,,Appropriate test for skewed data?,https://www.reddit.com/r/statistics/comments/8nqlxg/appropriate_test_for_skewed_data/,all_ads,2018-06-01 13:38:01,1 days 11:57:33.036584000,
"I'm trying to obtain multicollinearity statistics such as the variance inflation factor (VIF) in SPSS. I can only seem to find it for a linear regression, but I'm working with a logistic regression. How do I find the VIF?",9,1527881426.0,8nr5ju,False,"I'm trying to obtain multicollinearity statistics such as the variance inflation factor (VIF) in SPSS. I can only seem to find it for a linear regression, but I'm working with a logistic regression. How do I find the VIF?",0,"I'm trying to obtain multicollinearity statistics such as the variance inflation factor (VIF) in SPSS. I can only seem to find it for a linear regression, but I'm working with a logistic regression. How do I find the VIF?",3,statistics,54935,,VIF in SPSS,https://www.reddit.com/r/statistics/comments/8nr5ju/vif_in_spss/,all_ads,2018-06-01 15:30:26,1 days 10:05:08.036584000,
"I am trying to fit a `linear regression` model implemented in `statsmodels` library.

I have a doubt regarding the `fit()` method. Let's say I have data sample of size 15 and I broke it down into 3 parts and fit the model. Does call to each `fit()` will fit the model properly or will it overwrite previous values.

        import numpy as np
        import statsmodels.api as sm
        
        # First call
        X = [377, 295, 457, 495, 9] # independent variable
        y = [23, 79, 16, 41, 40]    # dependent variable
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        # Second call
        X = [243, 493, 106, 227, 334]
        y = [3, 5, 1, 62, 92]
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        # Third call
        X = [412, 332, 429, 96, 336] 
        y = [30, 1, 99, 4, 33]
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        scores = [9, 219, 200, 134, 499]
        scores = sm.add_constant(scores)
        print(ols.predict(scores))",2,1527880408.0,8nr2fy,False,"I am trying to fit a `linear regression` model implemented in `statsmodels` library.

I have a doubt regarding the `fit()` method. Let's say I have data sample of size 15 and I broke it down into 3 parts and fit the model. Does call to each `fit()` will fit the model properly or will it overwrite previous values.

        import numpy as np
        import statsmodels.api as sm
        
        # First call
        X = [377, 295, 457, 495, 9] # independent variable
        y = [23, 79, 16, 41, 40]    # dependent variable
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        # Second call
        X = [243, 493, 106, 227, 334]
        y = [3, 5, 1, 62, 92]
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        # Third call
        X = [412, 332, 429, 96, 336] 
        y = [30, 1, 99, 4, 33]
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        scores = [9, 219, 200, 134, 499]
        scores = sm.add_constant(scores)
        print(ols.predict(scores))",0,"I am trying to fit a `linear regression` model implemented in `statsmodels` library.

I have a doubt regarding the `fit()` method. Let's say I have data sample of size 15 and I broke it down into 3 parts and fit the model. Does call to each `fit()` will fit the model properly or will it overwrite previous values.

        import numpy as np
        import statsmodels.api as sm
        
        # First call
        X = [377, 295, 457, 495, 9] # independent variable
        y = [23, 79, 16, 41, 40]    # dependent variable
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        # Second call
        X = [243, 493, 106, 227, 334]
        y = [3, 5, 1, 62, 92]
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        # Third call
        X = [412, 332, 429, 96, 336] 
        y = [30, 1, 99, 4, 33]
        X = sm.add_constant(X)
        ols = sm.OLS(y,X).fit()
        #print(ols.summary())
        
        scores = [9, 219, 200, 134, 499]
        scores = sm.add_constant(scores)
        print(ols.predict(scores))",3,statistics,54935,,Understanding statsmodels linear regression,https://www.reddit.com/r/statistics/comments/8nr2fy/understanding_statsmodels_linear_regression/,all_ads,2018-06-01 15:13:28,1 days 10:22:06.036584000,
"Hi All,

I have organised a world cup sweep-stake with my mates. Each person involved has paid into the kitty and I have got them predict each of the match results up to the group stages. I have a points system set up and a few ideas for how I am going to do the group stages. 

Now I have everyones predictions I am wondering what I can do with the data? My statistics knowledge is a little rusty and I am hoping to use this as a little exercise to bring myself back up to scratch.

What ways could I compare each player? 
The odds of each person winning? 
Who is supporting who and any bias to the data? 
Any fun graphical ideas welcome too.

Cheers!",1,1527869458.0,8nq89k,False,"Hi All,

I have organised a world cup sweep-stake with my mates. Each person involved has paid into the kitty and I have got them predict each of the match results up to the group stages. I have a points system set up and a few ideas for how I am going to do the group stages. 

Now I have everyones predictions I am wondering what I can do with the data? My statistics knowledge is a little rusty and I am hoping to use this as a little exercise to bring myself back up to scratch.

What ways could I compare each player? 
The odds of each person winning? 
Who is supporting who and any bias to the data? 
Any fun graphical ideas welcome too.

Cheers!",0,"Hi All,

I have organised a world cup sweep-stake with my mates. Each person involved has paid into the kitty and I have got them predict each of the match results up to the group stages. I have a points system set up and a few ideas for how I am going to do the group stages. 

Now I have everyones predictions I am wondering what I can do with the data? My statistics knowledge is a little rusty and I am hoping to use this as a little exercise to bring myself back up to scratch.

What ways could I compare each player? 
The odds of each person winning? 
Who is supporting who and any bias to the data? 
Any fun graphical ideas welcome too.

Cheers!",4,statistics,54935,,Fun things I can do with my world cup sweepstake data,https://www.reddit.com/r/statistics/comments/8nq89k/fun_things_i_can_do_with_my_world_cup_sweepstake/,all_ads,2018-06-01 12:10:58,1 days 13:24:36.036584000,
"You can now use the combined power of the Mathpix iOS and MacOS apps to supercharge your document writing workflow. See a demo here:

[https://www.youtube.com/watch?v=Vmvb2\_6b6Rc](https://www.youtube.com/watch?v=Vmvb2_6b6Rc)",2,1527808722.0,8nj8o1,False,"You can now use the combined power of the Mathpix iOS and MacOS apps to supercharge your document writing workflow. See a demo here:

[https://www.youtube.com/watch?v=Vmvb2\_6b6Rc](https://www.youtube.com/watch?v=Vmvb2_6b6Rc)",0,"You can now use the combined power of the Mathpix iOS and MacOS apps to supercharge your document writing workflow. See a demo here:

[https://www.youtube.com/watch?v=Vmvb2\_6b6Rc](https://www.youtube.com/watch?v=Vmvb2_6b6Rc)",26,statistics,54935,,Use Mathpix to render LaTeX from PDFs/images on your desktop and handwritten notes instantly,https://www.reddit.com/r/statistics/comments/8nj8o1/use_mathpix_to_render_latex_from_pdfsimages_on/,all_ads,2018-05-31 19:18:42,2 days 06:16:52.036584000,
"I'm a materials engineer student and I've learned statistics but its not a required class, so I haven't had as rigorous of a course load as I'd like and now I can't fit it into my schedule before I graduate. A professor of mine recommended I purchase a textbook and teach myself which sounds fun, he recommended [this](https://www.amazon.com/Mind-Statistics-Jessica-M-Utts/dp/1285463188/ref=sr_1_1?ie=UTF8&qid=1527692537&sr=8-1&keywords=mind+of+statistics+5th+edition) textbook which looks great, but I've heard its lacking on the math side. Would this be fine or should I look into a different book? thanks!",4,1527829203.0,8nm1cx,False,"I'm a materials engineer student and I've learned statistics but its not a required class, so I haven't had as rigorous of a course load as I'd like and now I can't fit it into my schedule before I graduate. A professor of mine recommended I purchase a textbook and teach myself which sounds fun, he recommended [this](https://www.amazon.com/Mind-Statistics-Jessica-M-Utts/dp/1285463188/ref=sr_1_1?ie=UTF8&qid=1527692537&sr=8-1&keywords=mind+of+statistics+5th+edition) textbook which looks great, but I've heard its lacking on the math side. Would this be fine or should I look into a different book? thanks!",0,"I'm a materials engineer student and I've learned statistics but its not a required class, so I haven't had as rigorous of a course load as I'd like and now I can't fit it into my schedule before I graduate. A professor of mine recommended I purchase a textbook and teach myself which sounds fun, he recommended [this](https://www.amazon.com/Mind-Statistics-Jessica-M-Utts/dp/1285463188/ref=sr_1_1?ie=UTF8&qid=1527692537&sr=8-1&keywords=mind+of+statistics+5th+edition) textbook which looks great, but I've heard its lacking on the math side. Would this be fine or should I look into a different book? thanks!",7,statistics,54935,,Textbook Recommendations to teach myself?,https://www.reddit.com/r/statistics/comments/8nm1cx/textbook_recommendations_to_teach_myself/,all_ads,2018-06-01 01:00:03,2 days 00:35:31.036584000,
"Hi,

Say I have two exponentially moving averages, one with alpha 0.99 \(A\) and one with alpha 0.95 \(B\), which I use on some time series data, ie. B favours more recent observations than A.

Can I somehow use these two figures to estimate what B was at x iterations ago? I don't want to save B for every iteration for x steps back in time, but I would rather try to estimate it.

Thanks,

MK",2,1527811916.0,8njoi8,False,"Hi,

Say I have two exponentially moving averages, one with alpha 0.99 \(A\) and one with alpha 0.95 \(B\), which I use on some time series data, ie. B favours more recent observations than A.

Can I somehow use these two figures to estimate what B was at x iterations ago? I don't want to save B for every iteration for x steps back in time, but I would rather try to estimate it.

Thanks,

MK",0,"Hi,

Say I have two exponentially moving averages, one with alpha 0.99 \(A\) and one with alpha 0.95 \(B\), which I use on some time series data, ie. B favours more recent observations than A.

Can I somehow use these two figures to estimate what B was at x iterations ago? I don't want to save B for every iteration for x steps back in time, but I would rather try to estimate it.

Thanks,

MK",6,statistics,54935,,Estimating moving averages from moving averages,https://www.reddit.com/r/statistics/comments/8njoi8/estimating_moving_averages_from_moving_averages/,all_ads,2018-05-31 20:11:56,2 days 05:23:38.036584000,
"I've been working on some forecasting tools.  I've tried using ARIMA, and also developing some regression models based on market knowledge.

My issue is that I don't know how to use these together.  I'm wondering if there are ways to tune ARIMA with more standard predictive methods.    Or maybe it makes more sense to tune the regression with the ARIMA somehow?

Anyways, anyone have any experience about this?  It just seems weird to use both in a vacuum.",2,1527845578.0,8nnywb,False,"I've been working on some forecasting tools.  I've tried using ARIMA, and also developing some regression models based on market knowledge.

My issue is that I don't know how to use these together.  I'm wondering if there are ways to tune ARIMA with more standard predictive methods.    Or maybe it makes more sense to tune the regression with the ARIMA somehow?

Anyways, anyone have any experience about this?  It just seems weird to use both in a vacuum.",0,"I've been working on some forecasting tools.  I've tried using ARIMA, and also developing some regression models based on market knowledge.

My issue is that I don't know how to use these together.  I'm wondering if there are ways to tune ARIMA with more standard predictive methods.    Or maybe it makes more sense to tune the regression with the ARIMA somehow?

Anyways, anyone have any experience about this?  It just seems weird to use both in a vacuum.",1,statistics,54935,,How to combine ARIMA and a standard regression,https://www.reddit.com/r/statistics/comments/8nnywb/how_to_combine_arima_and_a_standard_regression/,all_ads,2018-06-01 05:32:58,1 days 20:02:36.036584000,
"I graduated earlier this month with a BS in Statistics from a fairly high ranking public university. My GPA was solid \(3.68\), I had quite a lot of exposure to R, some SAS, and some SQL through a job where I was helping to build a graphics system using D3.js at a university lab. I was also briefly involved with some astronomy research that used a little R and presented a poster at conference.

But I can't seem to get a job. At first I was struggling to apply to positions because I didn't meet every criterion on the listing, and I still worry that I'm not really proficient enough in any technology or analysis to actually be useful. Still, I feel like I'm a relatively qualified applicant for someone who just graduated college, but after spending 6 months looking for a job, I've only had one interview. I've had multiple career advisers look over my resume and cover letter, and I've been tailoring those to fit each job I've applied to, so I'm confused as to why I'm not hearing back on any of these positions.

I'm getting extra frustrated now because most the entry level and internship jobs in data analysis in my area seem to have dried up for the time being, and I'd really like to have something locked down by the end of July.

Any idea what I'm doing wrong? ",8,1527823146.0,8nl838,False,"I graduated earlier this month with a BS in Statistics from a fairly high ranking public university. My GPA was solid \(3.68\), I had quite a lot of exposure to R, some SAS, and some SQL through a job where I was helping to build a graphics system using D3.js at a university lab. I was also briefly involved with some astronomy research that used a little R and presented a poster at conference.

But I can't seem to get a job. At first I was struggling to apply to positions because I didn't meet every criterion on the listing, and I still worry that I'm not really proficient enough in any technology or analysis to actually be useful. Still, I feel like I'm a relatively qualified applicant for someone who just graduated college, but after spending 6 months looking for a job, I've only had one interview. I've had multiple career advisers look over my resume and cover letter, and I've been tailoring those to fit each job I've applied to, so I'm confused as to why I'm not hearing back on any of these positions.

I'm getting extra frustrated now because most the entry level and internship jobs in data analysis in my area seem to have dried up for the time being, and I'd really like to have something locked down by the end of July.

Any idea what I'm doing wrong? ",0,"I graduated earlier this month with a BS in Statistics from a fairly high ranking public university. My GPA was solid \(3.68\), I had quite a lot of exposure to R, some SAS, and some SQL through a job where I was helping to build a graphics system using D3.js at a university lab. I was also briefly involved with some astronomy research that used a little R and presented a poster at conference.

But I can't seem to get a job. At first I was struggling to apply to positions because I didn't meet every criterion on the listing, and I still worry that I'm not really proficient enough in any technology or analysis to actually be useful. Still, I feel like I'm a relatively qualified applicant for someone who just graduated college, but after spending 6 months looking for a job, I've only had one interview. I've had multiple career advisers look over my resume and cover letter, and I've been tailoring those to fit each job I've applied to, so I'm confused as to why I'm not hearing back on any of these positions.

I'm getting extra frustrated now because most the entry level and internship jobs in data analysis in my area seem to have dried up for the time being, and I'd really like to have something locked down by the end of July.

Any idea what I'm doing wrong? ",3,statistics,54935,,I'm really struggling on the job hunt. What do I do?,https://www.reddit.com/r/statistics/comments/8nl838/im_really_struggling_on_the_job_hunt_what_do_i_do/,all_ads,2018-05-31 23:19:06,2 days 02:16:28.036584000,
"I’m having a difficult time deriving the probability for each mean that it is the minimum mean of the set. Let’s say I have n vectors of values.  I want to know the probability for each of n that it’s mean is the minimum mean. I start by computing the t-stat that the mean(i) is less then each mean and convert into p-values.  I now have a matrix of p values and want to know the probability for each mean that it’s the minimum.  My intuition is that it’s proportional to the minimum p value for each vector excluding the diagonal (I.e 50% for i=j).  Would I just rescale this vector to sum to 1 or am I approaching the problem wrong. Thanks for your help. 
Edit: spellling",5,1527836969.0,8nmzou,False,"I’m having a difficult time deriving the probability for each mean that it is the minimum mean of the set. Let’s say I have n vectors of values.  I want to know the probability for each of n that it’s mean is the minimum mean. I start by computing the t-stat that the mean(i) is less then each mean and convert into p-values.  I now have a matrix of p values and want to know the probability for each mean that it’s the minimum.  My intuition is that it’s proportional to the minimum p value for each vector excluding the diagonal (I.e 50% for i=j).  Would I just rescale this vector to sum to 1 or am I approaching the problem wrong. Thanks for your help. 
Edit: spellling",0,"I’m having a difficult time deriving the probability for each mean that it is the minimum mean of the set. Let’s say I have n vectors of values.  I want to know the probability for each of n that it’s mean is the minimum mean. I start by computing the t-stat that the mean(i) is less then each mean and convert into p-values.  I now have a matrix of p values and want to know the probability for each mean that it’s the minimum.  My intuition is that it’s proportional to the minimum p value for each vector excluding the diagonal (I.e 50% for i=j).  Would I just rescale this vector to sum to 1 or am I approaching the problem wrong. Thanks for your help. 
Edit: spellling",0,statistics,54935,,Help deriving probability that mean of set of means is the minimium of the set,https://www.reddit.com/r/statistics/comments/8nmzou/help_deriving_probability_that_mean_of_set_of/,all_ads,2018-06-01 03:09:29,1 days 22:26:05.036584000,
"I've read a few conflicting ideas on what a confidence interval represents and combining it with the idea of a multiple regression (i.e. measuring the influence of several factors instead of a direct correlation between two) I don't see how a confidence interval can maintain the idea that 95% of the wider population are expected to fall within a certain distance of a line of best fit. 

Can anyone help me describe these two values simply in this context?",10,1527836501.0,8nmxp1,False,"I've read a few conflicting ideas on what a confidence interval represents and combining it with the idea of a multiple regression (i.e. measuring the influence of several factors instead of a direct correlation between two) I don't see how a confidence interval can maintain the idea that 95% of the wider population are expected to fall within a certain distance of a line of best fit. 

Can anyone help me describe these two values simply in this context?",0,"I've read a few conflicting ideas on what a confidence interval represents and combining it with the idea of a multiple regression (i.e. measuring the influence of several factors instead of a direct correlation between two) I don't see how a confidence interval can maintain the idea that 95% of the wider population are expected to fall within a certain distance of a line of best fit. 

Can anyone help me describe these two values simply in this context?",1,statistics,54935,,Multiple regression - R2 and confidence intervals,https://www.reddit.com/r/statistics/comments/8nmxp1/multiple_regression_r2_and_confidence_intervals/,all_ads,2018-06-01 03:01:41,1 days 22:33:53.036584000,
"Given the complaints of the ""over-generalized"" nature of this subreddit, I've created another dedicated to the discussion of more theoretical statistics. As such, discussions at /r/TheoreticalStatistics will not concern specific methodology questions or discussions related to data science/analysis, instead focusing on topics such as applied probability, mathematical statistics, and statistical theory.

https://www.reddit.com/r/TheoreticalStatistics/
",10,1527754660.0,8ndltc,False,"Given the complaints of the ""over-generalized"" nature of this subreddit, I've created another dedicated to the discussion of more theoretical statistics. As such, discussions at /r/TheoreticalStatistics will not concern specific methodology questions or discussions related to data science/analysis, instead focusing on topics such as applied probability, mathematical statistics, and statistical theory.

https://www.reddit.com/r/TheoreticalStatistics/
",0,"Given the complaints of the ""over-generalized"" nature of this subreddit, I've created another dedicated to the discussion of more theoretical statistics. As such, discussions at /r/TheoreticalStatistics will not concern specific methodology questions or discussions related to data science/analysis, instead focusing on topics such as applied probability, mathematical statistics, and statistical theory.

https://www.reddit.com/r/TheoreticalStatistics/
",46,statistics,54935,,/r/TheoreticalStatistics,https://www.reddit.com/r/statistics/comments/8ndltc/rtheoreticalstatistics/,all_ads,2018-05-31 04:17:40,2 days 21:17:54.036584000,
"So here is my situation:

I am doing a pass/fail experiment with a somewhat limited sample size. My goal is to compare 5 different brand's products and rank them accordingly. What I was thinking of doing was to take all of my samples and find an overhead failure rate, then use this overhead fail rate to find a sample size to test the individual brands and compare them to this 'standard' rate. I just don't know what equations or methods to use in order to accomplish this. Proportions were my first idea, but that is still a bit confusing. If it helps, I am using the JMP statistics software and I am pretty new to all this...",0,1527826567.0,8nloy7,False,"So here is my situation:

I am doing a pass/fail experiment with a somewhat limited sample size. My goal is to compare 5 different brand's products and rank them accordingly. What I was thinking of doing was to take all of my samples and find an overhead failure rate, then use this overhead fail rate to find a sample size to test the individual brands and compare them to this 'standard' rate. I just don't know what equations or methods to use in order to accomplish this. Proportions were my first idea, but that is still a bit confusing. If it helps, I am using the JMP statistics software and I am pretty new to all this...",0,"So here is my situation:

I am doing a pass/fail experiment with a somewhat limited sample size. My goal is to compare 5 different brand's products and rank them accordingly. What I was thinking of doing was to take all of my samples and find an overhead failure rate, then use this overhead fail rate to find a sample size to test the individual brands and compare them to this 'standard' rate. I just don't know what equations or methods to use in order to accomplish this. Proportions were my first idea, but that is still a bit confusing. If it helps, I am using the JMP statistics software and I am pretty new to all this...",1,statistics,54935,,Finding a sample size?,https://www.reddit.com/r/statistics/comments/8nloy7/finding_a_sample_size/,all_ads,2018-06-01 00:16:07,2 days 01:19:27.036584000,
"https://goo.gl/forms/XF9PgIEUkxYQIsNA3

above is a survey on extra circulars",1,1527821941.0,8nl22e,False,"https://goo.gl/forms/XF9PgIEUkxYQIsNA3

above is a survey on extra circulars",0,"https://goo.gl/forms/XF9PgIEUkxYQIsNA3

above is a survey on extra circulars",1,statistics,54935,,Hey I like statistics so help me with some of mine!,https://www.reddit.com/r/statistics/comments/8nl22e/hey_i_like_statistics_so_help_me_with_some_of_mine/,all_ads,2018-05-31 22:59:01,2 days 02:36:33.036584000,
"So I have been looking for 3 days now, and I can find everything on European Population Statistics, except for the one thing I need the most. A breakdown of European population based on race and gender. Just finding racial statistics is difficult, but to break it down into the two categories I am finding is near impossible. ",1,1527818695.0,8nkm1u,False,"So I have been looking for 3 days now, and I can find everything on European Population Statistics, except for the one thing I need the most. A breakdown of European population based on race and gender. Just finding racial statistics is difficult, but to break it down into the two categories I am finding is near impossible. ",0,"So I have been looking for 3 days now, and I can find everything on European Population Statistics, except for the one thing I need the most. A breakdown of European population based on race and gender. Just finding racial statistics is difficult, but to break it down into the two categories I am finding is near impossible. ",1,statistics,54935,,Finding population statistics?,https://www.reddit.com/r/statistics/comments/8nkm1u/finding_population_statistics/,all_ads,2018-05-31 22:04:55,2 days 03:30:39.036584000,
"Hello! I work in web analytics and SEO and manage websites for loan officers in 75\+ markets in the US. I'm looking to find the correlation between city size \(population\) and the &#37; of focus keywords that rank on the first page of Google. My general theory is that the smaller the city/less competition should equal a better percentage of our focus keywords showing up on the first page.

**Example**

* Denver, CO \(pop 600,000\+\) loan officers have approximately 13&#37; of their focus keywords on the first page of results
* But a Germantown, WI \(pop 20,000\) loan officer has about 40&#37; of focus keywords on the first page of results

Could anyone give some guidance on how to get a hard number from data like this? I can tell from browsing the data that there **is** something to my theory, but I'd like to be able to quantify it.

TIA!",2,1527815152.0,8nk45t,False,"Hello! I work in web analytics and SEO and manage websites for loan officers in 75\+ markets in the US. I'm looking to find the correlation between city size \(population\) and the &#37; of focus keywords that rank on the first page of Google. My general theory is that the smaller the city/less competition should equal a better percentage of our focus keywords showing up on the first page.

**Example**

* Denver, CO \(pop 600,000\+\) loan officers have approximately 13&#37; of their focus keywords on the first page of results
* But a Germantown, WI \(pop 20,000\) loan officer has about 40&#37; of focus keywords on the first page of results

Could anyone give some guidance on how to get a hard number from data like this? I can tell from browsing the data that there **is** something to my theory, but I'd like to be able to quantify it.

TIA!",0,"Hello! I work in web analytics and SEO and manage websites for loan officers in 75\+ markets in the US. I'm looking to find the correlation between city size \(population\) and the &#37; of focus keywords that rank on the first page of Google. My general theory is that the smaller the city/less competition should equal a better percentage of our focus keywords showing up on the first page.

**Example**

* Denver, CO \(pop 600,000\+\) loan officers have approximately 13&#37; of their focus keywords on the first page of results
* But a Germantown, WI \(pop 20,000\) loan officer has about 40&#37; of focus keywords on the first page of results

Could anyone give some guidance on how to get a hard number from data like this? I can tell from browsing the data that there **is** something to my theory, but I'd like to be able to quantify it.

TIA!",1,statistics,54935,,Newb to Stats - need some guidance,https://www.reddit.com/r/statistics/comments/8nk45t/newb_to_stats_need_some_guidance/,all_ads,2018-05-31 21:05:52,2 days 04:29:42.036584000,
"Ideally a data set with additional demographic and socio-economic variables, thanks!",6,1527790803.0,8nha62,False,"Ideally a data set with additional demographic and socio-economic variables, thanks!",0,"Ideally a data set with additional demographic and socio-economic variables, thanks!",2,statistics,54935,,"Is there a good data set where ""intelligence"" was measured? IQ, g factor etc...",https://www.reddit.com/r/statistics/comments/8nha62/is_there_a_good_data_set_where_intelligence_was/,all_ads,2018-05-31 14:20:03,2 days 11:15:31.036584000,
"So i've suspect that one of my independent variables has a mediating effect on several of my other independent ones. Therefore, i decided to try to find the indirect effect on the dependent variable through this one. 

I've never done this before, and think i might have done it wrong. 

When calculating the indirect effect with a regression, should i use one variable at a time when estimating the effect on the dependent and the mediator? Or should i use all the variables in the same regression, one for the dependent and one for the mediator? 

In the multiple regression there are only 4 out of 8 significant effects on the dependent, and 2 out of 7 significant effects on the mediator. However, when i did it one variable at a time, 5 out of 7 had a significant effect on the mediator. 

Also, in calculating the indirect effect. Would it be correct to subtract the effect when the mediator is included from the effect between the independent to the dependent? And should this also be done one variable at a time? 

Any help would be appreciated!",2,1527790348.0,8nh8uo,False,"So i've suspect that one of my independent variables has a mediating effect on several of my other independent ones. Therefore, i decided to try to find the indirect effect on the dependent variable through this one. 

I've never done this before, and think i might have done it wrong. 

When calculating the indirect effect with a regression, should i use one variable at a time when estimating the effect on the dependent and the mediator? Or should i use all the variables in the same regression, one for the dependent and one for the mediator? 

In the multiple regression there are only 4 out of 8 significant effects on the dependent, and 2 out of 7 significant effects on the mediator. However, when i did it one variable at a time, 5 out of 7 had a significant effect on the mediator. 

Also, in calculating the indirect effect. Would it be correct to subtract the effect when the mediator is included from the effect between the independent to the dependent? And should this also be done one variable at a time? 

Any help would be appreciated!",0,"So i've suspect that one of my independent variables has a mediating effect on several of my other independent ones. Therefore, i decided to try to find the indirect effect on the dependent variable through this one. 

I've never done this before, and think i might have done it wrong. 

When calculating the indirect effect with a regression, should i use one variable at a time when estimating the effect on the dependent and the mediator? Or should i use all the variables in the same regression, one for the dependent and one for the mediator? 

In the multiple regression there are only 4 out of 8 significant effects on the dependent, and 2 out of 7 significant effects on the mediator. However, when i did it one variable at a time, 5 out of 7 had a significant effect on the mediator. 

Also, in calculating the indirect effect. Would it be correct to subtract the effect when the mediator is included from the effect between the independent to the dependent? And should this also be done one variable at a time? 

Any help would be appreciated!",1,statistics,54935,,Some questions about mediation,https://www.reddit.com/r/statistics/comments/8nh8uo/some_questions_about_mediation/,all_ads,2018-05-31 14:12:28,2 days 11:23:06.036584000,
"I'm interested in finding some resources \- books, videos, or other materials to learn more about survey sampling, design of experiments, etc. 

I studied economics and so most of my statistics education, other than a couple of basic classes, focused on regression using econometric data.",1,1527756172.0,8ndrh4,False,"I'm interested in finding some resources \- books, videos, or other materials to learn more about survey sampling, design of experiments, etc. 

I studied economics and so most of my statistics education, other than a couple of basic classes, focused on regression using econometric data.",0,"I'm interested in finding some resources \- books, videos, or other materials to learn more about survey sampling, design of experiments, etc. 

I studied economics and so most of my statistics education, other than a couple of basic classes, focused on regression using econometric data.",6,statistics,54935,,Statistics for someone with econometrics background,https://www.reddit.com/r/statistics/comments/8ndrh4/statistics_for_someone_with_econometrics/,all_ads,2018-05-31 04:42:52,2 days 20:52:42.036584000,
,3,1527786793.0,8ngyod,False,,0,,0,statistics,54935,,How to calculate signed rank statistics for each day on excel?,https://www.reddit.com/r/statistics/comments/8ngyod/how_to_calculate_signed_rank_statistics_for_each/,all_ads,2018-05-31 13:13:13,2 days 12:22:21.036584000,
"Is it possible or wise to fit a probit model to a set of data like this:

Binary Outcome (Detection) | Response Variable (Amount)
---|---
1 | 100000
1 | 100000
1 | 100000
1 | 10000
1 | 10000
1 | 10000
1 | 1000
0 | 1000
1 | 1000
0 | 100
0 | 100
0 | 100
0 | 10
0 | 10
0 | 10

Originally I had several 'categories' of data like this, each with a slightly different starting value for the response variable (i.e. 123000 instead of 100000), but still decreasing by a factor of 10 each step.  With about 10 categories of data like this, I seemed to have fit a decent model which let me predict at which Amount the probit model would assign 95% probability of success.  

However, I'm now being told that it would be preferable to create a separate probit model for *each* category of data like above, but I'm uncomfortable doing so.  It seems to me that only the 'cutoff level' (1000 above) is really relevant for creating the model as given this data any number >1000 is an automatic success and any number <1000 is an automatic fail.  Indeed, when I try to fit a model to data like the above I get a very weak correlation between Amount and Detection where the prediction numbers are exact around the cutoff level (i.e. predicting the point 1000 gives exactly 2/3 chance of success) and pretty poor as you move away from that value.

Is there anything that can be done with a dataset like the above?  Is there a method other than probit (which is what I've been asked to use) that would be more descriptive about this data?  

I apologize if I'm not explaining the problem well, but thank you in advance for your help.",8,1527767641.0,8nf313,False,"Is it possible or wise to fit a probit model to a set of data like this:

Binary Outcome (Detection) | Response Variable (Amount)
---|---
1 | 100000
1 | 100000
1 | 100000
1 | 10000
1 | 10000
1 | 10000
1 | 1000
0 | 1000
1 | 1000
0 | 100
0 | 100
0 | 100
0 | 10
0 | 10
0 | 10

Originally I had several 'categories' of data like this, each with a slightly different starting value for the response variable (i.e. 123000 instead of 100000), but still decreasing by a factor of 10 each step.  With about 10 categories of data like this, I seemed to have fit a decent model which let me predict at which Amount the probit model would assign 95% probability of success.  

However, I'm now being told that it would be preferable to create a separate probit model for *each* category of data like above, but I'm uncomfortable doing so.  It seems to me that only the 'cutoff level' (1000 above) is really relevant for creating the model as given this data any number >1000 is an automatic success and any number <1000 is an automatic fail.  Indeed, when I try to fit a model to data like the above I get a very weak correlation between Amount and Detection where the prediction numbers are exact around the cutoff level (i.e. predicting the point 1000 gives exactly 2/3 chance of success) and pretty poor as you move away from that value.

Is there anything that can be done with a dataset like the above?  Is there a method other than probit (which is what I've been asked to use) that would be more descriptive about this data?  

I apologize if I'm not explaining the problem well, but thank you in advance for your help.",0,"Is it possible or wise to fit a probit model to a set of data like this:

Binary Outcome (Detection) | Response Variable (Amount)
---|---
1 | 100000
1 | 100000
1 | 100000
1 | 10000
1 | 10000
1 | 10000
1 | 1000
0 | 1000
1 | 1000
0 | 100
0 | 100
0 | 100
0 | 10
0 | 10
0 | 10

Originally I had several 'categories' of data like this, each with a slightly different starting value for the response variable (i.e. 123000 instead of 100000), but still decreasing by a factor of 10 each step.  With about 10 categories of data like this, I seemed to have fit a decent model which let me predict at which Amount the probit model would assign 95% probability of success.  

However, I'm now being told that it would be preferable to create a separate probit model for *each* category of data like above, but I'm uncomfortable doing so.  It seems to me that only the 'cutoff level' (1000 above) is really relevant for creating the model as given this data any number >1000 is an automatic success and any number <1000 is an automatic fail.  Indeed, when I try to fit a model to data like the above I get a very weak correlation between Amount and Detection where the prediction numbers are exact around the cutoff level (i.e. predicting the point 1000 gives exactly 2/3 chance of success) and pretty poor as you move away from that value.

Is there anything that can be done with a dataset like the above?  Is there a method other than probit (which is what I've been asked to use) that would be more descriptive about this data?  

I apologize if I'm not explaining the problem well, but thank you in advance for your help.",2,statistics,54935,,Probit Model Question,https://www.reddit.com/r/statistics/comments/8nf313/probit_model_question/,all_ads,2018-05-31 07:54:01,2 days 17:41:33.036584000,
"Can anyone provide me with a template of what I have to put into the syntax editor?

I have 2 IVs, 3 DV and 1 covariate.",0,1527775951.0,8ng426,False,"Can anyone provide me with a template of what I have to put into the syntax editor?

I have 2 IVs, 3 DV and 1 covariate.",0,"Can anyone provide me with a template of what I have to put into the syntax editor?

I have 2 IVs, 3 DV and 1 covariate.",1,statistics,54935,,How do I do a stepdown analysis in SPSS for a two-way MANCOVA?,https://www.reddit.com/r/statistics/comments/8ng426/how_do_i_do_a_stepdown_analysis_in_spss_for_a/,all_ads,2018-05-31 10:12:31,2 days 15:23:03.036584000,
"Hi, can you share your experience of writing down math stuff with *Word*, *OneNote*, *Notability*, *Goodnotes*, *LaTeX*, *Acrobat Reader*, etc?

I'd probably stick with *Word* as it supports formula input. My approach is to use *Notability* or *Acrobat Reader* in class as it's the quickest way to mark down on lecture notes with Apple Pencil. But this method costs me more time tidying up the handwritten notes and integrating them into a *Word* doc. So I say it's inefficient!

*OneNote* seems to be an ideal solution to this problem. I still haven't tried it. Any stat students use it?

*LaTeX* is elegant as far as I know. But I'm rather concerned about its compatibility with Word. I heard *MS Word* support formula input with *LaTeX* codes, so I might want to learn to input formula in *LaTeX* format.

**Productive statistics notes-taking?**",19,1527725774.0,8n9vg3,False,"Hi, can you share your experience of writing down math stuff with *Word*, *OneNote*, *Notability*, *Goodnotes*, *LaTeX*, *Acrobat Reader*, etc?

I'd probably stick with *Word* as it supports formula input. My approach is to use *Notability* or *Acrobat Reader* in class as it's the quickest way to mark down on lecture notes with Apple Pencil. But this method costs me more time tidying up the handwritten notes and integrating them into a *Word* doc. So I say it's inefficient!

*OneNote* seems to be an ideal solution to this problem. I still haven't tried it. Any stat students use it?

*LaTeX* is elegant as far as I know. But I'm rather concerned about its compatibility with Word. I heard *MS Word* support formula input with *LaTeX* codes, so I might want to learn to input formula in *LaTeX* format.

**Productive statistics notes-taking?**",0,"Hi, can you share your experience of writing down math stuff with *Word*, *OneNote*, *Notability*, *Goodnotes*, *LaTeX*, *Acrobat Reader*, etc?

I'd probably stick with *Word* as it supports formula input. My approach is to use *Notability* or *Acrobat Reader* in class as it's the quickest way to mark down on lecture notes with Apple Pencil. But this method costs me more time tidying up the handwritten notes and integrating them into a *Word* doc. So I say it's inefficient!

*OneNote* seems to be an ideal solution to this problem. I still haven't tried it. Any stat students use it?

*LaTeX* is elegant as far as I know. But I'm rather concerned about its compatibility with Word. I heard *MS Word* support formula input with *LaTeX* codes, so I might want to learn to input formula in *LaTeX* format.

**Productive statistics notes-taking?**",12,statistics,54935,,"As for statistics, is it inefficient to take notes on MS Word? Better suggestions?",https://www.reddit.com/r/statistics/comments/8n9vg3/as_for_statistics_is_it_inefficient_to_take_notes/,all_ads,2018-05-30 20:16:14,3 days 05:19:20.036584000,
"Hi guys 
If I were to extract genotype information from a genotype chip (I.e 450k) (one SNP only) would I still need to correct for genome wide significance?",3,1527755196.0,8ndnyk,False,"Hi guys 
If I were to extract genotype information from a genotype chip (I.e 450k) (one SNP only) would I still need to correct for genome wide significance?",0,"Hi guys 
If I were to extract genotype information from a genotype chip (I.e 450k) (one SNP only) would I still need to correct for genome wide significance?",2,statistics,54935,,Genome wide significance,https://www.reddit.com/r/statistics/comments/8ndnyk/genome_wide_significance/,all_ads,2018-05-31 04:26:36,2 days 21:08:58.036584000,
"I will be an incoming Masters student and would like some general advice to succeeding in the course work. 

Specifically, what can I do best to prepare myself? ",4,1527764377.0,8nenxf,False,"I will be an incoming Masters student and would like some general advice to succeeding in the course work. 

Specifically, what can I do best to prepare myself? ",0,"I will be an incoming Masters student and would like some general advice to succeeding in the course work. 

Specifically, what can I do best to prepare myself? ",1,statistics,54935,,Graduate School Advice,https://www.reddit.com/r/statistics/comments/8nenxf/graduate_school_advice/,all_ads,2018-05-31 06:59:37,2 days 18:35:57.036584000,
"If I were to use iodine to measure the Vitamin C content in various fruits, which would this be?",6,1527751360.0,8nd95g,False,"If I were to use iodine to measure the Vitamin C content in various fruits, which would this be?",0,"If I were to use iodine to measure the Vitamin C content in various fruits, which would this be?",1,statistics,54935,,Experiment or Observational Study and Why?,https://www.reddit.com/r/statistics/comments/8nd95g/experiment_or_observational_study_and_why/,all_ads,2018-05-31 03:22:40,2 days 22:12:54.036584000,
"As directly as I can state, I'd like to do forecasting on multiple time series. I don't have many data points so neural networks are out and I'd like to use dimensionality reduction appropriate to multivariate time series data.

I've been digging into a data set that consists of animal populations at 50 time points \(years\) across 10 locations and I'd like to forecast ahead in time by one unit \- I suspect that there are some latent variables influencing the traces which vary by location and possibly in time. Sorry, I’m brand new at this!

My general approach has been to apply [SSA](https://en.wikipedia.org/wiki/Singular_spectrum_analysis) on single locations and compare the output components and later to apply MSSA \(multivariate\-SSA\) which can handle multiple traces using the R package Rssa. I have also tried out using ForeCA  \(forecast canonical analysis\) but it can handle only handle single traces and I suspect that there is some relationship between the locations.

I considered doing something like multivariate Fourier techniques but I'd like not to be constrained to using sinusoids as models since I'm hoping that one or more output components \(purported latent variables\) will map onto something like temperature or prey populations.

There are also more species that interact on the food chain I can look at but I was unable to find any ""tensor\-ized""  versions of these techniques. Are VAR/VARIMA methods appropriate if I’m actually trying to find canonical traces shared by locations?

Does this all sound reasonable or am I just trying to wring blood from a stone? Thanks in advance.",5,1527684416.0,8n5ud9,False,"As directly as I can state, I'd like to do forecasting on multiple time series. I don't have many data points so neural networks are out and I'd like to use dimensionality reduction appropriate to multivariate time series data.

I've been digging into a data set that consists of animal populations at 50 time points \(years\) across 10 locations and I'd like to forecast ahead in time by one unit \- I suspect that there are some latent variables influencing the traces which vary by location and possibly in time. Sorry, I’m brand new at this!

My general approach has been to apply [SSA](https://en.wikipedia.org/wiki/Singular_spectrum_analysis) on single locations and compare the output components and later to apply MSSA \(multivariate\-SSA\) which can handle multiple traces using the R package Rssa. I have also tried out using ForeCA  \(forecast canonical analysis\) but it can handle only handle single traces and I suspect that there is some relationship between the locations.

I considered doing something like multivariate Fourier techniques but I'd like not to be constrained to using sinusoids as models since I'm hoping that one or more output components \(purported latent variables\) will map onto something like temperature or prey populations.

There are also more species that interact on the food chain I can look at but I was unable to find any ""tensor\-ized""  versions of these techniques. Are VAR/VARIMA methods appropriate if I’m actually trying to find canonical traces shared by locations?

Does this all sound reasonable or am I just trying to wring blood from a stone? Thanks in advance.",0,"As directly as I can state, I'd like to do forecasting on multiple time series. I don't have many data points so neural networks are out and I'd like to use dimensionality reduction appropriate to multivariate time series data.

I've been digging into a data set that consists of animal populations at 50 time points \(years\) across 10 locations and I'd like to forecast ahead in time by one unit \- I suspect that there are some latent variables influencing the traces which vary by location and possibly in time. Sorry, I’m brand new at this!

My general approach has been to apply [SSA](https://en.wikipedia.org/wiki/Singular_spectrum_analysis) on single locations and compare the output components and later to apply MSSA \(multivariate\-SSA\) which can handle multiple traces using the R package Rssa. I have also tried out using ForeCA  \(forecast canonical analysis\) but it can handle only handle single traces and I suspect that there is some relationship between the locations.

I considered doing something like multivariate Fourier techniques but I'd like not to be constrained to using sinusoids as models since I'm hoping that one or more output components \(purported latent variables\) will map onto something like temperature or prey populations.

There are also more species that interact on the food chain I can look at but I was unable to find any ""tensor\-ized""  versions of these techniques. Are VAR/VARIMA methods appropriate if I’m actually trying to find canonical traces shared by locations?

Does this all sound reasonable or am I just trying to wring blood from a stone? Thanks in advance.",19,statistics,54935,,Question on decomposing multiple time series using SSA (PCA in time): am I on the right track?,https://www.reddit.com/r/statistics/comments/8n5ud9/question_on_decomposing_multiple_time_series/,all_ads,2018-05-30 08:46:56,3 days 16:48:38.036584000,
"Been teaching myself statistics for a month. I now find myself lost as I started to be aware that there are too many topics.

I've gone through basics about sampling and population, some major distribution models, central limit theorem, correlation and dependence, hypothesis test, fundamental theoretical probability, etc.

However, I always want to learn advanced statistics. I have no idea which topic I should learn now after I realized there are so many [topics on Wiki](https://en.wikipedia.org/wiki/List_of_statistics_articles).

Maybe it's time for me to know each branch in statistics in general?",6,1527695316.0,8n6r1i,False,"Been teaching myself statistics for a month. I now find myself lost as I started to be aware that there are too many topics.

I've gone through basics about sampling and population, some major distribution models, central limit theorem, correlation and dependence, hypothesis test, fundamental theoretical probability, etc.

However, I always want to learn advanced statistics. I have no idea which topic I should learn now after I realized there are so many [topics on Wiki](https://en.wikipedia.org/wiki/List_of_statistics_articles).

Maybe it's time for me to know each branch in statistics in general?",0,"Been teaching myself statistics for a month. I now find myself lost as I started to be aware that there are too many topics.

I've gone through basics about sampling and population, some major distribution models, central limit theorem, correlation and dependence, hypothesis test, fundamental theoretical probability, etc.

However, I always want to learn advanced statistics. I have no idea which topic I should learn now after I realized there are so many [topics on Wiki](https://en.wikipedia.org/wiki/List_of_statistics_articles).

Maybe it's time for me to know each branch in statistics in general?",4,statistics,54935,,"Books/websites for general introduction to main branches, methods and concepts of statistics?",https://www.reddit.com/r/statistics/comments/8n6r1i/bookswebsites_for_general_introduction_to_main/,all_ads,2018-05-30 11:48:36,3 days 13:46:58.036584000,
"Hi there,

I have a group project in business which really has to start going soon. We are two people with no stats experience but it looks like we might have to use SPSS or something similar to get results. Basically, we want to test the relationship between different corporate governance factors on innovative performance on firm level, and compare between a group of 2 countries. I can get the variables from databased and reports, i.e. governance factors and also the dependent variable like innovation output. But due to limited time we will have a small sample size.

 I just wonder how I can do this in a spreadsheet etc. and apply correlation and/or regression analysis on this type of data in an ""easy"" way. It's not a thesis or anything but I thought if this could be done quick we can use this method and then state that it is limited due to sample and time and that further study could be made \(e.g. in my thesis\).

Thanks for any help.",3,1527703356.0,8n7d4g,False,"Hi there,

I have a group project in business which really has to start going soon. We are two people with no stats experience but it looks like we might have to use SPSS or something similar to get results. Basically, we want to test the relationship between different corporate governance factors on innovative performance on firm level, and compare between a group of 2 countries. I can get the variables from databased and reports, i.e. governance factors and also the dependent variable like innovation output. But due to limited time we will have a small sample size.

 I just wonder how I can do this in a spreadsheet etc. and apply correlation and/or regression analysis on this type of data in an ""easy"" way. It's not a thesis or anything but I thought if this could be done quick we can use this method and then state that it is limited due to sample and time and that further study could be made \(e.g. in my thesis\).

Thanks for any help.",0,"Hi there,

I have a group project in business which really has to start going soon. We are two people with no stats experience but it looks like we might have to use SPSS or something similar to get results. Basically, we want to test the relationship between different corporate governance factors on innovative performance on firm level, and compare between a group of 2 countries. I can get the variables from databased and reports, i.e. governance factors and also the dependent variable like innovation output. But due to limited time we will have a small sample size.

 I just wonder how I can do this in a spreadsheet etc. and apply correlation and/or regression analysis on this type of data in an ""easy"" way. It's not a thesis or anything but I thought if this could be done quick we can use this method and then state that it is limited due to sample and time and that further study could be made \(e.g. in my thesis\).

Thanks for any help.",3,statistics,54935,,Methodology question,https://www.reddit.com/r/statistics/comments/8n7d4g/methodology_question/,all_ads,2018-05-30 14:02:36,3 days 11:32:58.036584000,
"[https://academic.oup.com/annonc/advance\-article/doi/10.1093/annonc/mdy166/5004443](https://academic.oup.com/annonc/advance-article/doi/10.1093/annonc/mdy166/5004443)

Results:

\>In level\-I dermatologists achieved a mean \(±standard deviation\) sensitivity and specificity for lesion classification of 86.6&#37; \(±9.3&#37;\) and 71.3&#37; \(±11.2&#37;\), respectively. More clinical information \(level\-II\) improved the sensitivity to 88.9&#37; \(±9.6&#37;, *P* = 0.19*\)* and specificity to 75.7&#37; \(±11.7&#37;, P \< 0.05\). The CNN ROC curve revealed a higher specificity of 82.5&#37; when compared wit*h* dermatologists* *in level\-I \(71.3&#37;, P \< 0.01\) and level\-II \(75.7&#37;, P \< 0.01\) at their sensitivities of 86.6&#37; and 88.9&#37;, respectively. The CNN ROC AUC was greater than the *m*ean ROC area of dermatologists \(0.86 versus 0.79, P \< 0.01\). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge.",27,1527642730.0,8n0t0y,False,"[https://academic.oup.com/annonc/advance\-article/doi/10.1093/annonc/mdy166/5004443](https://academic.oup.com/annonc/advance-article/doi/10.1093/annonc/mdy166/5004443)

Results:

\>In level\-I dermatologists achieved a mean \(±standard deviation\) sensitivity and specificity for lesion classification of 86.6&#37; \(±9.3&#37;\) and 71.3&#37; \(±11.2&#37;\), respectively. More clinical information \(level\-II\) improved the sensitivity to 88.9&#37; \(±9.6&#37;, *P* = 0.19*\)* and specificity to 75.7&#37; \(±11.7&#37;, P \< 0.05\). The CNN ROC curve revealed a higher specificity of 82.5&#37; when compared wit*h* dermatologists* *in level\-I \(71.3&#37;, P \< 0.01\) and level\-II \(75.7&#37;, P \< 0.01\) at their sensitivities of 86.6&#37; and 88.9&#37;, respectively. The CNN ROC AUC was greater than the *m*ean ROC area of dermatologists \(0.86 versus 0.79, P \< 0.01\). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge.",0,"[https://academic.oup.com/annonc/advance\-article/doi/10.1093/annonc/mdy166/5004443](https://academic.oup.com/annonc/advance-article/doi/10.1093/annonc/mdy166/5004443)

Results:

\>In level\-I dermatologists achieved a mean \(±standard deviation\) sensitivity and specificity for lesion classification of 86.6&#37; \(±9.3&#37;\) and 71.3&#37; \(±11.2&#37;\), respectively. More clinical information \(level\-II\) improved the sensitivity to 88.9&#37; \(±9.6&#37;, *P* = 0.19*\)* and specificity to 75.7&#37; \(±11.7&#37;, P \< 0.05\). The CNN ROC curve revealed a higher specificity of 82.5&#37; when compared wit*h* dermatologists* *in level\-I \(71.3&#37;, P \< 0.01\) and level\-II \(75.7&#37;, P \< 0.01\) at their sensitivities of 86.6&#37; and 88.9&#37;, respectively. The CNN ROC AUC was greater than the *m*ean ROC area of dermatologists \(0.86 versus 0.79, P \< 0.01\). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge.",44,statistics,54935,,"Man against machine: diagnostic performance of a DL CNN for melanoma recognition in comparison to 58 dermatologists, ""(95% compared to 86.6%)""",https://www.reddit.com/r/statistics/comments/8n0t0y/man_against_machine_diagnostic_performance_of_a/,all_ads,2018-05-29 21:12:10,4 days 04:23:24.036584000,
"I have two, three-level, categorical variables in R. One is reference coded (A) and one is sequence coded (B).

The R output provides separate interaction terms for each level of the predictors. For example, terms for:

* Level 2 of A X Level 2 of B
* Level 3 of A X Level 2 of B
* Level 2 of A X Level 3 of B
* Level 2 of A X Level 3 of B

First of all, I have some trouble wording what these interactions mean in my personal dialogue. For instance, imagining that the first interaction is significant, it wouldn't be correct to say that ""the effect of variable A depends on variable B"" because the test is for specific levels of each. Would one say ""the effect of level 2 of variable A (relative to level 1) depends on the effect of level 2 of variable B (relative to level1)""? This is quite difficult for me to grok. 

In any case, assume again that this interaction is significant. What is the correct follow up? Is it to look at the effect of Variable B (level 2 vs 1) separately for levels 2 and 1 of Variable A? I.e., completely ignoring level 3 of each variable, and following up as though this were a significant 2x2 ANOVA?",4,1527667377.0,8n40fj,False,"I have two, three-level, categorical variables in R. One is reference coded (A) and one is sequence coded (B).

The R output provides separate interaction terms for each level of the predictors. For example, terms for:

* Level 2 of A X Level 2 of B
* Level 3 of A X Level 2 of B
* Level 2 of A X Level 3 of B
* Level 2 of A X Level 3 of B

First of all, I have some trouble wording what these interactions mean in my personal dialogue. For instance, imagining that the first interaction is significant, it wouldn't be correct to say that ""the effect of variable A depends on variable B"" because the test is for specific levels of each. Would one say ""the effect of level 2 of variable A (relative to level 1) depends on the effect of level 2 of variable B (relative to level1)""? This is quite difficult for me to grok. 

In any case, assume again that this interaction is significant. What is the correct follow up? Is it to look at the effect of Variable B (level 2 vs 1) separately for levels 2 and 1 of Variable A? I.e., completely ignoring level 3 of each variable, and following up as though this were a significant 2x2 ANOVA?",0,"I have two, three-level, categorical variables in R. One is reference coded (A) and one is sequence coded (B).

The R output provides separate interaction terms for each level of the predictors. For example, terms for:

* Level 2 of A X Level 2 of B
* Level 3 of A X Level 2 of B
* Level 2 of A X Level 3 of B
* Level 2 of A X Level 3 of B

First of all, I have some trouble wording what these interactions mean in my personal dialogue. For instance, imagining that the first interaction is significant, it wouldn't be correct to say that ""the effect of variable A depends on variable B"" because the test is for specific levels of each. Would one say ""the effect of level 2 of variable A (relative to level 1) depends on the effect of level 2 of variable B (relative to level1)""? This is quite difficult for me to grok. 

In any case, assume again that this interaction is significant. What is the correct follow up? Is it to look at the effect of Variable B (level 2 vs 1) separately for levels 2 and 1 of Variable A? I.e., completely ignoring level 3 of each variable, and following up as though this were a significant 2x2 ANOVA?",5,statistics,54935,,Question about interpreting interactions between three level dummy coded variables in R.,https://www.reddit.com/r/statistics/comments/8n40fj/question_about_interpreting_interactions_between/,all_ads,2018-05-30 04:02:57,3 days 21:32:37.036584000,
"I'm currently playing a deck that has a lot of strong combos that require a wide variety of two different cards. I'm curious on what I would do to calculate the chance that I start the game with the possibility to do the combo with my opening hand.

Some relevant information is:

Deck size is 40 cards

Opening hand size is 5 cards

I need 1 of each, drawing multiple is not an issue. 

The maximum limit of playing any certain card is 3, so when I say that I play 6 or 10 copies of the card it is just multiple different cards that serve the same function. 

For example, if I open with any of a card I play 6 copies of and any card I play 10 copies of, my combo will start. 

I know that trying to calculate the chance of opening X copies of a single card would be a hypergeometric distribution, what would this be categorized as and how would I calculate it?

Would it just be P(card A) * P(card B) + every combination?",1,1527672558.0,8n4lh4,False,"I'm currently playing a deck that has a lot of strong combos that require a wide variety of two different cards. I'm curious on what I would do to calculate the chance that I start the game with the possibility to do the combo with my opening hand.

Some relevant information is:

Deck size is 40 cards

Opening hand size is 5 cards

I need 1 of each, drawing multiple is not an issue. 

The maximum limit of playing any certain card is 3, so when I say that I play 6 or 10 copies of the card it is just multiple different cards that serve the same function. 

For example, if I open with any of a card I play 6 copies of and any card I play 10 copies of, my combo will start. 

I know that trying to calculate the chance of opening X copies of a single card would be a hypergeometric distribution, what would this be categorized as and how would I calculate it?

Would it just be P(card A) * P(card B) + every combination?",0,"I'm currently playing a deck that has a lot of strong combos that require a wide variety of two different cards. I'm curious on what I would do to calculate the chance that I start the game with the possibility to do the combo with my opening hand.

Some relevant information is:

Deck size is 40 cards

Opening hand size is 5 cards

I need 1 of each, drawing multiple is not an issue. 

The maximum limit of playing any certain card is 3, so when I say that I play 6 or 10 copies of the card it is just multiple different cards that serve the same function. 

For example, if I open with any of a card I play 6 copies of and any card I play 10 copies of, my combo will start. 

I know that trying to calculate the chance of opening X copies of a single card would be a hypergeometric distribution, what would this be categorized as and how would I calculate it?

Would it just be P(card A) * P(card B) + every combination?",4,statistics,54935,,Yu-Gi-Oh combo probability question,https://www.reddit.com/r/statistics/comments/8n4lh4/yugioh_combo_probability_question/,all_ads,2018-05-30 05:29:18,3 days 20:06:16.036584000,
,5,1527661472.0,8n3bhf,False,,0,,1,statistics,54935,,"Is Gait analysis possible in statistics using time series analysis in MATLAB or Python or any other language, and if possible can anyone point me in the direction i should be looking ?",https://www.reddit.com/r/statistics/comments/8n3bhf/is_gait_analysis_possible_in_statistics_using/,all_ads,2018-05-30 02:24:32,3 days 23:11:02.036584000,
So I would like to major in statistics in college and have looked at schools such as Ohio State. Next school year I will be taking AP Statistics at my high school. Anyway the purpose of this post is to ask you guys what I should start learning/researching over the summer to have a good understanding going into my class next year. I want to be successful in the course and just would like some beginner material to make myself successful.,10,1527594987.0,8mw3wt,False,So I would like to major in statistics in college and have looked at schools such as Ohio State. Next school year I will be taking AP Statistics at my high school. Anyway the purpose of this post is to ask you guys what I should start learning/researching over the summer to have a good understanding going into my class next year. I want to be successful in the course and just would like some beginner material to make myself successful.,0,So I would like to major in statistics in college and have looked at schools such as Ohio State. Next school year I will be taking AP Statistics at my high school. Anyway the purpose of this post is to ask you guys what I should start learning/researching over the summer to have a good understanding going into my class next year. I want to be successful in the course and just would like some beginner material to make myself successful.,10,statistics,54935,,Starting Material,https://www.reddit.com/r/statistics/comments/8mw3wt/starting_material/,all_ads,2018-05-29 07:56:27,4 days 17:39:07.036584000,
"Washington, D.C.– The Bipartisan Policy Center today released a letter signed by 36 former leaders in the U.S. Federal Statistical System calling on Congress and the Trump administration to improve how data are used in policymaking, which will result in more effective federal programs for the American public.

Signatories to the letter include seven former Census Bureau directors, seven former Bureau of Labor Statistics commissioners, four former Energy Information Administration administrators, and two former chief statisticians of the United States.

[continued...](https://bipartisanpolicy.org/press-release/former-government-statistical-agency-heads-call-for-better-use-of-data-in-policymaking/)",9,1527544541.0,8mq7qa,False,"Washington, D.C.– The Bipartisan Policy Center today released a letter signed by 36 former leaders in the U.S. Federal Statistical System calling on Congress and the Trump administration to improve how data are used in policymaking, which will result in more effective federal programs for the American public.

Signatories to the letter include seven former Census Bureau directors, seven former Bureau of Labor Statistics commissioners, four former Energy Information Administration administrators, and two former chief statisticians of the United States.

[continued...](https://bipartisanpolicy.org/press-release/former-government-statistical-agency-heads-call-for-better-use-of-data-in-policymaking/)",0,"Washington, D.C.– The Bipartisan Policy Center today released a letter signed by 36 former leaders in the U.S. Federal Statistical System calling on Congress and the Trump administration to improve how data are used in policymaking, which will result in more effective federal programs for the American public.

Signatories to the letter include seven former Census Bureau directors, seven former Bureau of Labor Statistics commissioners, four former Energy Information Administration administrators, and two former chief statisticians of the United States.

[continued...](https://bipartisanpolicy.org/press-release/former-government-statistical-agency-heads-call-for-better-use-of-data-in-policymaking/)",77,statistics,54935,,Former Government Statistical Agency Heads Call for Better Use of Data in Policymaking,https://www.reddit.com/r/statistics/comments/8mq7qa/former_government_statistical_agency_heads_call/,all_ads,2018-05-28 17:55:41,5 days 07:39:53.036584000,
"For my psychology class i was conducting a study to see whether or not the utilization of resources has a positive effect on transferable GPA. So from looking at the p\-value \(one tailed i think\) should i reject or not reject my null hypothesis and why?

My  p\-value one tailed is .207 and two tailed is .414 , my t is .82 and my DF is 103. 

My r is .0801 if that matters 

mean for resources is \(m = 2.8571 , SD = 1.6375\) and for GPA is \(m = 3.1793 , SD = .55\)",8,1527593743.0,8mvzc6,False,"For my psychology class i was conducting a study to see whether or not the utilization of resources has a positive effect on transferable GPA. So from looking at the p\-value \(one tailed i think\) should i reject or not reject my null hypothesis and why?

My  p\-value one tailed is .207 and two tailed is .414 , my t is .82 and my DF is 103. 

My r is .0801 if that matters 

mean for resources is \(m = 2.8571 , SD = 1.6375\) and for GPA is \(m = 3.1793 , SD = .55\)",0,"For my psychology class i was conducting a study to see whether or not the utilization of resources has a positive effect on transferable GPA. So from looking at the p\-value \(one tailed i think\) should i reject or not reject my null hypothesis and why?

My  p\-value one tailed is .207 and two tailed is .414 , my t is .82 and my DF is 103. 

My r is .0801 if that matters 

mean for resources is \(m = 2.8571 , SD = 1.6375\) and for GPA is \(m = 3.1793 , SD = .55\)",2,statistics,54935,,How do i know whether or not to reject my null hypothesis?,https://www.reddit.com/r/statistics/comments/8mvzc6/how_do_i_know_whether_or_not_to_reject_my_null/,all_ads,2018-05-29 07:35:43,4 days 17:59:51.036584000,
"Hey guys, I got 2 strange questions I need your help with:

1) What does: (x / mean) * 100 achieve? Where x = each row value, mean is the mean of the column.

2) https://imgur.com/a/nhQ96np I'm trying to create something like this. Use a common mean for all variables then describe the variation on different subsets, but I'm not sure how I would center the mean on for example 100 (lol standardize) and how to analyze the variation in terms of dispersion.

I'm not very strong at stats and this is making me nuts so don't mind any mistake, thanks!  
",4,1527589086.0,8mvgok,False,"Hey guys, I got 2 strange questions I need your help with:

1) What does: (x / mean) * 100 achieve? Where x = each row value, mean is the mean of the column.

2) https://imgur.com/a/nhQ96np I'm trying to create something like this. Use a common mean for all variables then describe the variation on different subsets, but I'm not sure how I would center the mean on for example 100 (lol standardize) and how to analyze the variation in terms of dispersion.

I'm not very strong at stats and this is making me nuts so don't mind any mistake, thanks!  
",0,"Hey guys, I got 2 strange questions I need your help with:

1) What does: (x / mean) * 100 achieve? Where x = each row value, mean is the mean of the column.

2) https://imgur.com/a/nhQ96np I'm trying to create something like this. Use a common mean for all variables then describe the variation on different subsets, but I'm not sure how I would center the mean on for example 100 (lol standardize) and how to analyze the variation in terms of dispersion.

I'm not very strong at stats and this is making me nuts so don't mind any mistake, thanks!  
",3,statistics,54935,,How do you center this on,https://www.reddit.com/r/statistics/comments/8mvgok/how_do_you_center_this_on/,all_ads,2018-05-29 06:18:06,4 days 19:17:28.036584000,
"To \*FINALLY\* get my degree in physics I need to finish a work, in my case will be about PCA analysis applied in FT\-IR\(infrared spectroscopy\).

BUTT i didnt had statistics yet, so i would like some books to start, like regression, covariances etc... and something about PCA/multivariate analysis too, i will be using R for the analysis.

thx",0,1527566102.0,8msva1,False,"To \*FINALLY\* get my degree in physics I need to finish a work, in my case will be about PCA analysis applied in FT\-IR\(infrared spectroscopy\).

BUTT i didnt had statistics yet, so i would like some books to start, like regression, covariances etc... and something about PCA/multivariate analysis too, i will be using R for the analysis.

thx",0,"To \*FINALLY\* get my degree in physics I need to finish a work, in my case will be about PCA analysis applied in FT\-IR\(infrared spectroscopy\).

BUTT i didnt had statistics yet, so i would like some books to start, like regression, covariances etc... and something about PCA/multivariate analysis too, i will be using R for the analysis.

thx",3,statistics,54935,,Book recomendations for my college,https://www.reddit.com/r/statistics/comments/8msva1/book_recomendations_for_my_college/,all_ads,2018-05-28 23:55:02,5 days 01:40:32.036584000,
"Every day this sub gets multiple posts asking how to become a statistician, what courses should they take, whether a particular Masters program is a good one, which introductory books people recommend, what software or code should they learn and whether they can be a biostatistician.  
  
It seems like the majority of active posts on this subreddit are one of the above and I barely have any posts about actual statistics (approaches to modelling, new techniques, questions about research design, etc) come through to my front page. 
  
I would just unsub but I'm unsure if there's another subreddit that fills this other niche of actually being about applied or theoretical statistics. If I actually go into this subreddit I see plenty of posts about what I'd like to see more of but it seems that the commonly asked easy questions get more replies and thus drown out the others.  
  
I think a good middle ground would be to select a particular day of the week in which the sub permits questions about uni/becoming a statistician/recommended textbooks, and the difference between a data analyst/statistician/data scientist, etc.  
  
I might be out of line here but wanted to get other's opinions on the matter. Thoughts?",21,1527518166.0,8mo21s,False,"Every day this sub gets multiple posts asking how to become a statistician, what courses should they take, whether a particular Masters program is a good one, which introductory books people recommend, what software or code should they learn and whether they can be a biostatistician.  
  
It seems like the majority of active posts on this subreddit are one of the above and I barely have any posts about actual statistics (approaches to modelling, new techniques, questions about research design, etc) come through to my front page. 
  
I would just unsub but I'm unsure if there's another subreddit that fills this other niche of actually being about applied or theoretical statistics. If I actually go into this subreddit I see plenty of posts about what I'd like to see more of but it seems that the commonly asked easy questions get more replies and thus drown out the others.  
  
I think a good middle ground would be to select a particular day of the week in which the sub permits questions about uni/becoming a statistician/recommended textbooks, and the difference between a data analyst/statistician/data scientist, etc.  
  
I might be out of line here but wanted to get other's opinions on the matter. Thoughts?",0,"Every day this sub gets multiple posts asking how to become a statistician, what courses should they take, whether a particular Masters program is a good one, which introductory books people recommend, what software or code should they learn and whether they can be a biostatistician.  
  
It seems like the majority of active posts on this subreddit are one of the above and I barely have any posts about actual statistics (approaches to modelling, new techniques, questions about research design, etc) come through to my front page. 
  
I would just unsub but I'm unsure if there's another subreddit that fills this other niche of actually being about applied or theoretical statistics. If I actually go into this subreddit I see plenty of posts about what I'd like to see more of but it seems that the commonly asked easy questions get more replies and thus drown out the others.  
  
I think a good middle ground would be to select a particular day of the week in which the sub permits questions about uni/becoming a statistician/recommended textbooks, and the difference between a data analyst/statistician/data scientist, etc.  
  
I might be out of line here but wanted to get other's opinions on the matter. Thoughts?",29,statistics,54935,,[meta] can we limit certain questions to specific days of the week?,https://www.reddit.com/r/statistics/comments/8mo21s/meta_can_we_limit_certain_questions_to_specific/,all_ads,2018-05-28 10:36:06,5 days 14:59:28.036584000,
"I'm trying to understand the procedure but I'm a bit stuck, and can't find any direct information in any papers. The closest I can find is in [this paper](https://www.researchgate.net/publication/47386959_How_to_fit_models_of_recognition_memory_data_using_maximum_likelihood), but in their example excel file, under the 'EXPECTED RATE' table, I am unable to replicate the values. I'm not sure how they achieve the values in cells L12:P12, and so I can't fit the signal detection model to the ROC data... Can anybody help?",0,1527578832.0,8mud5f,False,"I'm trying to understand the procedure but I'm a bit stuck, and can't find any direct information in any papers. The closest I can find is in [this paper](https://www.researchgate.net/publication/47386959_How_to_fit_models_of_recognition_memory_data_using_maximum_likelihood), but in their example excel file, under the 'EXPECTED RATE' table, I am unable to replicate the values. I'm not sure how they achieve the values in cells L12:P12, and so I can't fit the signal detection model to the ROC data... Can anybody help?",0,"I'm trying to understand the procedure but I'm a bit stuck, and can't find any direct information in any papers. The closest I can find is in [this paper](https://www.researchgate.net/publication/47386959_How_to_fit_models_of_recognition_memory_data_using_maximum_likelihood), but in their example excel file, under the 'EXPECTED RATE' table, I am unable to replicate the values. I'm not sure how they achieve the values in cells L12:P12, and so I can't fit the signal detection model to the ROC data... Can anybody help?",0,statistics,54935,,Fitting signal detection models to ROC data,https://www.reddit.com/r/statistics/comments/8mud5f/fitting_signal_detection_models_to_roc_data/,all_ads,2018-05-29 03:27:12,4 days 22:08:22.036584000,
"Hi all,

I'm working on improving my modeling skills in cases where the outcome is very unlikely \(and thus not very prominent in the training dataset\). I can build a basic logit without any problems, but was trying then to test out a few sampling methodologies that I have read are useful in the case of low incidence binary classification \(namely SMOTE, ROSE, and upsampling\). 

When I build these logits with differently sampled training data, however, I sometimes get a troublesome error that I fear is a sign that the model output and coefficients will lack interpretability / utility: ""fitted probabilities numerically 0 or 1 occurred"". I have seen this error before in cases when I accidentally left the outcome variable in the estimation dataset \(so of course I got 100&#37; accuracy\), or in cases where I model every possible interaction variable and thus my number of variables \> n, but neither is the case here. The code can all be seen at the link to follow; I'd really appreciate your help / suggestions: [https://github.com/pmaji/r\-stats\-and\-modeling/blob/master/classification/low\_incidence\_binary\_classification.md](https://github.com/pmaji/r-stats-and-modeling/blob/master/classification/low_incidence_binary_classification.md)",8,1527575678.0,8mu0ec,False,"Hi all,

I'm working on improving my modeling skills in cases where the outcome is very unlikely \(and thus not very prominent in the training dataset\). I can build a basic logit without any problems, but was trying then to test out a few sampling methodologies that I have read are useful in the case of low incidence binary classification \(namely SMOTE, ROSE, and upsampling\). 

When I build these logits with differently sampled training data, however, I sometimes get a troublesome error that I fear is a sign that the model output and coefficients will lack interpretability / utility: ""fitted probabilities numerically 0 or 1 occurred"". I have seen this error before in cases when I accidentally left the outcome variable in the estimation dataset \(so of course I got 100&#37; accuracy\), or in cases where I model every possible interaction variable and thus my number of variables \> n, but neither is the case here. The code can all be seen at the link to follow; I'd really appreciate your help / suggestions: [https://github.com/pmaji/r\-stats\-and\-modeling/blob/master/classification/low\_incidence\_binary\_classification.md](https://github.com/pmaji/r-stats-and-modeling/blob/master/classification/low_incidence_binary_classification.md)",0,"Hi all,

I'm working on improving my modeling skills in cases where the outcome is very unlikely \(and thus not very prominent in the training dataset\). I can build a basic logit without any problems, but was trying then to test out a few sampling methodologies that I have read are useful in the case of low incidence binary classification \(namely SMOTE, ROSE, and upsampling\). 

When I build these logits with differently sampled training data, however, I sometimes get a troublesome error that I fear is a sign that the model output and coefficients will lack interpretability / utility: ""fitted probabilities numerically 0 or 1 occurred"". I have seen this error before in cases when I accidentally left the outcome variable in the estimation dataset \(so of course I got 100&#37; accuracy\), or in cases where I model every possible interaction variable and thus my number of variables \> n, but neither is the case here. The code can all be seen at the link to follow; I'd really appreciate your help / suggestions: [https://github.com/pmaji/r\-stats\-and\-modeling/blob/master/classification/low\_incidence\_binary\_classification.md](https://github.com/pmaji/r-stats-and-modeling/blob/master/classification/low_incidence_binary_classification.md)",1,statistics,54935,,Sampling Methods Questions (Low Incidence Binary Classification),https://www.reddit.com/r/statistics/comments/8mu0ec/sampling_methods_questions_low_incidence_binary/,all_ads,2018-05-29 02:34:38,4 days 23:00:56.036584000,
"26M, US citizen. I want to get my master degree in statistics/public health. I also really want to travel for an extended period of time. I've only gone on trips for two weeks at a time. I've been working for 4 years but the degree will be to change careers. I've saved up money and have no debt at all. I'm not considering studying in the US because tuition is too expensive. I've applied and gotten into some programs in Europe for Sept 2018 that have low tuition costs (<= ~$1,500/year) but I'm second guessing going now or deferring for a year.

I have three options:

* Start a Master program in Europe this fall. Get degree in two years and hopefully be able to stay in Europe afterwards (somewhere) and get a job. But I got no funding, so cost of living is ~$10,000. Will probably work part-time anyways. Keep most of savings for future
* Travel for 5-6 months, then start a program in South Africa/Aus/NZ in January/February. I can apply for funding and see how it goes. Can still potentially move to Europe afterwards due to US working holiday agreements ([UK](https://www.bunac.org/usa/intern-abroad/professional/britain/visas)/Ireland). Try to network and get work permit. Can't find a program that fits what I want 90%-100% 
* Travel for a year. Trying to set up an internship abroad for Sept-Nov, travel freely Dec-Feb/Mar. I have no idea what to do from March-June. Probably work over June-Aug. Europe Sept 2019. Three years off is a long time. Worried about losing my mind traveling that long. Bad job prospects from so much time off. I'll be 29 by then :( Have considered studying part-time while travel but worry about logistics.

I'm not getting any younger so the scary options (2 & 3) could be good for self-development and I just want to go. But the responsible, pragmatic side of me is telling me to just go get the degree done and save my money (this is the side I usually listen to). Option 2 seems like the best of both worlds if I could find a solid program to apply to...
",11,1527574830.0,8mtwtm,False,"26M, US citizen. I want to get my master degree in statistics/public health. I also really want to travel for an extended period of time. I've only gone on trips for two weeks at a time. I've been working for 4 years but the degree will be to change careers. I've saved up money and have no debt at all. I'm not considering studying in the US because tuition is too expensive. I've applied and gotten into some programs in Europe for Sept 2018 that have low tuition costs (<= ~$1,500/year) but I'm second guessing going now or deferring for a year.

I have three options:

* Start a Master program in Europe this fall. Get degree in two years and hopefully be able to stay in Europe afterwards (somewhere) and get a job. But I got no funding, so cost of living is ~$10,000. Will probably work part-time anyways. Keep most of savings for future
* Travel for 5-6 months, then start a program in South Africa/Aus/NZ in January/February. I can apply for funding and see how it goes. Can still potentially move to Europe afterwards due to US working holiday agreements ([UK](https://www.bunac.org/usa/intern-abroad/professional/britain/visas)/Ireland). Try to network and get work permit. Can't find a program that fits what I want 90%-100% 
* Travel for a year. Trying to set up an internship abroad for Sept-Nov, travel freely Dec-Feb/Mar. I have no idea what to do from March-June. Probably work over June-Aug. Europe Sept 2019. Three years off is a long time. Worried about losing my mind traveling that long. Bad job prospects from so much time off. I'll be 29 by then :( Have considered studying part-time while travel but worry about logistics.

I'm not getting any younger so the scary options (2 & 3) could be good for self-development and I just want to go. But the responsible, pragmatic side of me is telling me to just go get the degree done and save my money (this is the side I usually listen to). Option 2 seems like the best of both worlds if I could find a solid program to apply to...
",0,"26M, US citizen. I want to get my master degree in statistics/public health. I also really want to travel for an extended period of time. I've only gone on trips for two weeks at a time. I've been working for 4 years but the degree will be to change careers. I've saved up money and have no debt at all. I'm not considering studying in the US because tuition is too expensive. I've applied and gotten into some programs in Europe for Sept 2018 that have low tuition costs (<= ~$1,500/year) but I'm second guessing going now or deferring for a year.

I have three options:

* Start a Master program in Europe this fall. Get degree in two years and hopefully be able to stay in Europe afterwards (somewhere) and get a job. But I got no funding, so cost of living is ~$10,000. Will probably work part-time anyways. Keep most of savings for future
* Travel for 5-6 months, then start a program in South Africa/Aus/NZ in January/February. I can apply for funding and see how it goes. Can still potentially move to Europe afterwards due to US working holiday agreements ([UK](https://www.bunac.org/usa/intern-abroad/professional/britain/visas)/Ireland). Try to network and get work permit. Can't find a program that fits what I want 90%-100% 
* Travel for a year. Trying to set up an internship abroad for Sept-Nov, travel freely Dec-Feb/Mar. I have no idea what to do from March-June. Probably work over June-Aug. Europe Sept 2019. Three years off is a long time. Worried about losing my mind traveling that long. Bad job prospects from so much time off. I'll be 29 by then :( Have considered studying part-time while travel but worry about logistics.

I'm not getting any younger so the scary options (2 & 3) could be good for self-development and I just want to go. But the responsible, pragmatic side of me is telling me to just go get the degree done and save my money (this is the side I usually listen to). Option 2 seems like the best of both worlds if I could find a solid program to apply to...
",0,statistics,54935,,Take a year off before graduate school?,https://www.reddit.com/r/statistics/comments/8mtwtm/take_a_year_off_before_graduate_school/,all_ads,2018-05-29 02:20:30,4 days 23:15:04.036584000,
"I'm trying to measure how ""exceptional"" a particular observation is based on several attributes of that observation among a population of observations. 

Each observation has several attributes, all numerical quantities but on different scales. To normalize these attributes to be on the same scale, I calculate the Z score of each attribute for each observation. 

Then, I combine the attributes together of one observation using a weighted average. 

For example, if I believe attribute 1 is twice as important as attribute 2, so attribute 1 will have a weight of two and attribute 2 will have a weight of one. I do the same for all observations. 

I call the result of the weighted average an ""observation Z Score."" I interpret an observation with a Z Score of 1 as being more ""exceptional"" than 84% of other observations.

Sanity check questions

 - Is this method ok?
 - Is the method to weight an attribute twice as important with a weight of 2 appropriate for Z Scores?
 - Is my interpretation of observation Z Score appropriate?
 - What should I do if I have binary data (or other data that is not normally distributed)?

Thanks",8,1527528482.0,8moti7,False,"I'm trying to measure how ""exceptional"" a particular observation is based on several attributes of that observation among a population of observations. 

Each observation has several attributes, all numerical quantities but on different scales. To normalize these attributes to be on the same scale, I calculate the Z score of each attribute for each observation. 

Then, I combine the attributes together of one observation using a weighted average. 

For example, if I believe attribute 1 is twice as important as attribute 2, so attribute 1 will have a weight of two and attribute 2 will have a weight of one. I do the same for all observations. 

I call the result of the weighted average an ""observation Z Score."" I interpret an observation with a Z Score of 1 as being more ""exceptional"" than 84% of other observations.

Sanity check questions

 - Is this method ok?
 - Is the method to weight an attribute twice as important with a weight of 2 appropriate for Z Scores?
 - Is my interpretation of observation Z Score appropriate?
 - What should I do if I have binary data (or other data that is not normally distributed)?

Thanks",0,"I'm trying to measure how ""exceptional"" a particular observation is based on several attributes of that observation among a population of observations. 

Each observation has several attributes, all numerical quantities but on different scales. To normalize these attributes to be on the same scale, I calculate the Z score of each attribute for each observation. 

Then, I combine the attributes together of one observation using a weighted average. 

For example, if I believe attribute 1 is twice as important as attribute 2, so attribute 1 will have a weight of two and attribute 2 will have a weight of one. I do the same for all observations. 

I call the result of the weighted average an ""observation Z Score."" I interpret an observation with a Z Score of 1 as being more ""exceptional"" than 84% of other observations.

Sanity check questions

 - Is this method ok?
 - Is the method to weight an attribute twice as important with a weight of 2 appropriate for Z Scores?
 - Is my interpretation of observation Z Score appropriate?
 - What should I do if I have binary data (or other data that is not normally distributed)?

Thanks",6,statistics,54935,,Combining Z Scores by Weighted Average. Sanity Check Please?,https://www.reddit.com/r/statistics/comments/8moti7/combining_z_scores_by_weighted_average_sanity/,all_ads,2018-05-28 13:28:02,5 days 12:07:32.036584000,
"Trying to minor in statistics. I waited too long to try double majoring, but I still want to have a rigorous statistics background to make me more employable.
Anything you guys would recommend?

So far I have applied regression, multivariate analysis, intro math statistics, intro probability, statistical packages (SQL, R, etc.), Intro to data science.

Thanks",14,1527560543.0,8ms6t7,False,"Trying to minor in statistics. I waited too long to try double majoring, but I still want to have a rigorous statistics background to make me more employable.
Anything you guys would recommend?

So far I have applied regression, multivariate analysis, intro math statistics, intro probability, statistical packages (SQL, R, etc.), Intro to data science.

Thanks",0,"Trying to minor in statistics. I waited too long to try double majoring, but I still want to have a rigorous statistics background to make me more employable.
Anything you guys would recommend?

So far I have applied regression, multivariate analysis, intro math statistics, intro probability, statistical packages (SQL, R, etc.), Intro to data science.

Thanks",0,statistics,54935,,Math major looking for useful stat classes,https://www.reddit.com/r/statistics/comments/8ms6t7/math_major_looking_for_useful_stat_classes/,all_ads,2018-05-28 22:22:23,5 days 03:13:11.036584000,
"I was wondering if someone could point me to a good paper or online source that discusses how to carry out regression analyses when the dependent variable is spatial distance, and thus always 0 or greater. When I carry out traditional regression, the best fit relationship can end up predicting negative distances, which violates the real world constraint. For some background, I am analyzing GPS data and trying to test hypotheses about what would / could predict distances between individuals at any given time. I thought of transforming the dependent variable from its raw current state \(a continuous non negative value\) into a binary variable \(e.g. 'A is within 5 meters of B'\), and then carrying binomial regression, treating this binary variable as a count of instances. But it would make me feel better if I didn't throw away the richer information contained in a continuous variable. Any tips very appreciated!",8,1527519209.0,8mo4yu,False,"I was wondering if someone could point me to a good paper or online source that discusses how to carry out regression analyses when the dependent variable is spatial distance, and thus always 0 or greater. When I carry out traditional regression, the best fit relationship can end up predicting negative distances, which violates the real world constraint. For some background, I am analyzing GPS data and trying to test hypotheses about what would / could predict distances between individuals at any given time. I thought of transforming the dependent variable from its raw current state \(a continuous non negative value\) into a binary variable \(e.g. 'A is within 5 meters of B'\), and then carrying binomial regression, treating this binary variable as a count of instances. But it would make me feel better if I didn't throw away the richer information contained in a continuous variable. Any tips very appreciated!",0,"I was wondering if someone could point me to a good paper or online source that discusses how to carry out regression analyses when the dependent variable is spatial distance, and thus always 0 or greater. When I carry out traditional regression, the best fit relationship can end up predicting negative distances, which violates the real world constraint. For some background, I am analyzing GPS data and trying to test hypotheses about what would / could predict distances between individuals at any given time. I thought of transforming the dependent variable from its raw current state \(a continuous non negative value\) into a binary variable \(e.g. 'A is within 5 meters of B'\), and then carrying binomial regression, treating this binary variable as a count of instances. But it would make me feel better if I didn't throw away the richer information contained in a continuous variable. Any tips very appreciated!",6,statistics,54935,,"Regression techniques for when the dependent variable is spatial distance, and thus always >= 0?",https://www.reddit.com/r/statistics/comments/8mo4yu/regression_techniques_for_when_the_dependent/,all_ads,2018-05-28 10:53:29,5 days 14:42:05.036584000,
"I need an explanation. I am running a linear mixed model with repeated measures in SPSS, and one of my covariates is expressed as a percentage. If I run the exact same model with my covariate expressed as a ratio (same data, but divided by 100), my significance level changes (from 0.069 to 0.029), as well the estimates in the fixed effects table.

Why would this occur? Why are percentages and ratios treated differently? How is 55% different from 0.55 when used as a covariate?

This might be a silly question, but I am fairly new to statistics and trying to learn the ins and outs :)

All help greatly appreciated!",0,1527525757.0,8momg9,False,"I need an explanation. I am running a linear mixed model with repeated measures in SPSS, and one of my covariates is expressed as a percentage. If I run the exact same model with my covariate expressed as a ratio (same data, but divided by 100), my significance level changes (from 0.069 to 0.029), as well the estimates in the fixed effects table.

Why would this occur? Why are percentages and ratios treated differently? How is 55% different from 0.55 when used as a covariate?

This might be a silly question, but I am fairly new to statistics and trying to learn the ins and outs :)

All help greatly appreciated!",0,"I need an explanation. I am running a linear mixed model with repeated measures in SPSS, and one of my covariates is expressed as a percentage. If I run the exact same model with my covariate expressed as a ratio (same data, but divided by 100), my significance level changes (from 0.069 to 0.029), as well the estimates in the fixed effects table.

Why would this occur? Why are percentages and ratios treated differently? How is 55% different from 0.55 when used as a covariate?

This might be a silly question, but I am fairly new to statistics and trying to learn the ins and outs :)

All help greatly appreciated!",6,statistics,54935,,Using percentage vs. ratio as a covariate in SPSS (linear mixed models) produces different output.,https://www.reddit.com/r/statistics/comments/8momg9/using_percentage_vs_ratio_as_a_covariate_in_spss/,all_ads,2018-05-28 12:42:37,5 days 12:52:57.036584000,
"This summer I’ve started to learn python on my own, but i was wondering which other programming languages y’all have found useful or helpful in your careers and experience. I’ve also heard sql is one.",21,1527508365.0,8mn7vt,False,"This summer I’ve started to learn python on my own, but i was wondering which other programming languages y’all have found useful or helpful in your careers and experience. I’ve also heard sql is one.",0,"This summer I’ve started to learn python on my own, but i was wondering which other programming languages y’all have found useful or helpful in your careers and experience. I’ve also heard sql is one.",4,statistics,54935,,Recommended Coding Languages,https://www.reddit.com/r/statistics/comments/8mn7vt/recommended_coding_languages/,all_ads,2018-05-28 07:52:45,5 days 17:42:49.036584000,
"Anyone on here who has applied to Biostatistics grad school, I would love any general advice you can give me! I am going into my senior year (majoring in Mathematical Biology), and I am thinking about getting a PhD in Biostats, but I am not sure my application will be competitive enough to apply right out of undergraduate, but tuition costs for MS is hefty. 

Anyone know what aspects of applications are most important? I did SIBS (Summer Institute in Biostats) last summer and loved it, but sometimes I think I should have just stayed at my university to do research that summer and try to get a paper. I am doing an REU this summer though.

If I am worried about my application not being competitive enough for a PhD program, should I apply to MS or just go for PhD anyway? Any advice would be greatly appreciated!",9,1527466484.0,8mird2,False,"Anyone on here who has applied to Biostatistics grad school, I would love any general advice you can give me! I am going into my senior year (majoring in Mathematical Biology), and I am thinking about getting a PhD in Biostats, but I am not sure my application will be competitive enough to apply right out of undergraduate, but tuition costs for MS is hefty. 

Anyone know what aspects of applications are most important? I did SIBS (Summer Institute in Biostats) last summer and loved it, but sometimes I think I should have just stayed at my university to do research that summer and try to get a paper. I am doing an REU this summer though.

If I am worried about my application not being competitive enough for a PhD program, should I apply to MS or just go for PhD anyway? Any advice would be greatly appreciated!",1,"Anyone on here who has applied to Biostatistics grad school, I would love any general advice you can give me! I am going into my senior year (majoring in Mathematical Biology), and I am thinking about getting a PhD in Biostats, but I am not sure my application will be competitive enough to apply right out of undergraduate, but tuition costs for MS is hefty. 

Anyone know what aspects of applications are most important? I did SIBS (Summer Institute in Biostats) last summer and loved it, but sometimes I think I should have just stayed at my university to do research that summer and try to get a paper. I am doing an REU this summer though.

If I am worried about my application not being competitive enough for a PhD program, should I apply to MS or just go for PhD anyway? Any advice would be greatly appreciated!",14,statistics,54935,,Biostatistics PhD Advice,https://www.reddit.com/r/statistics/comments/8mird2/biostatistics_phd_advice/,all_ads,2018-05-27 20:14:44,6 days 05:20:50.036584000,
"Hello!

I love the field of statistics (more applied than theoretical) and want to apply this sexy field to other fields. Within recent years, I've been considering biostatistics. (Is this field getting oversaturated?)

What other fields are statisticians becoming prominent in? I know that statistics can be used everywhere, but- as an undergraduate- I would love to explore applications of statistics before I apply for graduate programs. So, what domains are statistics really being applied to?

Thank you!",9,1527516880.0,8mnybw,False,"Hello!

I love the field of statistics (more applied than theoretical) and want to apply this sexy field to other fields. Within recent years, I've been considering biostatistics. (Is this field getting oversaturated?)

What other fields are statisticians becoming prominent in? I know that statistics can be used everywhere, but- as an undergraduate- I would love to explore applications of statistics before I apply for graduate programs. So, what domains are statistics really being applied to?

Thank you!",0,"Hello!

I love the field of statistics (more applied than theoretical) and want to apply this sexy field to other fields. Within recent years, I've been considering biostatistics. (Is this field getting oversaturated?)

What other fields are statisticians becoming prominent in? I know that statistics can be used everywhere, but- as an undergraduate- I would love to explore applications of statistics before I apply for graduate programs. So, what domains are statistics really being applied to?

Thank you!",1,statistics,54935,,Domains within statistics,https://www.reddit.com/r/statistics/comments/8mnybw/domains_within_statistics/,all_ads,2018-05-28 10:14:40,5 days 15:20:54.036584000,
"Hello,

I was recently accepted to NC State's online masters program for statistics.

While I'm still waiting on a few other decisions, I wanted to be proactive in mapping out a potential course plan during my time in grad school. 

I understand that multivariate analysis is important in upper-level statistics. While I don't see anything explicitly stated, I was wondering if any of these courses cover those topics

Here is a link to the available online courses: https://online.stat.ncsu.edu/online-programs/online-graduate-statistics-courses/#ST517

If I won't receive an adequate education with these offerings alone, please let me know. I'm open to any advice. Thank you!",4,1527490699.0,8mlh8k,False,"Hello,

I was recently accepted to NC State's online masters program for statistics.

While I'm still waiting on a few other decisions, I wanted to be proactive in mapping out a potential course plan during my time in grad school. 

I understand that multivariate analysis is important in upper-level statistics. While I don't see anything explicitly stated, I was wondering if any of these courses cover those topics

Here is a link to the available online courses: https://online.stat.ncsu.edu/online-programs/online-graduate-statistics-courses/#ST517

If I won't receive an adequate education with these offerings alone, please let me know. I'm open to any advice. Thank you!",0,"Hello,

I was recently accepted to NC State's online masters program for statistics.

While I'm still waiting on a few other decisions, I wanted to be proactive in mapping out a potential course plan during my time in grad school. 

I understand that multivariate analysis is important in upper-level statistics. While I don't see anything explicitly stated, I was wondering if any of these courses cover those topics

Here is a link to the available online courses: https://online.stat.ncsu.edu/online-programs/online-graduate-statistics-courses/#ST517

If I won't receive an adequate education with these offerings alone, please let me know. I'm open to any advice. Thank you!",3,statistics,54935,,Choosing Courses - Masters Program,https://www.reddit.com/r/statistics/comments/8mlh8k/choosing_courses_masters_program/,all_ads,2018-05-28 02:58:19,5 days 22:37:15.036584000,
"Hi r/statistics

I'm trying to compare some variables here and need some assistance! 

I have some patients who underwent a procedure, x, via two different methods. The clinical outcome is based on a score, y, which is numerical. The reason for getting the procedure is because of multiple conditions,z resulting in organ failure.

I'm trying to compare the two different methods to see if the degree of clinical outcome is related to the method in which the procedure was done. Initially I attempted to split these into two different groups and compare them with a two tailed t test, and a reviewer suggested that I use a regression model instead. I thought that the T test could be used to compare the two methods and that if i had two numerical variables I could use a regression model.

Then, if I wanted to ensure that none of the conditions resulting in organ failure contributed to the clinical outcome, of the procedure would I just use an ANOVA test to compare them?

Thanks,

Ajose001",18,1527493791.0,8mlsi5,False,"Hi r/statistics

I'm trying to compare some variables here and need some assistance! 

I have some patients who underwent a procedure, x, via two different methods. The clinical outcome is based on a score, y, which is numerical. The reason for getting the procedure is because of multiple conditions,z resulting in organ failure.

I'm trying to compare the two different methods to see if the degree of clinical outcome is related to the method in which the procedure was done. Initially I attempted to split these into two different groups and compare them with a two tailed t test, and a reviewer suggested that I use a regression model instead. I thought that the T test could be used to compare the two methods and that if i had two numerical variables I could use a regression model.

Then, if I wanted to ensure that none of the conditions resulting in organ failure contributed to the clinical outcome, of the procedure would I just use an ANOVA test to compare them?

Thanks,

Ajose001",0,"Hi r/statistics

I'm trying to compare some variables here and need some assistance! 

I have some patients who underwent a procedure, x, via two different methods. The clinical outcome is based on a score, y, which is numerical. The reason for getting the procedure is because of multiple conditions,z resulting in organ failure.

I'm trying to compare the two different methods to see if the degree of clinical outcome is related to the method in which the procedure was done. Initially I attempted to split these into two different groups and compare them with a two tailed t test, and a reviewer suggested that I use a regression model instead. I thought that the T test could be used to compare the two methods and that if i had two numerical variables I could use a regression model.

Then, if I wanted to ensure that none of the conditions resulting in organ failure contributed to the clinical outcome, of the procedure would I just use an ANOVA test to compare them?

Thanks,

Ajose001",3,statistics,54935,,Which method of comparison to use for statistical analysis,https://www.reddit.com/r/statistics/comments/8mlsi5/which_method_of_comparison_to_use_for_statistical/,all_ads,2018-05-28 03:49:51,5 days 21:45:43.036584000,
"I'm pretty novice right now. Think someone who has take intro to statistics and biostatistics but has forgotten most of that. I'm reading a text right now that promises to introduce time series to beginners, but some of the material is too advanced for me. As an example

""We can also conduct a test of whether there is any evidence that the time series data depart from a white noise process."" 

This is taken from really early on in the textbook. I'm not sure exactly what a white noise process is, or what the significance is. Why would I need to know if the time series data depart from a white noise process?

So, any suggestions that could help me? Would be much appreciated. ",12,1527429067.0,8mfy3o,False,"I'm pretty novice right now. Think someone who has take intro to statistics and biostatistics but has forgotten most of that. I'm reading a text right now that promises to introduce time series to beginners, but some of the material is too advanced for me. As an example

""We can also conduct a test of whether there is any evidence that the time series data depart from a white noise process."" 

This is taken from really early on in the textbook. I'm not sure exactly what a white noise process is, or what the significance is. Why would I need to know if the time series data depart from a white noise process?

So, any suggestions that could help me? Would be much appreciated. ",0,"I'm pretty novice right now. Think someone who has take intro to statistics and biostatistics but has forgotten most of that. I'm reading a text right now that promises to introduce time series to beginners, but some of the material is too advanced for me. As an example

""We can also conduct a test of whether there is any evidence that the time series data depart from a white noise process."" 

This is taken from really early on in the textbook. I'm not sure exactly what a white noise process is, or what the significance is. Why would I need to know if the time series data depart from a white noise process?

So, any suggestions that could help me? Would be much appreciated. ",36,statistics,54935,,Recommend me an introductory text for time series,https://www.reddit.com/r/statistics/comments/8mfy3o/recommend_me_an_introductory_text_for_time_series/,all_ads,2018-05-27 09:51:07,6 days 15:44:27.036584000,
I did Wilcoxon signed-rank test in SSPS with a variable testing to a null hypothesis. Now I want to get p-values for each observation (t).. T-statistics for all observations are quite easy to calculate in Excel but how to do it with Wilcoxon signed rank test?,0,1527480290.0,8mkc4n,False,I did Wilcoxon signed-rank test in SSPS with a variable testing to a null hypothesis. Now I want to get p-values for each observation (t).. T-statistics for all observations are quite easy to calculate in Excel but how to do it with Wilcoxon signed rank test?,0,I did Wilcoxon signed-rank test in SSPS with a variable testing to a null hypothesis. Now I want to get p-values for each observation (t).. T-statistics for all observations are quite easy to calculate in Excel but how to do it with Wilcoxon signed rank test?,1,statistics,54935,,Wilcoxon signed-rank test: how to calculate statistics and p-valuee in Excel for each time in the analysis,https://www.reddit.com/r/statistics/comments/8mkc4n/wilcoxon_signedrank_test_how_to_calculate/,all_ads,2018-05-28 00:04:50,6 days 01:30:44.036584000,
"I am trying to wrap my head around the concept of multiple comparisons. 

Assume that I have taken 20 measurements of the skulls of 5 primate species. The measurements might be the length of the skull, the breadth of the skull, the volume, and various lengths of specific morphological features of the skull. Now, I am interested to see if the 5 primate species are different using these measurements. 

So I do a MANOVA where my rows are the observations (let's say I have 20 specimens per species) and the columns are all the variables, so 20 in total. My MANOVA comes out significant. But, I want to do a post-hoc test, so I do a bunch of different pairwise comparisons between the species. So a MANOVA between species A and species B, then between species A and C, and so forth, to find out where the differences lie.  

Is that multiple comparisons? For some reason I imagine multiple comparisons would be taking species A and B, and then doing species(A,B) ~ variable A + variable B, species(A,B) ~ variable A + variable C, and so forth. Maybe they are both multiple comparisons, but the latter is just more specific while the former is more general? 

Edit: maybe a better example would have been all species ~ variable A + variable B, all species ~ variable A + variable C, etc.",3,1527476396.0,8mjw7r,False,"I am trying to wrap my head around the concept of multiple comparisons. 

Assume that I have taken 20 measurements of the skulls of 5 primate species. The measurements might be the length of the skull, the breadth of the skull, the volume, and various lengths of specific morphological features of the skull. Now, I am interested to see if the 5 primate species are different using these measurements. 

So I do a MANOVA where my rows are the observations (let's say I have 20 specimens per species) and the columns are all the variables, so 20 in total. My MANOVA comes out significant. But, I want to do a post-hoc test, so I do a bunch of different pairwise comparisons between the species. So a MANOVA between species A and species B, then between species A and C, and so forth, to find out where the differences lie.  

Is that multiple comparisons? For some reason I imagine multiple comparisons would be taking species A and B, and then doing species(A,B) ~ variable A + variable B, species(A,B) ~ variable A + variable C, and so forth. Maybe they are both multiple comparisons, but the latter is just more specific while the former is more general? 

Edit: maybe a better example would have been all species ~ variable A + variable B, all species ~ variable A + variable C, etc.",0,"I am trying to wrap my head around the concept of multiple comparisons. 

Assume that I have taken 20 measurements of the skulls of 5 primate species. The measurements might be the length of the skull, the breadth of the skull, the volume, and various lengths of specific morphological features of the skull. Now, I am interested to see if the 5 primate species are different using these measurements. 

So I do a MANOVA where my rows are the observations (let's say I have 20 specimens per species) and the columns are all the variables, so 20 in total. My MANOVA comes out significant. But, I want to do a post-hoc test, so I do a bunch of different pairwise comparisons between the species. So a MANOVA between species A and species B, then between species A and C, and so forth, to find out where the differences lie.  

Is that multiple comparisons? For some reason I imagine multiple comparisons would be taking species A and B, and then doing species(A,B) ~ variable A + variable B, species(A,B) ~ variable A + variable C, and so forth. Maybe they are both multiple comparisons, but the latter is just more specific while the former is more general? 

Edit: maybe a better example would have been all species ~ variable A + variable B, all species ~ variable A + variable C, etc.",1,statistics,54935,,Would that be multiple comparisons or something else altogether?,https://www.reddit.com/r/statistics/comments/8mjw7r/would_that_be_multiple_comparisons_or_something/,all_ads,2018-05-27 22:59:56,6 days 02:35:38.036584000,
"Was watching a game today, and the announcer said statistically the #2 hitter in the lineup is more likely to hit with a runner on base than the #3 hitter in the lineup.

I can't wrap my head around how this is possible. If both hitters have a 1/3 chance of getting on base, shouldn't having two cracks at a 1/3 chance be more likely to result in a baserunner than one attempt?

Is there some statistical calculation I am missing? Any help would be appreciated.

Thanks",12,1527406917.0,8me21y,False,"Was watching a game today, and the announcer said statistically the #2 hitter in the lineup is more likely to hit with a runner on base than the #3 hitter in the lineup.

I can't wrap my head around how this is possible. If both hitters have a 1/3 chance of getting on base, shouldn't having two cracks at a 1/3 chance be more likely to result in a baserunner than one attempt?

Is there some statistical calculation I am missing? Any help would be appreciated.

Thanks",0,"Was watching a game today, and the announcer said statistically the #2 hitter in the lineup is more likely to hit with a runner on base than the #3 hitter in the lineup.

I can't wrap my head around how this is possible. If both hitters have a 1/3 chance of getting on base, shouldn't having two cracks at a 1/3 chance be more likely to result in a baserunner than one attempt?

Is there some statistical calculation I am missing? Any help would be appreciated.

Thanks",9,statistics,54935,,Baseball batting order question,https://www.reddit.com/r/statistics/comments/8me21y/baseball_batting_order_question/,all_ads,2018-05-27 03:41:57,6 days 21:53:37.036584000,
"Hi all. I'm extey rusty on my statistics at the moment but a friend asked me a question a week back that is eating me up.

If I have a series 1, 2, 4, 10. The mean would be 17/4.
What happens if this is a timeseries?
2001 1
2002 2
2003 4
2004 10
- Is the mean still 17/4? 
- Do I fit a line of best fit and that represents the mean? *
- Should I think of it as I kind of weighted average, where the weight of each date is up to me

* My reasoning: I remember when check if mean of a time series in constant, we make a line of best fit and see if a trend exists or not.",10,1527414020.0,8meprv,False,"Hi all. I'm extey rusty on my statistics at the moment but a friend asked me a question a week back that is eating me up.

If I have a series 1, 2, 4, 10. The mean would be 17/4.
What happens if this is a timeseries?
2001 1
2002 2
2003 4
2004 10
- Is the mean still 17/4? 
- Do I fit a line of best fit and that represents the mean? *
- Should I think of it as I kind of weighted average, where the weight of each date is up to me

* My reasoning: I remember when check if mean of a time series in constant, we make a line of best fit and see if a trend exists or not.",0,"Hi all. I'm extey rusty on my statistics at the moment but a friend asked me a question a week back that is eating me up.

If I have a series 1, 2, 4, 10. The mean would be 17/4.
What happens if this is a timeseries?
2001 1
2002 2
2003 4
2004 10
- Is the mean still 17/4? 
- Do I fit a line of best fit and that represents the mean? *
- Should I think of it as I kind of weighted average, where the weight of each date is up to me

* My reasoning: I remember when check if mean of a time series in constant, we make a line of best fit and see if a trend exists or not.",2,statistics,54935,,How do you calculate the expected value of a time series,https://www.reddit.com/r/statistics/comments/8meprv/how_do_you_calculate_the_expected_value_of_a_time/,all_ads,2018-05-27 05:40:20,6 days 19:55:14.036584000,
"I get this from time to time when modelling GARCH\-models. What does it in theory mean, and how do I get around it without shrinking my sample size?",4,1527406708.0,8me1dg,False,"I get this from time to time when modelling GARCH\-models. What does it in theory mean, and how do I get around it without shrinking my sample size?",0,"I get this from time to time when modelling GARCH\-models. What does it in theory mean, and how do I get around it without shrinking my sample size?",4,statistics,54935,,"What does ""There was no convergence"" mean when modelling in statistic programs?",https://www.reddit.com/r/statistics/comments/8me1dg/what_does_there_was_no_convergence_mean_when/,all_ads,2018-05-27 03:38:28,6 days 21:57:06.036584000,
"Hello, I work for a company where we bid out projects but do our own internal estimates first to roughly gauge the costs.  Lately we’ve been finding our internal estimates are way off from the bidders and I’ve been tasked to help understand why this is.  

However, I’m unsure of how to go about this and we only have a history of roughly 20-30 projects.  Can you please give some advice on a) whether this is a feasible effort given the small population size and b) if it is possible, how best to go about determining how and where our estimates are off (we already know that our management and quality control estimates differ wildly from the bidders‘ proposals) 

Update/more information: The size of the projects vary - some are just upgrading small hardware components of a computer system that will only take a few months to complete but others may include construction or replacement of hardware and software components that are expected to last 1-2 years.  The budgets typically include labor costs, equipment but also project management costs, testing costs and facility charges.  ",10,1527380710.0,8mb8a5,False,"Hello, I work for a company where we bid out projects but do our own internal estimates first to roughly gauge the costs.  Lately we’ve been finding our internal estimates are way off from the bidders and I’ve been tasked to help understand why this is.  

However, I’m unsure of how to go about this and we only have a history of roughly 20-30 projects.  Can you please give some advice on a) whether this is a feasible effort given the small population size and b) if it is possible, how best to go about determining how and where our estimates are off (we already know that our management and quality control estimates differ wildly from the bidders‘ proposals) 

Update/more information: The size of the projects vary - some are just upgrading small hardware components of a computer system that will only take a few months to complete but others may include construction or replacement of hardware and software components that are expected to last 1-2 years.  The budgets typically include labor costs, equipment but also project management costs, testing costs and facility charges.  ",0,"Hello, I work for a company where we bid out projects but do our own internal estimates first to roughly gauge the costs.  Lately we’ve been finding our internal estimates are way off from the bidders and I’ve been tasked to help understand why this is.  

However, I’m unsure of how to go about this and we only have a history of roughly 20-30 projects.  Can you please give some advice on a) whether this is a feasible effort given the small population size and b) if it is possible, how best to go about determining how and where our estimates are off (we already know that our management and quality control estimates differ wildly from the bidders‘ proposals) 

Update/more information: The size of the projects vary - some are just upgrading small hardware components of a computer system that will only take a few months to complete but others may include construction or replacement of hardware and software components that are expected to last 1-2 years.  The budgets typically include labor costs, equipment but also project management costs, testing costs and facility charges.  ",11,statistics,54935,,Can you use statistics to forecast budgets?,https://www.reddit.com/r/statistics/comments/8mb8a5/can_you_use_statistics_to_forecast_budgets/,all_ads,2018-05-26 20:25:10,7 days 05:10:24.036584000,
"Obviosly i wont win 100% or even 20% but if i could win maybe even 10%-30% of the time then that could be great. Does anybody know a good statistical method that can help. I've tried using fibbonacci retracement, predictuve probability and so on and so forth, but to no avail. I realize that the lottery is totally random but I have seen some people who regularly win the pick 3 and 4 perhaps at least 2 times a month. So there is a method to it. I'm not a gambler and do not plan to just get random numbers from the computer. Does anybody have a quick tip to help me out here??",11,1527398467.0,8md6k6,False,"Obviosly i wont win 100% or even 20% but if i could win maybe even 10%-30% of the time then that could be great. Does anybody know a good statistical method that can help. I've tried using fibbonacci retracement, predictuve probability and so on and so forth, but to no avail. I realize that the lottery is totally random but I have seen some people who regularly win the pick 3 and 4 perhaps at least 2 times a month. So there is a method to it. I'm not a gambler and do not plan to just get random numbers from the computer. Does anybody have a quick tip to help me out here??",0,"Obviosly i wont win 100% or even 20% but if i could win maybe even 10%-30% of the time then that could be great. Does anybody know a good statistical method that can help. I've tried using fibbonacci retracement, predictuve probability and so on and so forth, but to no avail. I realize that the lottery is totally random but I have seen some people who regularly win the pick 3 and 4 perhaps at least 2 times a month. So there is a method to it. I'm not a gambler and do not plan to just get random numbers from the computer. Does anybody have a quick tip to help me out here??",0,statistics,54935,,I live in Ohio and was wondering what is the closest way of winning pick 3 and pick 4 using statistical methods,https://www.reddit.com/r/statistics/comments/8md6k6/i_live_in_ohio_and_was_wondering_what_is_the/,all_ads,2018-05-27 01:21:07,7 days 00:14:27.036584000,
" Hi everyone! I am currently a junior college student with a major in Statistics. Right after graduation, I consider going on to earn a Master in Statistics. Eventually, I want to work as a Biostatistician, hopefully dealing with genomic data analysis. Currently I don't have the plan to go to academia and I don't feel like I can commit to a PhD. I want to stop at the Masters and get industry experience

Having seen quite a few job postings, I only see 1 or 2 say it strictly requires PhD. I wonder if there are some reasons  that one should earn a Phd to work as a Biostatistician. Please advise me. Any input is welcomed. Thank you so much",31,1527318080.0,8m5wju,False," Hi everyone! I am currently a junior college student with a major in Statistics. Right after graduation, I consider going on to earn a Master in Statistics. Eventually, I want to work as a Biostatistician, hopefully dealing with genomic data analysis. Currently I don't have the plan to go to academia and I don't feel like I can commit to a PhD. I want to stop at the Masters and get industry experience

Having seen quite a few job postings, I only see 1 or 2 say it strictly requires PhD. I wonder if there are some reasons  that one should earn a Phd to work as a Biostatistician. Please advise me. Any input is welcomed. Thank you so much",0," Hi everyone! I am currently a junior college student with a major in Statistics. Right after graduation, I consider going on to earn a Master in Statistics. Eventually, I want to work as a Biostatistician, hopefully dealing with genomic data analysis. Currently I don't have the plan to go to academia and I don't feel like I can commit to a PhD. I want to stop at the Masters and get industry experience

Having seen quite a few job postings, I only see 1 or 2 say it strictly requires PhD. I wonder if there are some reasons  that one should earn a Phd to work as a Biostatistician. Please advise me. Any input is welcomed. Thank you so much",35,statistics,54935,,Is a Masters in Statistics enough to become a Biostatistician?,https://www.reddit.com/r/statistics/comments/8m5wju/is_a_masters_in_statistics_enough_to_become_a/,all_ads,2018-05-26 03:01:20,7 days 22:34:14.036584000,
"Hello.

I need some help on how to compare 3 sample proportions in R program.

Here is the question:

>At a significance level of 1&#37;, test whether there  
is evidence for a difference in the proportion of failed  
drives between the three drive models.

Here is the data set:

I have 3 hard drive models and I have to use the categorical variable ""failed"" to compare.

Data set

this is the first solution I found: using prop.test

    # 7 out of 18 SEAGATE drives failed # 3 out of 8 HITACHI drives failed # 15 out of 167 WDC drives failed  total <- c(18, 167, 8) failed  <- c( 7,15, 3) prop.test(failed,total) 

Second solution: use chisq.test function

>tbl = table\(ThreeModel\_Combined$model , ThreeModel\_Combined$failed\)  
chisq.test\(table\(tbl\)\)

They give different p\-value so maybe one of them is correct?

thank you.",3,1527361331.0,8m9g76,False,"Hello.

I need some help on how to compare 3 sample proportions in R program.

Here is the question:

>At a significance level of 1&#37;, test whether there  
is evidence for a difference in the proportion of failed  
drives between the three drive models.

Here is the data set:

I have 3 hard drive models and I have to use the categorical variable ""failed"" to compare.

Data set

this is the first solution I found: using prop.test

    # 7 out of 18 SEAGATE drives failed # 3 out of 8 HITACHI drives failed # 15 out of 167 WDC drives failed  total <- c(18, 167, 8) failed  <- c( 7,15, 3) prop.test(failed,total) 

Second solution: use chisq.test function

>tbl = table\(ThreeModel\_Combined$model , ThreeModel\_Combined$failed\)  
chisq.test\(table\(tbl\)\)

They give different p\-value so maybe one of them is correct?

thank you.",0,"Hello.

I need some help on how to compare 3 sample proportions in R program.

Here is the question:

>At a significance level of 1&#37;, test whether there  
is evidence for a difference in the proportion of failed  
drives between the three drive models.

Here is the data set:

I have 3 hard drive models and I have to use the categorical variable ""failed"" to compare.

Data set

this is the first solution I found: using prop.test

    # 7 out of 18 SEAGATE drives failed # 3 out of 8 HITACHI drives failed # 15 out of 167 WDC drives failed  total <- c(18, 167, 8) failed  <- c( 7,15, 3) prop.test(failed,total) 

Second solution: use chisq.test function

>tbl = table\(ThreeModel\_Combined$model , ThreeModel\_Combined$failed\)  
chisq.test\(table\(tbl\)\)

They give different p\-value so maybe one of them is correct?

thank you.",0,statistics,54935,,Is this the correct way to compare 3 sample proportions using Chi-square test?,https://www.reddit.com/r/statistics/comments/8m9g76/is_this_the_correct_way_to_compare_3_sample/,all_ads,2018-05-26 15:02:11,7 days 10:33:23.036584000,
"Hey all,

I just graduated from college and I'm hoping to get a masters in biostatistics in a year or 2, but my statistical knowledge is only decent. I minored in applied statistics and took an applied econometrics course in college so my knowledge of R, Stata and SAS is solid, but my knowledge of the theory is only decent and my knowledge of a lot of the math is subpar at best. I was thinking about brushing up on this before I start a program and teaching myself some more probability and statistical inference over the next year while I work.

Does anyone think this is worthwhile for an MS program? I'm starting to read Casella and Berger's Statistical Inference textbook, and would be open to suggestions about other good textbooks.

How much math should I brush up on and is learning more in depth statistical theory going to be useful in the long run?

Thanks.",23,1527295328.0,8m36ws,False,"Hey all,

I just graduated from college and I'm hoping to get a masters in biostatistics in a year or 2, but my statistical knowledge is only decent. I minored in applied statistics and took an applied econometrics course in college so my knowledge of R, Stata and SAS is solid, but my knowledge of the theory is only decent and my knowledge of a lot of the math is subpar at best. I was thinking about brushing up on this before I start a program and teaching myself some more probability and statistical inference over the next year while I work.

Does anyone think this is worthwhile for an MS program? I'm starting to read Casella and Berger's Statistical Inference textbook, and would be open to suggestions about other good textbooks.

How much math should I brush up on and is learning more in depth statistical theory going to be useful in the long run?

Thanks.",0,"Hey all,

I just graduated from college and I'm hoping to get a masters in biostatistics in a year or 2, but my statistical knowledge is only decent. I minored in applied statistics and took an applied econometrics course in college so my knowledge of R, Stata and SAS is solid, but my knowledge of the theory is only decent and my knowledge of a lot of the math is subpar at best. I was thinking about brushing up on this before I start a program and teaching myself some more probability and statistical inference over the next year while I work.

Does anyone think this is worthwhile for an MS program? I'm starting to read Casella and Berger's Statistical Inference textbook, and would be open to suggestions about other good textbooks.

How much math should I brush up on and is learning more in depth statistical theory going to be useful in the long run?

Thanks.",9,statistics,54935,,Thinking about applying to biostatistics graduate programs,https://www.reddit.com/r/statistics/comments/8m36ws/thinking_about_applying_to_biostatistics_graduate/,all_ads,2018-05-25 20:42:08,8 days 04:53:26.036584000,
"I have this table: https://imgur.com/a/Pptmgl8 Sheet link: https://drive.google.com/open?id=1l0h0st-6iisYgZLwT9hrbbScRXTVsQ7e

Each ID represents a user which participates in events, and in each event, they score a rating.

There's no way to score a zero in an event, so all the zeros are events which that specific ID did not participate.

The medium rating is calculating the arithmetic medium of the ratings of the events which that user participated.

The problem is that the medium rating is not properly ranking the users, since if a user manage to score a high rating in few events, he can simply stop attending to other events and stand on top of the ranking.

Are there any statistics calculus which takes into account the number of events attended (the sample size)?

Ps.: Adding the unattended events as ZEROS to all the mediums would cause another undesirable effect, as I don't want that the users that can attend to all events have an incontrovertible advantage over the users that score high ratings on all the events they attend to, even though they missed three or four events.",5,1527298249.0,8m3kfm,False,"I have this table: https://imgur.com/a/Pptmgl8 Sheet link: https://drive.google.com/open?id=1l0h0st-6iisYgZLwT9hrbbScRXTVsQ7e

Each ID represents a user which participates in events, and in each event, they score a rating.

There's no way to score a zero in an event, so all the zeros are events which that specific ID did not participate.

The medium rating is calculating the arithmetic medium of the ratings of the events which that user participated.

The problem is that the medium rating is not properly ranking the users, since if a user manage to score a high rating in few events, he can simply stop attending to other events and stand on top of the ranking.

Are there any statistics calculus which takes into account the number of events attended (the sample size)?

Ps.: Adding the unattended events as ZEROS to all the mediums would cause another undesirable effect, as I don't want that the users that can attend to all events have an incontrovertible advantage over the users that score high ratings on all the events they attend to, even though they missed three or four events.",0,"I have this table: https://imgur.com/a/Pptmgl8 Sheet link: https://drive.google.com/open?id=1l0h0st-6iisYgZLwT9hrbbScRXTVsQ7e

Each ID represents a user which participates in events, and in each event, they score a rating.

There's no way to score a zero in an event, so all the zeros are events which that specific ID did not participate.

The medium rating is calculating the arithmetic medium of the ratings of the events which that user participated.

The problem is that the medium rating is not properly ranking the users, since if a user manage to score a high rating in few events, he can simply stop attending to other events and stand on top of the ranking.

Are there any statistics calculus which takes into account the number of events attended (the sample size)?

Ps.: Adding the unattended events as ZEROS to all the mediums would cause another undesirable effect, as I don't want that the users that can attend to all events have an incontrovertible advantage over the users that score high ratings on all the events they attend to, even though they missed three or four events.",8,statistics,54935,,Ranking users according to RATINGS and FREQUENCY,https://www.reddit.com/r/statistics/comments/8m3kfm/ranking_users_according_to_ratings_and_frequency/,all_ads,2018-05-25 21:30:49,8 days 04:04:45.036584000,
Is there any intuitive and easy to read way to present these?,19,1527269323.0,8m0f8r,False,Is there any intuitive and easy to read way to present these?,0,Is there any intuitive and easy to read way to present these?,26,statistics,54935,,Best way to visually present a multiple regression results?,https://www.reddit.com/r/statistics/comments/8m0f8r/best_way_to_visually_present_a_multiple/,all_ads,2018-05-25 13:28:43,8 days 12:06:51.036584000,
"I'm starting a part time degree in October and I'm taking Computing & IT with a second subject of statistics. I know some basics, like the application of linear regression and polynomial regression and how to get it visualised in python. But I don't really know much statistical theory\(Or understand much of the maths behind it\), it's of course very relevant to what I want to do and would like to read a good book or course to get familiar with statistics before the uni module starts.",15,1527325549.0,8m6nr2,False,"I'm starting a part time degree in October and I'm taking Computing & IT with a second subject of statistics. I know some basics, like the application of linear regression and polynomial regression and how to get it visualised in python. But I don't really know much statistical theory\(Or understand much of the maths behind it\), it's of course very relevant to what I want to do and would like to read a good book or course to get familiar with statistics before the uni module starts.",0,"I'm starting a part time degree in October and I'm taking Computing & IT with a second subject of statistics. I know some basics, like the application of linear regression and polynomial regression and how to get it visualised in python. But I don't really know much statistical theory\(Or understand much of the maths behind it\), it's of course very relevant to what I want to do and would like to read a good book or course to get familiar with statistics before the uni module starts.",0,statistics,54935,,Good book or online course to get me started with statistics? Aspiring Data Scientist,https://www.reddit.com/r/statistics/comments/8m6nr2/good_book_or_online_course_to_get_me_started_with/,all_ads,2018-05-26 05:05:49,7 days 20:29:45.036584000,
"I have an assembly with 5 screws on it. If 1% of screws break, how many assemblies are scrapped?",3,1527294163.0,8m31g0,False,"I have an assembly with 5 screws on it. If 1% of screws break, how many assemblies are scrapped?",0,"I have an assembly with 5 screws on it. If 1% of screws break, how many assemblies are scrapped?",2,statistics,54935,,Simple Failure Rate Problem,https://www.reddit.com/r/statistics/comments/8m31g0/simple_failure_rate_problem/,all_ads,2018-05-25 20:22:43,8 days 05:12:51.036584000,
"The classic case of representing an interaction is with one continuous variable and one grouping variable. For example consider a graph of age vs income, broken up by sex. An interaction between sex and income would show up as different slopes. 

What about when both of your variables are continuous? Say, age and weight vs income? If there was an interaction, how could you demonstrate it?

(I'm actually plotting the effect of gene expression + drug sensitivity on a treatment outcome. All these are continuous.)",2,1527279045.0,8m18we,False,"The classic case of representing an interaction is with one continuous variable and one grouping variable. For example consider a graph of age vs income, broken up by sex. An interaction between sex and income would show up as different slopes. 

What about when both of your variables are continuous? Say, age and weight vs income? If there was an interaction, how could you demonstrate it?

(I'm actually plotting the effect of gene expression + drug sensitivity on a treatment outcome. All these are continuous.)",0,"The classic case of representing an interaction is with one continuous variable and one grouping variable. For example consider a graph of age vs income, broken up by sex. An interaction between sex and income would show up as different slopes. 

What about when both of your variables are continuous? Say, age and weight vs income? If there was an interaction, how could you demonstrate it?

(I'm actually plotting the effect of gene expression + drug sensitivity on a treatment outcome. All these are continuous.)",4,statistics,54935,,Visually representing an interaction term when both main effects are continuous?,https://www.reddit.com/r/statistics/comments/8m18we/visually_representing_an_interaction_term_when/,all_ads,2018-05-25 16:10:45,8 days 09:24:49.036584000,
"Hey everyone. 

I am a bit confused and I need some help. My work has around 200K clients classified as A. This is no statistical classification, it is based on business needs \(if client = A then A\). What I want to do is take a random sample of those 200K clients and check if there are indeed meeting the criteria A. 

Any thoughts? The client is either A or not. 

Thanks in advance.",8,1527275864.0,8m0yd0,False,"Hey everyone. 

I am a bit confused and I need some help. My work has around 200K clients classified as A. This is no statistical classification, it is based on business needs \(if client = A then A\). What I want to do is take a random sample of those 200K clients and check if there are indeed meeting the criteria A. 

Any thoughts? The client is either A or not. 

Thanks in advance.",0,"Hey everyone. 

I am a bit confused and I need some help. My work has around 200K clients classified as A. This is no statistical classification, it is based on business needs \(if client = A then A\). What I want to do is take a random sample of those 200K clients and check if there are indeed meeting the criteria A. 

Any thoughts? The client is either A or not. 

Thanks in advance.",4,statistics,54935,,Sample Size Calculation,https://www.reddit.com/r/statistics/comments/8m0yd0/sample_size_calculation/,all_ads,2018-05-25 15:17:44,8 days 10:17:50.036584000,
"Hi Everyone. This is my first time using this delightful looking sub. My question is— i want to create a shorter version of a longer test (100 Likert scale questions). My thought is to do a factor analysis and pick specific questions that load heavily on those factors, thus approximating the original 100 as best as possible. Is there a more elegant way to do this? Thanks in advance!",19,1527247643.0,8lyoed,False,"Hi Everyone. This is my first time using this delightful looking sub. My question is— i want to create a shorter version of a longer test (100 Likert scale questions). My thought is to do a factor analysis and pick specific questions that load heavily on those factors, thus approximating the original 100 as best as possible. Is there a more elegant way to do this? Thanks in advance!",0,"Hi Everyone. This is my first time using this delightful looking sub. My question is— i want to create a shorter version of a longer test (100 Likert scale questions). My thought is to do a factor analysis and pick specific questions that load heavily on those factors, thus approximating the original 100 as best as possible. Is there a more elegant way to do this? Thanks in advance!",11,statistics,54935,,Alternative approach other than factor analysis?,https://www.reddit.com/r/statistics/comments/8lyoed/alternative_approach_other_than_factor_analysis/,all_ads,2018-05-25 07:27:23,8 days 18:08:11.036584000,
"Hello statistics,

I did a linear regression and got the following [result](https://i.imgur.com/k2SOTzG.png). The p-value is 0.216 and the  regression coefficient is negative. I dont how i can interpret this. Can someone help me? ",3,1527309994.0,8m4zwz,False,"Hello statistics,

I did a linear regression and got the following [result](https://i.imgur.com/k2SOTzG.png). The p-value is 0.216 and the  regression coefficient is negative. I dont how i can interpret this. Can someone help me? ",0,"Hello statistics,

I did a linear regression and got the following [result](https://i.imgur.com/k2SOTzG.png). The p-value is 0.216 and the  regression coefficient is negative. I dont how i can interpret this. Can someone help me? ",0,statistics,54935,,Strange result: Need help interpreting it,https://www.reddit.com/r/statistics/comments/8m4zwz/strange_result_need_help_interpreting_it/,all_ads,2018-05-26 00:46:34,8 days 00:49:00.036584000,
"**View Full project(code+output) on Kaggle**

___

A Data Wrangling example for Twitter @dog_rates(Beginner's Guide)

URL: https://www.kaggle.com/sgonkaggle/data-wrangling-example-twitter-handle-dog-rates/
",0,1527267716.0,8m0axg,False,"**View Full project(code+output) on Kaggle**

___

A Data Wrangling example for Twitter @dog_rates(Beginner's Guide)

URL: https://www.kaggle.com/sgonkaggle/data-wrangling-example-twitter-handle-dog-rates/
",0,"**View Full project(code+output) on Kaggle**

___

A Data Wrangling example for Twitter @dog_rates(Beginner's Guide)

URL: https://www.kaggle.com/sgonkaggle/data-wrangling-example-twitter-handle-dog-rates/
",2,statistics,54935,,[Kaggle] Data Wrangling example for Twitter handle dog_rates,https://www.reddit.com/r/statistics/comments/8m0axg/kaggle_data_wrangling_example_for_twitter_handle/,all_ads,2018-05-25 13:01:56,8 days 12:33:38.036584000,
"I apologize in advance if the question is fairly basic: I come from a psych. background and am having some difficulty parsing the literature to find a definite answer on this.

1) I used ICC(2, 1) to calculate the interrater reliability of a binary (0 = do not apply, 1 = apply) dataset, with 3 raters. Is this right?

2) Could I also calculate Fleiss' kappa for this - or does this not provide any additional information?",3,1527268613.0,8m0dg3,False,"I apologize in advance if the question is fairly basic: I come from a psych. background and am having some difficulty parsing the literature to find a definite answer on this.

1) I used ICC(2, 1) to calculate the interrater reliability of a binary (0 = do not apply, 1 = apply) dataset, with 3 raters. Is this right?

2) Could I also calculate Fleiss' kappa for this - or does this not provide any additional information?",0,"I apologize in advance if the question is fairly basic: I come from a psych. background and am having some difficulty parsing the literature to find a definite answer on this.

1) I used ICC(2, 1) to calculate the interrater reliability of a binary (0 = do not apply, 1 = apply) dataset, with 3 raters. Is this right?

2) Could I also calculate Fleiss' kappa for this - or does this not provide any additional information?",1,statistics,54935,,"Interrater reliability for more than 2 raters: Fleiss' or ICC(2, 1)?",https://www.reddit.com/r/statistics/comments/8m0dg3/interrater_reliability_for_more_than_2_raters/,all_ads,2018-05-25 13:16:53,8 days 12:18:41.036584000,
"Looking for a good text book on causal inference.  Would like something easy to read, but sufficiently technical / not too fluffy.  Anyone have a good recommendation?  Thanks in advance!",17,1527208554.0,8lu1sr,False,"Looking for a good text book on causal inference.  Would like something easy to read, but sufficiently technical / not too fluffy.  Anyone have a good recommendation?  Thanks in advance!",0,"Looking for a good text book on causal inference.  Would like something easy to read, but sufficiently technical / not too fluffy.  Anyone have a good recommendation?  Thanks in advance!",15,statistics,54935,,Causal inference -- book recommendations?,https://www.reddit.com/r/statistics/comments/8lu1sr/causal_inference_book_recommendations/,all_ads,2018-05-24 20:35:54,9 days 04:59:40.036584000,
Given a dependent variable with a single continuous independent variable what is the relationship between the residual values of datapoints (e.g. taking the mean or sum of the absolute value of the residuals) and the r2 of the model?,8,1527220930.0,8lvolc,False,Given a dependent variable with a single continuous independent variable what is the relationship between the residual values of datapoints (e.g. taking the mean or sum of the absolute value of the residuals) and the r2 of the model?,0,Given a dependent variable with a single continuous independent variable what is the relationship between the residual values of datapoints (e.g. taking the mean or sum of the absolute value of the residuals) and the r2 of the model?,6,statistics,54935,,"In a linear regression model with one predictor, how does the r2 relate to the residuals?",https://www.reddit.com/r/statistics/comments/8lvolc/in_a_linear_regression_model_with_one_predictor/,all_ads,2018-05-25 00:02:10,9 days 01:33:24.036584000,
,3,1527208472.0,8lu1er,False,,0,,13,statistics,54935,,How to test robustness when the statistic test itself is Wilcoxon signed-rank test? I’m conducting an event study,https://www.reddit.com/r/statistics/comments/8lu1er/how_to_test_robustness_when_the_statistic_test/,all_ads,2018-05-24 20:34:32,9 days 05:01:02.036584000,
"I've been trying for some time to wrap my mind around PCA but some of the theory still makes my brain hurt. I'm writing my thesis on groundwater quality and am using PCA to analyze the chemistry of the samples. The output is shown below. The main thing I'm confused about is the negative loadings for all of the factors in PC1. I took it to mean that the first PC relates to a relatively fresh source of groundwater recharge \(which makes perfect sense considering the study area\), but my advisor seems to think that the loadings for all of the different PCs points to evaporative concentration of irrigation water. He also said that ""if all of the loadings are negative, then they might as well all be positive."" Both of us are hydrologists, not really expert statisticians, so I would love to hear input and interpretations from some of you on this sub. Thank you!

  
PC1            PC2                PC3                PC4              PC5              PC6

Ca   \-0.2918901  0.69208540 \-0.38089672  0.04589718 \-0.21405776  0.4291246

Mg   \-0.4419762  0.19639048 \-0.09656106  0.18483003 \-0.04449990 \-0.2959924

Na   \-0.4005622 \-0.40807415  0.21953091  0.01287829  0.13678670  0.7548061

K    \-0.3748620 \-0.23117606 \-0.30088689 \-0.81056739 \-0.14698013 \-0.1773789

HCO3 \-0.2573508  0.40890119  0.81096209 \-0.27076444  0.06163522 \-0.1261359

SO4  \-0.4311025 \-0.04487235 \-0.18515883  0.23435266  0.73191047 \-0.2460743

Cl   \-0.4079300 \-0.30539808  0.12216218  0.42226935 \-0.61024048 \-0.2249048

PC1       PC2        PC3         PC4          PC5         PC6    

Standard deviation     2.1734   0.9870   0.8709   0.56584   0.35954   0.26489 

Proportion of Variance 0.6748   0.1391   0.1084   0.04574   0.01847   0.01002 

Cumulative Proportion  0.6748   0.8140   0.9223   0.96806   0.98653   0.99655  ",3,1527221991.0,8lvtmc,False,"I've been trying for some time to wrap my mind around PCA but some of the theory still makes my brain hurt. I'm writing my thesis on groundwater quality and am using PCA to analyze the chemistry of the samples. The output is shown below. The main thing I'm confused about is the negative loadings for all of the factors in PC1. I took it to mean that the first PC relates to a relatively fresh source of groundwater recharge \(which makes perfect sense considering the study area\), but my advisor seems to think that the loadings for all of the different PCs points to evaporative concentration of irrigation water. He also said that ""if all of the loadings are negative, then they might as well all be positive."" Both of us are hydrologists, not really expert statisticians, so I would love to hear input and interpretations from some of you on this sub. Thank you!

  
PC1            PC2                PC3                PC4              PC5              PC6

Ca   \-0.2918901  0.69208540 \-0.38089672  0.04589718 \-0.21405776  0.4291246

Mg   \-0.4419762  0.19639048 \-0.09656106  0.18483003 \-0.04449990 \-0.2959924

Na   \-0.4005622 \-0.40807415  0.21953091  0.01287829  0.13678670  0.7548061

K    \-0.3748620 \-0.23117606 \-0.30088689 \-0.81056739 \-0.14698013 \-0.1773789

HCO3 \-0.2573508  0.40890119  0.81096209 \-0.27076444  0.06163522 \-0.1261359

SO4  \-0.4311025 \-0.04487235 \-0.18515883  0.23435266  0.73191047 \-0.2460743

Cl   \-0.4079300 \-0.30539808  0.12216218  0.42226935 \-0.61024048 \-0.2249048

PC1       PC2        PC3         PC4          PC5         PC6    

Standard deviation     2.1734   0.9870   0.8709   0.56584   0.35954   0.26489 

Proportion of Variance 0.6748   0.1391   0.1084   0.04574   0.01847   0.01002 

Cumulative Proportion  0.6748   0.8140   0.9223   0.96806   0.98653   0.99655  ",0,"I've been trying for some time to wrap my mind around PCA but some of the theory still makes my brain hurt. I'm writing my thesis on groundwater quality and am using PCA to analyze the chemistry of the samples. The output is shown below. The main thing I'm confused about is the negative loadings for all of the factors in PC1. I took it to mean that the first PC relates to a relatively fresh source of groundwater recharge \(which makes perfect sense considering the study area\), but my advisor seems to think that the loadings for all of the different PCs points to evaporative concentration of irrigation water. He also said that ""if all of the loadings are negative, then they might as well all be positive."" Both of us are hydrologists, not really expert statisticians, so I would love to hear input and interpretations from some of you on this sub. Thank you!

  
PC1            PC2                PC3                PC4              PC5              PC6

Ca   \-0.2918901  0.69208540 \-0.38089672  0.04589718 \-0.21405776  0.4291246

Mg   \-0.4419762  0.19639048 \-0.09656106  0.18483003 \-0.04449990 \-0.2959924

Na   \-0.4005622 \-0.40807415  0.21953091  0.01287829  0.13678670  0.7548061

K    \-0.3748620 \-0.23117606 \-0.30088689 \-0.81056739 \-0.14698013 \-0.1773789

HCO3 \-0.2573508  0.40890119  0.81096209 \-0.27076444  0.06163522 \-0.1261359

SO4  \-0.4311025 \-0.04487235 \-0.18515883  0.23435266  0.73191047 \-0.2460743

Cl   \-0.4079300 \-0.30539808  0.12216218  0.42226935 \-0.61024048 \-0.2249048

PC1       PC2        PC3         PC4          PC5         PC6    

Standard deviation     2.1734   0.9870   0.8709   0.56584   0.35954   0.26489 

Proportion of Variance 0.6748   0.1391   0.1084   0.04574   0.01847   0.01002 

Cumulative Proportion  0.6748   0.8140   0.9223   0.96806   0.98653   0.99655  ",7,statistics,54935,,Interpreting PCA results for water quality analysis: all negative loadings for PC1?,https://www.reddit.com/r/statistics/comments/8lvtmc/interpreting_pca_results_for_water_quality/,all_ads,2018-05-25 00:19:51,9 days 01:15:43.036584000,
"I'm looking at a data table that shows 2.5th, 5th, 50th, 95th, and 97.5th percentiles \(as well as mean, min, max, s.d, and n\). I don't have access to the actual dataset.

Given these data, can I get a rough estimate of the xth percentile \(say 25th\)? Or, take a given number, and determine roughly what percentile that number falls at?

The distribution appears to be normal with a positive skew.

Thank you!

Edit: I meant to say the distribution is bell-shaped and positively skewed.",26,1527224116.0,8lw3bl,False,"I'm looking at a data table that shows 2.5th, 5th, 50th, 95th, and 97.5th percentiles \(as well as mean, min, max, s.d, and n\). I don't have access to the actual dataset.

Given these data, can I get a rough estimate of the xth percentile \(say 25th\)? Or, take a given number, and determine roughly what percentile that number falls at?

The distribution appears to be normal with a positive skew.

Thank you!

Edit: I meant to say the distribution is bell-shaped and positively skewed.",0,"I'm looking at a data table that shows 2.5th, 5th, 50th, 95th, and 97.5th percentiles \(as well as mean, min, max, s.d, and n\). I don't have access to the actual dataset.

Given these data, can I get a rough estimate of the xth percentile \(say 25th\)? Or, take a given number, and determine roughly what percentile that number falls at?

The distribution appears to be normal with a positive skew.

Thank you!

Edit: I meant to say the distribution is bell-shaped and positively skewed.",5,statistics,54935,,Can I estimate the 25th percentile of a dataset if I know the 50th and 5th percentiles?,https://www.reddit.com/r/statistics/comments/8lw3bl/can_i_estimate_the_25th_percentile_of_a_dataset/,all_ads,2018-05-25 00:55:16,9 days 00:40:18.036584000,
I need to learn how to do this.,1,1527250381.0,8lyy49,False,I need to learn how to do this.,0,I need to learn how to do this.,0,statistics,54935,,Where is a good tutorial on sequential regression multiple imputation technique?,https://www.reddit.com/r/statistics/comments/8lyy49/where_is_a_good_tutorial_on_sequential_regression/,all_ads,2018-05-25 08:13:01,8 days 17:22:33.036584000,
"I have a test I'm running for website optimization, where the dependent variable is whether or not we can motivate someone to log into their account.  So basically, I have a binary outcome, and 3 test treatments that are also binary.

Would I just use logistic regression to determine if one of the 8 treatments is significant?  Is there another method I should use?  

When I've done full factorial before I normally have a continuous response variable and have used linear regression in combination with an ANOVA..   but I know that I can't use an ANOVA with the logistic regression, because obviously the assumptions do not hold.

Quick thoughts?  Any help is greatly appreciated, for realz.",13,1527220383.0,8lvlyq,False,"I have a test I'm running for website optimization, where the dependent variable is whether or not we can motivate someone to log into their account.  So basically, I have a binary outcome, and 3 test treatments that are also binary.

Would I just use logistic regression to determine if one of the 8 treatments is significant?  Is there another method I should use?  

When I've done full factorial before I normally have a continuous response variable and have used linear regression in combination with an ANOVA..   but I know that I can't use an ANOVA with the logistic regression, because obviously the assumptions do not hold.

Quick thoughts?  Any help is greatly appreciated, for realz.",0,"I have a test I'm running for website optimization, where the dependent variable is whether or not we can motivate someone to log into their account.  So basically, I have a binary outcome, and 3 test treatments that are also binary.

Would I just use logistic regression to determine if one of the 8 treatments is significant?  Is there another method I should use?  

When I've done full factorial before I normally have a continuous response variable and have used linear regression in combination with an ANOVA..   but I know that I can't use an ANOVA with the logistic regression, because obviously the assumptions do not hold.

Quick thoughts?  Any help is greatly appreciated, for realz.",3,statistics,54935,,2^3 Full Factorial test analysis help,https://www.reddit.com/r/statistics/comments/8lvlyq/23_full_factorial_test_analysis_help/,all_ads,2018-05-24 23:53:03,9 days 01:42:31.036584000,
"I have a list of over 200 events/trials I need to run a One-Sample Propotion test on in Minitab. Does anyone know of a way to batch process these numbers (maybe if I add my events/trials as columns) so that I do not need to manually enter the values in the Proportion box in Minitab. 

I apologize if this is not the place for this but the Minitab sub gets no traction and I thought some folks here might now.",3,1527220852.0,8lvo64,False,"I have a list of over 200 events/trials I need to run a One-Sample Propotion test on in Minitab. Does anyone know of a way to batch process these numbers (maybe if I add my events/trials as columns) so that I do not need to manually enter the values in the Proportion box in Minitab. 

I apologize if this is not the place for this but the Minitab sub gets no traction and I thought some folks here might now.",0,"I have a list of over 200 events/trials I need to run a One-Sample Propotion test on in Minitab. Does anyone know of a way to batch process these numbers (maybe if I add my events/trials as columns) so that I do not need to manually enter the values in the Proportion box in Minitab. 

I apologize if this is not the place for this but the Minitab sub gets no traction and I thought some folks here might now.",3,statistics,54935,,Batch Processing of One-Proportion Tests in Minitab,https://www.reddit.com/r/statistics/comments/8lvo64/batch_processing_of_oneproportion_tests_in_minitab/,all_ads,2018-05-25 00:00:52,9 days 01:34:42.036584000,
"I want to send my R code to the program. I will put a link to my github on my application. 

Also, I want to send some sort of write up even though the paper is not done.  How long should it be? Am I over doing it? I want to show that I know how to program. ",11,1527195468.0,8lsfe1,False,"I want to send my R code to the program. I will put a link to my github on my application. 

Also, I want to send some sort of write up even though the paper is not done.  How long should it be? Am I over doing it? I want to show that I know how to program. ",0,"I want to send my R code to the program. I will put a link to my github on my application. 

Also, I want to send some sort of write up even though the paper is not done.  How long should it be? Am I over doing it? I want to show that I know how to program. ",8,statistics,54935,,What is the best way to share a stats project that I am working on to the grad school that I am applying to?,https://www.reddit.com/r/statistics/comments/8lsfe1/what_is_the_best_way_to_share_a_stats_project/,all_ads,2018-05-24 16:57:48,9 days 08:37:46.036584000,
"I’m trying to construct latent variables from a series of survey questions, all of which have ordinal (Likert scale) responses. All of the packages and solutions I’ve found so far have only been for evaluating the correlation between two ordinal variables, not a data frame that is composed almost exclusively of ordinal variables.

Does anyone have any advice on what method, function, and package would work best in this scenario?

Thank you for your time!

EDIT:
Packages tried thus far:
-Hmisc
-coin
-polycor
-pspearman",3,1527203645.0,8ltevz,False,"I’m trying to construct latent variables from a series of survey questions, all of which have ordinal (Likert scale) responses. All of the packages and solutions I’ve found so far have only been for evaluating the correlation between two ordinal variables, not a data frame that is composed almost exclusively of ordinal variables.

Does anyone have any advice on what method, function, and package would work best in this scenario?

Thank you for your time!

EDIT:
Packages tried thus far:
-Hmisc
-coin
-polycor
-pspearman",0,"I’m trying to construct latent variables from a series of survey questions, all of which have ordinal (Likert scale) responses. All of the packages and solutions I’ve found so far have only been for evaluating the correlation between two ordinal variables, not a data frame that is composed almost exclusively of ordinal variables.

Does anyone have any advice on what method, function, and package would work best in this scenario?

Thank you for your time!

EDIT:
Packages tried thus far:
-Hmisc
-coin
-polycor
-pspearman",4,statistics,54935,,How do you create a correlation matrix for multiple ordinal variables (3+) in R?,https://www.reddit.com/r/statistics/comments/8ltevz/how_do_you_create_a_correlation_matrix_for/,all_ads,2018-05-24 19:14:05,9 days 06:21:29.036584000,
"I am doing a stats project on association and need more data pls help 




https://docs.google.com/forms/d/1cC7wlanf3b2I1Ra-PicgcbufYAaASNaGMvjNc161Y4o",6,1527194947.0,8lsdhw,False,"I am doing a stats project on association and need more data pls help 




https://docs.google.com/forms/d/1cC7wlanf3b2I1Ra-PicgcbufYAaASNaGMvjNc161Y4o",1,"I am doing a stats project on association and need more data pls help 




https://docs.google.com/forms/d/1cC7wlanf3b2I1Ra-PicgcbufYAaASNaGMvjNc161Y4o",6,statistics,54935,,AP Stats final project,https://www.reddit.com/r/statistics/comments/8lsdhw/ap_stats_final_project/,all_ads,2018-05-24 16:49:07,9 days 08:46:27.036584000,
"My graduate program doesn’t really cover real life applications and data analysis. We just learn from the book and cover theory and get tested on calculations and theory/concepts. After the tests I forget some of the concepts and proofs. I’m not really smart as other people because I have mental illness/disability and I get extra time on exams and homework/projects. However, I manage to pass my classes with B or higher and that’s all I really care about. Passing the classes and keeping my GPA above 3.0 so I can get the MS degree and getting a good job. 

&nbsp;

I have a little practice doing statistics in a work environment. Right now I’m working as an intern/co-op in an environmental company in Health and Safety (EHS) doing some data entry , administrative work and some basic statistical work with employee/contractor injury data (Doing basic statistics, finding trends, making bar graphs, pie charts, simple analysis, root cause analysis of injuries, running reports, document management, working with the incident management database and pulling data, updating the intranet site (some html required here), some Excel work. To summarize, I don’t do anything too advanced at all. Mainly what I do at my job is simple statistics that a first year college student learns about in an intro Statistics class. 

&nbsp;

I have a major in Sociology and a minor in Math. Math courses I’ve taken: Calc 1-3, Linear Algebra, Differential Equations, Numerical Analysis, and Probability & Statistics. Graduate courses I’ve taken so far: Experimental Design, Linear Algebra, Mathematical Statistics I (Probability part). I still have to take 7 more courses for the MS in Statistics, which include Mathematical Statistics II (Inference), Regression Analysis, Statistical Analysis using SAS/R, Sampling Theory, Statistical Consulting, and a few more electives of my choice.

&nbsp;

Will this be enough preparation for a good job or do I need to do bootcamps and data science classes? I’m kind of overwhelmed with the amount of material there is to learn, and it’s just too much. Right now my programming skills are very poor, but I’m sure that the SAS/R classes will help a little bit. I realize that my mental illness/disability will not make me very competitive and will potentially prevent me from getting jobs. There’s not much I can do about it. I don’t expect or want $100k+ salary, I’ll be happy with $60k starting out and ok yearly raises. So, will I be able to get a good job or is this the wrong field for someone like me? Thanks.
",9,1527198193.0,8lsqa8,False,"My graduate program doesn’t really cover real life applications and data analysis. We just learn from the book and cover theory and get tested on calculations and theory/concepts. After the tests I forget some of the concepts and proofs. I’m not really smart as other people because I have mental illness/disability and I get extra time on exams and homework/projects. However, I manage to pass my classes with B or higher and that’s all I really care about. Passing the classes and keeping my GPA above 3.0 so I can get the MS degree and getting a good job. 

&nbsp;

I have a little practice doing statistics in a work environment. Right now I’m working as an intern/co-op in an environmental company in Health and Safety (EHS) doing some data entry , administrative work and some basic statistical work with employee/contractor injury data (Doing basic statistics, finding trends, making bar graphs, pie charts, simple analysis, root cause analysis of injuries, running reports, document management, working with the incident management database and pulling data, updating the intranet site (some html required here), some Excel work. To summarize, I don’t do anything too advanced at all. Mainly what I do at my job is simple statistics that a first year college student learns about in an intro Statistics class. 

&nbsp;

I have a major in Sociology and a minor in Math. Math courses I’ve taken: Calc 1-3, Linear Algebra, Differential Equations, Numerical Analysis, and Probability & Statistics. Graduate courses I’ve taken so far: Experimental Design, Linear Algebra, Mathematical Statistics I (Probability part). I still have to take 7 more courses for the MS in Statistics, which include Mathematical Statistics II (Inference), Regression Analysis, Statistical Analysis using SAS/R, Sampling Theory, Statistical Consulting, and a few more electives of my choice.

&nbsp;

Will this be enough preparation for a good job or do I need to do bootcamps and data science classes? I’m kind of overwhelmed with the amount of material there is to learn, and it’s just too much. Right now my programming skills are very poor, but I’m sure that the SAS/R classes will help a little bit. I realize that my mental illness/disability will not make me very competitive and will potentially prevent me from getting jobs. There’s not much I can do about it. I don’t expect or want $100k+ salary, I’ll be happy with $60k starting out and ok yearly raises. So, will I be able to get a good job or is this the wrong field for someone like me? Thanks.
",0,"My graduate program doesn’t really cover real life applications and data analysis. We just learn from the book and cover theory and get tested on calculations and theory/concepts. After the tests I forget some of the concepts and proofs. I’m not really smart as other people because I have mental illness/disability and I get extra time on exams and homework/projects. However, I manage to pass my classes with B or higher and that’s all I really care about. Passing the classes and keeping my GPA above 3.0 so I can get the MS degree and getting a good job. 

&nbsp;

I have a little practice doing statistics in a work environment. Right now I’m working as an intern/co-op in an environmental company in Health and Safety (EHS) doing some data entry , administrative work and some basic statistical work with employee/contractor injury data (Doing basic statistics, finding trends, making bar graphs, pie charts, simple analysis, root cause analysis of injuries, running reports, document management, working with the incident management database and pulling data, updating the intranet site (some html required here), some Excel work. To summarize, I don’t do anything too advanced at all. Mainly what I do at my job is simple statistics that a first year college student learns about in an intro Statistics class. 

&nbsp;

I have a major in Sociology and a minor in Math. Math courses I’ve taken: Calc 1-3, Linear Algebra, Differential Equations, Numerical Analysis, and Probability & Statistics. Graduate courses I’ve taken so far: Experimental Design, Linear Algebra, Mathematical Statistics I (Probability part). I still have to take 7 more courses for the MS in Statistics, which include Mathematical Statistics II (Inference), Regression Analysis, Statistical Analysis using SAS/R, Sampling Theory, Statistical Consulting, and a few more electives of my choice.

&nbsp;

Will this be enough preparation for a good job or do I need to do bootcamps and data science classes? I’m kind of overwhelmed with the amount of material there is to learn, and it’s just too much. Right now my programming skills are very poor, but I’m sure that the SAS/R classes will help a little bit. I realize that my mental illness/disability will not make me very competitive and will potentially prevent me from getting jobs. There’s not much I can do about it. I don’t expect or want $100k+ salary, I’ll be happy with $60k starting out and ok yearly raises. So, will I be able to get a good job or is this the wrong field for someone like me? Thanks.
",5,statistics,54935,,Can I still be competitive in the job market and get a good statistics job if I have mental illness/disability?,https://www.reddit.com/r/statistics/comments/8lsqa8/can_i_still_be_competitive_in_the_job_market_and/,all_ads,2018-05-24 17:43:13,9 days 07:52:21.036584000,
"It's like Scholastic or something like that...


edit: to further explain, it's like the basis for all Statistics because if the word was not non-random, then there'd be no point in running these bell-curves and what not",9,1527240869.0,8ly0bj,False,"It's like Scholastic or something like that...


edit: to further explain, it's like the basis for all Statistics because if the word was not non-random, then there'd be no point in running these bell-curves and what not",0,"It's like Scholastic or something like that...


edit: to further explain, it's like the basis for all Statistics because if the word was not non-random, then there'd be no point in running these bell-curves and what not",0,statistics,54935,,What's that fancy word for non-random?,https://www.reddit.com/r/statistics/comments/8ly0bj/whats_that_fancy_word_for_nonrandom/,all_ads,2018-05-25 05:34:29,8 days 20:01:05.036584000,
"Hello! I'm having trouble with a dataset, hope someone can shed some light on it!

IV \- nominal categorical with 5 levels

DV1 \- continuous

DV2 \- ordinal categorical with 5 levels

I want to find out if accounting for DV1, does IV have an effect on DV2. Should I use an ordered multinomial logit model like this:

DV2 \~ \(1|DV1\) \+ IV

Or should I just look at IV and DV2 using ANOVA and separately look at the relationship between DV1 and DV2? Or should I just treat DV2 as continuous data?",9,1527168275.0,8lq86q,False,"Hello! I'm having trouble with a dataset, hope someone can shed some light on it!

IV \- nominal categorical with 5 levels

DV1 \- continuous

DV2 \- ordinal categorical with 5 levels

I want to find out if accounting for DV1, does IV have an effect on DV2. Should I use an ordered multinomial logit model like this:

DV2 \~ \(1|DV1\) \+ IV

Or should I just look at IV and DV2 using ANOVA and separately look at the relationship between DV1 and DV2? Or should I just treat DV2 as continuous data?",0,"Hello! I'm having trouble with a dataset, hope someone can shed some light on it!

IV \- nominal categorical with 5 levels

DV1 \- continuous

DV2 \- ordinal categorical with 5 levels

I want to find out if accounting for DV1, does IV have an effect on DV2. Should I use an ordered multinomial logit model like this:

DV2 \~ \(1|DV1\) \+ IV

Or should I just look at IV and DV2 using ANOVA and separately look at the relationship between DV1 and DV2? Or should I just treat DV2 as continuous data?",13,statistics,54935,,Should I use an ordered multinomial logit model or just use ANOVA?,https://www.reddit.com/r/statistics/comments/8lq86q/should_i_use_an_ordered_multinomial_logit_model/,all_ads,2018-05-24 09:24:35,9 days 16:10:59.036584000,
"So, I've been taking a fast paced statistics course online this Summer, and I have to say, the results of my first few quizzes are really disheartening. I'm sitting at a high C at this point, and everything I've lost points on, I can't say I'd get right next time. 

I'm not sure if I'm missing fundamental things, or if the questions are ambiguous. I though maybe some other opinions would help. Here are the questions I've gotten wrong.

[Imgur Link](https://imgur.com/a/YQda9RV)

So, with age, I understand that age progresses continuously, and that it could be expressed as a decimal or fraction, but it never is. I can't imagine any scenario in which a statistics for age would be expressed as anything but an integer. 

For days of the week, my understanding for Ordinal vs. Nominal is that Ordinal has some implied order. Do days of the week not? Again, I understand that the 'week' can be defined loosely and where is 'begins' and 'ends' is arbitrary, but in terms of statistics, is it not useful or standard to consider Tuesday as being *after* Monday?

I'm worried this class is going to wreck my GPA for Stats classes and I really feel like putting more time into studying this stuff isn't going to help me get questions like this right.
",3,1527165126.0,8lpy3s,False,"So, I've been taking a fast paced statistics course online this Summer, and I have to say, the results of my first few quizzes are really disheartening. I'm sitting at a high C at this point, and everything I've lost points on, I can't say I'd get right next time. 

I'm not sure if I'm missing fundamental things, or if the questions are ambiguous. I though maybe some other opinions would help. Here are the questions I've gotten wrong.

[Imgur Link](https://imgur.com/a/YQda9RV)

So, with age, I understand that age progresses continuously, and that it could be expressed as a decimal or fraction, but it never is. I can't imagine any scenario in which a statistics for age would be expressed as anything but an integer. 

For days of the week, my understanding for Ordinal vs. Nominal is that Ordinal has some implied order. Do days of the week not? Again, I understand that the 'week' can be defined loosely and where is 'begins' and 'ends' is arbitrary, but in terms of statistics, is it not useful or standard to consider Tuesday as being *after* Monday?

I'm worried this class is going to wreck my GPA for Stats classes and I really feel like putting more time into studying this stuff isn't going to help me get questions like this right.
",0,"So, I've been taking a fast paced statistics course online this Summer, and I have to say, the results of my first few quizzes are really disheartening. I'm sitting at a high C at this point, and everything I've lost points on, I can't say I'd get right next time. 

I'm not sure if I'm missing fundamental things, or if the questions are ambiguous. I though maybe some other opinions would help. Here are the questions I've gotten wrong.

[Imgur Link](https://imgur.com/a/YQda9RV)

So, with age, I understand that age progresses continuously, and that it could be expressed as a decimal or fraction, but it never is. I can't imagine any scenario in which a statistics for age would be expressed as anything but an integer. 

For days of the week, my understanding for Ordinal vs. Nominal is that Ordinal has some implied order. Do days of the week not? Again, I understand that the 'week' can be defined loosely and where is 'begins' and 'ends' is arbitrary, but in terms of statistics, is it not useful or standard to consider Tuesday as being *after* Monday?

I'm worried this class is going to wreck my GPA for Stats classes and I really feel like putting more time into studying this stuff isn't going to help me get questions like this right.
",6,statistics,54935,,Taking my first Statistics course. Need some insight.,https://www.reddit.com/r/statistics/comments/8lpy3s/taking_my_first_statistics_course_need_some/,all_ads,2018-05-24 08:32:06,9 days 17:03:28.036584000,
"Hello, I am currently studying archaeology at university, and one of my required courses is based on statistics. I am not particularly prepared for such a class, and a lot of the material is going over my head. I was hoping to get some insight. 

My essay is based on primates, their diets, and life histories. Right now, I am testing whether diet has significant effect on cranial capacity.

Using SPSS, I have run a Levene's test of homogeneity of variance. In the outcome, I get a significance based on mean of .032, and a significance based on median of .457. I am unclear if I should use the mean or median, and why the two are so different. 

I have other variables to test, but this is my first and only statistics class. I don't know which tests I should run for each, or what they tell me.

Thank you for reading. I know these are beginner-level questions, but I would really appreciate some help.",3,1527199910.0,8lsxxa,False,"Hello, I am currently studying archaeology at university, and one of my required courses is based on statistics. I am not particularly prepared for such a class, and a lot of the material is going over my head. I was hoping to get some insight. 

My essay is based on primates, their diets, and life histories. Right now, I am testing whether diet has significant effect on cranial capacity.

Using SPSS, I have run a Levene's test of homogeneity of variance. In the outcome, I get a significance based on mean of .032, and a significance based on median of .457. I am unclear if I should use the mean or median, and why the two are so different. 

I have other variables to test, but this is my first and only statistics class. I don't know which tests I should run for each, or what they tell me.

Thank you for reading. I know these are beginner-level questions, but I would really appreciate some help.",0,"Hello, I am currently studying archaeology at university, and one of my required courses is based on statistics. I am not particularly prepared for such a class, and a lot of the material is going over my head. I was hoping to get some insight. 

My essay is based on primates, their diets, and life histories. Right now, I am testing whether diet has significant effect on cranial capacity.

Using SPSS, I have run a Levene's test of homogeneity of variance. In the outcome, I get a significance based on mean of .032, and a significance based on median of .457. I am unclear if I should use the mean or median, and why the two are so different. 

I have other variables to test, but this is my first and only statistics class. I don't know which tests I should run for each, or what they tell me.

Thank you for reading. I know these are beginner-level questions, but I would really appreciate some help.",1,statistics,54935,,Statistics Final for a Non-Statistics Student,https://www.reddit.com/r/statistics/comments/8lsxxa/statistics_final_for_a_nonstatistics_student/,all_ads,2018-05-24 18:11:50,9 days 07:23:44.036584000,
"Hi all! Please help please, I am doing an experiment at university and my professors are having trouble deciding how I should present my data... 

I have got the time at which prey birds have visited bird feeders to see what pattern they follow, the data I have is literally just the time each visit occurs, performed in a few spots over a few days. I have around 200 points of data and I have been able to change the minutes into a decimal of an hour, plop this in a bar graph and see the general pattern frequency pattern based hour by hour. But would this best fit a Mann Whitney test? Or perhaps Wilcox? Please help! I will provide much karmas... and if the project goes well I may guild! Behold the bribery of Reddit gold!! 
A data sample:
Bird - time  (hh.mm)
1.          06.45
2.          07. 12
3.          14. 32
...
199.     13. 57
200.    20. 19",5,1527221887.0,8lvt37,False,"Hi all! Please help please, I am doing an experiment at university and my professors are having trouble deciding how I should present my data... 

I have got the time at which prey birds have visited bird feeders to see what pattern they follow, the data I have is literally just the time each visit occurs, performed in a few spots over a few days. I have around 200 points of data and I have been able to change the minutes into a decimal of an hour, plop this in a bar graph and see the general pattern frequency pattern based hour by hour. But would this best fit a Mann Whitney test? Or perhaps Wilcox? Please help! I will provide much karmas... and if the project goes well I may guild! Behold the bribery of Reddit gold!! 
A data sample:
Bird - time  (hh.mm)
1.          06.45
2.          07. 12
3.          14. 32
...
199.     13. 57
200.    20. 19",0,"Hi all! Please help please, I am doing an experiment at university and my professors are having trouble deciding how I should present my data... 

I have got the time at which prey birds have visited bird feeders to see what pattern they follow, the data I have is literally just the time each visit occurs, performed in a few spots over a few days. I have around 200 points of data and I have been able to change the minutes into a decimal of an hour, plop this in a bar graph and see the general pattern frequency pattern based hour by hour. But would this best fit a Mann Whitney test? Or perhaps Wilcox? Please help! I will provide much karmas... and if the project goes well I may guild! Behold the bribery of Reddit gold!! 
A data sample:
Bird - time  (hh.mm)
1.          06.45
2.          07. 12
3.          14. 32
...
199.     13. 57
200.    20. 19",0,statistics,54935,,HELP PLEASE!! Which statistical test to use?!,https://www.reddit.com/r/statistics/comments/8lvt37/help_please_which_statistical_test_to_use/,all_ads,2018-05-25 00:18:07,9 days 01:17:27.036584000,
"I want to replicate a political scientist's results that a decrease in rainfall (IV for economic shock) is positively correlated with the probability of a civil war breaking out. How do I do it?

The way I thought was to have one variable with the change in rainfall for each month of the years I'm looking at in the twelve months before (for example, for March 2002 I have a variable looking at the change in rainfall between March 2001 and March 2002, for April 2002 I look at the change between April 2001 and April 2002 etc.) and then I regress civil war (a dummy) with each month's rainfall variation.

Is there a quicker way?",7,1527178187.0,8lqzj0,False,"I want to replicate a political scientist's results that a decrease in rainfall (IV for economic shock) is positively correlated with the probability of a civil war breaking out. How do I do it?

The way I thought was to have one variable with the change in rainfall for each month of the years I'm looking at in the twelve months before (for example, for March 2002 I have a variable looking at the change in rainfall between March 2001 and March 2002, for April 2002 I look at the change between April 2001 and April 2002 etc.) and then I regress civil war (a dummy) with each month's rainfall variation.

Is there a quicker way?",0,"I want to replicate a political scientist's results that a decrease in rainfall (IV for economic shock) is positively correlated with the probability of a civil war breaking out. How do I do it?

The way I thought was to have one variable with the change in rainfall for each month of the years I'm looking at in the twelve months before (for example, for March 2002 I have a variable looking at the change in rainfall between March 2001 and March 2002, for April 2002 I look at the change between April 2001 and April 2002 etc.) and then I regress civil war (a dummy) with each month's rainfall variation.

Is there a quicker way?",2,statistics,54935,,What's the best way to run this regression?,https://www.reddit.com/r/statistics/comments/8lqzj0/whats_the_best_way_to_run_this_regression/,all_ads,2018-05-24 12:09:47,9 days 13:25:47.036584000,
"+++My Blog is down again due to the DSGVO in europe, Post when I'm online again++++
I don't know how to make up for not having a stricly statistical degree in a job interview. So what should I highlight? What can I do?
Further Information:
I'll get my bachelor degree in sociology, which includes statistic. I worked 2 months in a apprenticeship with databanks. I worked 4 months in another apprenticeship in a big company on a 40 page report (including presentation). I can just a little programming (javascript) and know R Basics. I speak english, german and a little russian and mandarin chinese.  
",52,1527109347.0,8lj68l,False,"+++My Blog is down again due to the DSGVO in europe, Post when I'm online again++++
I don't know how to make up for not having a stricly statistical degree in a job interview. So what should I highlight? What can I do?
Further Information:
I'll get my bachelor degree in sociology, which includes statistic. I worked 2 months in a apprenticeship with databanks. I worked 4 months in another apprenticeship in a big company on a 40 page report (including presentation). I can just a little programming (javascript) and know R Basics. I speak english, german and a little russian and mandarin chinese.  
",0,"+++My Blog is down again due to the DSGVO in europe, Post when I'm online again++++
I don't know how to make up for not having a stricly statistical degree in a job interview. So what should I highlight? What can I do?
Further Information:
I'll get my bachelor degree in sociology, which includes statistic. I worked 2 months in a apprenticeship with databanks. I worked 4 months in another apprenticeship in a big company on a 40 page report (including presentation). I can just a little programming (javascript) and know R Basics. I speak english, german and a little russian and mandarin chinese.  
",61,statistics,54935,,FOR EMPLOYED STATISTIANS: What are the most important skills for beginners?,https://www.reddit.com/r/statistics/comments/8lj68l/for_employed_statistians_what_are_the_most/,all_ads,2018-05-23 17:02:27,10 days 08:33:07.036584000,
"Hey everyone,

I recently graduated with a degree in political science and a minor in economics. I have decided that I want to pursue a masters in public policy in a couple years, so I attempted to apply for research positions. I even interviewed with a position that would have been a great fit. However, these positions strongly prefer a candidate with quantitative and qualitative analysis skills with a knowledge of how to use statistical software.

How would you recommend I learn, at least the basics, of this stuff to get a job in the field. I am really trying to make my grad school application strong.

Thank you so much for the help! ",10,1527152477.0,8lol9k,False,"Hey everyone,

I recently graduated with a degree in political science and a minor in economics. I have decided that I want to pursue a masters in public policy in a couple years, so I attempted to apply for research positions. I even interviewed with a position that would have been a great fit. However, these positions strongly prefer a candidate with quantitative and qualitative analysis skills with a knowledge of how to use statistical software.

How would you recommend I learn, at least the basics, of this stuff to get a job in the field. I am really trying to make my grad school application strong.

Thank you so much for the help! ",0,"Hey everyone,

I recently graduated with a degree in political science and a minor in economics. I have decided that I want to pursue a masters in public policy in a couple years, so I attempted to apply for research positions. I even interviewed with a position that would have been a great fit. However, these positions strongly prefer a candidate with quantitative and qualitative analysis skills with a knowledge of how to use statistical software.

How would you recommend I learn, at least the basics, of this stuff to get a job in the field. I am really trying to make my grad school application strong.

Thank you so much for the help! ",6,statistics,54935,,"Just graduated college, but have recently changed my mind on what I want to do, which requires statistical knowledge I do not have.",https://www.reddit.com/r/statistics/comments/8lol9k/just_graduated_college_but_have_recently_changed/,all_ads,2018-05-24 05:01:17,9 days 20:34:17.036584000,
"My textbooks say correlation coefficient is an indicator of a linear relationship and it doesn't make any sense when the relationship is nonlinear.

So for ""obvious"" linear trends (as you can see from its scatterplot), the correlation coefficient is near +1 or -1. This makes sense. 

But how about those ""linear"" trends having a correlation coefficient close to zero, how can we know there exists linearity whereas the scatterplot is just a mass? ",3,1527172655.0,8lqkob,False,"My textbooks say correlation coefficient is an indicator of a linear relationship and it doesn't make any sense when the relationship is nonlinear.

So for ""obvious"" linear trends (as you can see from its scatterplot), the correlation coefficient is near +1 or -1. This makes sense. 

But how about those ""linear"" trends having a correlation coefficient close to zero, how can we know there exists linearity whereas the scatterplot is just a mass? ",0,"My textbooks say correlation coefficient is an indicator of a linear relationship and it doesn't make any sense when the relationship is nonlinear.

So for ""obvious"" linear trends (as you can see from its scatterplot), the correlation coefficient is near +1 or -1. This makes sense. 

But how about those ""linear"" trends having a correlation coefficient close to zero, how can we know there exists linearity whereas the scatterplot is just a mass? ",2,statistics,54935,,"How does a correlation coefficient close to zero make sense, given that it is only interpretable for linear trends?",https://www.reddit.com/r/statistics/comments/8lqkob/how_does_a_correlation_coefficient_close_to_zero/,all_ads,2018-05-24 10:37:35,9 days 14:57:59.036584000,
"Hello all -- I proctored a Latin exam this morning during which events combined in a way that seemed to me highly improbable. The interesting (in my ignorant estimation) particulars are as follows:

1. There were 22 students in the class;
2. Ten of the students were female;
3. There were four lefties in the class;
4. All lefties were female;
5. The desks were arranged 5x5;
6. Each lefty sat in the second row; 
7. The arrangement, according to handedness, of the second row was LLRLL; and
8. The test was administered in America to Americans.

Other (perhaps) relevant info:

The percentage of Americans that are left handed is 10%. Men are 1.23 times more likely to be left-handed than women. Source: https://eric.ed.gov/?id=EJ807817

What statistical questions might this scenario pique? And what is the likelihood of this scenario occurring? Thanks!
",18,1527144926.0,8lnqzd,False,"Hello all -- I proctored a Latin exam this morning during which events combined in a way that seemed to me highly improbable. The interesting (in my ignorant estimation) particulars are as follows:

1. There were 22 students in the class;
2. Ten of the students were female;
3. There were four lefties in the class;
4. All lefties were female;
5. The desks were arranged 5x5;
6. Each lefty sat in the second row; 
7. The arrangement, according to handedness, of the second row was LLRLL; and
8. The test was administered in America to Americans.

Other (perhaps) relevant info:

The percentage of Americans that are left handed is 10%. Men are 1.23 times more likely to be left-handed than women. Source: https://eric.ed.gov/?id=EJ807817

What statistical questions might this scenario pique? And what is the likelihood of this scenario occurring? Thanks!
",0,"Hello all -- I proctored a Latin exam this morning during which events combined in a way that seemed to me highly improbable. The interesting (in my ignorant estimation) particulars are as follows:

1. There were 22 students in the class;
2. Ten of the students were female;
3. There were four lefties in the class;
4. All lefties were female;
5. The desks were arranged 5x5;
6. Each lefty sat in the second row; 
7. The arrangement, according to handedness, of the second row was LLRLL; and
8. The test was administered in America to Americans.

Other (perhaps) relevant info:

The percentage of Americans that are left handed is 10%. Men are 1.23 times more likely to be left-handed than women. Source: https://eric.ed.gov/?id=EJ807817

What statistical questions might this scenario pique? And what is the likelihood of this scenario occurring? Thanks!
",8,statistics,54935,,The Left-Handed Ladies of my Latin Class,https://www.reddit.com/r/statistics/comments/8lnqzd/the_lefthanded_ladies_of_my_latin_class/,all_ads,2018-05-24 02:55:26,9 days 22:40:08.036584000,
"I run across different things daily to read about, and after running across the ASA and learning some things, i ran across the Fellows list. I started looking deeper into it, and noticing the surnames, started wondering if the majority of new inductees are indeed Americans. 

I know its unlikely, but as your homework, make a graph of the names and (perceived) nationalities as they change over time, to see how the demographics have changed over the last 100 years of this academic field of study.",3,1527206647.0,8ltsq3,False,"I run across different things daily to read about, and after running across the ASA and learning some things, i ran across the Fellows list. I started looking deeper into it, and noticing the surnames, started wondering if the majority of new inductees are indeed Americans. 

I know its unlikely, but as your homework, make a graph of the names and (perceived) nationalities as they change over time, to see how the demographics have changed over the last 100 years of this academic field of study.",0,"I run across different things daily to read about, and after running across the ASA and learning some things, i ran across the Fellows list. I started looking deeper into it, and noticing the surnames, started wondering if the majority of new inductees are indeed Americans. 

I know its unlikely, but as your homework, make a graph of the names and (perceived) nationalities as they change over time, to see how the demographics have changed over the last 100 years of this academic field of study.",0,statistics,54935,,"A query of ""List of Fellows of the American Statistical Association""",https://www.reddit.com/r/statistics/comments/8ltsq3/a_query_of_list_of_fellows_of_the_american/,all_ads,2018-05-24 20:04:07,9 days 05:31:27.036584000,
"Hi everyone,

I am in Intro to stats and started hypothesis testing. I have covered hypothesis testing for population proportions, population means, and inference on means, populations for independent and dependent samples.

My professor keeps telling me that we will fail if we use the wrong formula, but I am worried.

Are there any tips or tricks on when to use Z-table vs T-table?
I know to use T-table when population std. deviation is not known, but in a lot of problems we don't know the population std. deviation and we don't use T-table.

Do I use T-table when I am dealing with the means and the formula for std. deviation requires population std. deviation that I do not know? Or do I use it when sample size is less than 30 because my professor said no in regards to the sample size?

Thanks for all your help, God bless! ",0,1527162838.0,8lpq8c,False,"Hi everyone,

I am in Intro to stats and started hypothesis testing. I have covered hypothesis testing for population proportions, population means, and inference on means, populations for independent and dependent samples.

My professor keeps telling me that we will fail if we use the wrong formula, but I am worried.

Are there any tips or tricks on when to use Z-table vs T-table?
I know to use T-table when population std. deviation is not known, but in a lot of problems we don't know the population std. deviation and we don't use T-table.

Do I use T-table when I am dealing with the means and the formula for std. deviation requires population std. deviation that I do not know? Or do I use it when sample size is less than 30 because my professor said no in regards to the sample size?

Thanks for all your help, God bless! ",0,"Hi everyone,

I am in Intro to stats and started hypothesis testing. I have covered hypothesis testing for population proportions, population means, and inference on means, populations for independent and dependent samples.

My professor keeps telling me that we will fail if we use the wrong formula, but I am worried.

Are there any tips or tricks on when to use Z-table vs T-table?
I know to use T-table when population std. deviation is not known, but in a lot of problems we don't know the population std. deviation and we don't use T-table.

Do I use T-table when I am dealing with the means and the formula for std. deviation requires population std. deviation that I do not know? Or do I use it when sample size is less than 30 because my professor said no in regards to the sample size?

Thanks for all your help, God bless! ",2,statistics,54935,,Introduction to Hypothesis Testing and Scenarios?,https://www.reddit.com/r/statistics/comments/8lpq8c/introduction_to_hypothesis_testing_and_scenarios/,all_ads,2018-05-24 07:53:58,9 days 17:41:36.036584000,
"Hi all Statistics Lovers :)

_on a udacity project , I completed an EDA(Exploratory Data Analysis) on Youtube Trending Videos_

You can view the full project on Kaggle : https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda

___

##Main findings:
I found some interesting facts about Youtube trending videos; these are:-

84% or more trending videos are using one of its tag on the video title for at least once.

Other than 604 trending videos,all trending videos are appeared in the trending list for more than 1 once.

Maximum number of Youtube videos are listed on trending, within 0 to 14 days of the video publishing date.

More users engaged in conversation when they were disliking a trending video rather than liking a trending video.

If difference between first trending date & publish date is less than 4 days,then there is a big chance,that video would not be re-trended for more than 3 times.

There is a impact on Youtube trending videos views count over tag_appeared_in_title or not.

Trending videos those have listed for more than 5 times got the highest number of views.

Videos belongs to categories where number of subscriber is/are most ;those videos are using at least one of its tag on the trending video title.

##Surprise! findings:
Many of Youube trending videos get listed on trending list for more than 1 time(or day), but they did not get higher number of traffics.

Another point I already discussed,many of the trending videos have lower number of subscriber(some of them have 0) & yet they managed to get greater number of viewers than top subscriber channels present in the Youtube.

Also I saw there are many trending videos managed to get higher number of views counts,but they have very few likes(many of them have 0).

___

Don't forget to visit below Kaggle Kernel 

#URL:
https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda


",4,1527097388.0,8li2mn,False,"Hi all Statistics Lovers :)

_on a udacity project , I completed an EDA(Exploratory Data Analysis) on Youtube Trending Videos_

You can view the full project on Kaggle : https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda

___

##Main findings:
I found some interesting facts about Youtube trending videos; these are:-

84% or more trending videos are using one of its tag on the video title for at least once.

Other than 604 trending videos,all trending videos are appeared in the trending list for more than 1 once.

Maximum number of Youtube videos are listed on trending, within 0 to 14 days of the video publishing date.

More users engaged in conversation when they were disliking a trending video rather than liking a trending video.

If difference between first trending date & publish date is less than 4 days,then there is a big chance,that video would not be re-trended for more than 3 times.

There is a impact on Youtube trending videos views count over tag_appeared_in_title or not.

Trending videos those have listed for more than 5 times got the highest number of views.

Videos belongs to categories where number of subscriber is/are most ;those videos are using at least one of its tag on the trending video title.

##Surprise! findings:
Many of Youube trending videos get listed on trending list for more than 1 time(or day), but they did not get higher number of traffics.

Another point I already discussed,many of the trending videos have lower number of subscriber(some of them have 0) & yet they managed to get greater number of viewers than top subscriber channels present in the Youtube.

Also I saw there are many trending videos managed to get higher number of views counts,but they have very few likes(many of them have 0).

___

Don't forget to visit below Kaggle Kernel 

#URL:
https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda


",0,"Hi all Statistics Lovers :)

_on a udacity project , I completed an EDA(Exploratory Data Analysis) on Youtube Trending Videos_

You can view the full project on Kaggle : https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda

___

##Main findings:
I found some interesting facts about Youtube trending videos; these are:-

84% or more trending videos are using one of its tag on the video title for at least once.

Other than 604 trending videos,all trending videos are appeared in the trending list for more than 1 once.

Maximum number of Youtube videos are listed on trending, within 0 to 14 days of the video publishing date.

More users engaged in conversation when they were disliking a trending video rather than liking a trending video.

If difference between first trending date & publish date is less than 4 days,then there is a big chance,that video would not be re-trended for more than 3 times.

There is a impact on Youtube trending videos views count over tag_appeared_in_title or not.

Trending videos those have listed for more than 5 times got the highest number of views.

Videos belongs to categories where number of subscriber is/are most ;those videos are using at least one of its tag on the trending video title.

##Surprise! findings:
Many of Youube trending videos get listed on trending list for more than 1 time(or day), but they did not get higher number of traffics.

Another point I already discussed,many of the trending videos have lower number of subscriber(some of them have 0) & yet they managed to get greater number of viewers than top subscriber channels present in the Youtube.

Also I saw there are many trending videos managed to get higher number of views counts,but they have very few likes(many of them have 0).

___

Don't forget to visit below Kaggle Kernel 

#URL:
https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda


",11,statistics,54935,,[Kaggle] Youtube Trending Videos - Detailed ANALYSIS (EDA),https://www.reddit.com/r/statistics/comments/8li2mn/kaggle_youtube_trending_videos_detailed_analysis/,all_ads,2018-05-23 13:43:08,10 days 11:52:26.036584000,
"Given a binary tree. If I randomly walk down the tree from root to a leaf \(i.e. randomly chose the left or right child\), what is the expected length of the path `E(P)` in terms of the tree height `h` and `n` the number of nodes?",7,1527122966.0,8lkw83,False,"Given a binary tree. If I randomly walk down the tree from root to a leaf \(i.e. randomly chose the left or right child\), what is the expected length of the path `E(P)` in terms of the tree height `h` and `n` the number of nodes?",0,"Given a binary tree. If I randomly walk down the tree from root to a leaf \(i.e. randomly chose the left or right child\), what is the expected length of the path `E(P)` in terms of the tree height `h` and `n` the number of nodes?",2,statistics,54935,,A statistics question about binary tree.,https://www.reddit.com/r/statistics/comments/8lkw83/a_statistics_question_about_binary_tree/,all_ads,2018-05-23 20:49:26,10 days 04:46:08.036584000,
"I'm about to be conducting a study consisting of one IV (categorical, 3 levels) and 8 DVs (continuous).

How do i use G*Power to calculate the recommended sample size?",1,1527121516.0,8lkpap,False,"I'm about to be conducting a study consisting of one IV (categorical, 3 levels) and 8 DVs (continuous).

How do i use G*Power to calculate the recommended sample size?",0,"I'm about to be conducting a study consisting of one IV (categorical, 3 levels) and 8 DVs (continuous).

How do i use G*Power to calculate the recommended sample size?",1,statistics,54935,,Need help regarding calculating recommended sample size using G*Power.,https://www.reddit.com/r/statistics/comments/8lkpap/need_help_regarding_calculating_recommended/,all_ads,2018-05-23 20:25:16,10 days 05:10:18.036584000,
"The following output comes from this [lme4 tutorial](https://it.unt.edu/sites/default/files/linearmixedmodels_jds_dec2010.pdf):

    Linear mixed model fit by REML
    Formula: extro ~ open + agree + social + class + (1 | school/class)
     Data: lmm.data
     AIC BIC logLik deviance REMLdev
     3548 3599 -1764 3509 3528
    Random effects:
     Groups Name Variance Std.Dev.
     class:school (Intercept) 2.88365 1.69813
     school (Intercept) 95.17339 9.75569
     Residual 0.96837 0.98406
    Number of obs: 1200, groups: class:school, 24; school, 6 

I didn't included fixed effects as my question is just about the random effects.

I have some intuition on how to interpret the estimated school variance. We can think of each school as having it's own intercept, and those values follow a normal distribution. The standard deviation of that distribution is 9.76.

I have a harder time understanding what the value for class:school means. It makes sense to me to think that we also want to allow each class to also have it's own intercept. But what is the value 1.69? Since classes are nested within schools, I would think that all 6 collections of classes have their own distribution with their own SD. What does this singular value mean?

Also, from an interpretation standpoint, since the σ\_school value is much higher than the σ\_class value, is the interpretation that average extraversion varies a lot between schools, but does not vary much between classes within a school?",1,1527121349.0,8lkojc,False,"The following output comes from this [lme4 tutorial](https://it.unt.edu/sites/default/files/linearmixedmodels_jds_dec2010.pdf):

    Linear mixed model fit by REML
    Formula: extro ~ open + agree + social + class + (1 | school/class)
     Data: lmm.data
     AIC BIC logLik deviance REMLdev
     3548 3599 -1764 3509 3528
    Random effects:
     Groups Name Variance Std.Dev.
     class:school (Intercept) 2.88365 1.69813
     school (Intercept) 95.17339 9.75569
     Residual 0.96837 0.98406
    Number of obs: 1200, groups: class:school, 24; school, 6 

I didn't included fixed effects as my question is just about the random effects.

I have some intuition on how to interpret the estimated school variance. We can think of each school as having it's own intercept, and those values follow a normal distribution. The standard deviation of that distribution is 9.76.

I have a harder time understanding what the value for class:school means. It makes sense to me to think that we also want to allow each class to also have it's own intercept. But what is the value 1.69? Since classes are nested within schools, I would think that all 6 collections of classes have their own distribution with their own SD. What does this singular value mean?

Also, from an interpretation standpoint, since the σ\_school value is much higher than the σ\_class value, is the interpretation that average extraversion varies a lot between schools, but does not vary much between classes within a school?",0,"The following output comes from this [lme4 tutorial](https://it.unt.edu/sites/default/files/linearmixedmodels_jds_dec2010.pdf):

    Linear mixed model fit by REML
    Formula: extro ~ open + agree + social + class + (1 | school/class)
     Data: lmm.data
     AIC BIC logLik deviance REMLdev
     3548 3599 -1764 3509 3528
    Random effects:
     Groups Name Variance Std.Dev.
     class:school (Intercept) 2.88365 1.69813
     school (Intercept) 95.17339 9.75569
     Residual 0.96837 0.98406
    Number of obs: 1200, groups: class:school, 24; school, 6 

I didn't included fixed effects as my question is just about the random effects.

I have some intuition on how to interpret the estimated school variance. We can think of each school as having it's own intercept, and those values follow a normal distribution. The standard deviation of that distribution is 9.76.

I have a harder time understanding what the value for class:school means. It makes sense to me to think that we also want to allow each class to also have it's own intercept. But what is the value 1.69? Since classes are nested within schools, I would think that all 6 collections of classes have their own distribution with their own SD. What does this singular value mean?

Also, from an interpretation standpoint, since the σ\_school value is much higher than the σ\_class value, is the interpretation that average extraversion varies a lot between schools, but does not vary much between classes within a school?",1,statistics,54935,,How To Interpret lme4 Variance Parameters in 3-Level Models,https://www.reddit.com/r/statistics/comments/8lkojc/how_to_interpret_lme4_variance_parameters_in/,all_ads,2018-05-23 20:22:29,10 days 05:13:05.036584000,
" Hi everyone! First of all, sorry if this is not the right place to ask about this! I'm working on a project, but I've been thrown into SPSS without much knowledge on it or in statistics in general, so I'm pretty lost right now. The thing is I have the following set of variables regarding some emails:

* The first two ones represent countries: US, UK, FR, IT...and so on.
* The third one represents email domains: [gmail.com](https://gmail.com/), [hotmail.com](https://hotmail.com/)...
* The fourth is coded 1, 2, 3, 4, 5, 6 or 7, based on time intervals.
* The final one is coded 1 or 0, if said email was spam or not.

I'd like to know the influence of the first 4 variables in the final one, in order to be able to build a pattern of spam emails \(e.g. US, [gmail.com](https://gmail.com/), 2\). How could I do this taking into account that the first 3 ones are string variables? I tried recoding them into numbers, but I don't know how to build the model after that. Could anybody help me out with this?

Thanks a lot!",1,1527119513.0,8lkfqu,False," Hi everyone! First of all, sorry if this is not the right place to ask about this! I'm working on a project, but I've been thrown into SPSS without much knowledge on it or in statistics in general, so I'm pretty lost right now. The thing is I have the following set of variables regarding some emails:

* The first two ones represent countries: US, UK, FR, IT...and so on.
* The third one represents email domains: [gmail.com](https://gmail.com/), [hotmail.com](https://hotmail.com/)...
* The fourth is coded 1, 2, 3, 4, 5, 6 or 7, based on time intervals.
* The final one is coded 1 or 0, if said email was spam or not.

I'd like to know the influence of the first 4 variables in the final one, in order to be able to build a pattern of spam emails \(e.g. US, [gmail.com](https://gmail.com/), 2\). How could I do this taking into account that the first 3 ones are string variables? I tried recoding them into numbers, but I don't know how to build the model after that. Could anybody help me out with this?

Thanks a lot!",0," Hi everyone! First of all, sorry if this is not the right place to ask about this! I'm working on a project, but I've been thrown into SPSS without much knowledge on it or in statistics in general, so I'm pretty lost right now. The thing is I have the following set of variables regarding some emails:

* The first two ones represent countries: US, UK, FR, IT...and so on.
* The third one represents email domains: [gmail.com](https://gmail.com/), [hotmail.com](https://hotmail.com/)...
* The fourth is coded 1, 2, 3, 4, 5, 6 or 7, based on time intervals.
* The final one is coded 1 or 0, if said email was spam or not.

I'd like to know the influence of the first 4 variables in the final one, in order to be able to build a pattern of spam emails \(e.g. US, [gmail.com](https://gmail.com/), 2\). How could I do this taking into account that the first 3 ones are string variables? I tried recoding them into numbers, but I don't know how to build the model after that. Could anybody help me out with this?

Thanks a lot!",1,statistics,54935,,Using string variables on a logistic binary regression.,https://www.reddit.com/r/statistics/comments/8lkfqu/using_string_variables_on_a_logistic_binary/,all_ads,2018-05-23 19:51:53,10 days 05:43:41.036584000,
"Hi  

I did an experiment for a study, where 19 individuals each answered 126 item, and I recorded their reaction time for each one.  

I need some help on the stats  

The research director asked us to remove item and individuals that are out of standards. He sets the standard at \+/\- 2 standard deviation.  

I did the average reaction time of each individuals \(from their 126 answers\), and the average reaction time of each item from the 19 individuals  

Then I did the average reaction time for all the group \(19 x 126 item\) and did the standard deviation for this group  

Now I don't know if i should remove item or individuals by comparing the average of the item/individual with the average of the group of item/individual \+/\- 2 SD

I welcome any help I can get on this, I nearly finished my project, and the director is not really open to questions.",0,1527118347.0,8lkaa9,False,"Hi  

I did an experiment for a study, where 19 individuals each answered 126 item, and I recorded their reaction time for each one.  

I need some help on the stats  

The research director asked us to remove item and individuals that are out of standards. He sets the standard at \+/\- 2 standard deviation.  

I did the average reaction time of each individuals \(from their 126 answers\), and the average reaction time of each item from the 19 individuals  

Then I did the average reaction time for all the group \(19 x 126 item\) and did the standard deviation for this group  

Now I don't know if i should remove item or individuals by comparing the average of the item/individual with the average of the group of item/individual \+/\- 2 SD

I welcome any help I can get on this, I nearly finished my project, and the director is not really open to questions.",0,"Hi  

I did an experiment for a study, where 19 individuals each answered 126 item, and I recorded their reaction time for each one.  

I need some help on the stats  

The research director asked us to remove item and individuals that are out of standards. He sets the standard at \+/\- 2 standard deviation.  

I did the average reaction time of each individuals \(from their 126 answers\), and the average reaction time of each item from the 19 individuals  

Then I did the average reaction time for all the group \(19 x 126 item\) and did the standard deviation for this group  

Now I don't know if i should remove item or individuals by comparing the average of the item/individual with the average of the group of item/individual \+/\- 2 SD

I welcome any help I can get on this, I nearly finished my project, and the director is not really open to questions.",1,statistics,54935,,A question about data processing for my research,https://www.reddit.com/r/statistics/comments/8lkaa9/a_question_about_data_processing_for_my_research/,all_ads,2018-05-23 19:32:27,10 days 06:03:07.036584000,
"I have a set of 3 data points where X represents the amount of gunpowder in a cartridge (by weight in grains), and Y represents the peak chamber pressure of the round when it is fired:

* (0, 200) - no powder, but >0 pressure because the casing still has a primer ignition when the trigger is pulled.
* (23.2, 37400)
* (25.8, 51200)

I'm trying to decide on which type of equation to use to find a curve that intersects each of the points in a way that allows me to most accurately estimate chamber pressure for any given powder charge above 0.

y = Ae^Bx + C seems like a good option, but then so does y = Ar^x + C.

Perhaps these are effectively the same, and somebody can educate me on reasons to use one or other other?",6,1527116796.0,8lk34i,False,"I have a set of 3 data points where X represents the amount of gunpowder in a cartridge (by weight in grains), and Y represents the peak chamber pressure of the round when it is fired:

* (0, 200) - no powder, but >0 pressure because the casing still has a primer ignition when the trigger is pulled.
* (23.2, 37400)
* (25.8, 51200)

I'm trying to decide on which type of equation to use to find a curve that intersects each of the points in a way that allows me to most accurately estimate chamber pressure for any given powder charge above 0.

y = Ae^Bx + C seems like a good option, but then so does y = Ar^x + C.

Perhaps these are effectively the same, and somebody can educate me on reasons to use one or other other?",0,"I have a set of 3 data points where X represents the amount of gunpowder in a cartridge (by weight in grains), and Y represents the peak chamber pressure of the round when it is fired:

* (0, 200) - no powder, but >0 pressure because the casing still has a primer ignition when the trigger is pulled.
* (23.2, 37400)
* (25.8, 51200)

I'm trying to decide on which type of equation to use to find a curve that intersects each of the points in a way that allows me to most accurately estimate chamber pressure for any given powder charge above 0.

y = Ae^Bx + C seems like a good option, but then so does y = Ar^x + C.

Perhaps these are effectively the same, and somebody can educate me on reasons to use one or other other?",1,statistics,54935,,Help me resolve some model uncertainty for a real-world application?,https://www.reddit.com/r/statistics/comments/8lk34i/help_me_resolve_some_model_uncertainty_for_a/,all_ads,2018-05-23 19:06:36,10 days 06:28:58.036584000,
"Hello everyone,

I have a question for the experienced people out there :

When you get started with a dataset, - let's say tabular data concerning credit fraud, churns or insurance - you need to analyze, what are your go-to reflexes and tools?

What are some tests you can rely on and interprete further down the road?

Thanks, a newbie",1,1527113601.0,8ljohf,False,"Hello everyone,

I have a question for the experienced people out there :

When you get started with a dataset, - let's say tabular data concerning credit fraud, churns or insurance - you need to analyze, what are your go-to reflexes and tools?

What are some tests you can rely on and interprete further down the road?

Thanks, a newbie",0,"Hello everyone,

I have a question for the experienced people out there :

When you get started with a dataset, - let's say tabular data concerning credit fraud, churns or insurance - you need to analyze, what are your go-to reflexes and tools?

What are some tests you can rely on and interprete further down the road?

Thanks, a newbie",0,statistics,54935,,Exploratory data analysis and tests,https://www.reddit.com/r/statistics/comments/8ljohf/exploratory_data_analysis_and_tests/,all_ads,2018-05-23 18:13:21,10 days 07:22:13.036584000,
"Sorry if this is not written with proper terminology, I am new to this and not exactly sure what it is I am looking for.

I have a data set of census tracts (the observations). In each census tracts, there are two values 1) the number of all tweets geocoded from within that tract and 2) the number of tweets about a particular topic, say, Trump.

I am using a calculation that is:

( [trump tweets in the census tract] / [sum of all trump tweets] ) / ( [ tweets in the census tract] / [sum of all tweets] )

There are census tracts where 75% of the tweets are about Trump but that only means three out of four tweets are about him. Whereas there are other census tracts where maybe only 10% of the tweets are about Trump but there are 60 tweets. **Is there a way to give greater weight to the census tracts that have a total number of observations?**

Again, if anything isn't clear, please let me know and I will try to clarify.

",1,1527111380.0,8ljemk,False,"Sorry if this is not written with proper terminology, I am new to this and not exactly sure what it is I am looking for.

I have a data set of census tracts (the observations). In each census tracts, there are two values 1) the number of all tweets geocoded from within that tract and 2) the number of tweets about a particular topic, say, Trump.

I am using a calculation that is:

( [trump tweets in the census tract] / [sum of all trump tweets] ) / ( [ tweets in the census tract] / [sum of all tweets] )

There are census tracts where 75% of the tweets are about Trump but that only means three out of four tweets are about him. Whereas there are other census tracts where maybe only 10% of the tweets are about Trump but there are 60 tweets. **Is there a way to give greater weight to the census tracts that have a total number of observations?**

Again, if anything isn't clear, please let me know and I will try to clarify.

",0,"Sorry if this is not written with proper terminology, I am new to this and not exactly sure what it is I am looking for.

I have a data set of census tracts (the observations). In each census tracts, there are two values 1) the number of all tweets geocoded from within that tract and 2) the number of tweets about a particular topic, say, Trump.

I am using a calculation that is:

( [trump tweets in the census tract] / [sum of all trump tweets] ) / ( [ tweets in the census tract] / [sum of all tweets] )

There are census tracts where 75% of the tweets are about Trump but that only means three out of four tweets are about him. Whereas there are other census tracts where maybe only 10% of the tweets are about Trump but there are 60 tweets. **Is there a way to give greater weight to the census tracts that have a total number of observations?**

Again, if anything isn't clear, please let me know and I will try to clarify.

",1,statistics,54935,,Is there a way I can give a greater weight to records with higher total observations for my odds ratio?,https://www.reddit.com/r/statistics/comments/8ljemk/is_there_a_way_i_can_give_a_greater_weight_to/,all_ads,2018-05-23 17:36:20,10 days 07:59:14.036584000,
"My dependent variable is first\-differenced \(log\-transformed returns\), and my two dependent variables are stationary at levels. The dependent variable is not. I am running a GARCH\-model. Have seen people do this before, but not sure if it allowed, since they are not in the same order. If it makes any difference, the dependent variables are standardized with mean = 0 and std = 1.

Would love help, thanks in advance!",2,1527104476.0,8linx9,False,"My dependent variable is first\-differenced \(log\-transformed returns\), and my two dependent variables are stationary at levels. The dependent variable is not. I am running a GARCH\-model. Have seen people do this before, but not sure if it allowed, since they are not in the same order. If it makes any difference, the dependent variables are standardized with mean = 0 and std = 1.

Would love help, thanks in advance!",0,"My dependent variable is first\-differenced \(log\-transformed returns\), and my two dependent variables are stationary at levels. The dependent variable is not. I am running a GARCH\-model. Have seen people do this before, but not sure if it allowed, since they are not in the same order. If it makes any difference, the dependent variables are standardized with mean = 0 and std = 1.

Would love help, thanks in advance!",1,statistics,54935,,Can I run a model with a first-differenced dependent variable and independent variable in levels?,https://www.reddit.com/r/statistics/comments/8linx9/can_i_run_a_model_with_a_firstdifferenced/,all_ads,2018-05-23 15:41:16,10 days 09:54:18.036584000,
"So for my homework I need to do a comparation of M-W and t test for two independent samples with simulations (with R). 
I have a problem how to start. Do I simulate a samples with random numbers or do I simulate the sums for the two samples. Also how should my simulation look under alternative hypothesis. I looked into some literature but I am only more confused.

Any hints maybe? :)",8,1527090298.0,8lhk8s,False,"So for my homework I need to do a comparation of M-W and t test for two independent samples with simulations (with R). 
I have a problem how to start. Do I simulate a samples with random numbers or do I simulate the sums for the two samples. Also how should my simulation look under alternative hypothesis. I looked into some literature but I am only more confused.

Any hints maybe? :)",0,"So for my homework I need to do a comparation of M-W and t test for two independent samples with simulations (with R). 
I have a problem how to start. Do I simulate a samples with random numbers or do I simulate the sums for the two samples. Also how should my simulation look under alternative hypothesis. I looked into some literature but I am only more confused.

Any hints maybe? :)",2,statistics,54935,,Mann-whitney and t test on rank comparation,https://www.reddit.com/r/statistics/comments/8lhk8s/mannwhitney_and_t_test_on_rank_comparation/,all_ads,2018-05-23 11:44:58,10 days 13:50:36.036584000,
"My boss is setting up a survey of 300 locations and of around 50,000 of our subscribers. The survey will contain 6 questions about customer satisfaction.

Right now, he wants to do 30 locations (10% of the network) and 20 customers per location. Now, intuitively it seems to me like a decent spread but the decision on this entirely lacked any methodology and they just kinda followed their gut.

Firstly, I'd like to ask... is this a decent/reasonable spread? Also, can anyone explain a bit about sampling methodology or point in a direction about where to find this information?",20,1527082027.0,8lgwc1,False,"My boss is setting up a survey of 300 locations and of around 50,000 of our subscribers. The survey will contain 6 questions about customer satisfaction.

Right now, he wants to do 30 locations (10% of the network) and 20 customers per location. Now, intuitively it seems to me like a decent spread but the decision on this entirely lacked any methodology and they just kinda followed their gut.

Firstly, I'd like to ask... is this a decent/reasonable spread? Also, can anyone explain a bit about sampling methodology or point in a direction about where to find this information?",0,"My boss is setting up a survey of 300 locations and of around 50,000 of our subscribers. The survey will contain 6 questions about customer satisfaction.

Right now, he wants to do 30 locations (10% of the network) and 20 customers per location. Now, intuitively it seems to me like a decent spread but the decision on this entirely lacked any methodology and they just kinda followed their gut.

Firstly, I'd like to ask... is this a decent/reasonable spread? Also, can anyone explain a bit about sampling methodology or point in a direction about where to find this information?",2,statistics,54935,,Sample Size for Customer Survey,https://www.reddit.com/r/statistics/comments/8lgwc1/sample_size_for_customer_survey/,all_ads,2018-05-23 09:27:07,10 days 16:08:27.036584000,
"I recently attended a talk about imputing missing data using machine learning and during the Q&A a random audience member commented that when using machine learning (or any predictive model) for imputation, we should use the resulting probability distribution to impute rather than the highest prediction probability. 

From what I understand, the commenter was saying if the model says there is a 60% chance it is ""A"" vs ""B."" Rather than assigning the missing value ""A"", you should flip a weighted coin (60%/40%) and impute the outcome that way.

Can anyone explain the reasoning here? Wouldn't it make sense to impute with the best prediction?",12,1527034263.0,8lb6jm,False,"I recently attended a talk about imputing missing data using machine learning and during the Q&A a random audience member commented that when using machine learning (or any predictive model) for imputation, we should use the resulting probability distribution to impute rather than the highest prediction probability. 

From what I understand, the commenter was saying if the model says there is a 60% chance it is ""A"" vs ""B."" Rather than assigning the missing value ""A"", you should flip a weighted coin (60%/40%) and impute the outcome that way.

Can anyone explain the reasoning here? Wouldn't it make sense to impute with the best prediction?",0,"I recently attended a talk about imputing missing data using machine learning and during the Q&A a random audience member commented that when using machine learning (or any predictive model) for imputation, we should use the resulting probability distribution to impute rather than the highest prediction probability. 

From what I understand, the commenter was saying if the model says there is a 60% chance it is ""A"" vs ""B."" Rather than assigning the missing value ""A"", you should flip a weighted coin (60%/40%) and impute the outcome that way.

Can anyone explain the reasoning here? Wouldn't it make sense to impute with the best prediction?",22,statistics,54935,,How to impute missing data using a predictive model?,https://www.reddit.com/r/statistics/comments/8lb6jm/how_to_impute_missing_data_using_a_predictive/,all_ads,2018-05-22 20:11:03,11 days 05:24:31.036584000,
"Is it possible to manually compute an ARIMA forecast in Excel, without having to use R or plugins? I have searched quite a lot for a thorough tutorial or guide, but almost every ressource is based on programming.",8,1527081811.0,8lgvod,False,"Is it possible to manually compute an ARIMA forecast in Excel, without having to use R or plugins? I have searched quite a lot for a thorough tutorial or guide, but almost every ressource is based on programming.",0,"Is it possible to manually compute an ARIMA forecast in Excel, without having to use R or plugins? I have searched quite a lot for a thorough tutorial or guide, but almost every ressource is based on programming.",0,statistics,54935,,ARIMA in Excel,https://www.reddit.com/r/statistics/comments/8lgvod/arima_in_excel/,all_ads,2018-05-23 09:23:31,10 days 16:12:03.036584000,
So I am comparing the mean damage amounts for specific weather types of the entire population of the US versus the means for a certain state. Do I do just a simple t test between two independent groups? What info do I need to get a confidence interval? ,4,1527046594.0,8lcw4t,False,So I am comparing the mean damage amounts for specific weather types of the entire population of the US versus the means for a certain state. Do I do just a simple t test between two independent groups? What info do I need to get a confidence interval? ,0,So I am comparing the mean damage amounts for specific weather types of the entire population of the US versus the means for a certain state. Do I do just a simple t test between two independent groups? What info do I need to get a confidence interval? ,5,statistics,54935,,Doing a simple statistical test for a research paper and need some help!,https://www.reddit.com/r/statistics/comments/8lcw4t/doing_a_simple_statistical_test_for_a_research/,all_ads,2018-05-22 23:36:34,11 days 01:59:00.036584000,
"I'm putting together a spreadsheet that helps determine how my opponent in fantasy baseball played one week versus his average over the current season. I do this by finding the percent deviation for each category we use.

EX: He averages 10 Homeruns per week and only hit 7 this week. That gives us a -30% difference. He also averages 30 RBIs per week and got 35 this week for a +17% difference. I do this for the other 10 categories then average these values for an overall % difference.

Is this the best way to do this? How can I factor in value magnitudes? It seems the lower value categories like Home Runs and Stolen Bases carry bigger weights than the larger valued categories due to the larger variances that arise more easily, i.e. it is much easier to get a -50% difference in Stolen Bases than in RBIs.",1,1527050472.0,8ldf4c,False,"I'm putting together a spreadsheet that helps determine how my opponent in fantasy baseball played one week versus his average over the current season. I do this by finding the percent deviation for each category we use.

EX: He averages 10 Homeruns per week and only hit 7 this week. That gives us a -30% difference. He also averages 30 RBIs per week and got 35 this week for a +17% difference. I do this for the other 10 categories then average these values for an overall % difference.

Is this the best way to do this? How can I factor in value magnitudes? It seems the lower value categories like Home Runs and Stolen Bases carry bigger weights than the larger valued categories due to the larger variances that arise more easily, i.e. it is much easier to get a -50% difference in Stolen Bases than in RBIs.",0,"I'm putting together a spreadsheet that helps determine how my opponent in fantasy baseball played one week versus his average over the current season. I do this by finding the percent deviation for each category we use.

EX: He averages 10 Homeruns per week and only hit 7 this week. That gives us a -30% difference. He also averages 30 RBIs per week and got 35 this week for a +17% difference. I do this for the other 10 categories then average these values for an overall % difference.

Is this the best way to do this? How can I factor in value magnitudes? It seems the lower value categories like Home Runs and Stolen Bases carry bigger weights than the larger valued categories due to the larger variances that arise more easily, i.e. it is much easier to get a -50% difference in Stolen Bases than in RBIs.",3,statistics,54935,,Overall deviation of separate categories including small and large values,https://www.reddit.com/r/statistics/comments/8ldf4c/overall_deviation_of_separate_categories/,all_ads,2018-05-23 00:41:12,11 days 00:54:22.036584000,
"I'm very much intrigued by this study but want to look at another medical condition. For somebody that has very limited knowledge on statistics, how difficult would it be to replicate this study?

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2984379/",4,1527063122.0,8ley4l,False,"I'm very much intrigued by this study but want to look at another medical condition. For somebody that has very limited knowledge on statistics, how difficult would it be to replicate this study?

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2984379/",0,"I'm very much intrigued by this study but want to look at another medical condition. For somebody that has very limited knowledge on statistics, how difficult would it be to replicate this study?

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2984379/",2,statistics,54935,,"How difficult is it to replicate this study, but with a different variable?",https://www.reddit.com/r/statistics/comments/8ley4l/how_difficult_is_it_to_replicate_this_study_but/,all_ads,2018-05-23 04:12:02,10 days 21:23:32.036584000,
"More background: I conducted a canonical correlation analysis and found the squared structure coefficients for each variable in the two variable sets. It is my understanding that these are eta squared effect sizes but I am unsure what chart to use when determining if it is small, medium, or large. Please help! Big thanks in advance. ",0,1527072341.0,8lfyfz,False,"More background: I conducted a canonical correlation analysis and found the squared structure coefficients for each variable in the two variable sets. It is my understanding that these are eta squared effect sizes but I am unsure what chart to use when determining if it is small, medium, or large. Please help! Big thanks in advance. ",0,"More background: I conducted a canonical correlation analysis and found the squared structure coefficients for each variable in the two variable sets. It is my understanding that these are eta squared effect sizes but I am unsure what chart to use when determining if it is small, medium, or large. Please help! Big thanks in advance. ",1,statistics,54935,,"What is considered small, medium, large effect size for an eta squared effect size from a canonical correlation analysis.",https://www.reddit.com/r/statistics/comments/8lfyfz/what_is_considered_small_medium_large_effect_size/,all_ads,2018-05-23 06:45:41,10 days 18:49:53.036584000,
"This is a bit complicated, but please bear with me. I'll get to the point as quickly as I can without sacrificing clarity.

I'm working on a project to era-adjust NFL statistics using standard deviations from the mean, rather than simply multiplying by a variable to arrive at a normalized number. This has worked wonderfully in categories where generally speaking, each team has one player who performs the task. (For instance, punting.)

The problem I'm having is dealing with datasets where there's a massive right skew. I can't figure out how to come up with a centerpoint from which to adjust.

Here's an example: In 2017, 316 different people had a rushing attempt in an NFL game. 127 of them had five or fewer attempts, while eight of them had between 250-325 attempts. 200 or so would be a normal-ish number of rushing attempts for a starting NFL running back, so you can see there's an enormous skew at work.

With a dataset like this, the mean (~43.5) doesn't seem like an accurate representation of a ""typical"" rusher. The median (9) is even less representative, IMO. I've looked into using IQR, but I still feel like that huge skew is affecting the numbers too much. Even if I eliminate the bottom 25%, a third of my remaining dataset has fewer than ten rushing attempts. I've considered arbitrarily picking a value and eliminating all the data below that number, but that sort of ruins my plan to do this completely objectively.

What can I do in order to take a dataset with a huge skew and come up with a representative mean-like number and, similarly, a representative standard deviation? I'm running out of ideas.

[Here's a link](https://www.pro-football-reference.com/years/2017/rushing.htm) to a sample dataset. I'm looking to come up with a useful sample-wide centerpoint and SD for Att.

Thanks in advance.",2,1527041733.0,8lc7x7,False,"This is a bit complicated, but please bear with me. I'll get to the point as quickly as I can without sacrificing clarity.

I'm working on a project to era-adjust NFL statistics using standard deviations from the mean, rather than simply multiplying by a variable to arrive at a normalized number. This has worked wonderfully in categories where generally speaking, each team has one player who performs the task. (For instance, punting.)

The problem I'm having is dealing with datasets where there's a massive right skew. I can't figure out how to come up with a centerpoint from which to adjust.

Here's an example: In 2017, 316 different people had a rushing attempt in an NFL game. 127 of them had five or fewer attempts, while eight of them had between 250-325 attempts. 200 or so would be a normal-ish number of rushing attempts for a starting NFL running back, so you can see there's an enormous skew at work.

With a dataset like this, the mean (~43.5) doesn't seem like an accurate representation of a ""typical"" rusher. The median (9) is even less representative, IMO. I've looked into using IQR, but I still feel like that huge skew is affecting the numbers too much. Even if I eliminate the bottom 25%, a third of my remaining dataset has fewer than ten rushing attempts. I've considered arbitrarily picking a value and eliminating all the data below that number, but that sort of ruins my plan to do this completely objectively.

What can I do in order to take a dataset with a huge skew and come up with a representative mean-like number and, similarly, a representative standard deviation? I'm running out of ideas.

[Here's a link](https://www.pro-football-reference.com/years/2017/rushing.htm) to a sample dataset. I'm looking to come up with a useful sample-wide centerpoint and SD for Att.

Thanks in advance.",0,"This is a bit complicated, but please bear with me. I'll get to the point as quickly as I can without sacrificing clarity.

I'm working on a project to era-adjust NFL statistics using standard deviations from the mean, rather than simply multiplying by a variable to arrive at a normalized number. This has worked wonderfully in categories where generally speaking, each team has one player who performs the task. (For instance, punting.)

The problem I'm having is dealing with datasets where there's a massive right skew. I can't figure out how to come up with a centerpoint from which to adjust.

Here's an example: In 2017, 316 different people had a rushing attempt in an NFL game. 127 of them had five or fewer attempts, while eight of them had between 250-325 attempts. 200 or so would be a normal-ish number of rushing attempts for a starting NFL running back, so you can see there's an enormous skew at work.

With a dataset like this, the mean (~43.5) doesn't seem like an accurate representation of a ""typical"" rusher. The median (9) is even less representative, IMO. I've looked into using IQR, but I still feel like that huge skew is affecting the numbers too much. Even if I eliminate the bottom 25%, a third of my remaining dataset has fewer than ten rushing attempts. I've considered arbitrarily picking a value and eliminating all the data below that number, but that sort of ruins my plan to do this completely objectively.

What can I do in order to take a dataset with a huge skew and come up with a representative mean-like number and, similarly, a representative standard deviation? I'm running out of ideas.

[Here's a link](https://www.pro-football-reference.com/years/2017/rushing.htm) to a sample dataset. I'm looking to come up with a useful sample-wide centerpoint and SD for Att.

Thanks in advance.",4,statistics,54935,,"Question regarding massive skews, means, and StDev",https://www.reddit.com/r/statistics/comments/8lc7x7/question_regarding_massive_skews_means_and_stdev/,all_ads,2018-05-22 22:15:33,11 days 03:20:01.036584000,
"My question revolves around understanding a breakdown of a change in an average rate. I am not sure if this belongs in r/statistics, but you all have always been so helpful in the past. Sorry for the long text, I have been at this on and off for 2 months and have hit a roadblock.  My head starts to hurt just thinking about it.  This data will eventually be used in an efficiency model to better allocate resources, but at the moment it is just gathering dust. 
**Situation:**  We have noticed a change in the average (weighted) rate of production for all of our facilities.  The rate is products/hour, and the rate is weighted based on total number of products produced.  In excel I use a the formula =sumproduct (Rate Column, Volume Column)/sum(Volume Column) to get the weighted average.  Then I take the difference between two periods’ weighted average to get the change in average rate.
 We have some equipment that is slower and some that is faster, same thing for the individual facilities (some are slower/some are faster).  Some equipment, while having a slower production rate, produce more product per year because it is used more (higher share in the weighting).  Overall, there are a little under 100 lines in total.  We have had some new initiatives in the facilities aimed at improving efficiency, not 100% sure they have all worked, but overall the result is that the average rate has increased.  Some facilities are producing more product, some facilities are producing less, some equipment has even had the rate slowed to improve quality, etc.  This really was a big change, and nobody was analyzing the results.  The equipment in the past has always produced items at a fairly constant rate, so this large of a change in the average rate is a weird occurrence to us. 
One theory is that this change is due purely to increased production rates (rate change), while other camps argue that rates are not changing all that much, but that the reason we are seeing a change is the company shift more production to the faster lines (a share change so to speak).  The answer is probably somewhere in the middle. What formula/model could we use to show what product(s) are driving the average rate change, and is it through real rate changes or a shift in the share of production to faster/slower equipment?  I have used variance analysis (volume, price, mix) in the past, but have never tried to apply it directly to a scenario like this (a rate). I tried using convex combination, but that is a stretch, especially since we can have negative changes in rates and share.  
**Example:** If we saw a 50 unit per hour change in the average production rate, what amount is due to rate change and what amount is due to a shift in production to faster lines?  I would want to be able to say:
* 40 of the 50 unit per hour change is due to increase in production rates
* the remaining 10 of the 50 is due to shifting production to other lines. 
*Of those other lines, Line A and Line B are contributing 12 of the 50 because they are faster lines, while Line C (a slower, but higher quality producing line) is bringing the rate down by -2.

I can get the answer at the total level, but I cannot figure out how to show which specific lines/facilities are causing the major changes.  Identifying those would help us determine where to focus our resources.

What complicates this, is that you can have some lines decreasing rates, some lines closing down, new lines being built, and some lines increasing rates (so positive and negative changes to both rate and share). I have access to data which gives me production volume and run-time of lines at custom intervals (monthly data).  From this I can easily calculate production rate and share.  At the moment, we are not worried about defect rates, down time, etc.

As always, thank you for your help.
",0,1527054481.0,8ldxmn,False,"My question revolves around understanding a breakdown of a change in an average rate. I am not sure if this belongs in r/statistics, but you all have always been so helpful in the past. Sorry for the long text, I have been at this on and off for 2 months and have hit a roadblock.  My head starts to hurt just thinking about it.  This data will eventually be used in an efficiency model to better allocate resources, but at the moment it is just gathering dust. 
**Situation:**  We have noticed a change in the average (weighted) rate of production for all of our facilities.  The rate is products/hour, and the rate is weighted based on total number of products produced.  In excel I use a the formula =sumproduct (Rate Column, Volume Column)/sum(Volume Column) to get the weighted average.  Then I take the difference between two periods’ weighted average to get the change in average rate.
 We have some equipment that is slower and some that is faster, same thing for the individual facilities (some are slower/some are faster).  Some equipment, while having a slower production rate, produce more product per year because it is used more (higher share in the weighting).  Overall, there are a little under 100 lines in total.  We have had some new initiatives in the facilities aimed at improving efficiency, not 100% sure they have all worked, but overall the result is that the average rate has increased.  Some facilities are producing more product, some facilities are producing less, some equipment has even had the rate slowed to improve quality, etc.  This really was a big change, and nobody was analyzing the results.  The equipment in the past has always produced items at a fairly constant rate, so this large of a change in the average rate is a weird occurrence to us. 
One theory is that this change is due purely to increased production rates (rate change), while other camps argue that rates are not changing all that much, but that the reason we are seeing a change is the company shift more production to the faster lines (a share change so to speak).  The answer is probably somewhere in the middle. What formula/model could we use to show what product(s) are driving the average rate change, and is it through real rate changes or a shift in the share of production to faster/slower equipment?  I have used variance analysis (volume, price, mix) in the past, but have never tried to apply it directly to a scenario like this (a rate). I tried using convex combination, but that is a stretch, especially since we can have negative changes in rates and share.  
**Example:** If we saw a 50 unit per hour change in the average production rate, what amount is due to rate change and what amount is due to a shift in production to faster lines?  I would want to be able to say:
* 40 of the 50 unit per hour change is due to increase in production rates
* the remaining 10 of the 50 is due to shifting production to other lines. 
*Of those other lines, Line A and Line B are contributing 12 of the 50 because they are faster lines, while Line C (a slower, but higher quality producing line) is bringing the rate down by -2.

I can get the answer at the total level, but I cannot figure out how to show which specific lines/facilities are causing the major changes.  Identifying those would help us determine where to focus our resources.

What complicates this, is that you can have some lines decreasing rates, some lines closing down, new lines being built, and some lines increasing rates (so positive and negative changes to both rate and share). I have access to data which gives me production volume and run-time of lines at custom intervals (monthly data).  From this I can easily calculate production rate and share.  At the moment, we are not worried about defect rates, down time, etc.

As always, thank you for your help.
",0,"My question revolves around understanding a breakdown of a change in an average rate. I am not sure if this belongs in r/statistics, but you all have always been so helpful in the past. Sorry for the long text, I have been at this on and off for 2 months and have hit a roadblock.  My head starts to hurt just thinking about it.  This data will eventually be used in an efficiency model to better allocate resources, but at the moment it is just gathering dust. 
**Situation:**  We have noticed a change in the average (weighted) rate of production for all of our facilities.  The rate is products/hour, and the rate is weighted based on total number of products produced.  In excel I use a the formula =sumproduct (Rate Column, Volume Column)/sum(Volume Column) to get the weighted average.  Then I take the difference between two periods’ weighted average to get the change in average rate.
 We have some equipment that is slower and some that is faster, same thing for the individual facilities (some are slower/some are faster).  Some equipment, while having a slower production rate, produce more product per year because it is used more (higher share in the weighting).  Overall, there are a little under 100 lines in total.  We have had some new initiatives in the facilities aimed at improving efficiency, not 100% sure they have all worked, but overall the result is that the average rate has increased.  Some facilities are producing more product, some facilities are producing less, some equipment has even had the rate slowed to improve quality, etc.  This really was a big change, and nobody was analyzing the results.  The equipment in the past has always produced items at a fairly constant rate, so this large of a change in the average rate is a weird occurrence to us. 
One theory is that this change is due purely to increased production rates (rate change), while other camps argue that rates are not changing all that much, but that the reason we are seeing a change is the company shift more production to the faster lines (a share change so to speak).  The answer is probably somewhere in the middle. What formula/model could we use to show what product(s) are driving the average rate change, and is it through real rate changes or a shift in the share of production to faster/slower equipment?  I have used variance analysis (volume, price, mix) in the past, but have never tried to apply it directly to a scenario like this (a rate). I tried using convex combination, but that is a stretch, especially since we can have negative changes in rates and share.  
**Example:** If we saw a 50 unit per hour change in the average production rate, what amount is due to rate change and what amount is due to a shift in production to faster lines?  I would want to be able to say:
* 40 of the 50 unit per hour change is due to increase in production rates
* the remaining 10 of the 50 is due to shifting production to other lines. 
*Of those other lines, Line A and Line B are contributing 12 of the 50 because they are faster lines, while Line C (a slower, but higher quality producing line) is bringing the rate down by -2.

I can get the answer at the total level, but I cannot figure out how to show which specific lines/facilities are causing the major changes.  Identifying those would help us determine where to focus our resources.

What complicates this, is that you can have some lines decreasing rates, some lines closing down, new lines being built, and some lines increasing rates (so positive and negative changes to both rate and share). I have access to data which gives me production volume and run-time of lines at custom intervals (monthly data).  From this I can easily calculate production rate and share.  At the moment, we are not worried about defect rates, down time, etc.

As always, thank you for your help.
",2,statistics,54935,,Question on understanding breaking down of a weighted average rate change between two periods.,https://www.reddit.com/r/statistics/comments/8ldxmn/question_on_understanding_breaking_down_of_a/,all_ads,2018-05-23 01:48:01,10 days 23:47:33.036584000,
"For example, running an ad campaign on certain products and seeing an uplift in the sales of those products during this campaign.",24,1527010893.0,8l8qsm,False,"For example, running an ad campaign on certain products and seeing an uplift in the sales of those products during this campaign.",0,"For example, running an ad campaign on certain products and seeing an uplift in the sales of those products during this campaign.",14,statistics,54935,,How do I calculate the correlation between an event and sales uplift?,https://www.reddit.com/r/statistics/comments/8l8qsm/how_do_i_calculate_the_correlation_between_an/,all_ads,2018-05-22 13:41:33,11 days 11:54:01.036584000,
"I have some sets of data and I would like to compare their means.

For the moment I just calculated their means and compared them but I think that viewing each set as a sample of a bigger population and using a statistical test to compare their mean would be more appropriate.

I would like to hear some opinions regarding this approach.

Besides that, I am not sure what statistical test to use. I can't say that these data sets follow a normal distribution. The data is continuous and some sets have a few hundred items but some have less than 10.

Could you please recommend a statistical test for comparing the mean of two samples for which one is sufficiently large \(more than 30 items\) but the other one has less than 10?

I was thinking about using a T test but since I can't say that the populations follow normal distributions and the samples aren't big enough in all cases, I'm not sure if that's appropriate.",18,1527034549.0,8lb7v5,False,"I have some sets of data and I would like to compare their means.

For the moment I just calculated their means and compared them but I think that viewing each set as a sample of a bigger population and using a statistical test to compare their mean would be more appropriate.

I would like to hear some opinions regarding this approach.

Besides that, I am not sure what statistical test to use. I can't say that these data sets follow a normal distribution. The data is continuous and some sets have a few hundred items but some have less than 10.

Could you please recommend a statistical test for comparing the mean of two samples for which one is sufficiently large \(more than 30 items\) but the other one has less than 10?

I was thinking about using a T test but since I can't say that the populations follow normal distributions and the samples aren't big enough in all cases, I'm not sure if that's appropriate.",0,"I have some sets of data and I would like to compare their means.

For the moment I just calculated their means and compared them but I think that viewing each set as a sample of a bigger population and using a statistical test to compare their mean would be more appropriate.

I would like to hear some opinions regarding this approach.

Besides that, I am not sure what statistical test to use. I can't say that these data sets follow a normal distribution. The data is continuous and some sets have a few hundred items but some have less than 10.

Could you please recommend a statistical test for comparing the mean of two samples for which one is sufficiently large \(more than 30 items\) but the other one has less than 10?

I was thinking about using a T test but since I can't say that the populations follow normal distributions and the samples aren't big enough in all cases, I'm not sure if that's appropriate.",5,statistics,54935,,Statistical test for comparing populations means based on a big sample and a small one,https://www.reddit.com/r/statistics/comments/8lb7v5/statistical_test_for_comparing_populations_means/,all_ads,2018-05-22 20:15:49,11 days 05:19:45.036584000,
"**The Golf Course Requirement** 

Your company, F Design has been awarded the contract to build the new golf course in Town. The new golf course needs to configure to *maximise golfer enjoyment* and meet the town’s needs. You are the golf course architect and your boss has called on you for your modelling skills. 

You will need to design an **18 hole** course to fit within **42 acres** of land. The shape of the 42 acres land is not a constraint. The golf course must be configure to maintain its natural beauty without cutting down trees. Hence, the golf hole should be built around the landscape. 

A golfing hole is a section of a golf course. Each hole comprises an area called the tee from which golfers start each hole. Golfers strike the ball from the tee and are aiming to move the ball towards a cup at the other end of the hole. The cup is located somewhere on a green, a small area of grass cut short. In between the tee and the green is called the fairway. Golfers aim to keep the ball on the fairway \(its not much fun trying to hit the ball in the trees or lakes on the edge of the fairway\). Each strike of the ball is called a stroke. The objective of golfer is to use as few strokes as possible to get the ball into the cup. 

The par of a hole is the number of strokes a golfer should use to move the ball from the tee to the cup. Golf courses usually have par three, four and five holes. A par three hole will require a three par to move the ball to the cup. Similarly four and five par with require four par and five par respectively. Par three hole is usually shorter than a par four hole. In addition, par four and five holes can have doglegs. A dogleg is a bend in the fairway roughly half way along the hole. A dogleg hole are holes that built around an obstruction on the land like trees, bushes and lakes. Doglegs add variety and difficulty to the golfing experience, and most courses usually have one or two. 

Each hole will take up an area depending upon its length \(indicated by par\), presence of a dogleg and other factors. 

A standard clubhouse is also to be built within the golf course. The standard clubhouse require a land size of 2 acres including parking and a construction cost of $350,000. 

A recent international survey on golfer has revealed the following enjoyment index and estimated cost of building the par holes. The enjoyment index is a measure of golfer enjoyment and satisfaction with the golf design, play difficulty and facilities. 

| Kind of hole  | Par |Acreage taken by hole|enjoyability index|building cost|
|:-|:-|:-|:-|:-|
|Straight Par 5|5|3|2|$50000|
|Dogleg par 5 |5|3.5|1.5|$60000|
|Straight par 4|4|2|1.5|$40000|
|Dogleg par 4|4|2.5|2|$45000|
| Long par 3 |3|1|1.75|$35000|
|Short par 3|3|0.75|2.25|$30000|

Based on the International golf course standard, the new golf course must also meet the following requirements. The course should have at least: 

1. a\)  One straight par 5, 
2. b\)  One dogleg par 5, 
3. c\)  Two straight par 4, 
4. d\)  Two dogleg par 4, 
5. e\)  One long par 3, and 
6. f\)  One short par 3. 
7. g\)  The course should have no more than 4 par 5. 
8. h\)  The course should have no more than 14 par 4. 
9. i\)  The course should have no more than 4 par 3. 
10. j\)  The total par must be between 70 and 72. 
11. k\)  The total number of holes must be exactly 18. 
12. l\)  The total acreage must be between 36 and 42 acres. 

After some discussion with the business community and examine survey results across similar style golf course, The Golf group shareholders is considering an option to build an *expanded clubhouse*. The expanded clubhouse would add *four enjoyability* points. However, the expanded clubhouse will cost $500,000 and a land size of 4 acres including parking. 

The Golf group has secured a $1.2 million funding from shareholders to construct the golf course. 

The Golf group management has requested your company F Design to produce decision models that maximise the golfer enjoyment for the standard clubhouse and the expanded clubhouse. In particular, given the pressure from shareholders, the management wants to know with the constraints given can the expanded clubhouse be built? 

If the expanded clubhouse cannot be constructed with the current constrains, the management request you to put forward three options for the expanded clubhouse for consideration. For the options, you may allow to change \(1\) the size of the clubhouse, \(2\) the construction cost of the clubhouse and, \(3\) the budget. 

**Note:** 

1. The construction cost for the golf course is not the actual cost. It is purposely formulated for this case study only. 
2. The case study is written based on the assumption there is no solution for the expanded clubhouse. ",6,1527067694.0,8lfg8g,False,"**The Golf Course Requirement** 

Your company, F Design has been awarded the contract to build the new golf course in Town. The new golf course needs to configure to *maximise golfer enjoyment* and meet the town’s needs. You are the golf course architect and your boss has called on you for your modelling skills. 

You will need to design an **18 hole** course to fit within **42 acres** of land. The shape of the 42 acres land is not a constraint. The golf course must be configure to maintain its natural beauty without cutting down trees. Hence, the golf hole should be built around the landscape. 

A golfing hole is a section of a golf course. Each hole comprises an area called the tee from which golfers start each hole. Golfers strike the ball from the tee and are aiming to move the ball towards a cup at the other end of the hole. The cup is located somewhere on a green, a small area of grass cut short. In between the tee and the green is called the fairway. Golfers aim to keep the ball on the fairway \(its not much fun trying to hit the ball in the trees or lakes on the edge of the fairway\). Each strike of the ball is called a stroke. The objective of golfer is to use as few strokes as possible to get the ball into the cup. 

The par of a hole is the number of strokes a golfer should use to move the ball from the tee to the cup. Golf courses usually have par three, four and five holes. A par three hole will require a three par to move the ball to the cup. Similarly four and five par with require four par and five par respectively. Par three hole is usually shorter than a par four hole. In addition, par four and five holes can have doglegs. A dogleg is a bend in the fairway roughly half way along the hole. A dogleg hole are holes that built around an obstruction on the land like trees, bushes and lakes. Doglegs add variety and difficulty to the golfing experience, and most courses usually have one or two. 

Each hole will take up an area depending upon its length \(indicated by par\), presence of a dogleg and other factors. 

A standard clubhouse is also to be built within the golf course. The standard clubhouse require a land size of 2 acres including parking and a construction cost of $350,000. 

A recent international survey on golfer has revealed the following enjoyment index and estimated cost of building the par holes. The enjoyment index is a measure of golfer enjoyment and satisfaction with the golf design, play difficulty and facilities. 

| Kind of hole  | Par |Acreage taken by hole|enjoyability index|building cost|
|:-|:-|:-|:-|:-|
|Straight Par 5|5|3|2|$50000|
|Dogleg par 5 |5|3.5|1.5|$60000|
|Straight par 4|4|2|1.5|$40000|
|Dogleg par 4|4|2.5|2|$45000|
| Long par 3 |3|1|1.75|$35000|
|Short par 3|3|0.75|2.25|$30000|

Based on the International golf course standard, the new golf course must also meet the following requirements. The course should have at least: 

1. a\)  One straight par 5, 
2. b\)  One dogleg par 5, 
3. c\)  Two straight par 4, 
4. d\)  Two dogleg par 4, 
5. e\)  One long par 3, and 
6. f\)  One short par 3. 
7. g\)  The course should have no more than 4 par 5. 
8. h\)  The course should have no more than 14 par 4. 
9. i\)  The course should have no more than 4 par 3. 
10. j\)  The total par must be between 70 and 72. 
11. k\)  The total number of holes must be exactly 18. 
12. l\)  The total acreage must be between 36 and 42 acres. 

After some discussion with the business community and examine survey results across similar style golf course, The Golf group shareholders is considering an option to build an *expanded clubhouse*. The expanded clubhouse would add *four enjoyability* points. However, the expanded clubhouse will cost $500,000 and a land size of 4 acres including parking. 

The Golf group has secured a $1.2 million funding from shareholders to construct the golf course. 

The Golf group management has requested your company F Design to produce decision models that maximise the golfer enjoyment for the standard clubhouse and the expanded clubhouse. In particular, given the pressure from shareholders, the management wants to know with the constraints given can the expanded clubhouse be built? 

If the expanded clubhouse cannot be constructed with the current constrains, the management request you to put forward three options for the expanded clubhouse for consideration. For the options, you may allow to change \(1\) the size of the clubhouse, \(2\) the construction cost of the clubhouse and, \(3\) the budget. 

**Note:** 

1. The construction cost for the golf course is not the actual cost. It is purposely formulated for this case study only. 
2. The case study is written based on the assumption there is no solution for the expanded clubhouse. ",0,"**The Golf Course Requirement** 

Your company, F Design has been awarded the contract to build the new golf course in Town. The new golf course needs to configure to *maximise golfer enjoyment* and meet the town’s needs. You are the golf course architect and your boss has called on you for your modelling skills. 

You will need to design an **18 hole** course to fit within **42 acres** of land. The shape of the 42 acres land is not a constraint. The golf course must be configure to maintain its natural beauty without cutting down trees. Hence, the golf hole should be built around the landscape. 

A golfing hole is a section of a golf course. Each hole comprises an area called the tee from which golfers start each hole. Golfers strike the ball from the tee and are aiming to move the ball towards a cup at the other end of the hole. The cup is located somewhere on a green, a small area of grass cut short. In between the tee and the green is called the fairway. Golfers aim to keep the ball on the fairway \(its not much fun trying to hit the ball in the trees or lakes on the edge of the fairway\). Each strike of the ball is called a stroke. The objective of golfer is to use as few strokes as possible to get the ball into the cup. 

The par of a hole is the number of strokes a golfer should use to move the ball from the tee to the cup. Golf courses usually have par three, four and five holes. A par three hole will require a three par to move the ball to the cup. Similarly four and five par with require four par and five par respectively. Par three hole is usually shorter than a par four hole. In addition, par four and five holes can have doglegs. A dogleg is a bend in the fairway roughly half way along the hole. A dogleg hole are holes that built around an obstruction on the land like trees, bushes and lakes. Doglegs add variety and difficulty to the golfing experience, and most courses usually have one or two. 

Each hole will take up an area depending upon its length \(indicated by par\), presence of a dogleg and other factors. 

A standard clubhouse is also to be built within the golf course. The standard clubhouse require a land size of 2 acres including parking and a construction cost of $350,000. 

A recent international survey on golfer has revealed the following enjoyment index and estimated cost of building the par holes. The enjoyment index is a measure of golfer enjoyment and satisfaction with the golf design, play difficulty and facilities. 

| Kind of hole  | Par |Acreage taken by hole|enjoyability index|building cost|
|:-|:-|:-|:-|:-|
|Straight Par 5|5|3|2|$50000|
|Dogleg par 5 |5|3.5|1.5|$60000|
|Straight par 4|4|2|1.5|$40000|
|Dogleg par 4|4|2.5|2|$45000|
| Long par 3 |3|1|1.75|$35000|
|Short par 3|3|0.75|2.25|$30000|

Based on the International golf course standard, the new golf course must also meet the following requirements. The course should have at least: 

1. a\)  One straight par 5, 
2. b\)  One dogleg par 5, 
3. c\)  Two straight par 4, 
4. d\)  Two dogleg par 4, 
5. e\)  One long par 3, and 
6. f\)  One short par 3. 
7. g\)  The course should have no more than 4 par 5. 
8. h\)  The course should have no more than 14 par 4. 
9. i\)  The course should have no more than 4 par 3. 
10. j\)  The total par must be between 70 and 72. 
11. k\)  The total number of holes must be exactly 18. 
12. l\)  The total acreage must be between 36 and 42 acres. 

After some discussion with the business community and examine survey results across similar style golf course, The Golf group shareholders is considering an option to build an *expanded clubhouse*. The expanded clubhouse would add *four enjoyability* points. However, the expanded clubhouse will cost $500,000 and a land size of 4 acres including parking. 

The Golf group has secured a $1.2 million funding from shareholders to construct the golf course. 

The Golf group management has requested your company F Design to produce decision models that maximise the golfer enjoyment for the standard clubhouse and the expanded clubhouse. In particular, given the pressure from shareholders, the management wants to know with the constraints given can the expanded clubhouse be built? 

If the expanded clubhouse cannot be constructed with the current constrains, the management request you to put forward three options for the expanded clubhouse for consideration. For the options, you may allow to change \(1\) the size of the clubhouse, \(2\) the construction cost of the clubhouse and, \(3\) the budget. 

**Note:** 

1. The construction cost for the golf course is not the actual cost. It is purposely formulated for this case study only. 
2. The case study is written based on the assumption there is no solution for the expanded clubhouse. ",0,statistics,54935,,Anyone know how to tackle this Question-,https://www.reddit.com/r/statistics/comments/8lfg8g/anyone_know_how_to_tackle_this_question/,all_ads,2018-05-23 05:28:14,10 days 20:07:20.036584000,
"Hello! I am currently a college student pursuing a degree in Data Analytics (its a mix of comp sci classes and stats classes). I like statistics as it allows me to help people and solve problems using math. However, I have seen plenty of people say that they dislike their stats jobs. They mainly say that they are boring, require too much education, and feel pointless. Should I stay in stats or are they onto something? Thank you!

EDIT: I apologize for saying ""worthwhile"" instead of ""good"" or ""wise"". I now realize that ""worthwhile"" kind of diminishes the field as a whole. ",24,1526979227.0,8l5uli,False,"Hello! I am currently a college student pursuing a degree in Data Analytics (its a mix of comp sci classes and stats classes). I like statistics as it allows me to help people and solve problems using math. However, I have seen plenty of people say that they dislike their stats jobs. They mainly say that they are boring, require too much education, and feel pointless. Should I stay in stats or are they onto something? Thank you!

EDIT: I apologize for saying ""worthwhile"" instead of ""good"" or ""wise"". I now realize that ""worthwhile"" kind of diminishes the field as a whole. ",0,"Hello! I am currently a college student pursuing a degree in Data Analytics (its a mix of comp sci classes and stats classes). I like statistics as it allows me to help people and solve problems using math. However, I have seen plenty of people say that they dislike their stats jobs. They mainly say that they are boring, require too much education, and feel pointless. Should I stay in stats or are they onto something? Thank you!

EDIT: I apologize for saying ""worthwhile"" instead of ""good"" or ""wise"". I now realize that ""worthwhile"" kind of diminishes the field as a whole. ",18,statistics,54935,,Is statistics a worthwhile career choice?,https://www.reddit.com/r/statistics/comments/8l5uli/is_statistics_a_worthwhile_career_choice/,all_ads,2018-05-22 04:53:47,11 days 20:41:47.036584000,
"I have a question that may or may not have an answer, but here it is:

If Team A wins 95% of their home games, and Team B wins 77% of their away games, whats the % chance that Team A or Team B wins?

Team A would be the home team vs Team B.

Thanks!",4,1527036769.0,8lbi94,False,"I have a question that may or may not have an answer, but here it is:

If Team A wins 95% of their home games, and Team B wins 77% of their away games, whats the % chance that Team A or Team B wins?

Team A would be the home team vs Team B.

Thanks!",0,"I have a question that may or may not have an answer, but here it is:

If Team A wins 95% of their home games, and Team B wins 77% of their away games, whats the % chance that Team A or Team B wins?

Team A would be the home team vs Team B.

Thanks!",1,statistics,54935,,Statistically computing % odds a team wins in football?,https://www.reddit.com/r/statistics/comments/8lbi94/statistically_computing_odds_a_team_wins_in/,all_ads,2018-05-22 20:52:49,11 days 04:42:45.036584000,
"Hello everyone, 

one item on my latest survey study has some outliers. I asked people to report how much they think one ought to give to charity. The majority responded in the range of 0 \- 25, though one person each entered 35, 50, and 95. The mean is 5.65 and the SD is 7.1I get that one might exclude those outliers due to their statistical anomaly, but would I not need an argument to do so? Like it being error, it having been recorded under exceptional circumstances, or it just being part of a different population. 

But I have no reason to assume either of this as all values are valid possibilities present in the literature of charitable giving. 

My question: Do I have to exclude these values simply due to their statistical anomaly or can I keep them given I see no reason to exclude them?

Thanks!",5,1527000883.0,8l80v8,False,"Hello everyone, 

one item on my latest survey study has some outliers. I asked people to report how much they think one ought to give to charity. The majority responded in the range of 0 \- 25, though one person each entered 35, 50, and 95. The mean is 5.65 and the SD is 7.1I get that one might exclude those outliers due to their statistical anomaly, but would I not need an argument to do so? Like it being error, it having been recorded under exceptional circumstances, or it just being part of a different population. 

But I have no reason to assume either of this as all values are valid possibilities present in the literature of charitable giving. 

My question: Do I have to exclude these values simply due to their statistical anomaly or can I keep them given I see no reason to exclude them?

Thanks!",0,"Hello everyone, 

one item on my latest survey study has some outliers. I asked people to report how much they think one ought to give to charity. The majority responded in the range of 0 \- 25, though one person each entered 35, 50, and 95. The mean is 5.65 and the SD is 7.1I get that one might exclude those outliers due to their statistical anomaly, but would I not need an argument to do so? Like it being error, it having been recorded under exceptional circumstances, or it just being part of a different population. 

But I have no reason to assume either of this as all values are valid possibilities present in the literature of charitable giving. 

My question: Do I have to exclude these values simply due to their statistical anomaly or can I keep them given I see no reason to exclude them?

Thanks!",2,statistics,54935,,How to decide when to exclude outliers?,https://www.reddit.com/r/statistics/comments/8l80v8/how_to_decide_when_to_exclude_outliers/,all_ads,2018-05-22 10:54:43,11 days 14:40:51.036584000,
"I am analysing a dataset and trying to determine how much variance in the outcome variable (continuous) is accounted for by three variables of different types: age (continuous), sex (dichotomous), and education level (categorical ordinal).
I know that if all the variables were continuous, then I could perform a multiple regression analysis and look at R^2 values for estimates of explained variance for each predictor. 
I performed a linear regression model with the data types explained above (using fitlm in matlab which supports categorical predictors), but I get a coefficient for the continuous predictor (obviously) and a coefficient for each categorical variable (-1 in each category, if that makes sense). However, this approach seems to tell me that sex accounts for more variance than age, which seems wrong from looking at the data graphically. I then performed an ANOVA on the output of the regression model 
 
Anyway, I really want to understand how to analyse data like this with different types of data and most importantly, how interpret the results correctly! because I feel like this is a very common problem (and maybe very basic... but I'm banging my head on a wall!). I appreciate, any advice on ""quick and dirty"" ways to gain a good quantitative intuition and ""real ways"" to do it, or where to look for good examples? Thanks!",6,1526957270.0,8l38gg,False,"I am analysing a dataset and trying to determine how much variance in the outcome variable (continuous) is accounted for by three variables of different types: age (continuous), sex (dichotomous), and education level (categorical ordinal).
I know that if all the variables were continuous, then I could perform a multiple regression analysis and look at R^2 values for estimates of explained variance for each predictor. 
I performed a linear regression model with the data types explained above (using fitlm in matlab which supports categorical predictors), but I get a coefficient for the continuous predictor (obviously) and a coefficient for each categorical variable (-1 in each category, if that makes sense). However, this approach seems to tell me that sex accounts for more variance than age, which seems wrong from looking at the data graphically. I then performed an ANOVA on the output of the regression model 
 
Anyway, I really want to understand how to analyse data like this with different types of data and most importantly, how interpret the results correctly! because I feel like this is a very common problem (and maybe very basic... but I'm banging my head on a wall!). I appreciate, any advice on ""quick and dirty"" ways to gain a good quantitative intuition and ""real ways"" to do it, or where to look for good examples? Thanks!",0,"I am analysing a dataset and trying to determine how much variance in the outcome variable (continuous) is accounted for by three variables of different types: age (continuous), sex (dichotomous), and education level (categorical ordinal).
I know that if all the variables were continuous, then I could perform a multiple regression analysis and look at R^2 values for estimates of explained variance for each predictor. 
I performed a linear regression model with the data types explained above (using fitlm in matlab which supports categorical predictors), but I get a coefficient for the continuous predictor (obviously) and a coefficient for each categorical variable (-1 in each category, if that makes sense). However, this approach seems to tell me that sex accounts for more variance than age, which seems wrong from looking at the data graphically. I then performed an ANOVA on the output of the regression model 
 
Anyway, I really want to understand how to analyse data like this with different types of data and most importantly, how interpret the results correctly! because I feel like this is a very common problem (and maybe very basic... but I'm banging my head on a wall!). I appreciate, any advice on ""quick and dirty"" ways to gain a good quantitative intuition and ""real ways"" to do it, or where to look for good examples? Thanks!",14,statistics,54935,,"How to determine variance explained in a dependent variable from dependent variable of different types (continuous, categorical dichotomous, and categorical ordinal)?",https://www.reddit.com/r/statistics/comments/8l38gg/how_to_determine_variance_explained_in_a/,all_ads,2018-05-21 22:47:50,12 days 02:47:44.036584000,
"By EDA, I mean things like ""plot relationships between variables and the response"", ""imputed missing variables and fixed input issues"", and ""transform features to make relationships linear on the chance that we're going to actually use a linear model instead of an SVM/random forest/etc.""  I'm assuming you have a number of columns that makes it difficult to look at all of them at once.

Typically what I do is stick everything into a regression as a first step to see what pops up as statistically significant.  If it's a really big number of predictors, I might stick it into an elastic net or similar penalized regression that cuts features.",11,1526951295.0,8l2g1p,False,"By EDA, I mean things like ""plot relationships between variables and the response"", ""imputed missing variables and fixed input issues"", and ""transform features to make relationships linear on the chance that we're going to actually use a linear model instead of an SVM/random forest/etc.""  I'm assuming you have a number of columns that makes it difficult to look at all of them at once.

Typically what I do is stick everything into a regression as a first step to see what pops up as statistically significant.  If it's a really big number of predictors, I might stick it into an elastic net or similar penalized regression that cuts features.",0,"By EDA, I mean things like ""plot relationships between variables and the response"", ""imputed missing variables and fixed input issues"", and ""transform features to make relationships linear on the chance that we're going to actually use a linear model instead of an SVM/random forest/etc.""  I'm assuming you have a number of columns that makes it difficult to look at all of them at once.

Typically what I do is stick everything into a regression as a first step to see what pops up as statistically significant.  If it's a really big number of predictors, I might stick it into an elastic net or similar penalized regression that cuts features.",8,statistics,54935,,You've done all the EDA on your relatively complex problem. What's the first thing you do afterwards?,https://www.reddit.com/r/statistics/comments/8l2g1p/youve_done_all_the_eda_on_your_relatively_complex/,all_ads,2018-05-21 21:08:15,12 days 04:27:19.036584000,
"I'm currently doing an introductory course on statistics, and specifically a module on hypothesis testing. 

I can follow along with the examples just fine, but what I struggle with is intuitively understanding why H0 is rejected when the test statistic falls within the rejection region.

My current best understanding is as follows: if the test statistic (which is a standardised measure of how far a sample mean is removed from the population mean) falls within the rejection region (which is determined by how much confidence you want in the inference; significance level alpha) then it means that, since the distribution is normal, the sample mean differs from the population mean *due to something more than luck* (this is as far as my intuition goes 😐). 

Any ideas for how I can better understand what's going on here? Maybe (likely) I'm missing some basics that I need to go back to.",34,1526923693.0,8kzjdw,False,"I'm currently doing an introductory course on statistics, and specifically a module on hypothesis testing. 

I can follow along with the examples just fine, but what I struggle with is intuitively understanding why H0 is rejected when the test statistic falls within the rejection region.

My current best understanding is as follows: if the test statistic (which is a standardised measure of how far a sample mean is removed from the population mean) falls within the rejection region (which is determined by how much confidence you want in the inference; significance level alpha) then it means that, since the distribution is normal, the sample mean differs from the population mean *due to something more than luck* (this is as far as my intuition goes 😐). 

Any ideas for how I can better understand what's going on here? Maybe (likely) I'm missing some basics that I need to go back to.",0,"I'm currently doing an introductory course on statistics, and specifically a module on hypothesis testing. 

I can follow along with the examples just fine, but what I struggle with is intuitively understanding why H0 is rejected when the test statistic falls within the rejection region.

My current best understanding is as follows: if the test statistic (which is a standardised measure of how far a sample mean is removed from the population mean) falls within the rejection region (which is determined by how much confidence you want in the inference; significance level alpha) then it means that, since the distribution is normal, the sample mean differs from the population mean *due to something more than luck* (this is as far as my intuition goes 😐). 

Any ideas for how I can better understand what's going on here? Maybe (likely) I'm missing some basics that I need to go back to.",22,statistics,54935,,Please help me develop a better intuition for understanding the basics hypothesis testing,https://www.reddit.com/r/statistics/comments/8kzjdw/please_help_me_develop_a_better_intuition_for/,all_ads,2018-05-21 13:28:13,12 days 12:07:21.036584000,
"Does anyone know of a definitive source I could cite that offers guidelines for weighting data? Something like, “weighting should not be done when sample size is smaller than x,” “the magnitude of individual weights should not exceed x,” etc. I am looking to cite a source to justify my choice to weight survey data for a project I am working on. Thank you!",10,1526956814.0,8l3691,False,"Does anyone know of a definitive source I could cite that offers guidelines for weighting data? Something like, “weighting should not be done when sample size is smaller than x,” “the magnitude of individual weights should not exceed x,” etc. I am looking to cite a source to justify my choice to weight survey data for a project I am working on. Thank you!",0,"Does anyone know of a definitive source I could cite that offers guidelines for weighting data? Something like, “weighting should not be done when sample size is smaller than x,” “the magnitude of individual weights should not exceed x,” etc. I am looking to cite a source to justify my choice to weight survey data for a project I am working on. Thank you!",4,statistics,54935,,Official guidelines for weighting data?,https://www.reddit.com/r/statistics/comments/8l3691/official_guidelines_for_weighting_data/,all_ads,2018-05-21 22:40:14,12 days 02:55:20.036584000,
"Forgive me if the terminology I use is not highly technical. 

I have a series of projects that had an initial budget, and an actual cost. As such, I can create a table like the example below. The standard deviation on any individual project's change between budget and actual is 16%, but what would the standard deviation be at the bottom line/total level for the whole portfolio? In this example, despite all the variability of the individual projects, the portfolio overall only saw a 1% increase. I'm pretty sure this 1% wouldn't have a standard deviation of 16% like each individual project, but can the portfolio's standard deviation be calculated with the information available, and if so how?

		Budget	Actual	Delta
		10          12           20%
        19	        17	        -11%
		20	        26	         30%
	    30	        30	          0%
		10	         8	        -20%
		15	        12	        -20%
		17	        18	          6%
		100	        95	         -5%
		74	        80	          8%
		16	        15	         -6%
		14	        14	          0%
		8	        7.5	         -6%
		55	        52	         -5%
		60	        70	         17%
		66	        62	         -6%
		41	        40	         -2%
		40	        41	          3%
		91	        89	         -2%
		84	        92	         10%
		16	         8	        -50%
		14	        16	         14%
Total	      800	        804.5	          1%
				
			STD Dev	16%

",0,1526964254.0,8l448d,False,"Forgive me if the terminology I use is not highly technical. 

I have a series of projects that had an initial budget, and an actual cost. As such, I can create a table like the example below. The standard deviation on any individual project's change between budget and actual is 16%, but what would the standard deviation be at the bottom line/total level for the whole portfolio? In this example, despite all the variability of the individual projects, the portfolio overall only saw a 1% increase. I'm pretty sure this 1% wouldn't have a standard deviation of 16% like each individual project, but can the portfolio's standard deviation be calculated with the information available, and if so how?

		Budget	Actual	Delta
		10          12           20%
        19	        17	        -11%
		20	        26	         30%
	    30	        30	          0%
		10	         8	        -20%
		15	        12	        -20%
		17	        18	          6%
		100	        95	         -5%
		74	        80	          8%
		16	        15	         -6%
		14	        14	          0%
		8	        7.5	         -6%
		55	        52	         -5%
		60	        70	         17%
		66	        62	         -6%
		41	        40	         -2%
		40	        41	          3%
		91	        89	         -2%
		84	        92	         10%
		16	         8	        -50%
		14	        16	         14%
Total	      800	        804.5	          1%
				
			STD Dev	16%

",0,"Forgive me if the terminology I use is not highly technical. 

I have a series of projects that had an initial budget, and an actual cost. As such, I can create a table like the example below. The standard deviation on any individual project's change between budget and actual is 16%, but what would the standard deviation be at the bottom line/total level for the whole portfolio? In this example, despite all the variability of the individual projects, the portfolio overall only saw a 1% increase. I'm pretty sure this 1% wouldn't have a standard deviation of 16% like each individual project, but can the portfolio's standard deviation be calculated with the information available, and if so how?

		Budget	Actual	Delta
		10          12           20%
        19	        17	        -11%
		20	        26	         30%
	    30	        30	          0%
		10	         8	        -20%
		15	        12	        -20%
		17	        18	          6%
		100	        95	         -5%
		74	        80	          8%
		16	        15	         -6%
		14	        14	          0%
		8	        7.5	         -6%
		55	        52	         -5%
		60	        70	         17%
		66	        62	         -6%
		41	        40	         -2%
		40	        41	          3%
		91	        89	         -2%
		84	        92	         10%
		16	         8	        -50%
		14	        16	         14%
Total	      800	        804.5	          1%
				
			STD Dev	16%

",2,statistics,54935,,Standard deviation of a portfolio of underlying projects,https://www.reddit.com/r/statistics/comments/8l448d/standard_deviation_of_a_portfolio_of_underlying/,all_ads,2018-05-22 00:44:14,12 days 00:51:20.036584000,
"So with differing questions that are scored on the same likert scale, is there a way to compare them?

So if for example theres one question with an average of 2 out of 5 on one question and 2 out of 5 on another one, we could say that the result of those questions are pretty much equal. 

But what if theres one question with an average of 3 out of 5, then one with 4 out of 5, could you still compare them? What would the relation be? Double the reaction? x times the reaction? I assume its not a linear relation where the factor would be 33%

So can you compare different averages on the same likert scale and if so, how?",4,1526937711.0,8l0rjy,False,"So with differing questions that are scored on the same likert scale, is there a way to compare them?

So if for example theres one question with an average of 2 out of 5 on one question and 2 out of 5 on another one, we could say that the result of those questions are pretty much equal. 

But what if theres one question with an average of 3 out of 5, then one with 4 out of 5, could you still compare them? What would the relation be? Double the reaction? x times the reaction? I assume its not a linear relation where the factor would be 33%

So can you compare different averages on the same likert scale and if so, how?",0,"So with differing questions that are scored on the same likert scale, is there a way to compare them?

So if for example theres one question with an average of 2 out of 5 on one question and 2 out of 5 on another one, we could say that the result of those questions are pretty much equal. 

But what if theres one question with an average of 3 out of 5, then one with 4 out of 5, could you still compare them? What would the relation be? Double the reaction? x times the reaction? I assume its not a linear relation where the factor would be 33%

So can you compare different averages on the same likert scale and if so, how?",6,statistics,54935,,Likert Statistical theory: Comparing likert scale averages between different questions? Linear?,https://www.reddit.com/r/statistics/comments/8l0rjy/likert_statistical_theory_comparing_likert_scale/,all_ads,2018-05-21 17:21:51,12 days 08:13:43.036584000,
"Hello, I find myself having quite limited knowledge in A/B testing and related subjects involving experiments performed on multiple groups. 

Can you recommend any books/ articles/ video lectures that cover these subjects. Prefferably aimed for people with statistical education (not social scientists). More math and deeper analysis is welcome.",2,1526927597.0,8kztit,False,"Hello, I find myself having quite limited knowledge in A/B testing and related subjects involving experiments performed on multiple groups. 

Can you recommend any books/ articles/ video lectures that cover these subjects. Prefferably aimed for people with statistical education (not social scientists). More math and deeper analysis is welcome.",0,"Hello, I find myself having quite limited knowledge in A/B testing and related subjects involving experiments performed on multiple groups. 

Can you recommend any books/ articles/ video lectures that cover these subjects. Prefferably aimed for people with statistical education (not social scientists). More math and deeper analysis is welcome.",5,statistics,54935,,Resources to deepen understanding on A/B testing (and related subjects),https://www.reddit.com/r/statistics/comments/8kztit/resources_to_deepen_understanding_on_ab_testing/,all_ads,2018-05-21 14:33:17,12 days 11:02:17.036584000,
"Essentially, my question is whether it is ever appropriate to use a continuous measure as a random factor in a mixed model analysis.   A quick example: We administer a spatial orientation task among children in several schools in several different counties. In comparing task performance, we would like to account for the variance associated with IQ. In this example, would it be more appropriate to include IQ as a fixed factor, or *could* we include it as a random factor? I know that we could include (for instance) IQ within each school as a random factor, but I cannot find any examples or literature which directly discuss the use of covariates as random factors. If anyone can answer this question, or provide some literature/documentation that addresses it, I would be grateful. Thanks.",14,1526926635.0,8kzqwa,False,"Essentially, my question is whether it is ever appropriate to use a continuous measure as a random factor in a mixed model analysis.   A quick example: We administer a spatial orientation task among children in several schools in several different counties. In comparing task performance, we would like to account for the variance associated with IQ. In this example, would it be more appropriate to include IQ as a fixed factor, or *could* we include it as a random factor? I know that we could include (for instance) IQ within each school as a random factor, but I cannot find any examples or literature which directly discuss the use of covariates as random factors. If anyone can answer this question, or provide some literature/documentation that addresses it, I would be grateful. Thanks.",0,"Essentially, my question is whether it is ever appropriate to use a continuous measure as a random factor in a mixed model analysis.   A quick example: We administer a spatial orientation task among children in several schools in several different counties. In comparing task performance, we would like to account for the variance associated with IQ. In this example, would it be more appropriate to include IQ as a fixed factor, or *could* we include it as a random factor? I know that we could include (for instance) IQ within each school as a random factor, but I cannot find any examples or literature which directly discuss the use of covariates as random factors. If anyone can answer this question, or provide some literature/documentation that addresses it, I would be grateful. Thanks.",4,statistics,54935,,Question about covariates as random factors in mixed models,https://www.reddit.com/r/statistics/comments/8kzqwa/question_about_covariates_as_random_factors_in/,all_ads,2018-05-21 14:17:15,12 days 11:18:19.036584000,
"In the course catalog there is a multivariate statistics course listed, but I asked a professor about it and they said it hasn't been taught in years. I inquired because I was curious about principal component analysis.

I'm worried I'm missing something essential.",13,1526916885.0,8kz2l1,False,"In the course catalog there is a multivariate statistics course listed, but I asked a professor about it and they said it hasn't been taught in years. I inquired because I was curious about principal component analysis.

I'm worried I'm missing something essential.",0,"In the course catalog there is a multivariate statistics course listed, but I asked a professor about it and they said it hasn't been taught in years. I inquired because I was curious about principal component analysis.

I'm worried I'm missing something essential.",7,statistics,54935,,My MS program doesn't offer multivariate statistics. Will I be missing out?,https://www.reddit.com/r/statistics/comments/8kz2l1/my_ms_program_doesnt_offer_multivariate/,all_ads,2018-05-21 11:34:45,12 days 14:00:49.036584000,
"Hi r/statistics,

I am trying to gain more understanding around a concept I don't fully understand. Example:

Business A \- $1000 profit; 10 employees; fixed costs generated by employees \- $600

Business B \- $600 profit; 5 employees; fixed costs generated by employees \-$200  

Business C \- $3000 loss; 15 employees; fixed costs generated by employees \-$1000

The average of the group is a $466.67 loss. The leader of the organization says ""I want to reduce employees, which will decrease fixed costs and make the average of the group breakeven."" So we create a blended pool:

Total Business \- $1400 loss ; 30 employees; fixed costs generated by employees \- $1,800

Each employee generates $60 in fixed costs on average. To breakeven on the business, we would have to cut around 23 of the 30 heads \(this is under the assumption that profit will remain unchanged, only fixed costs would change\).

I feel a statistical fallacy is being committed somewhere above. Creating a blended group will bucket profitable and unprofitable business together. I keep trying to rationalize against it by saying, ""If I were to get a flat tire while driving a car, I wouldn't look to replace all 4 tires. Just the flat one."" The parallel being, we can't meaningful gather a takeaway on the group and have to look only at business C. If anyone has knowledge on the underlying theory surrounding this, I would love to hear it! Thanks. ",6,1526950062.0,8l29vh,False,"Hi r/statistics,

I am trying to gain more understanding around a concept I don't fully understand. Example:

Business A \- $1000 profit; 10 employees; fixed costs generated by employees \- $600

Business B \- $600 profit; 5 employees; fixed costs generated by employees \-$200  

Business C \- $3000 loss; 15 employees; fixed costs generated by employees \-$1000

The average of the group is a $466.67 loss. The leader of the organization says ""I want to reduce employees, which will decrease fixed costs and make the average of the group breakeven."" So we create a blended pool:

Total Business \- $1400 loss ; 30 employees; fixed costs generated by employees \- $1,800

Each employee generates $60 in fixed costs on average. To breakeven on the business, we would have to cut around 23 of the 30 heads \(this is under the assumption that profit will remain unchanged, only fixed costs would change\).

I feel a statistical fallacy is being committed somewhere above. Creating a blended group will bucket profitable and unprofitable business together. I keep trying to rationalize against it by saying, ""If I were to get a flat tire while driving a car, I wouldn't look to replace all 4 tires. Just the flat one."" The parallel being, we can't meaningful gather a takeaway on the group and have to look only at business C. If anyone has knowledge on the underlying theory surrounding this, I would love to hear it! Thanks. ",0,"Hi r/statistics,

I am trying to gain more understanding around a concept I don't fully understand. Example:

Business A \- $1000 profit; 10 employees; fixed costs generated by employees \- $600

Business B \- $600 profit; 5 employees; fixed costs generated by employees \-$200  

Business C \- $3000 loss; 15 employees; fixed costs generated by employees \-$1000

The average of the group is a $466.67 loss. The leader of the organization says ""I want to reduce employees, which will decrease fixed costs and make the average of the group breakeven."" So we create a blended pool:

Total Business \- $1400 loss ; 30 employees; fixed costs generated by employees \- $1,800

Each employee generates $60 in fixed costs on average. To breakeven on the business, we would have to cut around 23 of the 30 heads \(this is under the assumption that profit will remain unchanged, only fixed costs would change\).

I feel a statistical fallacy is being committed somewhere above. Creating a blended group will bucket profitable and unprofitable business together. I keep trying to rationalize against it by saying, ""If I were to get a flat tire while driving a car, I wouldn't look to replace all 4 tires. Just the flat one."" The parallel being, we can't meaningful gather a takeaway on the group and have to look only at business C. If anyone has knowledge on the underlying theory surrounding this, I would love to hear it! Thanks. ",1,statistics,54935,,Blended Averages Help,https://www.reddit.com/r/statistics/comments/8l29vh/blended_averages_help/,all_ads,2018-05-21 20:47:42,12 days 04:47:52.036584000,
"I'm pursuing a ML degree and aside from the normal CS courses, there's a ton of stats courses as well. I can handle the cs courses but I just took Intro to Probability and Intro to Statistics over the last year and it was pretty difficult for me. I still don't think I properly learned anything.

Going forward, I have to take ~10 more stats courses and looking at the overview this is what I have to know

* Stochastic Processes - Topics covered include finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* Regression Analysis - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models. Instruction in the use of SAS.

* Statistical Inference - Principles of statistical reasoning and theories of statistical analysis. Topics include: statistical models, likelihood theory, repeated sampling theories of inference, prior elicitation, Bayesian theories of inference, decision theory, asymptotic theory, model checking, and checking for prior-data conflict. Advantages and disadvantages of the different theories.

So it's pretty safe to say I'm at a very basic knowledge/understanding of statistics. I remember and understood a bit of probability, but the stats course was a complete blur.

It's currently the summer semester so I'm hoping to read atleast an hour or two every day and try to strengthen my knowledge.

Thanks everyone!",14,1526888781.0,8kwkrb,False,"I'm pursuing a ML degree and aside from the normal CS courses, there's a ton of stats courses as well. I can handle the cs courses but I just took Intro to Probability and Intro to Statistics over the last year and it was pretty difficult for me. I still don't think I properly learned anything.

Going forward, I have to take ~10 more stats courses and looking at the overview this is what I have to know

* Stochastic Processes - Topics covered include finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* Regression Analysis - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models. Instruction in the use of SAS.

* Statistical Inference - Principles of statistical reasoning and theories of statistical analysis. Topics include: statistical models, likelihood theory, repeated sampling theories of inference, prior elicitation, Bayesian theories of inference, decision theory, asymptotic theory, model checking, and checking for prior-data conflict. Advantages and disadvantages of the different theories.

So it's pretty safe to say I'm at a very basic knowledge/understanding of statistics. I remember and understood a bit of probability, but the stats course was a complete blur.

It's currently the summer semester so I'm hoping to read atleast an hour or two every day and try to strengthen my knowledge.

Thanks everyone!",0,"I'm pursuing a ML degree and aside from the normal CS courses, there's a ton of stats courses as well. I can handle the cs courses but I just took Intro to Probability and Intro to Statistics over the last year and it was pretty difficult for me. I still don't think I properly learned anything.

Going forward, I have to take ~10 more stats courses and looking at the overview this is what I have to know

* Stochastic Processes - Topics covered include finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* Regression Analysis - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models. Instruction in the use of SAS.

* Statistical Inference - Principles of statistical reasoning and theories of statistical analysis. Topics include: statistical models, likelihood theory, repeated sampling theories of inference, prior elicitation, Bayesian theories of inference, decision theory, asymptotic theory, model checking, and checking for prior-data conflict. Advantages and disadvantages of the different theories.

So it's pretty safe to say I'm at a very basic knowledge/understanding of statistics. I remember and understood a bit of probability, but the stats course was a complete blur.

It's currently the summer semester so I'm hoping to read atleast an hour or two every day and try to strengthen my knowledge.

Thanks everyone!",18,statistics,54935,,Good book to read over the summer to prepare,https://www.reddit.com/r/statistics/comments/8kwkrb/good_book_to_read_over_the_summer_to_prepare/,all_ads,2018-05-21 03:46:21,12 days 21:49:13.036584000,
"They seems like the same. I'm self teaching myself time series and ACF's formula look exactly like Correlation formula. Am I missing something here ? Mis remembering? I googled and I think they're the same.

In context of time series I've found a good explanation:

https://stats.stackexchange.com/questions/77248/what-is-autocorrelation-function

But my main question is: am I crazy or does ACF and Correlation formula is the same math formula?

Edit:

Thank you in advance!",10,1526916966.0,8kz2t5,False,"They seems like the same. I'm self teaching myself time series and ACF's formula look exactly like Correlation formula. Am I missing something here ? Mis remembering? I googled and I think they're the same.

In context of time series I've found a good explanation:

https://stats.stackexchange.com/questions/77248/what-is-autocorrelation-function

But my main question is: am I crazy or does ACF and Correlation formula is the same math formula?

Edit:

Thank you in advance!",0,"They seems like the same. I'm self teaching myself time series and ACF's formula look exactly like Correlation formula. Am I missing something here ? Mis remembering? I googled and I think they're the same.

In context of time series I've found a good explanation:

https://stats.stackexchange.com/questions/77248/what-is-autocorrelation-function

But my main question is: am I crazy or does ACF and Correlation formula is the same math formula?

Edit:

Thank you in advance!",4,statistics,54935,,Time series: What's the difference between ACF (autocorrelation function) and Correlation?,https://www.reddit.com/r/statistics/comments/8kz2t5/time_series_whats_the_difference_between_acf/,all_ads,2018-05-21 11:36:06,12 days 13:59:28.036584000,
Any advice for a senior stats major?,18,1526897669.0,8kxgkk,False,Any advice for a senior stats major?,0,Any advice for a senior stats major?,6,statistics,54935,,What do you wish you did in college as an undergraduate?,https://www.reddit.com/r/statistics/comments/8kxgkk/what_do_you_wish_you_did_in_college_as_an/,all_ads,2018-05-21 06:14:29,12 days 19:21:05.036584000,
"Bit late on posting this one as the talk was a while ago, but here it is anyway. I went to a talk organised by my local statistics association a few weeks ago, and the topic of the session was ""What I wish I'd known"". The goal of the session was for several speakers who currently work in statistics to describe how they got there, and what they wish they'd known at the beginning of their career. Roger Peng was one of the speakers, and he's recently released a podcast episode covering the same thing on his [Effort Report](https://effortreport.libsyn.com/72-what-i-wish-id-known) podcast.

Of the thinks that were spoken about, these are the points that I took away:

* You might think at the beginning of your career that you have a lot of time, but you really don't. Roger made the point of thinking about your time spent doing things as a pie\-chart rather than a to\-do list. The to\-do list in a way can be endless, and you can keep adding more and more things to it, but it's easy to keep putting things off. Thinking about your time in a pie\-chart sense forces you to think about it in a finite sense. If you've got many projects that you want to work on, you've got to divide your pie chart up into big enough slices to get them done. If you're thinking of taking on more work, you've got to figure out what slice of the pie chart is going to have to be shrunk/removed.
* Use your networks to get a foot in the door. Many of the early jobs that the speakers had were found through their professors and academic contacts. Asking professors in your school, or even other universities, if they know of any jobs going is a good idea, especially since a lot of jobs might not end up ever being advertised. If you manage to ask the right person at the right time, you might end up being recommended for a job that you otherwise might've never seen.
* What you think you'll really enjoy might turn out to be something you hate doing, and something you might have never even considered might turn out to be something you love. Don't turn down opportunities just because they don't sound interesting. In your early career it's worth trying out a variety of jobs \(if possible\) to get a better idea of where you might like to end up in the future.
* You probably can't afford to be too picky at the beginning of your career, and the goal really for your first job is to get some experience that you can use as a stepping stone to move onto your next job that might be more relevant to your interests.

I can't remember any more at the moment, but to continue this thread, what do you wish you'd known at the beginning of your career? This could be statistical knowledge, career knowledge, or anything other practical advice.",0,1526896083.0,8kxawf,False,"Bit late on posting this one as the talk was a while ago, but here it is anyway. I went to a talk organised by my local statistics association a few weeks ago, and the topic of the session was ""What I wish I'd known"". The goal of the session was for several speakers who currently work in statistics to describe how they got there, and what they wish they'd known at the beginning of their career. Roger Peng was one of the speakers, and he's recently released a podcast episode covering the same thing on his [Effort Report](https://effortreport.libsyn.com/72-what-i-wish-id-known) podcast.

Of the thinks that were spoken about, these are the points that I took away:

* You might think at the beginning of your career that you have a lot of time, but you really don't. Roger made the point of thinking about your time spent doing things as a pie\-chart rather than a to\-do list. The to\-do list in a way can be endless, and you can keep adding more and more things to it, but it's easy to keep putting things off. Thinking about your time in a pie\-chart sense forces you to think about it in a finite sense. If you've got many projects that you want to work on, you've got to divide your pie chart up into big enough slices to get them done. If you're thinking of taking on more work, you've got to figure out what slice of the pie chart is going to have to be shrunk/removed.
* Use your networks to get a foot in the door. Many of the early jobs that the speakers had were found through their professors and academic contacts. Asking professors in your school, or even other universities, if they know of any jobs going is a good idea, especially since a lot of jobs might not end up ever being advertised. If you manage to ask the right person at the right time, you might end up being recommended for a job that you otherwise might've never seen.
* What you think you'll really enjoy might turn out to be something you hate doing, and something you might have never even considered might turn out to be something you love. Don't turn down opportunities just because they don't sound interesting. In your early career it's worth trying out a variety of jobs \(if possible\) to get a better idea of where you might like to end up in the future.
* You probably can't afford to be too picky at the beginning of your career, and the goal really for your first job is to get some experience that you can use as a stepping stone to move onto your next job that might be more relevant to your interests.

I can't remember any more at the moment, but to continue this thread, what do you wish you'd known at the beginning of your career? This could be statistical knowledge, career knowledge, or anything other practical advice.",0,"Bit late on posting this one as the talk was a while ago, but here it is anyway. I went to a talk organised by my local statistics association a few weeks ago, and the topic of the session was ""What I wish I'd known"". The goal of the session was for several speakers who currently work in statistics to describe how they got there, and what they wish they'd known at the beginning of their career. Roger Peng was one of the speakers, and he's recently released a podcast episode covering the same thing on his [Effort Report](https://effortreport.libsyn.com/72-what-i-wish-id-known) podcast.

Of the thinks that were spoken about, these are the points that I took away:

* You might think at the beginning of your career that you have a lot of time, but you really don't. Roger made the point of thinking about your time spent doing things as a pie\-chart rather than a to\-do list. The to\-do list in a way can be endless, and you can keep adding more and more things to it, but it's easy to keep putting things off. Thinking about your time in a pie\-chart sense forces you to think about it in a finite sense. If you've got many projects that you want to work on, you've got to divide your pie chart up into big enough slices to get them done. If you're thinking of taking on more work, you've got to figure out what slice of the pie chart is going to have to be shrunk/removed.
* Use your networks to get a foot in the door. Many of the early jobs that the speakers had were found through their professors and academic contacts. Asking professors in your school, or even other universities, if they know of any jobs going is a good idea, especially since a lot of jobs might not end up ever being advertised. If you manage to ask the right person at the right time, you might end up being recommended for a job that you otherwise might've never seen.
* What you think you'll really enjoy might turn out to be something you hate doing, and something you might have never even considered might turn out to be something you love. Don't turn down opportunities just because they don't sound interesting. In your early career it's worth trying out a variety of jobs \(if possible\) to get a better idea of where you might like to end up in the future.
* You probably can't afford to be too picky at the beginning of your career, and the goal really for your first job is to get some experience that you can use as a stepping stone to move onto your next job that might be more relevant to your interests.

I can't remember any more at the moment, but to continue this thread, what do you wish you'd known at the beginning of your career? This could be statistical knowledge, career knowledge, or anything other practical advice.",5,statistics,54935,,What I wish I'd known,https://www.reddit.com/r/statistics/comments/8kxawf/what_i_wish_id_known/,all_ads,2018-05-21 05:48:03,12 days 19:47:31.036584000,
"I need help at gathering data to update my teachers outdated and incomplete materials on the subject. So help me gather data.

[https://docs.google.com/forms/d/e/1FAIpQLSefD11Ox2JPVd\-QAUx\_l2nGFbcxOI4r\-GmyMUqnPQjcv5gbow/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSefD11Ox2JPVd-QAUx_l2nGFbcxOI4r-GmyMUqnPQjcv5gbow/viewform?usp=sf_link)

The form takes around 5\+ minutes to complete.

Thanks in advance!",1,1526939549.0,8l0ypa,False,"I need help at gathering data to update my teachers outdated and incomplete materials on the subject. So help me gather data.

[https://docs.google.com/forms/d/e/1FAIpQLSefD11Ox2JPVd\-QAUx\_l2nGFbcxOI4r\-GmyMUqnPQjcv5gbow/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSefD11Ox2JPVd-QAUx_l2nGFbcxOI4r-GmyMUqnPQjcv5gbow/viewform?usp=sf_link)

The form takes around 5\+ minutes to complete.

Thanks in advance!",0,"I need help at gathering data to update my teachers outdated and incomplete materials on the subject. So help me gather data.

[https://docs.google.com/forms/d/e/1FAIpQLSefD11Ox2JPVd\-QAUx\_l2nGFbcxOI4r\-GmyMUqnPQjcv5gbow/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSefD11Ox2JPVd-QAUx_l2nGFbcxOI4r-GmyMUqnPQjcv5gbow/viewform?usp=sf_link)

The form takes around 5\+ minutes to complete.

Thanks in advance!",0,statistics,54935,,I need help at gathering data about Social media usage.,https://www.reddit.com/r/statistics/comments/8l0ypa/i_need_help_at_gathering_data_about_social_media/,all_ads,2018-05-21 17:52:29,12 days 07:43:05.036584000,
"Hello,

I’m a forty year old programmer who works primary on database applications. I have the opportunity to enroll in a MS program for statistics and have it mostly paid for. I’m interested in analyzing public programs to see if they’re achieving their goals, but I’d be happy doing any kind of work that had a positive social impact. 

Is what I’d like to do practical? Who employs people who do what I want to do? 

Thanks!",15,1526816150.0,8kq9gd,False,"Hello,

I’m a forty year old programmer who works primary on database applications. I have the opportunity to enroll in a MS program for statistics and have it mostly paid for. I’m interested in analyzing public programs to see if they’re achieving their goals, but I’d be happy doing any kind of work that had a positive social impact. 

Is what I’d like to do practical? Who employs people who do what I want to do? 

Thanks!",0,"Hello,

I’m a forty year old programmer who works primary on database applications. I have the opportunity to enroll in a MS program for statistics and have it mostly paid for. I’m interested in analyzing public programs to see if they’re achieving their goals, but I’d be happy doing any kind of work that had a positive social impact. 

Is what I’d like to do practical? Who employs people who do what I want to do? 

Thanks!",18,statistics,54935,,Graduate Degree Opportunity,https://www.reddit.com/r/statistics/comments/8kq9gd/graduate_degree_opportunity/,all_ads,2018-05-20 07:35:50,13 days 17:59:44.036584000,
"Hi,

So I will be starting my PhD in Statistics this fall and I was wondering how much knowledge programs assume entering students have. The main reason for asking is because the prof I want to work with gave me some of his current papers to read and there is decent amount of material in the papers that I have only vaguely seen (specifically concentration inequalities and martingales). I have done some measure theoretic probability but not to the point where I can totally understand what is being done in the papers.

Thanks",16,1526747848.0,8kk1ih,False,"Hi,

So I will be starting my PhD in Statistics this fall and I was wondering how much knowledge programs assume entering students have. The main reason for asking is because the prof I want to work with gave me some of his current papers to read and there is decent amount of material in the papers that I have only vaguely seen (specifically concentration inequalities and martingales). I have done some measure theoretic probability but not to the point where I can totally understand what is being done in the papers.

Thanks",0,"Hi,

So I will be starting my PhD in Statistics this fall and I was wondering how much knowledge programs assume entering students have. The main reason for asking is because the prof I want to work with gave me some of his current papers to read and there is decent amount of material in the papers that I have only vaguely seen (specifically concentration inequalities and martingales). I have done some measure theoretic probability but not to the point where I can totally understand what is being done in the papers.

Thanks",41,statistics,54935,,Prerequisite Knowledge Going into PhD,https://www.reddit.com/r/statistics/comments/8kk1ih/prerequisite_knowledge_going_into_phd/,all_ads,2018-05-19 12:37:28,14 days 12:58:06.036584000,
"I have survey data that contains participants' intent to purchase products from several countries. The countries are the independent variable and the purchase intent score (7 point likert scale) is the dependent variable (the purpose is to find out if the country of origin affects the purchase intent).

I had weighted the purchase intent rating according to the response rate of the sample (age and gender) compared to the population. The weighting was simply weight * purchase intent rating. Some groups were very under represented and had weightings of over 16.

I ran an ANOVA test on the weighted data and it was (very) not statisitically significant. I ran an ANOVA test with the original unweighted data and it was statistically significant.

Am I using the wrong statistical significance test? Have I weighted the data incorrectly? Should I not have weighted the data at all based on the sample?

Sorry if I haven't explained this well. I'm not a stats expert (as you can probably tell!)

Thanks for any advice :)",4,1526819219.0,8kqhxj,False,"I have survey data that contains participants' intent to purchase products from several countries. The countries are the independent variable and the purchase intent score (7 point likert scale) is the dependent variable (the purpose is to find out if the country of origin affects the purchase intent).

I had weighted the purchase intent rating according to the response rate of the sample (age and gender) compared to the population. The weighting was simply weight * purchase intent rating. Some groups were very under represented and had weightings of over 16.

I ran an ANOVA test on the weighted data and it was (very) not statisitically significant. I ran an ANOVA test with the original unweighted data and it was statistically significant.

Am I using the wrong statistical significance test? Have I weighted the data incorrectly? Should I not have weighted the data at all based on the sample?

Sorry if I haven't explained this well. I'm not a stats expert (as you can probably tell!)

Thanks for any advice :)",0,"I have survey data that contains participants' intent to purchase products from several countries. The countries are the independent variable and the purchase intent score (7 point likert scale) is the dependent variable (the purpose is to find out if the country of origin affects the purchase intent).

I had weighted the purchase intent rating according to the response rate of the sample (age and gender) compared to the population. The weighting was simply weight * purchase intent rating. Some groups were very under represented and had weightings of over 16.

I ran an ANOVA test on the weighted data and it was (very) not statisitically significant. I ran an ANOVA test with the original unweighted data and it was statistically significant.

Am I using the wrong statistical significance test? Have I weighted the data incorrectly? Should I not have weighted the data at all based on the sample?

Sorry if I haven't explained this well. I'm not a stats expert (as you can probably tell!)

Thanks for any advice :)",1,statistics,54935,,Question - Not sure which statistical significance test to use and whether to use weighted sample,https://www.reddit.com/r/statistics/comments/8kqhxj/question_not_sure_which_statistical_significance/,all_ads,2018-05-20 08:26:59,13 days 17:08:35.036584000,
"Hi all, I'm trying to get into a top 5 program in statistics (dream would be Stanford). I just got 163 quant on my GRE (168 verbal, yay...), which was unexpected but oh well. 

I did research in machine learning and applied math in college, graduated with a BA in math and physics from a top 5 LAC, and my GPA is above 3.7. I'm currently working in data analysis. I'm going to make sure I get as best a score as I can on the subject test of course, but is this score going to bring me down? Is it worth spending another $200?

I probably will get a higher score if I take it again because I skipped studying the math section altogether after I got 170 on the practice test I took while I was still at school. Only at the testing center did I realize how out of practice I was... I did bring my verbal score up from the mid 150s though.",3,1526797448.0,8kol07,False,"Hi all, I'm trying to get into a top 5 program in statistics (dream would be Stanford). I just got 163 quant on my GRE (168 verbal, yay...), which was unexpected but oh well. 

I did research in machine learning and applied math in college, graduated with a BA in math and physics from a top 5 LAC, and my GPA is above 3.7. I'm currently working in data analysis. I'm going to make sure I get as best a score as I can on the subject test of course, but is this score going to bring me down? Is it worth spending another $200?

I probably will get a higher score if I take it again because I skipped studying the math section altogether after I got 170 on the practice test I took while I was still at school. Only at the testing center did I realize how out of practice I was... I did bring my verbal score up from the mid 150s though.",0,"Hi all, I'm trying to get into a top 5 program in statistics (dream would be Stanford). I just got 163 quant on my GRE (168 verbal, yay...), which was unexpected but oh well. 

I did research in machine learning and applied math in college, graduated with a BA in math and physics from a top 5 LAC, and my GPA is above 3.7. I'm currently working in data analysis. I'm going to make sure I get as best a score as I can on the subject test of course, but is this score going to bring me down? Is it worth spending another $200?

I probably will get a higher score if I take it again because I skipped studying the math section altogether after I got 170 on the practice test I took while I was still at school. Only at the testing center did I realize how out of practice I was... I did bring my verbal score up from the mid 150s though.",0,statistics,54935,,How much does the general GRE score matter?,https://www.reddit.com/r/statistics/comments/8kol07/how_much_does_the_general_gre_score_matter/,all_ads,2018-05-20 02:24:08,13 days 23:11:26.036584000,
[USA crude oil export to UK 2018-2017](https://www.nlsql.com/static/img/skype/pic_9oKBZBTj.png) ,1,1526794666.0,8koao4,False,[USA crude oil export to UK 2018-2017](https://www.nlsql.com/static/img/skype/pic_9oKBZBTj.png) ,0,[USA crude oil export to UK 2018-2017](https://www.nlsql.com/static/img/skype/pic_9oKBZBTj.png) ,0,statistics,54935,,What is the reason of huge growth of crude oil export to UK from USA for last 2 years?,https://www.reddit.com/r/statistics/comments/8koao4/what_is_the_reason_of_huge_growth_of_crude_oil/,all_ads,2018-05-20 01:37:46,13 days 23:57:48.036584000,
"Is Pierre Gy’s theory of sampling mathematically valid? 

I have a test that reports an uncertainty of +/- 4%. The process involved taking  15 incremental samples, combining them, and testing two sub-samples. 

I look at this and see n=1 for Student’s t-test. How does Gy’s theory allow us to determine uncertainty when a t-test wouldn’t be able to?


",0,1526765158.0,8kl8xi,False,"Is Pierre Gy’s theory of sampling mathematically valid? 

I have a test that reports an uncertainty of +/- 4%. The process involved taking  15 incremental samples, combining them, and testing two sub-samples. 

I look at this and see n=1 for Student’s t-test. How does Gy’s theory allow us to determine uncertainty when a t-test wouldn’t be able to?


",0,"Is Pierre Gy’s theory of sampling mathematically valid? 

I have a test that reports an uncertainty of +/- 4%. The process involved taking  15 incremental samples, combining them, and testing two sub-samples. 

I look at this and see n=1 for Student’s t-test. How does Gy’s theory allow us to determine uncertainty when a t-test wouldn’t be able to?


",2,statistics,54935,,Validity of Pierre Gy’s theory of sampling,https://www.reddit.com/r/statistics/comments/8kl8xi/validity_of_pierre_gys_theory_of_sampling/,all_ads,2018-05-19 17:25:58,14 days 08:09:36.036584000,
"https://i.imgur.com/bVUqrhQ.png

Anyone know of some online service or something that can automatically try multiple different functions, and find the best fitting?",8,1526760852.0,8kkwdp,False,"https://i.imgur.com/bVUqrhQ.png

Anyone know of some online service or something that can automatically try multiple different functions, and find the best fitting?",0,"https://i.imgur.com/bVUqrhQ.png

Anyone know of some online service or something that can automatically try multiple different functions, and find the best fitting?",0,statistics,54935,,"I have a plot here, and want to find what function it best fits..",https://www.reddit.com/r/statistics/comments/8kkwdp/i_have_a_plot_here_and_want_to_find_what_function/,all_ads,2018-05-19 16:14:12,14 days 09:21:22.036584000,
"I'm testing to what extent the relationship between my independent continuous variable (x) and dummy dependent variable (y) is influenced by a dummy moderator variable (z).

Would this be the correct way to perform this regression in SPSS? https://imgur.com/a/HbO9CQc

Any help is appreciated, thanks a lot!",6,1526759804.0,8kkt3f,False,"I'm testing to what extent the relationship between my independent continuous variable (x) and dummy dependent variable (y) is influenced by a dummy moderator variable (z).

Would this be the correct way to perform this regression in SPSS? https://imgur.com/a/HbO9CQc

Any help is appreciated, thanks a lot!",0,"I'm testing to what extent the relationship between my independent continuous variable (x) and dummy dependent variable (y) is influenced by a dummy moderator variable (z).

Would this be the correct way to perform this regression in SPSS? https://imgur.com/a/HbO9CQc

Any help is appreciated, thanks a lot!",0,statistics,54935,,Moderation in SPSS,https://www.reddit.com/r/statistics/comments/8kkt3f/moderation_in_spss/,all_ads,2018-05-19 15:56:44,14 days 09:38:50.036584000,
"For example, if the difference between data sets A and B is significant, while B and C are not significant, does that imply that A and C are statistically different as well? ",16,1526681552.0,8kdgx3,False,"For example, if the difference between data sets A and B is significant, while B and C are not significant, does that imply that A and C are statistically different as well? ",0,"For example, if the difference between data sets A and B is significant, while B and C are not significant, does that imply that A and C are statistically different as well? ",22,statistics,54935,,Is statistical difference commutative?,https://www.reddit.com/r/statistics/comments/8kdgx3/is_statistical_difference_commutative/,all_ads,2018-05-18 18:12:32,15 days 07:23:02.036584000,
,4,1526699341.0,8kfohx,False,,0,,5,statistics,54935,,Need EE statistics lab ideas!,https://www.reddit.com/r/ElectricalEngineering/comments/8kfh2i/need_ee_statistics_lab_ideas/,all_ads,2018-05-18 23:09:01,15 days 02:26:33.036584000,19600.0
"Hi everyone,

I got one question wrong on my stats exam that I have a question on. 

I got the first question right:
The IQ of humans is approximately normally distributed with mean 100 and population standard deviation 15. Compute the probability that a simple random sample of size n-10 results in a sample mean greater than 110.  

Approach: I got the standard deviation of the sampling distribution of my sample means, Z-score, and conducted probability.

The second question I got wrong:
A doctor calculates that the mean height of his 200 patients is 28.72 inches and their standard deviation is 3.17 inches. Find the height of the patients at the 20th percentile. Find the probability that a randomly selected patient is between 35 to 40 inches tall.

Approach: I used the formula for the standard deviation of the sampling distribution of the sample means 3.17/Rad200, but apparently I had to use the standard deviation of 3.17 I was only given. Why is that?

Thank you for your answers! :) God bless and have a wonderful weekend.",5,1526729520.0,8kiqif,False,"Hi everyone,

I got one question wrong on my stats exam that I have a question on. 

I got the first question right:
The IQ of humans is approximately normally distributed with mean 100 and population standard deviation 15. Compute the probability that a simple random sample of size n-10 results in a sample mean greater than 110.  

Approach: I got the standard deviation of the sampling distribution of my sample means, Z-score, and conducted probability.

The second question I got wrong:
A doctor calculates that the mean height of his 200 patients is 28.72 inches and their standard deviation is 3.17 inches. Find the height of the patients at the 20th percentile. Find the probability that a randomly selected patient is between 35 to 40 inches tall.

Approach: I used the formula for the standard deviation of the sampling distribution of the sample means 3.17/Rad200, but apparently I had to use the standard deviation of 3.17 I was only given. Why is that?

Thank you for your answers! :) God bless and have a wonderful weekend.",0,"Hi everyone,

I got one question wrong on my stats exam that I have a question on. 

I got the first question right:
The IQ of humans is approximately normally distributed with mean 100 and population standard deviation 15. Compute the probability that a simple random sample of size n-10 results in a sample mean greater than 110.  

Approach: I got the standard deviation of the sampling distribution of my sample means, Z-score, and conducted probability.

The second question I got wrong:
A doctor calculates that the mean height of his 200 patients is 28.72 inches and their standard deviation is 3.17 inches. Find the height of the patients at the 20th percentile. Find the probability that a randomly selected patient is between 35 to 40 inches tall.

Approach: I used the formula for the standard deviation of the sampling distribution of the sample means 3.17/Rad200, but apparently I had to use the standard deviation of 3.17 I was only given. Why is that?

Thank you for your answers! :) God bless and have a wonderful weekend.",1,statistics,54935,,Question I got wrong on my stats exam? Introductory Stats?,https://www.reddit.com/r/statistics/comments/8kiqif/question_i_got_wrong_on_my_stats_exam/,all_ads,2018-05-19 07:32:00,14 days 18:03:34.036584000,
"Hi all,

Imagine we have a dataset of 500 rows. Within that 500, most represent a single customer, however some will represent a single customer returning multiple times. We want to use this data to start doing some preliminary analysis to identify some predictors of risk, and whether we can use these in predictive analysis going forward.

The data points that represent the same returning customer will each have a seperate risk result. However they obviously have some underlying correlation in that they represent the same customer.

Does this phenomena have a name, and is it an issue? I know of multicollinearity where the predictors are correlated, but what about the data itself?

Feel free to ask further questions if I haven't explained this very clearly.",7,1526678147.0,8kd2rr,False,"Hi all,

Imagine we have a dataset of 500 rows. Within that 500, most represent a single customer, however some will represent a single customer returning multiple times. We want to use this data to start doing some preliminary analysis to identify some predictors of risk, and whether we can use these in predictive analysis going forward.

The data points that represent the same returning customer will each have a seperate risk result. However they obviously have some underlying correlation in that they represent the same customer.

Does this phenomena have a name, and is it an issue? I know of multicollinearity where the predictors are correlated, but what about the data itself?

Feel free to ask further questions if I haven't explained this very clearly.",0,"Hi all,

Imagine we have a dataset of 500 rows. Within that 500, most represent a single customer, however some will represent a single customer returning multiple times. We want to use this data to start doing some preliminary analysis to identify some predictors of risk, and whether we can use these in predictive analysis going forward.

The data points that represent the same returning customer will each have a seperate risk result. However they obviously have some underlying correlation in that they represent the same customer.

Does this phenomena have a name, and is it an issue? I know of multicollinearity where the predictors are correlated, but what about the data itself?

Feel free to ask further questions if I haven't explained this very clearly.",11,statistics,54935,,Using data that is correlated to some other pieces of data more strongly than others?,https://www.reddit.com/r/statistics/comments/8kd2rr/using_data_that_is_correlated_to_some_other/,all_ads,2018-05-18 17:15:47,15 days 08:19:47.036584000,
"I'm a 1st year stats major. So I'm thinking about going into data science for graduate school. I'm currently struggling in my physics class, and I will get a 'withdraw' notation on my transcript if i drop right now. But if I try really hard, the most I can get is probably a B- or C. Should I choose drop or C? Thanks",16,1526716821.0,8khlku,False,"I'm a 1st year stats major. So I'm thinking about going into data science for graduate school. I'm currently struggling in my physics class, and I will get a 'withdraw' notation on my transcript if i drop right now. But if I try really hard, the most I can get is probably a B- or C. Should I choose drop or C? Thanks",0,"I'm a 1st year stats major. So I'm thinking about going into data science for graduate school. I'm currently struggling in my physics class, and I will get a 'withdraw' notation on my transcript if i drop right now. But if I try really hard, the most I can get is probably a B- or C. Should I choose drop or C? Thanks",1,statistics,54935,,'C' or 'W' on transcript for graduate school,https://www.reddit.com/r/statistics/comments/8khlku/c_or_w_on_transcript_for_graduate_school/,all_ads,2018-05-19 04:00:21,14 days 21:35:13.036584000,
"I'm a marketing major entering my senior year in the fall, and I want to spend the summer reading more about stats given that I'm about to move into specializing into market research. What suggestions do you have for books to read to learn more about entry level statistics topics (regression, chi-square, f-tests, etc) that explain things in more common language? For context, my math experience up to this point consists of pre-cal, stats 1 & 2, and a basic market research course.",8,1526692425.0,8keteh,False,"I'm a marketing major entering my senior year in the fall, and I want to spend the summer reading more about stats given that I'm about to move into specializing into market research. What suggestions do you have for books to read to learn more about entry level statistics topics (regression, chi-square, f-tests, etc) that explain things in more common language? For context, my math experience up to this point consists of pre-cal, stats 1 & 2, and a basic market research course.",0,"I'm a marketing major entering my senior year in the fall, and I want to spend the summer reading more about stats given that I'm about to move into specializing into market research. What suggestions do you have for books to read to learn more about entry level statistics topics (regression, chi-square, f-tests, etc) that explain things in more common language? For context, my math experience up to this point consists of pre-cal, stats 1 & 2, and a basic market research course.",2,statistics,54935,,Learning Statistics,https://www.reddit.com/r/statistics/comments/8keteh/learning_statistics/,all_ads,2018-05-18 21:13:45,15 days 04:21:49.036584000,
"As the title says, I have a friend that claimed both of her sisters got pregnant while using condoms, twice.  That is 4 children conceived while using condoms in a single family.  Presuming that on all 4 occasions, the condoms used were properly stored, not expired, etc., what are the chances that this would happen given the current statistics on condom effectiveness during proper use?  ",13,1526706353.0,8kgidf,False,"As the title says, I have a friend that claimed both of her sisters got pregnant while using condoms, twice.  That is 4 children conceived while using condoms in a single family.  Presuming that on all 4 occasions, the condoms used were properly stored, not expired, etc., what are the chances that this would happen given the current statistics on condom effectiveness during proper use?  ",0,"As the title says, I have a friend that claimed both of her sisters got pregnant while using condoms, twice.  That is 4 children conceived while using condoms in a single family.  Presuming that on all 4 occasions, the condoms used were properly stored, not expired, etc., what are the chances that this would happen given the current statistics on condom effectiveness during proper use?  ",0,statistics,54935,,"Can someone help me figure out this probability? Friend claimed both sisters got pregnant while using condoms, twice.",https://www.reddit.com/r/statistics/comments/8kgidf/can_someone_help_me_figure_out_this_probability/,all_ads,2018-05-19 01:05:53,15 days 00:29:41.036584000,
"Hi,

I am trying to build a script \(in SQL\) that will return the p value of \(many\) 2x2 contingency tables... 

My problem is some cells can be \<5 therefore I should not use the chi square calculation and compute the Fisher test. However that makes the calculation impossible in my environment as some other cells are likely to be \>100

Ex of a table that I have

Cat1    Cat2    

GrA     150     98

GrB    12        2

The Fisher test will require for me to calculate the factorial of 262 which is not possible . How can I deal with contingency table having both large and small cells value? Is there a reduced formulas of the Fisher exact test that I can easily deploy in a non scientific programming environment? Is there another test more adapted to my data? I know that there is a possibility to apply the Yate's correction to the Chi Square but it seems that this solution is very conservative too...

Thanks,",1,1526705359.0,8kged3,False,"Hi,

I am trying to build a script \(in SQL\) that will return the p value of \(many\) 2x2 contingency tables... 

My problem is some cells can be \<5 therefore I should not use the chi square calculation and compute the Fisher test. However that makes the calculation impossible in my environment as some other cells are likely to be \>100

Ex of a table that I have

Cat1    Cat2    

GrA     150     98

GrB    12        2

The Fisher test will require for me to calculate the factorial of 262 which is not possible . How can I deal with contingency table having both large and small cells value? Is there a reduced formulas of the Fisher exact test that I can easily deploy in a non scientific programming environment? Is there another test more adapted to my data? I know that there is a possibility to apply the Yate's correction to the Chi Square but it seems that this solution is very conservative too...

Thanks,",0,"Hi,

I am trying to build a script \(in SQL\) that will return the p value of \(many\) 2x2 contingency tables... 

My problem is some cells can be \<5 therefore I should not use the chi square calculation and compute the Fisher test. However that makes the calculation impossible in my environment as some other cells are likely to be \>100

Ex of a table that I have

Cat1    Cat2    

GrA     150     98

GrB    12        2

The Fisher test will require for me to calculate the factorial of 262 which is not possible . How can I deal with contingency table having both large and small cells value? Is there a reduced formulas of the Fisher exact test that I can easily deploy in a non scientific programming environment? Is there another test more adapted to my data? I know that there is a possibility to apply the Yate's correction to the Chi Square but it seems that this solution is very conservative too...

Thanks,",1,statistics,54935,,Contingency table analysis when some cells can be large and others very small?,https://www.reddit.com/r/statistics/comments/8kged3/contingency_table_analysis_when_some_cells_can_be/,all_ads,2018-05-19 00:49:19,15 days 00:46:15.036584000,
What’s the difference?,38,1526608584.0,8k6a58,False,What’s the difference?,0,What’s the difference?,40,statistics,54935,,Difference between data analyst and data scientist?,https://www.reddit.com/r/statistics/comments/8k6a58/difference_between_data_analyst_and_data_scientist/,all_ads,2018-05-17 21:56:24,16 days 03:39:10.036584000,
"Hi!

I just wrote my Stats 1 exam which I've been really nervous about. We were allowed to bring the paper with us home and the first question is 5 true or false questions. 

I'm really eager to see how I did and was wondering if someone could help provide the answers? No explanation necessary. Can also send proof that I had the exam today so you don't think I cheat or anything.

Questions: https://imgur.com/a/6PcZgO6

Thanks a lot!",7,1526667068.0,8kc1yt,False,"Hi!

I just wrote my Stats 1 exam which I've been really nervous about. We were allowed to bring the paper with us home and the first question is 5 true or false questions. 

I'm really eager to see how I did and was wondering if someone could help provide the answers? No explanation necessary. Can also send proof that I had the exam today so you don't think I cheat or anything.

Questions: https://imgur.com/a/6PcZgO6

Thanks a lot!",0,"Hi!

I just wrote my Stats 1 exam which I've been really nervous about. We were allowed to bring the paper with us home and the first question is 5 true or false questions. 

I'm really eager to see how I did and was wondering if someone could help provide the answers? No explanation necessary. Can also send proof that I had the exam today so you don't think I cheat or anything.

Questions: https://imgur.com/a/6PcZgO6

Thanks a lot!",1,statistics,54935,,True and false questions from my paper,https://www.reddit.com/r/statistics/comments/8kc1yt/true_and_false_questions_from_my_paper/,all_ads,2018-05-18 14:11:08,15 days 11:24:26.036584000,
"I have a statistics question I was hoping some smart Redditor could help me with.

We are doing usability testing of 3 different workflows: A, B, C.

We want the same users to go through 3 different workflows to see which one is faster.

In order to fairly compare the different workflows we want to randomly select which order the users will go through the workflows \[A,B,C vs C,A,B, vs B,C,A etc\].

The users will perform 10 of the same task in each workflow.

Each task should take 20 seconds to 3 minutes to complete. 

How do I determine what sample size of users I will need in order to compare these workflows and say which one is faster?

\(if you care\- we are comparing a desktop vs iphone vs ipad workflow\)

Sorry if this is a basic question. I looked online but could only find information about comparing 2 things and I got a little confused by the rest of the stuff out there. I appreciate any help. Thanks so much Reddit!",23,1526595780.0,8k4ml4,False,"I have a statistics question I was hoping some smart Redditor could help me with.

We are doing usability testing of 3 different workflows: A, B, C.

We want the same users to go through 3 different workflows to see which one is faster.

In order to fairly compare the different workflows we want to randomly select which order the users will go through the workflows \[A,B,C vs C,A,B, vs B,C,A etc\].

The users will perform 10 of the same task in each workflow.

Each task should take 20 seconds to 3 minutes to complete. 

How do I determine what sample size of users I will need in order to compare these workflows and say which one is faster?

\(if you care\- we are comparing a desktop vs iphone vs ipad workflow\)

Sorry if this is a basic question. I looked online but could only find information about comparing 2 things and I got a little confused by the rest of the stuff out there. I appreciate any help. Thanks so much Reddit!",0,"I have a statistics question I was hoping some smart Redditor could help me with.

We are doing usability testing of 3 different workflows: A, B, C.

We want the same users to go through 3 different workflows to see which one is faster.

In order to fairly compare the different workflows we want to randomly select which order the users will go through the workflows \[A,B,C vs C,A,B, vs B,C,A etc\].

The users will perform 10 of the same task in each workflow.

Each task should take 20 seconds to 3 minutes to complete. 

How do I determine what sample size of users I will need in order to compare these workflows and say which one is faster?

\(if you care\- we are comparing a desktop vs iphone vs ipad workflow\)

Sorry if this is a basic question. I looked online but could only find information about comparing 2 things and I got a little confused by the rest of the stuff out there. I appreciate any help. Thanks so much Reddit!",23,statistics,54935,,Reddit how do I figure out sample size?,https://www.reddit.com/r/statistics/comments/8k4ml4/reddit_how_do_i_figure_out_sample_size/,all_ads,2018-05-17 18:23:00,16 days 07:12:34.036584000,
"There is the plot: https://i.imgur.com/wi6KFGh.png

I looked around online, and it seems this is a log-normal plot.

I'd like to find the formula for this plot, but I'm having difficulty. Various sites are noting that I have to get the average, standard deviation, normalize the information, etc.

Is there some free service that will automatically do that for me and spit out a formula that best fits this?",8,1526618999.0,8k7mtr,False,"There is the plot: https://i.imgur.com/wi6KFGh.png

I looked around online, and it seems this is a log-normal plot.

I'd like to find the formula for this plot, but I'm having difficulty. Various sites are noting that I have to get the average, standard deviation, normalize the information, etc.

Is there some free service that will automatically do that for me and spit out a formula that best fits this?",0,"There is the plot: https://i.imgur.com/wi6KFGh.png

I looked around online, and it seems this is a log-normal plot.

I'd like to find the formula for this plot, but I'm having difficulty. Various sites are noting that I have to get the average, standard deviation, normalize the information, etc.

Is there some free service that will automatically do that for me and spit out a formula that best fits this?",3,statistics,54935,,I have a whole bunch of points that look like they'd fit a log-normal plot. Any simple free tools I can use to get the formula?,https://www.reddit.com/r/statistics/comments/8k7mtr/i_have_a_whole_bunch_of_points_that_look_like/,all_ads,2018-05-18 00:49:59,16 days 00:45:35.036584000,
"Hello, Stats!

I'll have to ask that you forgive me for the nerdiness of the subject, but also... I certainly wish now that I'd paid better attention in class.

I'm looking for how to write an equation, with maybe some examples, when it comes into creating dice\-based games, as in, RPG tabletop and the like.

For this one example, it's very basic, and it's for a friend and whatever group he's doing stuff with. In a duel setting, each player has a d100, where 1\-10 is failure \(why don't they just use d10?\), so I understand that \(p=0.9, q=0.1\). Of course, each sample is independent, and the dice are assumed to be balanced. I suppose the problem here is that a certain ability in this game allows and entire section of player\-characters to re\-roll the die once per encounter, and the moderator is unconvinced that this is an unfair advantage even if nobody else has it, and I think:a.\) Hard factsb.\) I could brush up on stats before I forget it forever

I don't even know what model I'm dealing with anymore. Do I fabricate a sample size? Is this Hypothesis Testing? 

Again, I apologize if I've just barged in here making demands or if this is a bother. I thought that dice would be an excellent application of a skill I never thought I'd use!",5,1526635120.0,8k9fb2,False,"Hello, Stats!

I'll have to ask that you forgive me for the nerdiness of the subject, but also... I certainly wish now that I'd paid better attention in class.

I'm looking for how to write an equation, with maybe some examples, when it comes into creating dice\-based games, as in, RPG tabletop and the like.

For this one example, it's very basic, and it's for a friend and whatever group he's doing stuff with. In a duel setting, each player has a d100, where 1\-10 is failure \(why don't they just use d10?\), so I understand that \(p=0.9, q=0.1\). Of course, each sample is independent, and the dice are assumed to be balanced. I suppose the problem here is that a certain ability in this game allows and entire section of player\-characters to re\-roll the die once per encounter, and the moderator is unconvinced that this is an unfair advantage even if nobody else has it, and I think:a.\) Hard factsb.\) I could brush up on stats before I forget it forever

I don't even know what model I'm dealing with anymore. Do I fabricate a sample size? Is this Hypothesis Testing? 

Again, I apologize if I've just barged in here making demands or if this is a bother. I thought that dice would be an excellent application of a skill I never thought I'd use!",0,"Hello, Stats!

I'll have to ask that you forgive me for the nerdiness of the subject, but also... I certainly wish now that I'd paid better attention in class.

I'm looking for how to write an equation, with maybe some examples, when it comes into creating dice\-based games, as in, RPG tabletop and the like.

For this one example, it's very basic, and it's for a friend and whatever group he's doing stuff with. In a duel setting, each player has a d100, where 1\-10 is failure \(why don't they just use d10?\), so I understand that \(p=0.9, q=0.1\). Of course, each sample is independent, and the dice are assumed to be balanced. I suppose the problem here is that a certain ability in this game allows and entire section of player\-characters to re\-roll the die once per encounter, and the moderator is unconvinced that this is an unfair advantage even if nobody else has it, and I think:a.\) Hard factsb.\) I could brush up on stats before I forget it forever

I don't even know what model I'm dealing with anymore. Do I fabricate a sample size? Is this Hypothesis Testing? 

Again, I apologize if I've just barged in here making demands or if this is a bother. I thought that dice would be an excellent application of a skill I never thought I'd use!",1,statistics,54935,,Creating a probability model for dice games?,https://www.reddit.com/r/statistics/comments/8k9fb2/creating_a_probability_model_for_dice_games/,all_ads,2018-05-18 05:18:40,15 days 20:16:54.036584000,
"I have an introductory knowledge of statistics but have played and watched basketball for a very long time. I know much more then the average fan about the sport and could provide some good insight with the basketball portion, I just need someone a lot more knowledgable about statistics to assist me with that portion.",4,1526629044.0,8k8ss9,False,"I have an introductory knowledge of statistics but have played and watched basketball for a very long time. I know much more then the average fan about the sport and could provide some good insight with the basketball portion, I just need someone a lot more knowledgable about statistics to assist me with that portion.",0,"I have an introductory knowledge of statistics but have played and watched basketball for a very long time. I know much more then the average fan about the sport and could provide some good insight with the basketball portion, I just need someone a lot more knowledgable about statistics to assist me with that portion.",0,statistics,54935,,"With the legalization of sports betting, would anyone be interested in creating a model to predict the outcome of basketball games?",https://www.reddit.com/r/statistics/comments/8k8ss9/with_the_legalization_of_sports_betting_would/,all_ads,2018-05-18 03:37:24,15 days 21:58:10.036584000,
"The purpose of this reading assignment is to give you a sense of the sort of things that someone consulting a statistician will want to know, at least one that holds or is applying for a research grant.

Read Chapter 22 – Writing the Data Analysis Plan, by A.T. Panter, of the book How to Write a Successful Research Grant Application – A Guide for Social and Behavioral Scientists, Eds.

Pequegnat et al., 2nd ed., and answer the questions that appear after the preliminary notes.

 Note that this book is directed toward researchers in the social sciences, and this chapter is about the sort of questions to ask of a statistics expert. The book is written with the national granting bodies of the United States in mind, but a lot of the material, including the data analysis chapters, is relevant to grant applications to bodies in other countries such as Canada’s NSERC and SSHRC.


Before reading the chapter, know these definitions:


Grant: An amount of money awarded to a university or organization in order to do a proposed research project. Typically a competition is run for grants, in which a certain number of grants meant to advance research in a particular field or towards a particular goal are available, and researchers need to submit applications for these grants. In most cases, there are more applicants than grants available.


Granting body: An organization that awards grants. These include government bodies like NIH in the United States or NSERC in Canada, but a non-government organization like the Bill and Melinda Gates Foundation can also award grants.


Principal Investigator (PI): The person whom will be in charge of the proposed research. Also the person who actually submits the grant application. Typically this is the most qualified person on a research team. Every other researcher on the team is a co-investigator.


Reviewer: Person who reads grants applications and assigns scores to them based on their merit and relevance to the goal of the competition. Typically an application will be seen by several reviewers and an aggregate score is given.

Be sure to write the answers in your own words; they don’t have to be long.


Q1. What are three things that grant applicants must demonstrate to reviewers that they are able to do?
 

Q2. What is an advantage of thinking about your data analysis plan when doing a grant application?
 

Q3. What is one way to use the scores of many closely related variables or items together?
 

Q4. What is the main advantage of using a known measure over creating a new one?


Q5. What are two reasons why you would make a new scale, rather than rely on one in the literature?


Q6. What are three of the possible roles that a variable can have?

 
Q7. Name three of the special features of a research design. For each feature, name something that you should study or know about before using each of those features.
 

Q8. What is something that should be included in a descriptive analysis?

 
Q9. What is a question to ask yourself during model development?


Q10. How much of a grant application should be allocated to the data analysis plan?

Blog Mirror: http://www.stats-et-al.com/2018/05/reading-assignment-grant-applications.html",0,1526616495.0,8k7ba8,False,"The purpose of this reading assignment is to give you a sense of the sort of things that someone consulting a statistician will want to know, at least one that holds or is applying for a research grant.

Read Chapter 22 – Writing the Data Analysis Plan, by A.T. Panter, of the book How to Write a Successful Research Grant Application – A Guide for Social and Behavioral Scientists, Eds.

Pequegnat et al., 2nd ed., and answer the questions that appear after the preliminary notes.

 Note that this book is directed toward researchers in the social sciences, and this chapter is about the sort of questions to ask of a statistics expert. The book is written with the national granting bodies of the United States in mind, but a lot of the material, including the data analysis chapters, is relevant to grant applications to bodies in other countries such as Canada’s NSERC and SSHRC.


Before reading the chapter, know these definitions:


Grant: An amount of money awarded to a university or organization in order to do a proposed research project. Typically a competition is run for grants, in which a certain number of grants meant to advance research in a particular field or towards a particular goal are available, and researchers need to submit applications for these grants. In most cases, there are more applicants than grants available.


Granting body: An organization that awards grants. These include government bodies like NIH in the United States or NSERC in Canada, but a non-government organization like the Bill and Melinda Gates Foundation can also award grants.


Principal Investigator (PI): The person whom will be in charge of the proposed research. Also the person who actually submits the grant application. Typically this is the most qualified person on a research team. Every other researcher on the team is a co-investigator.


Reviewer: Person who reads grants applications and assigns scores to them based on their merit and relevance to the goal of the competition. Typically an application will be seen by several reviewers and an aggregate score is given.

Be sure to write the answers in your own words; they don’t have to be long.


Q1. What are three things that grant applicants must demonstrate to reviewers that they are able to do?
 

Q2. What is an advantage of thinking about your data analysis plan when doing a grant application?
 

Q3. What is one way to use the scores of many closely related variables or items together?
 

Q4. What is the main advantage of using a known measure over creating a new one?


Q5. What are two reasons why you would make a new scale, rather than rely on one in the literature?


Q6. What are three of the possible roles that a variable can have?

 
Q7. Name three of the special features of a research design. For each feature, name something that you should study or know about before using each of those features.
 

Q8. What is something that should be included in a descriptive analysis?

 
Q9. What is a question to ask yourself during model development?


Q10. How much of a grant application should be allocated to the data analysis plan?

Blog Mirror: http://www.stats-et-al.com/2018/05/reading-assignment-grant-applications.html",0,"The purpose of this reading assignment is to give you a sense of the sort of things that someone consulting a statistician will want to know, at least one that holds or is applying for a research grant.

Read Chapter 22 – Writing the Data Analysis Plan, by A.T. Panter, of the book How to Write a Successful Research Grant Application – A Guide for Social and Behavioral Scientists, Eds.

Pequegnat et al., 2nd ed., and answer the questions that appear after the preliminary notes.

 Note that this book is directed toward researchers in the social sciences, and this chapter is about the sort of questions to ask of a statistics expert. The book is written with the national granting bodies of the United States in mind, but a lot of the material, including the data analysis chapters, is relevant to grant applications to bodies in other countries such as Canada’s NSERC and SSHRC.


Before reading the chapter, know these definitions:


Grant: An amount of money awarded to a university or organization in order to do a proposed research project. Typically a competition is run for grants, in which a certain number of grants meant to advance research in a particular field or towards a particular goal are available, and researchers need to submit applications for these grants. In most cases, there are more applicants than grants available.


Granting body: An organization that awards grants. These include government bodies like NIH in the United States or NSERC in Canada, but a non-government organization like the Bill and Melinda Gates Foundation can also award grants.


Principal Investigator (PI): The person whom will be in charge of the proposed research. Also the person who actually submits the grant application. Typically this is the most qualified person on a research team. Every other researcher on the team is a co-investigator.


Reviewer: Person who reads grants applications and assigns scores to them based on their merit and relevance to the goal of the competition. Typically an application will be seen by several reviewers and an aggregate score is given.

Be sure to write the answers in your own words; they don’t have to be long.


Q1. What are three things that grant applicants must demonstrate to reviewers that they are able to do?
 

Q2. What is an advantage of thinking about your data analysis plan when doing a grant application?
 

Q3. What is one way to use the scores of many closely related variables or items together?
 

Q4. What is the main advantage of using a known measure over creating a new one?


Q5. What are two reasons why you would make a new scale, rather than rely on one in the literature?


Q6. What are three of the possible roles that a variable can have?

 
Q7. Name three of the special features of a research design. For each feature, name something that you should study or know about before using each of those features.
 

Q8. What is something that should be included in a descriptive analysis?

 
Q9. What is a question to ask yourself during model development?


Q10. How much of a grant application should be allocated to the data analysis plan?

Blog Mirror: http://www.stats-et-al.com/2018/05/reading-assignment-grant-applications.html",0,statistics,54935,,Reading Assignment for Novice Statistical Consultants and Grant Applicants,https://www.reddit.com/r/statistics/comments/8k7ba8/reading_assignment_for_novice_statistical/,all_ads,2018-05-18 00:08:15,16 days 01:27:19.036584000,
"Looking to see medical complications in patients with a certain disease undergoing total knee arthroplasty (TKA). Wanted to see if there was a difference between the years of patients with (TKAw) and without (TKAno) the disease undergoing TKA.

    Years    TKAw    TKAno
    2005    1507        897
    2006    1585        963
    2007    1713        1157
    2008    1727        1112
    2009    2012        1126
    2010    2111        1221
    2011    2315        1128
    2012    2363        1239 
    2013    2307        1221
    2014    2159        1250

When I use the Pearson's Correlation Coefficient, I get:

R = 0.810
Rsquared = 0.65
p = 0.004

Just want to make sure I did this correctly. 

Edit 2: 
Study type: Case-Control
",13,1526610997.0,8k6lpc,False,"Looking to see medical complications in patients with a certain disease undergoing total knee arthroplasty (TKA). Wanted to see if there was a difference between the years of patients with (TKAw) and without (TKAno) the disease undergoing TKA.

    Years    TKAw    TKAno
    2005    1507        897
    2006    1585        963
    2007    1713        1157
    2008    1727        1112
    2009    2012        1126
    2010    2111        1221
    2011    2315        1128
    2012    2363        1239 
    2013    2307        1221
    2014    2159        1250

When I use the Pearson's Correlation Coefficient, I get:

R = 0.810
Rsquared = 0.65
p = 0.004

Just want to make sure I did this correctly. 

Edit 2: 
Study type: Case-Control
",0,"Looking to see medical complications in patients with a certain disease undergoing total knee arthroplasty (TKA). Wanted to see if there was a difference between the years of patients with (TKAw) and without (TKAno) the disease undergoing TKA.

    Years    TKAw    TKAno
    2005    1507        897
    2006    1585        963
    2007    1713        1157
    2008    1727        1112
    2009    2012        1126
    2010    2111        1221
    2011    2315        1128
    2012    2363        1239 
    2013    2307        1221
    2014    2159        1250

When I use the Pearson's Correlation Coefficient, I get:

R = 0.810
Rsquared = 0.65
p = 0.004

Just want to make sure I did this correctly. 

Edit 2: 
Study type: Case-Control
",1,statistics,54935,,Is this correct for Pearson's Correlation Coefficient?,https://www.reddit.com/r/statistics/comments/8k6lpc/is_this_correct_for_pearsons_correlation/,all_ads,2018-05-17 22:36:37,16 days 02:58:57.036584000,
,0,1526609365.0,8k6e19,False,,0,,0,statistics,54935,,A Beneficial Case For The Blind Leading The Blind?,https://www.reddit.com/r/probabilitytheory/comments/8k68hi/a_beneficial_case_for_the_blind_leading_the_blind/,all_ads,2018-05-17 22:09:25,16 days 03:26:09.036584000,19600.0
"I am unable to find much literature on the one-tailed test, most are two-tailed.  Thanks!",4,1526609322.0,8k6dub,False,"I am unable to find much literature on the one-tailed test, most are two-tailed.  Thanks!",0,"I am unable to find much literature on the one-tailed test, most are two-tailed.  Thanks!",1,statistics,54935,,How do I know the directionality in a one-tailed Mann Whitney-U test?,https://www.reddit.com/r/statistics/comments/8k6dub/how_do_i_know_the_directionality_in_a_onetailed/,all_ads,2018-05-17 22:08:42,16 days 03:26:52.036584000,
"Edit: Added relevant numbers.

Hi, I am writing my thesis, and my experiment groups are rather small \( N=7 and 8 respectively\)

There was no significant difference between them, so I could combine them into one bigger group.In my field of study \(sport science\) there is usually a valid reason to use the 90&#37; confidence interval, because of the small groups and the exercises not being harmful \(like medicine has a potential to be harmful\).

With N= 28 with 15 experiment and 13 control.I compared the effectiveness of some exercises, but it showed a p=,075 at 95&#37; confidence interval.Can I safely say that if something is not significant at ,075 at 95&#37; means that it is significant at 90&#37;?Of course with the added sidenote that a 95&#37; would be a more accurate figure, but that's besides the point I am trying to make.

Thank you for reading this, and I hope to recieve your input.

PS Sorry for my English, it is my second language.",22,1526578732.0,8k2yrh,False,"Edit: Added relevant numbers.

Hi, I am writing my thesis, and my experiment groups are rather small \( N=7 and 8 respectively\)

There was no significant difference between them, so I could combine them into one bigger group.In my field of study \(sport science\) there is usually a valid reason to use the 90&#37; confidence interval, because of the small groups and the exercises not being harmful \(like medicine has a potential to be harmful\).

With N= 28 with 15 experiment and 13 control.I compared the effectiveness of some exercises, but it showed a p=,075 at 95&#37; confidence interval.Can I safely say that if something is not significant at ,075 at 95&#37; means that it is significant at 90&#37;?Of course with the added sidenote that a 95&#37; would be a more accurate figure, but that's besides the point I am trying to make.

Thank you for reading this, and I hope to recieve your input.

PS Sorry for my English, it is my second language.",0,"Edit: Added relevant numbers.

Hi, I am writing my thesis, and my experiment groups are rather small \( N=7 and 8 respectively\)

There was no significant difference between them, so I could combine them into one bigger group.In my field of study \(sport science\) there is usually a valid reason to use the 90&#37; confidence interval, because of the small groups and the exercises not being harmful \(like medicine has a potential to be harmful\).

With N= 28 with 15 experiment and 13 control.I compared the effectiveness of some exercises, but it showed a p=,075 at 95&#37; confidence interval.Can I safely say that if something is not significant at ,075 at 95&#37; means that it is significant at 90&#37;?Of course with the added sidenote that a 95&#37; would be a more accurate figure, but that's besides the point I am trying to make.

Thank you for reading this, and I hope to recieve your input.

PS Sorry for my English, it is my second language.",5,statistics,54935,,90 versus 95 confidence interval (basic knowledge),https://www.reddit.com/r/statistics/comments/8k2yrh/90_versus_95_confidence_interval_basic_knowledge/,all_ads,2018-05-17 13:38:52,16 days 11:56:42.036584000,
"hello, Im slightly confused as to how to interpret some findings involving a hierarchical regression. 

I am looking at a paper that has conducted a hierarchical regression which involves four steps. With each step (more variables being added) the adjusted R squared decreases indicating to me that the gains from adding another variable are smaller than the costs, per the adjusted R^2 method. But im not sure how to interpret it if it looks like these different scenarios (pulled from the paper).

Model 1- Step 1 Ar2 .6(sig), Step 2 Ar2 .03(sig), Step 3 Ar2 .04 (sig), Step 4 Ar2 .02 (sig)

Model 2- Step 1 Ar2 .59(sig), Step 2 Ar2 .002(NONsig), Step 3 Ar2 .01 (NONsig), Step 4 Ar2 .02 (sig)

if you could help me understand how to interpret this that would be amazing. The authors of the paper make no attempt however, I believe it is important to explain and understand. ",2,1526601120.0,8k5am1,False,"hello, Im slightly confused as to how to interpret some findings involving a hierarchical regression. 

I am looking at a paper that has conducted a hierarchical regression which involves four steps. With each step (more variables being added) the adjusted R squared decreases indicating to me that the gains from adding another variable are smaller than the costs, per the adjusted R^2 method. But im not sure how to interpret it if it looks like these different scenarios (pulled from the paper).

Model 1- Step 1 Ar2 .6(sig), Step 2 Ar2 .03(sig), Step 3 Ar2 .04 (sig), Step 4 Ar2 .02 (sig)

Model 2- Step 1 Ar2 .59(sig), Step 2 Ar2 .002(NONsig), Step 3 Ar2 .01 (NONsig), Step 4 Ar2 .02 (sig)

if you could help me understand how to interpret this that would be amazing. The authors of the paper make no attempt however, I believe it is important to explain and understand. ",0,"hello, Im slightly confused as to how to interpret some findings involving a hierarchical regression. 

I am looking at a paper that has conducted a hierarchical regression which involves four steps. With each step (more variables being added) the adjusted R squared decreases indicating to me that the gains from adding another variable are smaller than the costs, per the adjusted R^2 method. But im not sure how to interpret it if it looks like these different scenarios (pulled from the paper).

Model 1- Step 1 Ar2 .6(sig), Step 2 Ar2 .03(sig), Step 3 Ar2 .04 (sig), Step 4 Ar2 .02 (sig)

Model 2- Step 1 Ar2 .59(sig), Step 2 Ar2 .002(NONsig), Step 3 Ar2 .01 (NONsig), Step 4 Ar2 .02 (sig)

if you could help me understand how to interpret this that would be amazing. The authors of the paper make no attempt however, I believe it is important to explain and understand. ",0,statistics,54935,,Hierarchical Regression- Adjusted R squared decrease but also significant.,https://www.reddit.com/r/statistics/comments/8k5am1/hierarchical_regression_adjusted_r_squared/,all_ads,2018-05-17 19:52:00,16 days 05:43:34.036584000,
"I have a BSc in math \(didn't take any stats courses though!\) but I work in the marketing field now. Our company has a lot of marketing data \(Adwords primarily\) and I've been tasked to analyze it, find trends, make bidding recommendations, and possibly build models.

I've read and worked through the OpenIntro textbook, which is great for an initial overview and is very basic.

What textbook should I read next?

I'm not interested in grinding through integrals or proving theorems. I'm familiar with R \- not a pro but I can figure things out as I go along. I'd prefer a book that has solutions to the exercises available \(either in the back of the book, or somewhere online\) since I learn best by solving problems. Thank you!",4,1526599839.0,8k54rd,False,"I have a BSc in math \(didn't take any stats courses though!\) but I work in the marketing field now. Our company has a lot of marketing data \(Adwords primarily\) and I've been tasked to analyze it, find trends, make bidding recommendations, and possibly build models.

I've read and worked through the OpenIntro textbook, which is great for an initial overview and is very basic.

What textbook should I read next?

I'm not interested in grinding through integrals or proving theorems. I'm familiar with R \- not a pro but I can figure things out as I go along. I'd prefer a book that has solutions to the exercises available \(either in the back of the book, or somewhere online\) since I learn best by solving problems. Thank you!",0,"I have a BSc in math \(didn't take any stats courses though!\) but I work in the marketing field now. Our company has a lot of marketing data \(Adwords primarily\) and I've been tasked to analyze it, find trends, make bidding recommendations, and possibly build models.

I've read and worked through the OpenIntro textbook, which is great for an initial overview and is very basic.

What textbook should I read next?

I'm not interested in grinding through integrals or proving theorems. I'm familiar with R \- not a pro but I can figure things out as I go along. I'd prefer a book that has solutions to the exercises available \(either in the back of the book, or somewhere online\) since I learn best by solving problems. Thank you!",0,statistics,54935,,[Textbook] Which textbook should I read next?,https://www.reddit.com/r/statistics/comments/8k54rd/textbook_which_textbook_should_i_read_next/,all_ads,2018-05-17 19:30:39,16 days 06:04:55.036584000,
"Hey!

The last few years, I've come to the understanding that crimes like random rape or assault towards women at night and/or in public is much more rare than random rape or assault on men.. And that the crimes against females in a huge majority is actually committed in the privacy of their home, by acquaintances. Can anyone help me provide sources for these claims today, or is it just BS? If what I'm taught is true, isn't it our responsibility to bring forth the truth, and thus, help women steer away from irrational fear in society today, instead helping men prepare for assault instead of leading then into false securities on a night out?",2,1526624808.0,8k8c3y,False,"Hey!

The last few years, I've come to the understanding that crimes like random rape or assault towards women at night and/or in public is much more rare than random rape or assault on men.. And that the crimes against females in a huge majority is actually committed in the privacy of their home, by acquaintances. Can anyone help me provide sources for these claims today, or is it just BS? If what I'm taught is true, isn't it our responsibility to bring forth the truth, and thus, help women steer away from irrational fear in society today, instead helping men prepare for assault instead of leading then into false securities on a night out?",0,"Hey!

The last few years, I've come to the understanding that crimes like random rape or assault towards women at night and/or in public is much more rare than random rape or assault on men.. And that the crimes against females in a huge majority is actually committed in the privacy of their home, by acquaintances. Can anyone help me provide sources for these claims today, or is it just BS? If what I'm taught is true, isn't it our responsibility to bring forth the truth, and thus, help women steer away from irrational fear in society today, instead helping men prepare for assault instead of leading then into false securities on a night out?",0,statistics,54935,,Fear mongering and incorrect statistics in society,https://www.reddit.com/r/statistics/comments/8k8c3y/fear_mongering_and_incorrect_statistics_in_society/,all_ads,2018-05-18 02:26:48,15 days 23:08:46.036584000,
"Programs like Duke, Chicago, UCLA. Any other thesis optional, non-pro masters?",2,1526620891.0,8k7vd5,False,"Programs like Duke, Chicago, UCLA. Any other thesis optional, non-pro masters?",0,"Programs like Duke, Chicago, UCLA. Any other thesis optional, non-pro masters?",0,statistics,54935,,What are the best thesis optional master programs in the country?,https://www.reddit.com/r/statistics/comments/8k7vd5/what_are_the_best_thesis_optional_master_programs/,all_ads,2018-05-18 01:21:31,16 days 00:14:03.036584000,
"I was hoping for some advice! I go to LSE in the UK am debating between applying directly for a MSc in Statistics after undergrad, or taking a 1\-2 year break to work for a bit in Investment banking.

 My degree is Mathematics with economics \-\- i've taken 2 economics courses, upper div calculus/linear algebra course a couple statistics courses, real analysis, and discrete maths. I've gotten 75\+ in most of my courses \(equivalent of a US 4.0\). Next year is my last year at school, and it is the only year when I'll be able to take 5 statistics courses \(most of my classes have been chosen for me thus far\).  Also over this summer I have an internship with an investment bank in London, and they may offer me a job at the end of the summer.

I was wondering if it was better to apply straight for a Msc in stats during my 3rd year or take the 1\-2 years to work. My reasoning for working is that I can save some money and I can have a better idea of the business side of things. I have heard that going from IB to MSc in Stats is unusual and I might not get into as good a program. ",2,1526591141.0,8k43hi,False,"I was hoping for some advice! I go to LSE in the UK am debating between applying directly for a MSc in Statistics after undergrad, or taking a 1\-2 year break to work for a bit in Investment banking.

 My degree is Mathematics with economics \-\- i've taken 2 economics courses, upper div calculus/linear algebra course a couple statistics courses, real analysis, and discrete maths. I've gotten 75\+ in most of my courses \(equivalent of a US 4.0\). Next year is my last year at school, and it is the only year when I'll be able to take 5 statistics courses \(most of my classes have been chosen for me thus far\).  Also over this summer I have an internship with an investment bank in London, and they may offer me a job at the end of the summer.

I was wondering if it was better to apply straight for a Msc in stats during my 3rd year or take the 1\-2 years to work. My reasoning for working is that I can save some money and I can have a better idea of the business side of things. I have heard that going from IB to MSc in Stats is unusual and I might not get into as good a program. ",0,"I was hoping for some advice! I go to LSE in the UK am debating between applying directly for a MSc in Statistics after undergrad, or taking a 1\-2 year break to work for a bit in Investment banking.

 My degree is Mathematics with economics \-\- i've taken 2 economics courses, upper div calculus/linear algebra course a couple statistics courses, real analysis, and discrete maths. I've gotten 75\+ in most of my courses \(equivalent of a US 4.0\). Next year is my last year at school, and it is the only year when I'll be able to take 5 statistics courses \(most of my classes have been chosen for me thus far\).  Also over this summer I have an internship with an investment bank in London, and they may offer me a job at the end of the summer.

I was wondering if it was better to apply straight for a Msc in stats during my 3rd year or take the 1\-2 years to work. My reasoning for working is that I can save some money and I can have a better idea of the business side of things. I have heard that going from IB to MSc in Stats is unusual and I might not get into as good a program. ",0,statistics,54935,,MSC decision: 1-2-year Gap of Investment Banking between Undergrad and MSc Statistics,https://www.reddit.com/r/statistics/comments/8k43hi/msc_decision_12year_gap_of_investment_banking/,all_ads,2018-05-17 17:05:41,16 days 08:29:53.036584000,
"* [Link to the article](https://fivethirtyeight.com/features/how-shoddy-statistics-found-a-home-in-sports-research/). 
* [Abstract from Sainani's critique of Magnitude-Based Inference](https://www.ncbi.nlm.nih.gov/pubmed/29683920); I haven't found a full PDF not behind a paywall.
* [Link to page with PDF of Sainani's Critique](https://journals.lww.com/acsm-msse/Abstract/publishahead/The_Problem_with__Magnitude_Based_Inference_.96919.aspx); Courtesy of /u/efrique 
* I believe this [link is the original paper](http://sportsci.org/jour/05/ambwgh.htm) by Batterham and Hopkins if you'd like to read about what MBI is exactly (/r/badstatistics)
",26,1526498360.0,8ju3ng,False,"* [Link to the article](https://fivethirtyeight.com/features/how-shoddy-statistics-found-a-home-in-sports-research/). 
* [Abstract from Sainani's critique of Magnitude-Based Inference](https://www.ncbi.nlm.nih.gov/pubmed/29683920); I haven't found a full PDF not behind a paywall.
* [Link to page with PDF of Sainani's Critique](https://journals.lww.com/acsm-msse/Abstract/publishahead/The_Problem_with__Magnitude_Based_Inference_.96919.aspx); Courtesy of /u/efrique 
* I believe this [link is the original paper](http://sportsci.org/jour/05/ambwgh.htm) by Batterham and Hopkins if you'd like to read about what MBI is exactly (/r/badstatistics)
",0,"* [Link to the article](https://fivethirtyeight.com/features/how-shoddy-statistics-found-a-home-in-sports-research/). 
* [Abstract from Sainani's critique of Magnitude-Based Inference](https://www.ncbi.nlm.nih.gov/pubmed/29683920); I haven't found a full PDF not behind a paywall.
* [Link to page with PDF of Sainani's Critique](https://journals.lww.com/acsm-msse/Abstract/publishahead/The_Problem_with__Magnitude_Based_Inference_.96919.aspx); Courtesy of /u/efrique 
* I believe this [link is the original paper](http://sportsci.org/jour/05/ambwgh.htm) by Batterham and Hopkins if you'd like to read about what MBI is exactly (/r/badstatistics)
",105,statistics,54935,,Fivethirtyeight: How Shoddy Statistics Found a Home in Sports Research,https://www.reddit.com/r/statistics/comments/8ju3ng/fivethirtyeight_how_shoddy_statistics_found_a/,all_ads,2018-05-16 15:19:20,17 days 10:16:14.036584000,
"Say 5 is best, and 1 is worst. I get the following number of responses for each answer:

    5   4   3   2   1
    -----------------
    25  13  2   0   0

In this case, the trend is obvious: the participants think 5. However, what about a case like:

    5   4   3   2   1
    -----------------
    15  13  6   3   3
5 is still the most chosen, but only by two votes - a weak preference. Or worse, when two answers have the same number of votes:

    5   4   3   2   1
    -----------------
    16  16  6   2   0

Or when all of the numbers are basically the same? How can I make sense of all of these cases?",11,1526545401.0,8jzw2k,False,"Say 5 is best, and 1 is worst. I get the following number of responses for each answer:

    5   4   3   2   1
    -----------------
    25  13  2   0   0

In this case, the trend is obvious: the participants think 5. However, what about a case like:

    5   4   3   2   1
    -----------------
    15  13  6   3   3
5 is still the most chosen, but only by two votes - a weak preference. Or worse, when two answers have the same number of votes:

    5   4   3   2   1
    -----------------
    16  16  6   2   0

Or when all of the numbers are basically the same? How can I make sense of all of these cases?",0,"Say 5 is best, and 1 is worst. I get the following number of responses for each answer:

    5   4   3   2   1
    -----------------
    25  13  2   0   0

In this case, the trend is obvious: the participants think 5. However, what about a case like:

    5   4   3   2   1
    -----------------
    15  13  6   3   3
5 is still the most chosen, but only by two votes - a weak preference. Or worse, when two answers have the same number of votes:

    5   4   3   2   1
    -----------------
    16  16  6   2   0

Or when all of the numbers are basically the same? How can I make sense of all of these cases?",10,statistics,54935,,How do you interpret a Likert scale distribution?,https://www.reddit.com/r/statistics/comments/8jzw2k/how_do_you_interpret_a_likert_scale_distribution/,all_ads,2018-05-17 04:23:21,16 days 21:12:13.036584000,
"I am a political scientist trying to explain variations in perception of corruption in the seventeen provinces of Spain. My universe is just that, seventeen cases. Is there a way to justify a linear regression with such small N?

(I am already getting a significant variable at the 1% confidence levels in a linear regression).",17,1526574653.0,8k2o59,False,"I am a political scientist trying to explain variations in perception of corruption in the seventeen provinces of Spain. My universe is just that, seventeen cases. Is there a way to justify a linear regression with such small N?

(I am already getting a significant variable at the 1% confidence levels in a linear regression).",0,"I am a political scientist trying to explain variations in perception of corruption in the seventeen provinces of Spain. My universe is just that, seventeen cases. Is there a way to justify a linear regression with such small N?

(I am already getting a significant variable at the 1% confidence levels in a linear regression).",1,statistics,54935,,Question about a linear regression with small N,https://www.reddit.com/r/statistics/comments/8k2o59/question_about_a_linear_regression_with_small_n/,all_ads,2018-05-17 12:30:53,16 days 13:04:41.036584000,
"My dream after graduation is to become a data scientist. My school offers two kinds of degrees in statistics, one is the BA of statistics, the other one is the BBA \(Bachelor of Business Administration\) of statistics, which teaches mostly quantitative stats. Which one should I choose for data scientist route?",8,1526543777.0,8jzpkr,False,"My dream after graduation is to become a data scientist. My school offers two kinds of degrees in statistics, one is the BA of statistics, the other one is the BBA \(Bachelor of Business Administration\) of statistics, which teaches mostly quantitative stats. Which one should I choose for data scientist route?",0,"My dream after graduation is to become a data scientist. My school offers two kinds of degrees in statistics, one is the BA of statistics, the other one is the BBA \(Bachelor of Business Administration\) of statistics, which teaches mostly quantitative stats. Which one should I choose for data scientist route?",2,statistics,54935,,Business Statistics Going Into Data Scientist,https://www.reddit.com/r/statistics/comments/8jzpkr/business_statistics_going_into_data_scientist/,all_ads,2018-05-17 03:56:17,16 days 21:39:17.036584000,
"Can someone describe in layman's terms (only took 1-2 courses of stats in college) why running a function such as findCorrelation in R to reduce/remove pair-wise correlations is important? If I'm running a step-wise regression, what could be the worst thing that could happen to my analysis? And is that the same thing as the most likely thing to happen to my analysis?",4,1526525029.0,8jxeed,False,"Can someone describe in layman's terms (only took 1-2 courses of stats in college) why running a function such as findCorrelation in R to reduce/remove pair-wise correlations is important? If I'm running a step-wise regression, what could be the worst thing that could happen to my analysis? And is that the same thing as the most likely thing to happen to my analysis?",1,"Can someone describe in layman's terms (only took 1-2 courses of stats in college) why running a function such as findCorrelation in R to reduce/remove pair-wise correlations is important? If I'm running a step-wise regression, what could be the worst thing that could happen to my analysis? And is that the same thing as the most likely thing to happen to my analysis?",7,statistics,54935,,Importance of reducing pair-wise correlations?,https://www.reddit.com/r/statistics/comments/8jxeed/importance_of_reducing_pairwise_correlations/,all_ads,2018-05-16 22:43:49,17 days 02:51:45.036584000,
"If I observe in the literature a Cohen's d of 1.78 in Study 1, and -0.22 in Study 2, without access to the raw data how could I assess whether this difference is significant or not?

One intuition I had was to perform a simulation generating a difference score between the two ds, creating a new distribution which I can then calculate 95% highest density intervals. 

Here's some R code. S = ""Study"". C = ""condition""

s1_c1 <- rnorm(1000, 0, 1)

s1_c2 <- rnorm(1000, 1.78, 1)

s1_d <- s1_c2 - s1_c1

s2_c1 <- rnorm(1000, 0, 1)

s2_c2 <- rnorm(1000, -0.22, 1)

s2_d <- s2_c2 - s2_c1

diff <- s2_d - s1_d

Is this even wrong?

*Edit - R formatting & added a bit of code.",1,1526530864.0,8jy662,False,"If I observe in the literature a Cohen's d of 1.78 in Study 1, and -0.22 in Study 2, without access to the raw data how could I assess whether this difference is significant or not?

One intuition I had was to perform a simulation generating a difference score between the two ds, creating a new distribution which I can then calculate 95% highest density intervals. 

Here's some R code. S = ""Study"". C = ""condition""

s1_c1 <- rnorm(1000, 0, 1)

s1_c2 <- rnorm(1000, 1.78, 1)

s1_d <- s1_c2 - s1_c1

s2_c1 <- rnorm(1000, 0, 1)

s2_c2 <- rnorm(1000, -0.22, 1)

s2_d <- s2_c2 - s2_c1

diff <- s2_d - s1_d

Is this even wrong?

*Edit - R formatting & added a bit of code.",0,"If I observe in the literature a Cohen's d of 1.78 in Study 1, and -0.22 in Study 2, without access to the raw data how could I assess whether this difference is significant or not?

One intuition I had was to perform a simulation generating a difference score between the two ds, creating a new distribution which I can then calculate 95% highest density intervals. 

Here's some R code. S = ""Study"". C = ""condition""

s1_c1 <- rnorm(1000, 0, 1)

s1_c2 <- rnorm(1000, 1.78, 1)

s1_d <- s1_c2 - s1_c1

s2_c1 <- rnorm(1000, 0, 1)

s2_c2 <- rnorm(1000, -0.22, 1)

s2_d <- s2_c2 - s2_c1

diff <- s2_d - s1_d

Is this even wrong?

*Edit - R formatting & added a bit of code.",4,statistics,54935,,Testing difference between two Cohen's d values,https://www.reddit.com/r/statistics/comments/8jy662/testing_difference_between_two_cohens_d_values/,all_ads,2018-05-17 00:21:04,17 days 01:14:30.036584000,
"When forecasting time-series data, one would only use part of the available data to build their model (""Training Data"") while keeping the most recent data on reserve (""Test Data"") in order to compare the actual, recent data with forecasts made from the model. Does this mean that after model checking and building, the Test Data will still go unutilized for model creation? 

For example, let's say I have data from 2004 - 2018 relating to sales of a product and I want to forecast sales for 2019-2020. I will use 2004-2016 numbers for my Training Data and 2017-2018 for my Test Data. After model building and model checking, will I essentially still use 2004-2016 data to forecast 2019-2020 numbers? It seems like it would be a waste to not use 2017-2018 data. Should I forecast with all available data points? 

Thank you ",5,1526545358.0,8jzvw8,False,"When forecasting time-series data, one would only use part of the available data to build their model (""Training Data"") while keeping the most recent data on reserve (""Test Data"") in order to compare the actual, recent data with forecasts made from the model. Does this mean that after model checking and building, the Test Data will still go unutilized for model creation? 

For example, let's say I have data from 2004 - 2018 relating to sales of a product and I want to forecast sales for 2019-2020. I will use 2004-2016 numbers for my Training Data and 2017-2018 for my Test Data. After model building and model checking, will I essentially still use 2004-2016 data to forecast 2019-2020 numbers? It seems like it would be a waste to not use 2017-2018 data. Should I forecast with all available data points? 

Thank you ",0,"When forecasting time-series data, one would only use part of the available data to build their model (""Training Data"") while keeping the most recent data on reserve (""Test Data"") in order to compare the actual, recent data with forecasts made from the model. Does this mean that after model checking and building, the Test Data will still go unutilized for model creation? 

For example, let's say I have data from 2004 - 2018 relating to sales of a product and I want to forecast sales for 2019-2020. I will use 2004-2016 numbers for my Training Data and 2017-2018 for my Test Data. After model building and model checking, will I essentially still use 2004-2016 data to forecast 2019-2020 numbers? It seems like it would be a waste to not use 2017-2018 data. Should I forecast with all available data points? 

Thank you ",0,statistics,54935,,Time Series Forecasting With All Data Points,https://www.reddit.com/r/statistics/comments/8jzvw8/time_series_forecasting_with_all_data_points/,all_ads,2018-05-17 04:22:38,16 days 21:12:56.036584000,
"Hey guys i need some help regarding a test that im going to conduct; and to be honest im pretty stupid when it comes to statistical analysis (already read some books explaining and comparing the methods but still cant comprehend a shit), thats why im this desprate for some insight.

I want to study the relationship between dialysis vintage / duration which consist of 3 groups (1-8 months, 9-69months, and 70-207 months) and nutrional status that has 3 variables; SGA Score (Mild, Moderate, Severe), Serum albumin concentration (<3.6 g/dL and >3.6 g/dL), and Body Fat Percentage (Male : Adequate >25%  and Inadequate <25%, Female : Adequare >30% and Inadequate <30%).

What statistical analysis should i use and why?

Thanks in advance, dear reddit citizen",11,1526525148.0,8jxez8,False,"Hey guys i need some help regarding a test that im going to conduct; and to be honest im pretty stupid when it comes to statistical analysis (already read some books explaining and comparing the methods but still cant comprehend a shit), thats why im this desprate for some insight.

I want to study the relationship between dialysis vintage / duration which consist of 3 groups (1-8 months, 9-69months, and 70-207 months) and nutrional status that has 3 variables; SGA Score (Mild, Moderate, Severe), Serum albumin concentration (<3.6 g/dL and >3.6 g/dL), and Body Fat Percentage (Male : Adequate >25%  and Inadequate <25%, Female : Adequare >30% and Inadequate <30%).

What statistical analysis should i use and why?

Thanks in advance, dear reddit citizen",0,"Hey guys i need some help regarding a test that im going to conduct; and to be honest im pretty stupid when it comes to statistical analysis (already read some books explaining and comparing the methods but still cant comprehend a shit), thats why im this desprate for some insight.

I want to study the relationship between dialysis vintage / duration which consist of 3 groups (1-8 months, 9-69months, and 70-207 months) and nutrional status that has 3 variables; SGA Score (Mild, Moderate, Severe), Serum albumin concentration (<3.6 g/dL and >3.6 g/dL), and Body Fat Percentage (Male : Adequate >25%  and Inadequate <25%, Female : Adequare >30% and Inadequate <30%).

What statistical analysis should i use and why?

Thanks in advance, dear reddit citizen",1,statistics,54935,,[ask] What statistical analysis should i use?,https://www.reddit.com/r/statistics/comments/8jxez8/ask_what_statistical_analysis_should_i_use/,all_ads,2018-05-16 22:45:48,17 days 02:49:46.036584000,
"So im trying to practice critically reviewing papers statistics in an attempt to enhance my skills of evaluation. However, I think I may have bitten off more than i can chew in the sense that i haven't worked with MANCOVA or moderation analysis before and therefore I am unsure whether I have a broad knowledge to be able to critically review this paper. I was wondering if I could get some pointers and help regarding the statistical analysis that was conducted and what the authors could have done to improve, their statistical analysis.

The paper in question: https://www.ncbi.nlm.nih.gov/pubmed/27054501

Let me know if you can not gain access to the paper I will then send you the PDF. 
Thank you in advance for your help and guidance i am really keen to advance my learning and you will be helping me a ton. 
P.s sorry if i dont understand concepts that you mention immediately",15,1526536369.0,8jyvet,False,"So im trying to practice critically reviewing papers statistics in an attempt to enhance my skills of evaluation. However, I think I may have bitten off more than i can chew in the sense that i haven't worked with MANCOVA or moderation analysis before and therefore I am unsure whether I have a broad knowledge to be able to critically review this paper. I was wondering if I could get some pointers and help regarding the statistical analysis that was conducted and what the authors could have done to improve, their statistical analysis.

The paper in question: https://www.ncbi.nlm.nih.gov/pubmed/27054501

Let me know if you can not gain access to the paper I will then send you the PDF. 
Thank you in advance for your help and guidance i am really keen to advance my learning and you will be helping me a ton. 
P.s sorry if i dont understand concepts that you mention immediately",0,"So im trying to practice critically reviewing papers statistics in an attempt to enhance my skills of evaluation. However, I think I may have bitten off more than i can chew in the sense that i haven't worked with MANCOVA or moderation analysis before and therefore I am unsure whether I have a broad knowledge to be able to critically review this paper. I was wondering if I could get some pointers and help regarding the statistical analysis that was conducted and what the authors could have done to improve, their statistical analysis.

The paper in question: https://www.ncbi.nlm.nih.gov/pubmed/27054501

Let me know if you can not gain access to the paper I will then send you the PDF. 
Thank you in advance for your help and guidance i am really keen to advance my learning and you will be helping me a ton. 
P.s sorry if i dont understand concepts that you mention immediately",1,statistics,54935,,Unsure whether this paper executed their statistical analysis correctly?,https://www.reddit.com/r/statistics/comments/8jyvet/unsure_whether_this_paper_executed_their/,all_ads,2018-05-17 01:52:49,16 days 23:42:45.036584000,
"I studied math and computer science in college, relatively strong math background with a focus in analysis, and a solid probability theory foundation. I've seen a lot of resources for self\-studying machine learning and the like, but where would one best learn the skills that a Statistics Masters/PhD might have in applied data analysis? Suppose that you're willing to put in the work in slogging through texts and doing problem sets. I guess there's sort of two parallel components:

1. Theoretical Knowledge \(Textbook recommendations? I've heard good things about Gelman's BDA. What would be the best way to use this book?\)
2. Practical Knowledge \(What are some resources for applying classical statistical models such as linear regression rather than fancy random forests or whatever?\)

EDIT:

Here's a list of books I think I might be useful. What would you cut out / add in? Would you reorder things at all? My main interest is predictive modeling. Do you think there might be some relevant data mining type books out there?

1. ~~Casella & Bergers's Statistical Inference~~ Wasserman's All of Statistics \(these seem to cover similar content; I've taken graduate courses in stochastic processes so a decent chunk of it might be review\)
2. Kutner's Applied Statistical Linear Models
3. Hair et. al's Multivariate Data Analysis
4. Gelman's Bayesian Data Analysis
5. Gelman's Data Analysis Using Regression and Multilevel Models \(do I need this \+ BDA?\)
6. Bishops's Pattern Recognition & Machine Learning OR Hastie et. al's Elements of Statistical Learning
7. Stoffer's Time Series Analysis and its Applications
8. Hoeting's Computational Statistics",19,1526461336.0,8jqv3l,False,"I studied math and computer science in college, relatively strong math background with a focus in analysis, and a solid probability theory foundation. I've seen a lot of resources for self\-studying machine learning and the like, but where would one best learn the skills that a Statistics Masters/PhD might have in applied data analysis? Suppose that you're willing to put in the work in slogging through texts and doing problem sets. I guess there's sort of two parallel components:

1. Theoretical Knowledge \(Textbook recommendations? I've heard good things about Gelman's BDA. What would be the best way to use this book?\)
2. Practical Knowledge \(What are some resources for applying classical statistical models such as linear regression rather than fancy random forests or whatever?\)

EDIT:

Here's a list of books I think I might be useful. What would you cut out / add in? Would you reorder things at all? My main interest is predictive modeling. Do you think there might be some relevant data mining type books out there?

1. ~~Casella & Bergers's Statistical Inference~~ Wasserman's All of Statistics \(these seem to cover similar content; I've taken graduate courses in stochastic processes so a decent chunk of it might be review\)
2. Kutner's Applied Statistical Linear Models
3. Hair et. al's Multivariate Data Analysis
4. Gelman's Bayesian Data Analysis
5. Gelman's Data Analysis Using Regression and Multilevel Models \(do I need this \+ BDA?\)
6. Bishops's Pattern Recognition & Machine Learning OR Hastie et. al's Elements of Statistical Learning
7. Stoffer's Time Series Analysis and its Applications
8. Hoeting's Computational Statistics",0,"I studied math and computer science in college, relatively strong math background with a focus in analysis, and a solid probability theory foundation. I've seen a lot of resources for self\-studying machine learning and the like, but where would one best learn the skills that a Statistics Masters/PhD might have in applied data analysis? Suppose that you're willing to put in the work in slogging through texts and doing problem sets. I guess there's sort of two parallel components:

1. Theoretical Knowledge \(Textbook recommendations? I've heard good things about Gelman's BDA. What would be the best way to use this book?\)
2. Practical Knowledge \(What are some resources for applying classical statistical models such as linear regression rather than fancy random forests or whatever?\)

EDIT:

Here's a list of books I think I might be useful. What would you cut out / add in? Would you reorder things at all? My main interest is predictive modeling. Do you think there might be some relevant data mining type books out there?

1. ~~Casella & Bergers's Statistical Inference~~ Wasserman's All of Statistics \(these seem to cover similar content; I've taken graduate courses in stochastic processes so a decent chunk of it might be review\)
2. Kutner's Applied Statistical Linear Models
3. Hair et. al's Multivariate Data Analysis
4. Gelman's Bayesian Data Analysis
5. Gelman's Data Analysis Using Regression and Multilevel Models \(do I need this \+ BDA?\)
6. Bishops's Pattern Recognition & Machine Learning OR Hastie et. al's Elements of Statistical Learning
7. Stoffer's Time Series Analysis and its Applications
8. Hoeting's Computational Statistics",42,statistics,54935,,Best resources to self-study graduate statistics?,https://www.reddit.com/r/statistics/comments/8jqv3l/best_resources_to_selfstudy_graduate_statistics/,all_ads,2018-05-16 05:02:16,17 days 20:33:18.036584000,
"(x-post from [here](https://www.reddit.com/r/datascience/comments/8jt6ek/learning_about_data_science_through_doing_a_hobby/))

Hi everyone,

I have an interest in the world of data science, even though a) I have almost no formal training in a quantitive field and b) I don't know a lot about data science.

In order to develop my quantitative skills incrementally and learn more about data science, I want a hobby project to work on that I hope would help me with this.

**Here are some things about me:**

- I have a business degree and did a year of statistics in university
- I'm currently doing a basic statistics course to reinforce the basics (and I'll probably continue doing 'basics learning' until I'm more comfortable with stats)
- I've done a bit of R, and know a lot of Ruby (but I can figure things out)
- I'm patient and in it for the long haul - I'm happy for this journey to take multiple years
- I appreciate the steep learning curve ahead of me

**The prospective hobby project:**

Currently, I work as a scrum master in a software company of 300 engineers that make a complex product. The company is organised into autonomous squads. Predictability of these squads is important because it allows us to make certain commitments to their customers. Squads work in fixed, two-week iterations (sprints). As people do the work, the work transitions through phases (""Open"" -> ""In Development"" -> ""Testing"" -> ""Done""). Work is quantified using story points. What I want to know is: given specific conditions and an amount of work, how likely is it that a squad would be able to complete all its work inside one sprint?

**My questions about the hobby project to this sub:**

- Is this a good approach to developing quantitative skills and learn about data science in the first place? (more of a meta question)
- Is the question the hobby project aims to answer a good one?
- Apart from this sub, where can I find help if I need stuck?
- And finally, how do I start breaking down this problem and start designing a possible solution?",4,1526494109.0,8jtr6p,False,"(x-post from [here](https://www.reddit.com/r/datascience/comments/8jt6ek/learning_about_data_science_through_doing_a_hobby/))

Hi everyone,

I have an interest in the world of data science, even though a) I have almost no formal training in a quantitive field and b) I don't know a lot about data science.

In order to develop my quantitative skills incrementally and learn more about data science, I want a hobby project to work on that I hope would help me with this.

**Here are some things about me:**

- I have a business degree and did a year of statistics in university
- I'm currently doing a basic statistics course to reinforce the basics (and I'll probably continue doing 'basics learning' until I'm more comfortable with stats)
- I've done a bit of R, and know a lot of Ruby (but I can figure things out)
- I'm patient and in it for the long haul - I'm happy for this journey to take multiple years
- I appreciate the steep learning curve ahead of me

**The prospective hobby project:**

Currently, I work as a scrum master in a software company of 300 engineers that make a complex product. The company is organised into autonomous squads. Predictability of these squads is important because it allows us to make certain commitments to their customers. Squads work in fixed, two-week iterations (sprints). As people do the work, the work transitions through phases (""Open"" -> ""In Development"" -> ""Testing"" -> ""Done""). Work is quantified using story points. What I want to know is: given specific conditions and an amount of work, how likely is it that a squad would be able to complete all its work inside one sprint?

**My questions about the hobby project to this sub:**

- Is this a good approach to developing quantitative skills and learn about data science in the first place? (more of a meta question)
- Is the question the hobby project aims to answer a good one?
- Apart from this sub, where can I find help if I need stuck?
- And finally, how do I start breaking down this problem and start designing a possible solution?",0,"(x-post from [here](https://www.reddit.com/r/datascience/comments/8jt6ek/learning_about_data_science_through_doing_a_hobby/))

Hi everyone,

I have an interest in the world of data science, even though a) I have almost no formal training in a quantitive field and b) I don't know a lot about data science.

In order to develop my quantitative skills incrementally and learn more about data science, I want a hobby project to work on that I hope would help me with this.

**Here are some things about me:**

- I have a business degree and did a year of statistics in university
- I'm currently doing a basic statistics course to reinforce the basics (and I'll probably continue doing 'basics learning' until I'm more comfortable with stats)
- I've done a bit of R, and know a lot of Ruby (but I can figure things out)
- I'm patient and in it for the long haul - I'm happy for this journey to take multiple years
- I appreciate the steep learning curve ahead of me

**The prospective hobby project:**

Currently, I work as a scrum master in a software company of 300 engineers that make a complex product. The company is organised into autonomous squads. Predictability of these squads is important because it allows us to make certain commitments to their customers. Squads work in fixed, two-week iterations (sprints). As people do the work, the work transitions through phases (""Open"" -> ""In Development"" -> ""Testing"" -> ""Done""). Work is quantified using story points. What I want to know is: given specific conditions and an amount of work, how likely is it that a squad would be able to complete all its work inside one sprint?

**My questions about the hobby project to this sub:**

- Is this a good approach to developing quantitative skills and learn about data science in the first place? (more of a meta question)
- Is the question the hobby project aims to answer a good one?
- Apart from this sub, where can I find help if I need stuck?
- And finally, how do I start breaking down this problem and start designing a possible solution?",5,statistics,54935,,Learning about statistics through doing a hobby project,https://www.reddit.com/r/statistics/comments/8jtr6p/learning_about_statistics_through_doing_a_hobby/,all_ads,2018-05-16 14:08:29,17 days 11:27:05.036584000,
"Hello statistics,

My topic is the impact of Corporate social responsibility on the customer loyalty.

This is my research [model](https://i.imgur.com/yIOn1lE.png) (Sorry just did it fast). The arrows show a positiv and hypothesised relationship between the constructs. For example: 
Hypothesis 1 (H1)= CSR has a positive impact on Service Quality.
There are 7 hypothesis to test.

In order to test them I did a survey with a 5-point Likert scale (1=Strongly disagree; 5=Strondly agree). The Construct CSR has 12 Questions/Items, SQ 20 etc.

Control variables like gender or age are available. 
________________

The paper i read used SEM (a similar paper PLS) for the research. Im too noobish to understand the method they used. So I ask if a multiple regression analysis is possible? ",9,1526536727.0,8jyww8,False,"Hello statistics,

My topic is the impact of Corporate social responsibility on the customer loyalty.

This is my research [model](https://i.imgur.com/yIOn1lE.png) (Sorry just did it fast). The arrows show a positiv and hypothesised relationship between the constructs. For example: 
Hypothesis 1 (H1)= CSR has a positive impact on Service Quality.
There are 7 hypothesis to test.

In order to test them I did a survey with a 5-point Likert scale (1=Strongly disagree; 5=Strondly agree). The Construct CSR has 12 Questions/Items, SQ 20 etc.

Control variables like gender or age are available. 
________________

The paper i read used SEM (a similar paper PLS) for the research. Im too noobish to understand the method they used. So I ask if a multiple regression analysis is possible? ",0,"Hello statistics,

My topic is the impact of Corporate social responsibility on the customer loyalty.

This is my research [model](https://i.imgur.com/yIOn1lE.png) (Sorry just did it fast). The arrows show a positiv and hypothesised relationship between the constructs. For example: 
Hypothesis 1 (H1)= CSR has a positive impact on Service Quality.
There are 7 hypothesis to test.

In order to test them I did a survey with a 5-point Likert scale (1=Strongly disagree; 5=Strondly agree). The Construct CSR has 12 Questions/Items, SQ 20 etc.

Control variables like gender or age are available. 
________________

The paper i read used SEM (a similar paper PLS) for the research. Im too noobish to understand the method they used. So I ask if a multiple regression analysis is possible? ",0,statistics,54935,,Bachelor Thesis: Is a multiple regression analysis possible with my Data?,https://www.reddit.com/r/statistics/comments/8jyww8/bachelor_thesis_is_a_multiple_regression_analysis/,all_ads,2018-05-17 01:58:47,16 days 23:36:47.036584000,
"I understand the slightly more jargon\-y explanation of r\-squared, that it represents the amount of variance in Y that is accounted for by the predictor\(s\). But how do we make sense of that in layman's terms, to somebody who has no prior training in statistics?

If I have an r\-squared value of 1.0, I think I'm right in saying that if we know values for all the predictors, then we can make a perfect prediction for the outcome. That's quite easy to explain in layman's terms. But what if my r\-squared value is 0.5? What does it actually mean if we can explain just 50&#37; of the variance in the outcome using the predictors? Any help in simplifying this is greatly appreciated",5,1526517742.0,8jwej9,False,"I understand the slightly more jargon\-y explanation of r\-squared, that it represents the amount of variance in Y that is accounted for by the predictor\(s\). But how do we make sense of that in layman's terms, to somebody who has no prior training in statistics?

If I have an r\-squared value of 1.0, I think I'm right in saying that if we know values for all the predictors, then we can make a perfect prediction for the outcome. That's quite easy to explain in layman's terms. But what if my r\-squared value is 0.5? What does it actually mean if we can explain just 50&#37; of the variance in the outcome using the predictors? Any help in simplifying this is greatly appreciated",0,"I understand the slightly more jargon\-y explanation of r\-squared, that it represents the amount of variance in Y that is accounted for by the predictor\(s\). But how do we make sense of that in layman's terms, to somebody who has no prior training in statistics?

If I have an r\-squared value of 1.0, I think I'm right in saying that if we know values for all the predictors, then we can make a perfect prediction for the outcome. That's quite easy to explain in layman's terms. But what if my r\-squared value is 0.5? What does it actually mean if we can explain just 50&#37; of the variance in the outcome using the predictors? Any help in simplifying this is greatly appreciated",1,statistics,54935,,How would you explain r-squared to a layman?,https://www.reddit.com/r/statistics/comments/8jwej9/how_would_you_explain_rsquared_to_a_layman/,all_ads,2018-05-16 20:42:22,17 days 04:53:12.036584000,
"How do you write a statistical model (mathematical) and hypothesis for a moderation with covariates?
The IV is work-family conflict, DV is burnout and the moderator is the type of policies implemented (3 types). Then there are also 2 covariates of education and gender. ",1,1526515422.0,8jw3bg,False,"How do you write a statistical model (mathematical) and hypothesis for a moderation with covariates?
The IV is work-family conflict, DV is burnout and the moderator is the type of policies implemented (3 types). Then there are also 2 covariates of education and gender. ",0,"How do you write a statistical model (mathematical) and hypothesis for a moderation with covariates?
The IV is work-family conflict, DV is burnout and the moderator is the type of policies implemented (3 types). Then there are also 2 covariates of education and gender. ",0,statistics,54935,,How do you write a statistical model and hypothesis for a moderation with covariates?,https://www.reddit.com/r/statistics/comments/8jw3bg/how_do_you_write_a_statistical_model_and/,all_ads,2018-05-16 20:03:42,17 days 05:31:52.036584000,
"Hi all, I am at a loss for searching when it comes to figuring out where to begin with my analysis. 

I have a list of subject lines and various click rates within each of the emails. I would like to run an analysis to find any correlation between words in the subject lines and click rates within the email.

I have access to SPSS, R, Python, and Excel. Any advice on where to begin or even just key words to search for in order to get a lead on how to do this?

Thanks in advance.  
",3,1526511320.0,8jvjn3,False,"Hi all, I am at a loss for searching when it comes to figuring out where to begin with my analysis. 

I have a list of subject lines and various click rates within each of the emails. I would like to run an analysis to find any correlation between words in the subject lines and click rates within the email.

I have access to SPSS, R, Python, and Excel. Any advice on where to begin or even just key words to search for in order to get a lead on how to do this?

Thanks in advance.  
",0,"Hi all, I am at a loss for searching when it comes to figuring out where to begin with my analysis. 

I have a list of subject lines and various click rates within each of the emails. I would like to run an analysis to find any correlation between words in the subject lines and click rates within the email.

I have access to SPSS, R, Python, and Excel. Any advice on where to begin or even just key words to search for in order to get a lead on how to do this?

Thanks in advance.  
",0,statistics,54935,,Analysis on email subject lines vs specific click rates within emails. - Where to start?,https://www.reddit.com/r/statistics/comments/8jvjn3/analysis_on_email_subject_lines_vs_specific_click/,all_ads,2018-05-16 18:55:20,17 days 06:40:14.036584000,
"I find this question somewhat tricky. How exactly do you calculate a general outcome in a probability tree?

For example, making a probability tree for a deck of 52 cards, how would you calculate all the chances of getting 2 Aces and one other card, regardless of order?",1,1526504547.0,8juq7j,False,"I find this question somewhat tricky. How exactly do you calculate a general outcome in a probability tree?

For example, making a probability tree for a deck of 52 cards, how would you calculate all the chances of getting 2 Aces and one other card, regardless of order?",0,"I find this question somewhat tricky. How exactly do you calculate a general outcome in a probability tree?

For example, making a probability tree for a deck of 52 cards, how would you calculate all the chances of getting 2 Aces and one other card, regardless of order?",1,statistics,54935,,Probability tree question.,https://www.reddit.com/r/statistics/comments/8juq7j/probability_tree_question/,all_ads,2018-05-16 17:02:27,17 days 08:33:07.036584000,
"Hi, I'm calculating the inter\-rater reliability for my study. It is a study measuring the size of a part of the foetal brain in mm so I have continuous data. 

I remeasured 30 patients and another rater measured the same 30 patients. In order to calculate the inter\-rater reliability, I am using the intraclass correlation coefficient \(ICC\) the Two\-Way Random\-Effects model so that my data can be generalised to other clinicians. 

I just can't seem to find anywhere whether I should use the single measure or average measure values from my SPSS output. Any help would be great thank you!! ",0,1526504215.0,8juovj,False,"Hi, I'm calculating the inter\-rater reliability for my study. It is a study measuring the size of a part of the foetal brain in mm so I have continuous data. 

I remeasured 30 patients and another rater measured the same 30 patients. In order to calculate the inter\-rater reliability, I am using the intraclass correlation coefficient \(ICC\) the Two\-Way Random\-Effects model so that my data can be generalised to other clinicians. 

I just can't seem to find anywhere whether I should use the single measure or average measure values from my SPSS output. Any help would be great thank you!! ",0,"Hi, I'm calculating the inter\-rater reliability for my study. It is a study measuring the size of a part of the foetal brain in mm so I have continuous data. 

I remeasured 30 patients and another rater measured the same 30 patients. In order to calculate the inter\-rater reliability, I am using the intraclass correlation coefficient \(ICC\) the Two\-Way Random\-Effects model so that my data can be generalised to other clinicians. 

I just can't seem to find anywhere whether I should use the single measure or average measure values from my SPSS output. Any help would be great thank you!! ",1,statistics,54935,,ICC for inter-rater reliability - single measures or average measures?,https://www.reddit.com/r/statistics/comments/8juovj/icc_for_interrater_reliability_single_measures_or/,all_ads,2018-05-16 16:56:55,17 days 08:38:39.036584000,
"I’ve recently been doing a lot of analysis on pigouvian taxes, such as carbon taxes and pollution taxes for my work.

The tax rates in question are only increasing, accounting for inflation, over time (this is largely driven by policy goals reflecting increasing desire to reduce pollutants).

My question is, when will (if ever) only positive changes in a variable ever effect the analysis?  For example, can I still correctly use and interpret OLS outputs when a variable has only positive changes?

Apologies if this is very basic or too vague.",2,1526452393.0,8jptya,False,"I’ve recently been doing a lot of analysis on pigouvian taxes, such as carbon taxes and pollution taxes for my work.

The tax rates in question are only increasing, accounting for inflation, over time (this is largely driven by policy goals reflecting increasing desire to reduce pollutants).

My question is, when will (if ever) only positive changes in a variable ever effect the analysis?  For example, can I still correctly use and interpret OLS outputs when a variable has only positive changes?

Apologies if this is very basic or too vague.",0,"I’ve recently been doing a lot of analysis on pigouvian taxes, such as carbon taxes and pollution taxes for my work.

The tax rates in question are only increasing, accounting for inflation, over time (this is largely driven by policy goals reflecting increasing desire to reduce pollutants).

My question is, when will (if ever) only positive changes in a variable ever effect the analysis?  For example, can I still correctly use and interpret OLS outputs when a variable has only positive changes?

Apologies if this is very basic or too vague.",6,statistics,54935,,When will only positive changes in a variable effect the validity of analysis?,https://www.reddit.com/r/statistics/comments/8jptya/when_will_only_positive_changes_in_a_variable/,all_ads,2018-05-16 02:33:13,17 days 23:02:21.036584000,
"For a statistics project, I have to find a correlation between the amount of coins minted in a year a coin was made, and the amount of coins in my sample of 100 coins.

I understand that this is a chi-square of sorts, but which one? ",7,1526453943.0,8jq0d9,False,"For a statistics project, I have to find a correlation between the amount of coins minted in a year a coin was made, and the amount of coins in my sample of 100 coins.

I understand that this is a chi-square of sorts, but which one? ",0,"For a statistics project, I have to find a correlation between the amount of coins minted in a year a coin was made, and the amount of coins in my sample of 100 coins.

I understand that this is a chi-square of sorts, but which one? ",6,statistics,54935,,Which test would be most appropriate?,https://www.reddit.com/r/statistics/comments/8jq0d9/which_test_would_be_most_appropriate/,all_ads,2018-05-16 02:59:03,17 days 22:36:31.036584000,
"I was going through an explanation of gap statistic [here](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/) when I happened to come across the **phrase null reference distribution of the data**. I don't have a statistics background and have been having trouble deciphering the texts online. 

From what I understand that to conduct a hypothesis testing, you'll need data points to compare against the actual data and these points are provided by a particular distribution of data. Using these two a statistical test will be conducted which will provide us with some information about the actual data we have. 

Am I understanding this correctly? If so, then how do we select this null reference distribution of data? The actual distribution could be anything. How do we select one then?

PS: I don't have a background in statistics. Whatever I know comes from reading stuff online or watching videos. It'd be nice if you could see me as a simpleton while answering. ",0,1526478099.0,8jsk9e,False,"I was going through an explanation of gap statistic [here](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/) when I happened to come across the **phrase null reference distribution of the data**. I don't have a statistics background and have been having trouble deciphering the texts online. 

From what I understand that to conduct a hypothesis testing, you'll need data points to compare against the actual data and these points are provided by a particular distribution of data. Using these two a statistical test will be conducted which will provide us with some information about the actual data we have. 

Am I understanding this correctly? If so, then how do we select this null reference distribution of data? The actual distribution could be anything. How do we select one then?

PS: I don't have a background in statistics. Whatever I know comes from reading stuff online or watching videos. It'd be nice if you could see me as a simpleton while answering. ",0,"I was going through an explanation of gap statistic [here](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/) when I happened to come across the **phrase null reference distribution of the data**. I don't have a statistics background and have been having trouble deciphering the texts online. 

From what I understand that to conduct a hypothesis testing, you'll need data points to compare against the actual data and these points are provided by a particular distribution of data. Using these two a statistical test will be conducted which will provide us with some information about the actual data we have. 

Am I understanding this correctly? If so, then how do we select this null reference distribution of data? The actual distribution could be anything. How do we select one then?

PS: I don't have a background in statistics. Whatever I know comes from reading stuff online or watching videos. It'd be nice if you could see me as a simpleton while answering. ",1,statistics,54935,,Null reference distribution of data,https://www.reddit.com/r/statistics/comments/8jsk9e/null_reference_distribution_of_data/,all_ads,2018-05-16 09:41:39,17 days 15:53:55.036584000,
"I'm doing statistical analysis in R on a dataset and the result has me slightly confused. In this test im comparing two yes-no outcomes, and seeing whether the first answer correlates to the second. Regardless, R says this has only one degree of freedom.

In the past, I've done a similar comparison of two animal behavior contests scored by a win or loss. Again seeing whether the outcome of the first is correlated to the outcome of the second. However, in this instance, R said this test has 3 degrees of freedom.

My issue is I fail to see how these are different in their number of degrees of freedom, as they're both a comparison of two nominal outcomes. You can have Y-Y, N-N, Y-N, N-Y for the first, and for the second you can have W-W, L-L, W-L, L-W, again 4 possible outcomes. So why does the first only have 1 degree of freedom while the second has 3? How are these not analogous?",2,1526477392.0,8jsi4h,False,"I'm doing statistical analysis in R on a dataset and the result has me slightly confused. In this test im comparing two yes-no outcomes, and seeing whether the first answer correlates to the second. Regardless, R says this has only one degree of freedom.

In the past, I've done a similar comparison of two animal behavior contests scored by a win or loss. Again seeing whether the outcome of the first is correlated to the outcome of the second. However, in this instance, R said this test has 3 degrees of freedom.

My issue is I fail to see how these are different in their number of degrees of freedom, as they're both a comparison of two nominal outcomes. You can have Y-Y, N-N, Y-N, N-Y for the first, and for the second you can have W-W, L-L, W-L, L-W, again 4 possible outcomes. So why does the first only have 1 degree of freedom while the second has 3? How are these not analogous?",0,"I'm doing statistical analysis in R on a dataset and the result has me slightly confused. In this test im comparing two yes-no outcomes, and seeing whether the first answer correlates to the second. Regardless, R says this has only one degree of freedom.

In the past, I've done a similar comparison of two animal behavior contests scored by a win or loss. Again seeing whether the outcome of the first is correlated to the outcome of the second. However, in this instance, R said this test has 3 degrees of freedom.

My issue is I fail to see how these are different in their number of degrees of freedom, as they're both a comparison of two nominal outcomes. You can have Y-Y, N-N, Y-N, N-Y for the first, and for the second you can have W-W, L-L, W-L, L-W, again 4 possible outcomes. So why does the first only have 1 degree of freedom while the second has 3? How are these not analogous?",1,statistics,54935,,Quick question regarding Chi Square degrees of freedom.,https://www.reddit.com/r/statistics/comments/8jsi4h/quick_question_regarding_chi_square_degrees_of/,all_ads,2018-05-16 09:29:52,17 days 16:05:42.036584000,
"Is there a test/effect size, or a need to test, percent changes between 2 groups?  I'm interested in testing if there is a difference between men's percent changes \-19.61 vs. 53.10.  Say for data that looks like this:

||Year 1 admissions|Year 2 Admissions |GROUP 1       &#37; Change |Year 1 admissions|Year 2 Admissions |GROUP 2       &#37; Change |
|:-|:-|:-|:-|:-|:-|:-|
|  Male    |50977 |40978 |\-19.61  |436518 |668306   |53.10    |
|Female |3492 |5417 | 55.13 |219892 |385224    |75.19    |

I read into a two sample test for proportions but that feels wrong because the actual props. are often larger than |1|.  Thanks!!",12,1526447752.0,8jp90h,False,"Is there a test/effect size, or a need to test, percent changes between 2 groups?  I'm interested in testing if there is a difference between men's percent changes \-19.61 vs. 53.10.  Say for data that looks like this:

||Year 1 admissions|Year 2 Admissions |GROUP 1       &#37; Change |Year 1 admissions|Year 2 Admissions |GROUP 2       &#37; Change |
|:-|:-|:-|:-|:-|:-|:-|
|  Male    |50977 |40978 |\-19.61  |436518 |668306   |53.10    |
|Female |3492 |5417 | 55.13 |219892 |385224    |75.19    |

I read into a two sample test for proportions but that feels wrong because the actual props. are often larger than |1|.  Thanks!!",0,"Is there a test/effect size, or a need to test, percent changes between 2 groups?  I'm interested in testing if there is a difference between men's percent changes \-19.61 vs. 53.10.  Say for data that looks like this:

||Year 1 admissions|Year 2 Admissions |GROUP 1       &#37; Change |Year 1 admissions|Year 2 Admissions |GROUP 2       &#37; Change |
|:-|:-|:-|:-|:-|:-|:-|
|  Male    |50977 |40978 |\-19.61  |436518 |668306   |53.10    |
|Female |3492 |5417 | 55.13 |219892 |385224    |75.19    |

I read into a two sample test for proportions but that feels wrong because the actual props. are often larger than |1|.  Thanks!!",5,statistics,54935,,Testing percent changes between two groups?,https://www.reddit.com/r/statistics/comments/8jp90h/testing_percent_changes_between_two_groups/,all_ads,2018-05-16 01:15:52,18 days 00:19:42.036584000,
"Hi all,

I was recently accepted into a M.S program for Statistics, and they make clear that the courses use software packages such as ""...Maple, MATLAB, Minitab, Python, SAS, and R—programs often used in places of employment—and expose students to real\-life data. ""

I am in the market for something that I can take notes and draw on \(e.g. a Surface Pro\) but I am unfamiliar with these programs and their potential load on a PC. I am worried about a scenario in which I have to wait for something to process while a cohort with a faster machine does not. This was a *significant* issue when I briefly worked with bioinformatics pipelines, such as MOTHUR and QIIME.

Does anyone have experience with some or all of these programs, and can recommend a minimum computing power?",35,1526423858.0,8jm29m,False,"Hi all,

I was recently accepted into a M.S program for Statistics, and they make clear that the courses use software packages such as ""...Maple, MATLAB, Minitab, Python, SAS, and R—programs often used in places of employment—and expose students to real\-life data. ""

I am in the market for something that I can take notes and draw on \(e.g. a Surface Pro\) but I am unfamiliar with these programs and their potential load on a PC. I am worried about a scenario in which I have to wait for something to process while a cohort with a faster machine does not. This was a *significant* issue when I briefly worked with bioinformatics pipelines, such as MOTHUR and QIIME.

Does anyone have experience with some or all of these programs, and can recommend a minimum computing power?",0,"Hi all,

I was recently accepted into a M.S program for Statistics, and they make clear that the courses use software packages such as ""...Maple, MATLAB, Minitab, Python, SAS, and R—programs often used in places of employment—and expose students to real\-life data. ""

I am in the market for something that I can take notes and draw on \(e.g. a Surface Pro\) but I am unfamiliar with these programs and their potential load on a PC. I am worried about a scenario in which I have to wait for something to process while a cohort with a faster machine does not. This was a *significant* issue when I briefly worked with bioinformatics pipelines, such as MOTHUR and QIIME.

Does anyone have experience with some or all of these programs, and can recommend a minimum computing power?",13,statistics,54935,,Tech Question: How much computing power is necessary?,https://www.reddit.com/r/statistics/comments/8jm29m/tech_question_how_much_computing_power_is/,all_ads,2018-05-15 18:37:38,18 days 06:57:56.036584000,
"I’m researching 50 movies that showcase if black people dying first or not is a myth, what would be a good formula or way I could make this project great? ",9,1526463836.0,8jr5cl,False,"I’m researching 50 movies that showcase if black people dying first or not is a myth, what would be a good formula or way I could make this project great? ",0,"I’m researching 50 movies that showcase if black people dying first or not is a myth, what would be a good formula or way I could make this project great? ",0,statistics,54935,,"I’m working on a stats project, I need some help...",https://www.reddit.com/r/statistics/comments/8jr5cl/im_working_on_a_stats_project_i_need_some_help/,all_ads,2018-05-16 05:43:56,17 days 19:51:38.036584000,
"Hello All,

I was wondering if there was a way to have multiple ROCs plotted on one graph in SPSS. 

Such as this [https://www.medcalc.org/manual/\_help/images/roc\_c\_3.png](https://www.medcalc.org/manual/_help/images/roc_c_3.png)

Thanks!",0,1526462857.0,8jr1c0,False,"Hello All,

I was wondering if there was a way to have multiple ROCs plotted on one graph in SPSS. 

Such as this [https://www.medcalc.org/manual/\_help/images/roc\_c\_3.png](https://www.medcalc.org/manual/_help/images/roc_c_3.png)

Thanks!",0,"Hello All,

I was wondering if there was a way to have multiple ROCs plotted on one graph in SPSS. 

Such as this [https://www.medcalc.org/manual/\_help/images/roc\_c\_3.png](https://www.medcalc.org/manual/_help/images/roc_c_3.png)

Thanks!",1,statistics,54935,,ROC Curve Graphs in SPSS,https://www.reddit.com/r/statistics/comments/8jr1c0/roc_curve_graphs_in_spss/,all_ads,2018-05-16 05:27:37,17 days 20:07:57.036584000,
"I'm thinking about running a logistic regression using genetic data, so there's about 30,000 predictors \(but only \~500 observations\). Will that probably just crash my little i5 laptop? I know I could just try it and see but I don't actually have the data yet, and still not sure what kind of logistic I would run. I'm thinking an elastic net.",8,1526435254.0,8jnkn6,False,"I'm thinking about running a logistic regression using genetic data, so there's about 30,000 predictors \(but only \~500 observations\). Will that probably just crash my little i5 laptop? I know I could just try it and see but I don't actually have the data yet, and still not sure what kind of logistic I would run. I'm thinking an elastic net.",0,"I'm thinking about running a logistic regression using genetic data, so there's about 30,000 predictors \(but only \~500 observations\). Will that probably just crash my little i5 laptop? I know I could just try it and see but I don't actually have the data yet, and still not sure what kind of logistic I would run. I'm thinking an elastic net.",3,statistics,54935,,How big can a dataset be before you generally need to start using cloud services?,https://www.reddit.com/r/statistics/comments/8jnkn6/how_big_can_a_dataset_be_before_you_generally/,all_ads,2018-05-15 21:47:34,18 days 03:48:00.036584000,
"Ok I have a highschool project and the data I have is large and I don't know how to use any of these programs. Can anyone help me arrange this data so that I get a table that has total Adult white arrested for marijuana possession vs total adult black arrested for marijuana possession. I really can't figure this out, I need some guidance please. Thank you. 

The codebook is on this page:
https://www.icpsr.umich.edu/icpsrweb/NACJD/legacy/series/57/studies/35018?archive=NACJD&sortBy=7",1,1526461897.0,8jqxf2,False,"Ok I have a highschool project and the data I have is large and I don't know how to use any of these programs. Can anyone help me arrange this data so that I get a table that has total Adult white arrested for marijuana possession vs total adult black arrested for marijuana possession. I really can't figure this out, I need some guidance please. Thank you. 

The codebook is on this page:
https://www.icpsr.umich.edu/icpsrweb/NACJD/legacy/series/57/studies/35018?archive=NACJD&sortBy=7",0,"Ok I have a highschool project and the data I have is large and I don't know how to use any of these programs. Can anyone help me arrange this data so that I get a table that has total Adult white arrested for marijuana possession vs total adult black arrested for marijuana possession. I really can't figure this out, I need some guidance please. Thank you. 

The codebook is on this page:
https://www.icpsr.umich.edu/icpsrweb/NACJD/legacy/series/57/studies/35018?archive=NACJD&sortBy=7",1,statistics,54935,,SPSS HELP whats going on??,https://www.reddit.com/r/statistics/comments/8jqxf2/spss_help_whats_going_on/,all_ads,2018-05-16 05:11:37,17 days 20:23:57.036584000,
"Hello all, I am working through a homework that I have using SAS, I am a novice with SAS and have been trying to code this but can not get the first result in the data set to show. I also am wondering what the proper coding would be for this assignment I am working on?  [http://math.usu.edu/jrstevens/biostat/Homework1.pdf](http://math.usu.edu/jrstevens/biostat/Homework1.pdf) 

Any help with SAS coding is appreciated.",4,1526440717.0,8jobgw,False,"Hello all, I am working through a homework that I have using SAS, I am a novice with SAS and have been trying to code this but can not get the first result in the data set to show. I also am wondering what the proper coding would be for this assignment I am working on?  [http://math.usu.edu/jrstevens/biostat/Homework1.pdf](http://math.usu.edu/jrstevens/biostat/Homework1.pdf) 

Any help with SAS coding is appreciated.",0,"Hello all, I am working through a homework that I have using SAS, I am a novice with SAS and have been trying to code this but can not get the first result in the data set to show. I also am wondering what the proper coding would be for this assignment I am working on?  [http://math.usu.edu/jrstevens/biostat/Homework1.pdf](http://math.usu.edu/jrstevens/biostat/Homework1.pdf) 

Any help with SAS coding is appreciated.",0,statistics,54935,,SAS Help,https://www.reddit.com/r/statistics/comments/8jobgw/sas_help/,all_ads,2018-05-15 23:18:37,18 days 02:16:57.036584000,
I'm using lassopack in stata but if doesn't provide p-values. Is this typical for lasso models?,15,1526388780.0,8jizat,False,I'm using lassopack in stata but if doesn't provide p-values. Is this typical for lasso models?,0,I'm using lassopack in stata but if doesn't provide p-values. Is this typical for lasso models?,15,statistics,54935,,Are p values generally reported in LASSO regressions?,https://www.reddit.com/r/statistics/comments/8jizat/are_p_values_generally_reported_in_lasso/,all_ads,2018-05-15 08:53:00,18 days 16:42:34.036584000,
"I'm not sure to what extent stats questions are appropriate on this sub, but /r/AskStatistics seems to be mostly dead, and I'm getting desperate with my dissertation due in 10 days. So I'd really appreciate anyone who could just quickly tell me if I'm doing the right thing.

I'm trying to do an ordinal regression analysis on my data. I have a sample size of 40 participants. Each of the participants are divided into 3 roughly equal groups (containing 15, 13 and 12 members,) which I'll just call Group 1, Group 2 and Group 3 Each participant answered a Likert-type survey containing 19 questions, and each question had a response from 1 to 5 (5-point Likert, 1 is the worst option, 5 is the best option.) I want to establish whether the **Group** and the **Question Number** (Independent variables) have an effect on the **Response** (Dependent) to the survey.

I want to use the following data structure to perform the regression in SPSS:

https://i.stack.imgur.com/G7y77.png

So in the first column, I would enter every Participant 19 times, one for every question 1-19. The same for whichever Group the participant is in. Then the Question would be 1-19 for every participant, and the Response would be the number corresponding to whichever response that participant picked for that question.

Someone who know statistics told me that reduplicating my participants and groups like this would be bad though, but I wasn't sure how else to do it, nor whether doing a regression like this wouldn't at least tell me something useful.

The question is: is there a better way to do a regression analysis for this kind of data? Or will this work?

Thank you so much for any kind soul who can help.",3,1526425963.0,8jmc21,False,"I'm not sure to what extent stats questions are appropriate on this sub, but /r/AskStatistics seems to be mostly dead, and I'm getting desperate with my dissertation due in 10 days. So I'd really appreciate anyone who could just quickly tell me if I'm doing the right thing.

I'm trying to do an ordinal regression analysis on my data. I have a sample size of 40 participants. Each of the participants are divided into 3 roughly equal groups (containing 15, 13 and 12 members,) which I'll just call Group 1, Group 2 and Group 3 Each participant answered a Likert-type survey containing 19 questions, and each question had a response from 1 to 5 (5-point Likert, 1 is the worst option, 5 is the best option.) I want to establish whether the **Group** and the **Question Number** (Independent variables) have an effect on the **Response** (Dependent) to the survey.

I want to use the following data structure to perform the regression in SPSS:

https://i.stack.imgur.com/G7y77.png

So in the first column, I would enter every Participant 19 times, one for every question 1-19. The same for whichever Group the participant is in. Then the Question would be 1-19 for every participant, and the Response would be the number corresponding to whichever response that participant picked for that question.

Someone who know statistics told me that reduplicating my participants and groups like this would be bad though, but I wasn't sure how else to do it, nor whether doing a regression like this wouldn't at least tell me something useful.

The question is: is there a better way to do a regression analysis for this kind of data? Or will this work?

Thank you so much for any kind soul who can help.",0,"I'm not sure to what extent stats questions are appropriate on this sub, but /r/AskStatistics seems to be mostly dead, and I'm getting desperate with my dissertation due in 10 days. So I'd really appreciate anyone who could just quickly tell me if I'm doing the right thing.

I'm trying to do an ordinal regression analysis on my data. I have a sample size of 40 participants. Each of the participants are divided into 3 roughly equal groups (containing 15, 13 and 12 members,) which I'll just call Group 1, Group 2 and Group 3 Each participant answered a Likert-type survey containing 19 questions, and each question had a response from 1 to 5 (5-point Likert, 1 is the worst option, 5 is the best option.) I want to establish whether the **Group** and the **Question Number** (Independent variables) have an effect on the **Response** (Dependent) to the survey.

I want to use the following data structure to perform the regression in SPSS:

https://i.stack.imgur.com/G7y77.png

So in the first column, I would enter every Participant 19 times, one for every question 1-19. The same for whichever Group the participant is in. Then the Question would be 1-19 for every participant, and the Response would be the number corresponding to whichever response that participant picked for that question.

Someone who know statistics told me that reduplicating my participants and groups like this would be bad though, but I wasn't sure how else to do it, nor whether doing a regression like this wouldn't at least tell me something useful.

The question is: is there a better way to do a regression analysis for this kind of data? Or will this work?

Thank you so much for any kind soul who can help.",2,statistics,54935,,Will this data format work for an ordinal regression in SPSS?,https://www.reddit.com/r/statistics/comments/8jmc21/will_this_data_format_work_for_an_ordinal/,all_ads,2018-05-15 19:12:43,18 days 06:22:51.036584000,
"I need some help in interpreting coefficients of a sqrt transformed dependent variable (transformation was needed to meet assumptions for linear regression).

[Coefficients table in question](https://imgur.com/gallery/j2eMy4L)

I am wondering how to interpret the marked coefficients? Do I just square them, or is there another trick?

If my dependent variable wasn’t transformed, I would simply say that for every unit of meal progression (point of consumption percentage), slope steepness (conservative slope) declines by 0.280 units for the hungry group. How do I interpret it when sqrt-transformed?

Any help highly appreciated :)
",3,1526401739.0,8jjzhr,False,"I need some help in interpreting coefficients of a sqrt transformed dependent variable (transformation was needed to meet assumptions for linear regression).

[Coefficients table in question](https://imgur.com/gallery/j2eMy4L)

I am wondering how to interpret the marked coefficients? Do I just square them, or is there another trick?

If my dependent variable wasn’t transformed, I would simply say that for every unit of meal progression (point of consumption percentage), slope steepness (conservative slope) declines by 0.280 units for the hungry group. How do I interpret it when sqrt-transformed?

Any help highly appreciated :)
",0,"I need some help in interpreting coefficients of a sqrt transformed dependent variable (transformation was needed to meet assumptions for linear regression).

[Coefficients table in question](https://imgur.com/gallery/j2eMy4L)

I am wondering how to interpret the marked coefficients? Do I just square them, or is there another trick?

If my dependent variable wasn’t transformed, I would simply say that for every unit of meal progression (point of consumption percentage), slope steepness (conservative slope) declines by 0.280 units for the hungry group. How do I interpret it when sqrt-transformed?

Any help highly appreciated :)
",5,statistics,54935,,How to interpret square-root transformed dependent variable coefficients?,https://www.reddit.com/r/statistics/comments/8jjzhr/how_to_interpret_squareroot_transformed_dependent/,all_ads,2018-05-15 12:28:59,18 days 13:06:35.036584000,
"Hello, I'd like to ask for advice & opinions from the people who participate in this subreddit on what would be considered appropriate to ask.

I read a post here not so long ago where the poster seems to feel as though they are being asked to do free consulting, or that others are performing free consulting. This seemed to genuinely be a concern to them, which I can understand.

&nbsp;

With this, and the subreddit guidelines in mind, I've avoided asking questions in this subreddit.

I'm studying statistics at the moment so my questions would fall under the homework category. 

That being said, I have been reaching out to r/homeworkhelp, r/askstatistics, and a couple of other places. The types of questions I'm asking seem to often get up-voted but not responded to.

&nbsp;

I was wondering how people would feel if after first trying everything that I can think of, asking everywhere that I'm able to find people offering advice and _still_ not coming any closer to finding an answer, I posted questions here?

The places I've been asking for assistance outside of reddit are mostly maths-focused discord servers. If anyone could recommend somewhere else I can add to my list of places to ask for advice I'll gladly take that help.

&nbsp;

Thank you for taking the time to read this, and for your thoughts and opinions. I greatly appreciate it.",8,1526391875.0,8jj8u5,False,"Hello, I'd like to ask for advice & opinions from the people who participate in this subreddit on what would be considered appropriate to ask.

I read a post here not so long ago where the poster seems to feel as though they are being asked to do free consulting, or that others are performing free consulting. This seemed to genuinely be a concern to them, which I can understand.

&nbsp;

With this, and the subreddit guidelines in mind, I've avoided asking questions in this subreddit.

I'm studying statistics at the moment so my questions would fall under the homework category. 

That being said, I have been reaching out to r/homeworkhelp, r/askstatistics, and a couple of other places. The types of questions I'm asking seem to often get up-voted but not responded to.

&nbsp;

I was wondering how people would feel if after first trying everything that I can think of, asking everywhere that I'm able to find people offering advice and _still_ not coming any closer to finding an answer, I posted questions here?

The places I've been asking for assistance outside of reddit are mostly maths-focused discord servers. If anyone could recommend somewhere else I can add to my list of places to ask for advice I'll gladly take that help.

&nbsp;

Thank you for taking the time to read this, and for your thoughts and opinions. I greatly appreciate it.",0,"Hello, I'd like to ask for advice & opinions from the people who participate in this subreddit on what would be considered appropriate to ask.

I read a post here not so long ago where the poster seems to feel as though they are being asked to do free consulting, or that others are performing free consulting. This seemed to genuinely be a concern to them, which I can understand.

&nbsp;

With this, and the subreddit guidelines in mind, I've avoided asking questions in this subreddit.

I'm studying statistics at the moment so my questions would fall under the homework category. 

That being said, I have been reaching out to r/homeworkhelp, r/askstatistics, and a couple of other places. The types of questions I'm asking seem to often get up-voted but not responded to.

&nbsp;

I was wondering how people would feel if after first trying everything that I can think of, asking everywhere that I'm able to find people offering advice and _still_ not coming any closer to finding an answer, I posted questions here?

The places I've been asking for assistance outside of reddit are mostly maths-focused discord servers. If anyone could recommend somewhere else I can add to my list of places to ask for advice I'll gladly take that help.

&nbsp;

Thank you for taking the time to read this, and for your thoughts and opinions. I greatly appreciate it.",6,statistics,54935,,On asking for assistance,https://www.reddit.com/r/statistics/comments/8jj8u5/on_asking_for_assistance/,all_ads,2018-05-15 09:44:35,18 days 15:50:59.036584000,
"I'm not sure exactly how it goes but I heard a story once along the lines of --

A man was afraid to fly on a plane because he was scared someone would have a bomb on board. Odds of someone having one is (let's just say) 1 million/1..so the man figures if he takes a bomb on the plane he will be safer because the odds of there being 2 bombs on a plane are 100 million/1 ...

Not sure if I explained it right or if it even makes much sense ... But I'm wondering
 1) if it makes any sense in the 1st place.
 2) Would he really be any safer ??
3) is there a name for something like this ? 
4) do any other factors come into play in something like this ? 



I'm high af ",6,1526436303.0,8jnpjq,False,"I'm not sure exactly how it goes but I heard a story once along the lines of --

A man was afraid to fly on a plane because he was scared someone would have a bomb on board. Odds of someone having one is (let's just say) 1 million/1..so the man figures if he takes a bomb on the plane he will be safer because the odds of there being 2 bombs on a plane are 100 million/1 ...

Not sure if I explained it right or if it even makes much sense ... But I'm wondering
 1) if it makes any sense in the 1st place.
 2) Would he really be any safer ??
3) is there a name for something like this ? 
4) do any other factors come into play in something like this ? 



I'm high af ",0,"I'm not sure exactly how it goes but I heard a story once along the lines of --

A man was afraid to fly on a plane because he was scared someone would have a bomb on board. Odds of someone having one is (let's just say) 1 million/1..so the man figures if he takes a bomb on the plane he will be safer because the odds of there being 2 bombs on a plane are 100 million/1 ...

Not sure if I explained it right or if it even makes much sense ... But I'm wondering
 1) if it makes any sense in the 1st place.
 2) Would he really be any safer ??
3) is there a name for something like this ? 
4) do any other factors come into play in something like this ? 



I'm high af ",0,statistics,54935,,How & does this even work?,https://www.reddit.com/r/statistics/comments/8jnpjq/how_does_this_even_work/,all_ads,2018-05-15 22:05:03,18 days 03:30:31.036584000,
"I'm testing the relationship between a continuous independent (x) variable and a dummy dependent (y) variable, which is influenced by a dummy moderator (z). How do I go about testing such a regression in SPSS, do I need to create interaction terms? If so, how do you create an interaction term with a dummy moderator variable? Sorry if these are beginner questions - I've used Google but I feel a bit lost, so I appreciate any help!

Edit: would this be the right way of performing the regression in SPSS? https://imgur.com/a/HbO9CQc",4,1526421681.0,8jlsgu,False,"I'm testing the relationship between a continuous independent (x) variable and a dummy dependent (y) variable, which is influenced by a dummy moderator (z). How do I go about testing such a regression in SPSS, do I need to create interaction terms? If so, how do you create an interaction term with a dummy moderator variable? Sorry if these are beginner questions - I've used Google but I feel a bit lost, so I appreciate any help!

Edit: would this be the right way of performing the regression in SPSS? https://imgur.com/a/HbO9CQc",0,"I'm testing the relationship between a continuous independent (x) variable and a dummy dependent (y) variable, which is influenced by a dummy moderator (z). How do I go about testing such a regression in SPSS, do I need to create interaction terms? If so, how do you create an interaction term with a dummy moderator variable? Sorry if these are beginner questions - I've used Google but I feel a bit lost, so I appreciate any help!

Edit: would this be the right way of performing the regression in SPSS? https://imgur.com/a/HbO9CQc",1,statistics,54935,,Moderation with a dummy variable,https://www.reddit.com/r/statistics/comments/8jlsgu/moderation_with_a_dummy_variable/,all_ads,2018-05-15 18:01:21,18 days 07:34:13.036584000,
"I have a binary ordinal variable (high raters vs low raters) that I created from likert responses on an 11pt scale, such that scores 0-5 were classed as low raters and 6-10 as high raters. I also have categorized customer complaints. There are about 10 of such categoriess (e.g service complaints, product complaints, etc.).

I want to find out which category of customer complaints to focus on the most, so I was thinking of using a chi-square test of association in SPSS. Would this be correct, and if so, which statistic should I use? Phi and Cramers V or contingency coefficient?

Alternatively, is there a better test than a chi square? Maybe instead of transforming my likert rating to high vs low raters, I can use a one way ANOVA with customer complaint categories as a factor?

Thanks.",7,1526405534.0,8jk99t,False,"I have a binary ordinal variable (high raters vs low raters) that I created from likert responses on an 11pt scale, such that scores 0-5 were classed as low raters and 6-10 as high raters. I also have categorized customer complaints. There are about 10 of such categoriess (e.g service complaints, product complaints, etc.).

I want to find out which category of customer complaints to focus on the most, so I was thinking of using a chi-square test of association in SPSS. Would this be correct, and if so, which statistic should I use? Phi and Cramers V or contingency coefficient?

Alternatively, is there a better test than a chi square? Maybe instead of transforming my likert rating to high vs low raters, I can use a one way ANOVA with customer complaint categories as a factor?

Thanks.",0,"I have a binary ordinal variable (high raters vs low raters) that I created from likert responses on an 11pt scale, such that scores 0-5 were classed as low raters and 6-10 as high raters. I also have categorized customer complaints. There are about 10 of such categoriess (e.g service complaints, product complaints, etc.).

I want to find out which category of customer complaints to focus on the most, so I was thinking of using a chi-square test of association in SPSS. Would this be correct, and if so, which statistic should I use? Phi and Cramers V or contingency coefficient?

Alternatively, is there a better test than a chi square? Maybe instead of transforming my likert rating to high vs low raters, I can use a one way ANOVA with customer complaint categories as a factor?

Thanks.",2,statistics,54935,,Test of association between binary ordinal variable and nominal variable with several levels,https://www.reddit.com/r/statistics/comments/8jk99t/test_of_association_between_binary_ordinal/,all_ads,2018-05-15 13:32:14,18 days 12:03:20.036584000,
"Pm me if you’re interested 

Temecula, CA",7,1526440782.0,8jobsd,False,"Pm me if you’re interested 

Temecula, CA",0,"Pm me if you’re interested 

Temecula, CA",0,statistics,54935,,Statistician job opening - Southern California,https://www.reddit.com/r/statistics/comments/8jobsd/statistician_job_opening_southern_california/,all_ads,2018-05-15 23:19:42,18 days 02:15:52.036584000,
"So I have the option of taking either modeling in regression or nonparametirc statistics this fall. I want to take nonparametric, however from what I understand the professor for it does not the present the information well at all it's an easy A but eh I would like to challenge myself. The professor for the other course is supposed to be fantastic, however I'm curious as the end of the day what will hm contribute more towards my work like is one easier to learn on my own than the other?

I guess that depends on my own skills but eh thanks in advance for any input. ",14,1526364657.0,8jgdet,False,"So I have the option of taking either modeling in regression or nonparametirc statistics this fall. I want to take nonparametric, however from what I understand the professor for it does not the present the information well at all it's an easy A but eh I would like to challenge myself. The professor for the other course is supposed to be fantastic, however I'm curious as the end of the day what will hm contribute more towards my work like is one easier to learn on my own than the other?

I guess that depends on my own skills but eh thanks in advance for any input. ",0,"So I have the option of taking either modeling in regression or nonparametirc statistics this fall. I want to take nonparametric, however from what I understand the professor for it does not the present the information well at all it's an easy A but eh I would like to challenge myself. The professor for the other course is supposed to be fantastic, however I'm curious as the end of the day what will hm contribute more towards my work like is one easier to learn on my own than the other?

I guess that depends on my own skills but eh thanks in advance for any input. ",6,statistics,54935,,Kinda of a silly question which class will better prepare for designing experiments?,https://www.reddit.com/r/statistics/comments/8jgdet/kinda_of_a_silly_question_which_class_will_better/,all_ads,2018-05-15 02:10:57,18 days 23:24:37.036584000,
"I have a linear regression model with an ordinary scaled categorical predictor with three levels ('low', 'medium', & 'high'). Is it possible to test for linearity (Gauss-Markov)? I've done it visually and it looks fine, but I'm unsure whether linearity even applies to (non-binary) categorical predictors like mine. I've looked around this site but can't find an answer. Any help is greatly appreciated!",5,1526360209.0,8jft3s,False,"I have a linear regression model with an ordinary scaled categorical predictor with three levels ('low', 'medium', & 'high'). Is it possible to test for linearity (Gauss-Markov)? I've done it visually and it looks fine, but I'm unsure whether linearity even applies to (non-binary) categorical predictors like mine. I've looked around this site but can't find an answer. Any help is greatly appreciated!",0,"I have a linear regression model with an ordinary scaled categorical predictor with three levels ('low', 'medium', & 'high'). Is it possible to test for linearity (Gauss-Markov)? I've done it visually and it looks fine, but I'm unsure whether linearity even applies to (non-binary) categorical predictors like mine. I've looked around this site but can't find an answer. Any help is greatly appreciated!",8,statistics,54935,,Linearity assumption with categorical predictor,https://www.reddit.com/r/statistics/comments/8jft3s/linearity_assumption_with_categorical_predictor/,all_ads,2018-05-15 00:56:49,19 days 00:38:45.036584000,
"on the anova table my error SS is way higher than my regression SS, though both are significant according to p value.  ",9,1526336363.0,8jclws,False,"on the anova table my error SS is way higher than my regression SS, though both are significant according to p value.  ",0,"on the anova table my error SS is way higher than my regression SS, though both are significant according to p value.  ",16,statistics,54935,,does it matter in a regression if the error sum of squares is higher than the regression sum of squares?,https://www.reddit.com/r/statistics/comments/8jclws/does_it_matter_in_a_regression_if_the_error_sum/,all_ads,2018-05-14 18:19:23,19 days 07:16:11.036584000,
"I have 2 explanatory variables that I put in both the mean and variance equation of a GARCH\(1,1\)\-model. However, I do not include lag values of the dependent varible since it is not significant in the model. Is this okay? Or must the mean equation be an AR or ARMA model?",1,1526353060.0,8jeuhb,False,"I have 2 explanatory variables that I put in both the mean and variance equation of a GARCH\(1,1\)\-model. However, I do not include lag values of the dependent varible since it is not significant in the model. Is this okay? Or must the mean equation be an AR or ARMA model?",0,"I have 2 explanatory variables that I put in both the mean and variance equation of a GARCH\(1,1\)\-model. However, I do not include lag values of the dependent varible since it is not significant in the model. Is this okay? Or must the mean equation be an AR or ARMA model?",6,statistics,54935,,"Can the mean equation of a GARCH(1,1) model be just dependent on explanatory variables?",https://www.reddit.com/r/statistics/comments/8jeuhb/can_the_mean_equation_of_a_garch11_model_be_just/,all_ads,2018-05-14 22:57:40,19 days 02:37:54.036584000,
"Assuming there is only 1 correct way to place the remaining numbers. 

Because if you have the same situation (two numbers and two spaces remaining) for the row and 3x3 block, and there’s one space in the intersection... You can't have a probability divided into .5 three times. 

So maybe what I'm really asking is: are you guaranteed enough information to solve this row, column, and block? ",9,1526376204.0,8jhnhx,False,"Assuming there is only 1 correct way to place the remaining numbers. 

Because if you have the same situation (two numbers and two spaces remaining) for the row and 3x3 block, and there’s one space in the intersection... You can't have a probability divided into .5 three times. 

So maybe what I'm really asking is: are you guaranteed enough information to solve this row, column, and block? ",0,"Assuming there is only 1 correct way to place the remaining numbers. 

Because if you have the same situation (two numbers and two spaces remaining) for the row and 3x3 block, and there’s one space in the intersection... You can't have a probability divided into .5 three times. 

So maybe what I'm really asking is: are you guaranteed enough information to solve this row, column, and block? ",0,statistics,54935,,"Stupid probability question: if there are two numbers left in a row in Sudoku, and a perfect Sudoku-solving program can't be certain which number goes where, is the probability 50/50 that a particular number belongs in one of those remaining squares?",https://www.reddit.com/r/statistics/comments/8jhnhx/stupid_probability_question_if_there_are_two/,all_ads,2018-05-15 05:23:24,18 days 20:12:10.036584000,
"Hi guys, I'm not sure if this is the right forum to post this in but it's the first I thought of. Basically I'm sitting an exam in 2 days and the module has 13 topics. The exam has 8 questions of which we must answer 4. If I learn 8 topics is this enough so I'll always be able to answer 4 on the paper no matter what?",5,1526367936.0,8jgqvb,False,"Hi guys, I'm not sure if this is the right forum to post this in but it's the first I thought of. Basically I'm sitting an exam in 2 days and the module has 13 topics. The exam has 8 questions of which we must answer 4. If I learn 8 topics is this enough so I'll always be able to answer 4 on the paper no matter what?",0,"Hi guys, I'm not sure if this is the right forum to post this in but it's the first I thought of. Basically I'm sitting an exam in 2 days and the module has 13 topics. The exam has 8 questions of which we must answer 4. If I learn 8 topics is this enough so I'll always be able to answer 4 on the paper no matter what?",0,statistics,54935,,Exam preperation,https://www.reddit.com/r/statistics/comments/8jgqvb/exam_preperation/,all_ads,2018-05-15 03:05:36,18 days 22:29:58.036584000,
"I am comparing sample means to a population mean for the purposes of statistical replication with a subset of the population.

Finding the tracking error of the mean is simple enough. However, I have also been asked to find the tracking error of the standard deviation. This question seems slightly illogical to me because it's a descriptive statistic about the dispersion of the data. How would I quantify that into a statistic?",3,1526351266.0,8jelri,False,"I am comparing sample means to a population mean for the purposes of statistical replication with a subset of the population.

Finding the tracking error of the mean is simple enough. However, I have also been asked to find the tracking error of the standard deviation. This question seems slightly illogical to me because it's a descriptive statistic about the dispersion of the data. How would I quantify that into a statistic?",0,"I am comparing sample means to a population mean for the purposes of statistical replication with a subset of the population.

Finding the tracking error of the mean is simple enough. However, I have also been asked to find the tracking error of the standard deviation. This question seems slightly illogical to me because it's a descriptive statistic about the dispersion of the data. How would I quantify that into a statistic?",2,statistics,54935,,Comparing Standard Deviations,https://www.reddit.com/r/statistics/comments/8jelri/comparing_standard_deviations/,all_ads,2018-05-14 22:27:46,19 days 03:07:48.036584000,
"Trying to wrap my head around what kind of test I could do to determine if there was a significant difference in provincial mean graduation rates year over year.

For example, from 2010-2018 there was a new program put in place to increase highschool graduation rates. Knowing only the graduation rates for each year, and nothing about the population within each year, can you test whether the mean is higher for 2010-2018 as opposed to 2000-2009?

Would this just be a paired t test, where the groups are just the years/means?",7,1526339524.0,8jd0ue,False,"Trying to wrap my head around what kind of test I could do to determine if there was a significant difference in provincial mean graduation rates year over year.

For example, from 2010-2018 there was a new program put in place to increase highschool graduation rates. Knowing only the graduation rates for each year, and nothing about the population within each year, can you test whether the mean is higher for 2010-2018 as opposed to 2000-2009?

Would this just be a paired t test, where the groups are just the years/means?",0,"Trying to wrap my head around what kind of test I could do to determine if there was a significant difference in provincial mean graduation rates year over year.

For example, from 2010-2018 there was a new program put in place to increase highschool graduation rates. Knowing only the graduation rates for each year, and nothing about the population within each year, can you test whether the mean is higher for 2010-2018 as opposed to 2000-2009?

Would this just be a paired t test, where the groups are just the years/means?",3,statistics,54935,,How would you determine if a year over year change for the overall population was significant?,https://www.reddit.com/r/statistics/comments/8jd0ue/how_would_you_determine_if_a_year_over_year/,all_ads,2018-05-14 19:12:04,19 days 06:23:30.036584000,
"I came across a website a long time ago that would display all the economic sectors of a country in a grid-type graphic where the size of each rectangle would represent how big a part of the country's economy those sectors are. 

Does anyone know what site it is? Or what kind of graphic I am talking about so I can better google it?

Any help is appreciated.",3,1526311495.0,8jafpm,False,"I came across a website a long time ago that would display all the economic sectors of a country in a grid-type graphic where the size of each rectangle would represent how big a part of the country's economy those sectors are. 

Does anyone know what site it is? Or what kind of graphic I am talking about so I can better google it?

Any help is appreciated.",0,"I came across a website a long time ago that would display all the economic sectors of a country in a grid-type graphic where the size of each rectangle would represent how big a part of the country's economy those sectors are. 

Does anyone know what site it is? Or what kind of graphic I am talking about so I can better google it?

Any help is appreciated.",11,statistics,54935,,Looking for a website I saw long ago that displays a graphic of economic sectors per country,https://www.reddit.com/r/statistics/comments/8jafpm/looking_for_a_website_i_saw_long_ago_that/,all_ads,2018-05-14 11:24:55,19 days 14:10:39.036584000,
"Hi everyone,

I have been given a statistics project and I do not know how to approach it. 

I have to select a model car (Toyota Camry) and find 500 cars and the prices of those cars from years 2008-2018. 

When I find the 500 cars and the prices for each car, I need to calculate the mean for the price, the standard deviation, the equation of the regression line plot them in a graph, find the residuals and tell whether the car does or doesn't keep its value.

How can I approach this? My stats professor is not helpful at all and gets angry when we ask him too many questions. :/",4,1526351876.0,8jeoq5,False,"Hi everyone,

I have been given a statistics project and I do not know how to approach it. 

I have to select a model car (Toyota Camry) and find 500 cars and the prices of those cars from years 2008-2018. 

When I find the 500 cars and the prices for each car, I need to calculate the mean for the price, the standard deviation, the equation of the regression line plot them in a graph, find the residuals and tell whether the car does or doesn't keep its value.

How can I approach this? My stats professor is not helpful at all and gets angry when we ask him too many questions. :/",0,"Hi everyone,

I have been given a statistics project and I do not know how to approach it. 

I have to select a model car (Toyota Camry) and find 500 cars and the prices of those cars from years 2008-2018. 

When I find the 500 cars and the prices for each car, I need to calculate the mean for the price, the standard deviation, the equation of the regression line plot them in a graph, find the residuals and tell whether the car does or doesn't keep its value.

How can I approach this? My stats professor is not helpful at all and gets angry when we ask him too many questions. :/",0,statistics,54935,,Help with an Introduction to Stats project?,https://www.reddit.com/r/statistics/comments/8jeoq5/help_with_an_introduction_to_stats_project/,all_ads,2018-05-14 22:37:56,19 days 02:57:38.036584000,
I have a data set with the features contains both categorical variables and numerical variables and a numerical value I need to predict. Does it exist a test to rank (or better say how much related) the correlation between the value I need to predict and all the other features (contains both categorical variables and numerical variables)? ,1,1526310503.0,8jad38,False,I have a data set with the features contains both categorical variables and numerical variables and a numerical value I need to predict. Does it exist a test to rank (or better say how much related) the correlation between the value I need to predict and all the other features (contains both categorical variables and numerical variables)? ,0,I have a data set with the features contains both categorical variables and numerical variables and a numerical value I need to predict. Does it exist a test to rank (or better say how much related) the correlation between the value I need to predict and all the other features (contains both categorical variables and numerical variables)? ,7,statistics,54935,,Correlation of mixture (categorical and numerical ) variables with numerical variable.,https://www.reddit.com/r/statistics/comments/8jad38/correlation_of_mixture_categorical_and_numerical/,all_ads,2018-05-14 11:08:23,19 days 14:27:11.036584000,
"I am a High School senior equivalent and I really like statistics. I think my knowledge is somewhat basic, I learnt various distributions which some applicable to this scenario could be normal or student t distribution, learnt hypothesis testing and bivariate distributions. Thanks!",1,1526331694.0,8jc1id,False,"I am a High School senior equivalent and I really like statistics. I think my knowledge is somewhat basic, I learnt various distributions which some applicable to this scenario could be normal or student t distribution, learnt hypothesis testing and bivariate distributions. Thanks!",0,"I am a High School senior equivalent and I really like statistics. I think my knowledge is somewhat basic, I learnt various distributions which some applicable to this scenario could be normal or student t distribution, learnt hypothesis testing and bivariate distributions. Thanks!",3,statistics,54935,,How can I use the knowledge I gained about statistics practically on my finances and expenditure?,https://www.reddit.com/r/statistics/comments/8jc1id/how_can_i_use_the_knowledge_i_gained_about/,all_ads,2018-05-14 17:01:34,19 days 08:34:00.036584000,
"So I have a population that I would like to divide in 4 segments. And I want to do this on a (preferably) mathematically sound basis. I cannot share what kind of data is concerned exactly but I’ve thought of an example to help illustrate what I want to achieve. Imagine a population of househould expenditures ranging from 1.000 to 1.000.000. I want to divide this expenditure in ‘’small’’ and ‘’large’’. 

 
Then a certain percentage of this expenditure is spend on for example groceries. These proportions have to be divided into two groups as well. Say ‘’Low’’ and ‘’High’’. By doing so you will get 4 different segments illustrated below:

https://imgur.com/c3cXe9y

Now my question is: what is the best method to achieve this segmentation? 


It doesn’t have to be a split straight down the middle, preferably would even like to be able to ‘tweak’ the segmentation. I'm curious to hear your thoughts!

I don't know if it is in any way relevelant, but it concerns a population with a poisson distribution. 

",5,1526327529.0,8jbmgv,False,"So I have a population that I would like to divide in 4 segments. And I want to do this on a (preferably) mathematically sound basis. I cannot share what kind of data is concerned exactly but I’ve thought of an example to help illustrate what I want to achieve. Imagine a population of househould expenditures ranging from 1.000 to 1.000.000. I want to divide this expenditure in ‘’small’’ and ‘’large’’. 

 
Then a certain percentage of this expenditure is spend on for example groceries. These proportions have to be divided into two groups as well. Say ‘’Low’’ and ‘’High’’. By doing so you will get 4 different segments illustrated below:

https://imgur.com/c3cXe9y

Now my question is: what is the best method to achieve this segmentation? 


It doesn’t have to be a split straight down the middle, preferably would even like to be able to ‘tweak’ the segmentation. I'm curious to hear your thoughts!

I don't know if it is in any way relevelant, but it concerns a population with a poisson distribution. 

",0,"So I have a population that I would like to divide in 4 segments. And I want to do this on a (preferably) mathematically sound basis. I cannot share what kind of data is concerned exactly but I’ve thought of an example to help illustrate what I want to achieve. Imagine a population of househould expenditures ranging from 1.000 to 1.000.000. I want to divide this expenditure in ‘’small’’ and ‘’large’’. 

 
Then a certain percentage of this expenditure is spend on for example groceries. These proportions have to be divided into two groups as well. Say ‘’Low’’ and ‘’High’’. By doing so you will get 4 different segments illustrated below:

https://imgur.com/c3cXe9y

Now my question is: what is the best method to achieve this segmentation? 


It doesn’t have to be a split straight down the middle, preferably would even like to be able to ‘tweak’ the segmentation. I'm curious to hear your thoughts!

I don't know if it is in any way relevelant, but it concerns a population with a poisson distribution. 

",1,statistics,54935,,Dividing a population in 4,https://www.reddit.com/r/statistics/comments/8jbmgv/dividing_a_population_in_4/,all_ads,2018-05-14 15:52:09,19 days 09:43:25.036584000,
"I'm currently an undergraduate hoping to eventually study machine learning in graduate school. Next school year, I'll be taking my first statistics courses but I want to learn some this summer to supplement my machine learning studying. What are some textbooks I should look into? 

Preferably, I would like something that incorporates coding exercises (R or Python would be great). The courses I'll be starting this Fall use Rice's *Mathematical Statistics and Data Analysis* IIRC. I'm currently considering *All of Statistics* or *Elements of Statistical Learning*. If it's relevant, my current mathematical background in single-variable calculus and linear algebra.  ",13,1526273828.0,8j6wt1,False,"I'm currently an undergraduate hoping to eventually study machine learning in graduate school. Next school year, I'll be taking my first statistics courses but I want to learn some this summer to supplement my machine learning studying. What are some textbooks I should look into? 

Preferably, I would like something that incorporates coding exercises (R or Python would be great). The courses I'll be starting this Fall use Rice's *Mathematical Statistics and Data Analysis* IIRC. I'm currently considering *All of Statistics* or *Elements of Statistical Learning*. If it's relevant, my current mathematical background in single-variable calculus and linear algebra.  ",0,"I'm currently an undergraduate hoping to eventually study machine learning in graduate school. Next school year, I'll be taking my first statistics courses but I want to learn some this summer to supplement my machine learning studying. What are some textbooks I should look into? 

Preferably, I would like something that incorporates coding exercises (R or Python would be great). The courses I'll be starting this Fall use Rice's *Mathematical Statistics and Data Analysis* IIRC. I'm currently considering *All of Statistics* or *Elements of Statistical Learning*. If it's relevant, my current mathematical background in single-variable calculus and linear algebra.  ",16,statistics,54935,,Undergraduate textbook recommendations,https://www.reddit.com/r/statistics/comments/8j6wt1/undergraduate_textbook_recommendations/,all_ads,2018-05-14 00:57:08,20 days 00:38:26.036584000,
"This is more of a thinking problem and I was wondering why the answer given is correct.

**Q: A random experiment was conducted to see if a newly formulated drug produced a different effect on the mean time to recover for the standard drug is 26 days. Following an extensive random experiment involving 65 patients, the data gathered were used to construct a 95% confidence interval estimate for the mean recovery time in days for patients on the new drug. The 95% confidence interval was found to be (24.6, 27.8). What conclusion can be reached in this case concerning the new drug relative to the standard drug?**

Answer: There is insufficient evidence to reject the claim that there is no difference between the new drug and the standard drug with respect to mean recovery time. 

Is this because the mean time for the standard drug is 26 days and 26 fits in between the confidence interval given for the new drug and therefore we cant reject that there are no differences based on whats provided?",5,1526276049.0,8j76c3,False,"This is more of a thinking problem and I was wondering why the answer given is correct.

**Q: A random experiment was conducted to see if a newly formulated drug produced a different effect on the mean time to recover for the standard drug is 26 days. Following an extensive random experiment involving 65 patients, the data gathered were used to construct a 95% confidence interval estimate for the mean recovery time in days for patients on the new drug. The 95% confidence interval was found to be (24.6, 27.8). What conclusion can be reached in this case concerning the new drug relative to the standard drug?**

Answer: There is insufficient evidence to reject the claim that there is no difference between the new drug and the standard drug with respect to mean recovery time. 

Is this because the mean time for the standard drug is 26 days and 26 fits in between the confidence interval given for the new drug and therefore we cant reject that there are no differences based on whats provided?",0,"This is more of a thinking problem and I was wondering why the answer given is correct.

**Q: A random experiment was conducted to see if a newly formulated drug produced a different effect on the mean time to recover for the standard drug is 26 days. Following an extensive random experiment involving 65 patients, the data gathered were used to construct a 95% confidence interval estimate for the mean recovery time in days for patients on the new drug. The 95% confidence interval was found to be (24.6, 27.8). What conclusion can be reached in this case concerning the new drug relative to the standard drug?**

Answer: There is insufficient evidence to reject the claim that there is no difference between the new drug and the standard drug with respect to mean recovery time. 

Is this because the mean time for the standard drug is 26 days and 26 fits in between the confidence interval given for the new drug and therefore we cant reject that there are no differences based on whats provided?",5,statistics,54935,,Drawing a conclusion to a comparison of two drugs,https://www.reddit.com/r/statistics/comments/8j76c3/drawing_a_conclusion_to_a_comparison_of_two_drugs/,all_ads,2018-05-14 01:34:09,20 days 00:01:25.036584000,
"Hey guys, Coursera published a list of 10 blogs..... [https://blog.coursera.org/top-10-blogs-data-scientists/](https://blog.coursera.org/top-10-blogs-data-scientists/)",5,1526225890.0,8j2co2,False,"Hey guys, Coursera published a list of 10 blogs..... [https://blog.coursera.org/top-10-blogs-data-scientists/](https://blog.coursera.org/top-10-blogs-data-scientists/)",0,"Hey guys, Coursera published a list of 10 blogs..... [https://blog.coursera.org/top-10-blogs-data-scientists/](https://blog.coursera.org/top-10-blogs-data-scientists/)",55,statistics,54935,,10 Data Science Blogs by Coursera,https://www.reddit.com/r/statistics/comments/8j2co2/10_data_science_blogs_by_coursera/,all_ads,2018-05-13 11:38:10,20 days 13:57:24.036584000,
"The Five Factor Model is a personality theory used in psychological research and states that each individual falls on a spectrum in five facets of personality: extraversion, neuroticism, agreeableness, conscientiousness, and openness. Multiple studies have applied the five factor model to compare different groups of people (for example, people in different careers, age groups, cultures, etc.).

Suppose I have the percentiles of each of the five factors for a particular person and I wanted to compare it to the average percentiles of individuals of a certain culture. Which calculation(s) would give me the most ""similar"" results? So far, I'm only aware of the sum of squared differences, but I was wondering if there was a better way to evaluate similarity between two data sets in this particular situation.",5,1526303289.0,8j9tld,False,"The Five Factor Model is a personality theory used in psychological research and states that each individual falls on a spectrum in five facets of personality: extraversion, neuroticism, agreeableness, conscientiousness, and openness. Multiple studies have applied the five factor model to compare different groups of people (for example, people in different careers, age groups, cultures, etc.).

Suppose I have the percentiles of each of the five factors for a particular person and I wanted to compare it to the average percentiles of individuals of a certain culture. Which calculation(s) would give me the most ""similar"" results? So far, I'm only aware of the sum of squared differences, but I was wondering if there was a better way to evaluate similarity between two data sets in this particular situation.",0,"The Five Factor Model is a personality theory used in psychological research and states that each individual falls on a spectrum in five facets of personality: extraversion, neuroticism, agreeableness, conscientiousness, and openness. Multiple studies have applied the five factor model to compare different groups of people (for example, people in different careers, age groups, cultures, etc.).

Suppose I have the percentiles of each of the five factors for a particular person and I wanted to compare it to the average percentiles of individuals of a certain culture. Which calculation(s) would give me the most ""similar"" results? So far, I'm only aware of the sum of squared differences, but I was wondering if there was a better way to evaluate similarity between two data sets in this particular situation.",1,statistics,54935,,Comparing data sets for similarity,https://www.reddit.com/r/statistics/comments/8j9tld/comparing_data_sets_for_similarity/,all_ads,2018-05-14 09:08:09,19 days 16:27:25.036584000,
"Hey guys,

I was wondering what do you think the best resource is to start learning statistical genomics and/or population genetics? Ideally, I'm looking for some statistical guide/lessons from a genomics perspective. ",3,1526255332.0,8j4oe1,False,"Hey guys,

I was wondering what do you think the best resource is to start learning statistical genomics and/or population genetics? Ideally, I'm looking for some statistical guide/lessons from a genomics perspective. ",0,"Hey guys,

I was wondering what do you think the best resource is to start learning statistical genomics and/or population genetics? Ideally, I'm looking for some statistical guide/lessons from a genomics perspective. ",9,statistics,54935,,Statistical Genomics/Population Genetics,https://www.reddit.com/r/statistics/comments/8j4oe1/statistical_genomicspopulation_genetics/,all_ads,2018-05-13 19:48:52,20 days 05:46:42.036584000,
"survfit(outcome~1) 

Call: survfit(formula = outcome ~ 1)

      n  events  median 0.95LCL 0.95UCL 
    228     165     310     285     363 

I know this is a survival fit curve but I am having difficult understanding other parts of the summary besides (n) and understanding what is survfit().",1,1526289144.0,8j8ib1,False,"survfit(outcome~1) 

Call: survfit(formula = outcome ~ 1)

      n  events  median 0.95LCL 0.95UCL 
    228     165     310     285     363 

I know this is a survival fit curve but I am having difficult understanding other parts of the summary besides (n) and understanding what is survfit().",0,"survfit(outcome~1) 

Call: survfit(formula = outcome ~ 1)

      n  events  median 0.95LCL 0.95UCL 
    228     165     310     285     363 

I know this is a survival fit curve but I am having difficult understanding other parts of the summary besides (n) and understanding what is survfit().",0,statistics,54935,,Understanding survfit() function?,https://www.reddit.com/r/statistics/comments/8j8ib1/understanding_survfit_function/,all_ads,2018-05-14 05:12:24,19 days 20:23:10.036584000,
"Hey all, I was wondering what are some computer software programs recommended to learn? 

I’m interested in biostatistics / data science if that helps out. ",2,1526287551.0,8j8coz,False,"Hey all, I was wondering what are some computer software programs recommended to learn? 

I’m interested in biostatistics / data science if that helps out. ",0,"Hey all, I was wondering what are some computer software programs recommended to learn? 

I’m interested in biostatistics / data science if that helps out. ",0,statistics,54935,,Computer Software Programs,https://www.reddit.com/r/statistics/comments/8j8coz/computer_software_programs/,all_ads,2018-05-14 04:45:51,19 days 20:49:43.036584000,
"Dear statistics whizzes of Reddit, I need help in choosing a suitable data analysis test.

Which test would I use to see if the slopes of two regression lines are significantly different from each other?

I have a dependent variable, which changes over time (exerted forces, which gradually drop as time progresses). I have two different groups, and with linear regression I was able to determine that one group’s exerted forces fell more than the other group’s. I now want to test if this difference is significant. 
I have to mention that although I have equal numbers of subjects in both groups, each subject responded a different number of times. All in all, group one has 800 responses (from which exerted forces were obtained), whilst the other groups has 600 responses. 

Any suggestions?
",5,1526253743.0,8j4hnw,False,"Dear statistics whizzes of Reddit, I need help in choosing a suitable data analysis test.

Which test would I use to see if the slopes of two regression lines are significantly different from each other?

I have a dependent variable, which changes over time (exerted forces, which gradually drop as time progresses). I have two different groups, and with linear regression I was able to determine that one group’s exerted forces fell more than the other group’s. I now want to test if this difference is significant. 
I have to mention that although I have equal numbers of subjects in both groups, each subject responded a different number of times. All in all, group one has 800 responses (from which exerted forces were obtained), whilst the other groups has 600 responses. 

Any suggestions?
",0,"Dear statistics whizzes of Reddit, I need help in choosing a suitable data analysis test.

Which test would I use to see if the slopes of two regression lines are significantly different from each other?

I have a dependent variable, which changes over time (exerted forces, which gradually drop as time progresses). I have two different groups, and with linear regression I was able to determine that one group’s exerted forces fell more than the other group’s. I now want to test if this difference is significant. 
I have to mention that although I have equal numbers of subjects in both groups, each subject responded a different number of times. All in all, group one has 800 responses (from which exerted forces were obtained), whilst the other groups has 600 responses. 

Any suggestions?
",5,statistics,54935,,"Which statistical test to use, if I want to see if two regression lines are SIG. different from each other - if one slope is steeper?",https://www.reddit.com/r/statistics/comments/8j4hnw/which_statistical_test_to_use_if_i_want_to_see_if/,all_ads,2018-05-13 19:22:23,20 days 06:13:11.036584000,
"I spoke with Harry Crane, professor of statistics at Rutgers who's endorsed by Nassim Taleb, a few weeks ago. He had a rather radical but I think fascinating proposal for how to finally resolve the replication crisis.

It didn't involve changing p\-values. It didn't involve pre\-registration. Instead, it was about giving researchers ""skin in the game"" for their predictions.

We turned our conversation about this into a highly popular [2\-hour podcast episode](https://thejollyswagmen.com/new-blog/harrycrane) on probability and statistics, but if you want to go straight to his solution to the replication crisis,  skip to 59:27. I'd be fascinated to hear whether people think this would work. You can also listen on iTunes [here](https://itunes.apple.com/au/podcast/the-jolly-swagmen-podcast/id1236553683?mt=2).",15,1526314683.0,8jane2,False,"I spoke with Harry Crane, professor of statistics at Rutgers who's endorsed by Nassim Taleb, a few weeks ago. He had a rather radical but I think fascinating proposal for how to finally resolve the replication crisis.

It didn't involve changing p\-values. It didn't involve pre\-registration. Instead, it was about giving researchers ""skin in the game"" for their predictions.

We turned our conversation about this into a highly popular [2\-hour podcast episode](https://thejollyswagmen.com/new-blog/harrycrane) on probability and statistics, but if you want to go straight to his solution to the replication crisis,  skip to 59:27. I'd be fascinated to hear whether people think this would work. You can also listen on iTunes [here](https://itunes.apple.com/au/podcast/the-jolly-swagmen-podcast/id1236553683?mt=2).",0,"I spoke with Harry Crane, professor of statistics at Rutgers who's endorsed by Nassim Taleb, a few weeks ago. He had a rather radical but I think fascinating proposal for how to finally resolve the replication crisis.

It didn't involve changing p\-values. It didn't involve pre\-registration. Instead, it was about giving researchers ""skin in the game"" for their predictions.

We turned our conversation about this into a highly popular [2\-hour podcast episode](https://thejollyswagmen.com/new-blog/harrycrane) on probability and statistics, but if you want to go straight to his solution to the replication crisis,  skip to 59:27. I'd be fascinated to hear whether people think this would work. You can also listen on iTunes [here](https://itunes.apple.com/au/podcast/the-jolly-swagmen-podcast/id1236553683?mt=2).",0,statistics,54935,,A Radical Proposal to Fix the Replication Crisis,https://www.reddit.com/r/statistics/comments/8jane2/a_radical_proposal_to_fix_the_replication_crisis/,all_ads,2018-05-14 12:18:03,19 days 13:17:31.036584000,
"I'm using SPSS for the first time right now. I have an open & closed question with one response (tick the box that applies) and an ""other"" option. How should I analyze this data in SPSS? The question asks ""How often do you use cannabis?"" and the options are ""only tried it once"", ""special occasions"", ""daily"", ""weekly"", ""monthly"",""annually"", and ""other"". What I've done so far is created one numerical variable for the question and responses, including ""other"", and another string variable for the other responses given.",0,1526268956.0,8j6bqd,False,"I'm using SPSS for the first time right now. I have an open & closed question with one response (tick the box that applies) and an ""other"" option. How should I analyze this data in SPSS? The question asks ""How often do you use cannabis?"" and the options are ""only tried it once"", ""special occasions"", ""daily"", ""weekly"", ""monthly"",""annually"", and ""other"". What I've done so far is created one numerical variable for the question and responses, including ""other"", and another string variable for the other responses given.",0,"I'm using SPSS for the first time right now. I have an open & closed question with one response (tick the box that applies) and an ""other"" option. How should I analyze this data in SPSS? The question asks ""How often do you use cannabis?"" and the options are ""only tried it once"", ""special occasions"", ""daily"", ""weekly"", ""monthly"",""annually"", and ""other"". What I've done so far is created one numerical variable for the question and responses, including ""other"", and another string variable for the other responses given.",0,statistics,54935,,How do I analyze data from a closed & open question in a questionnaire in SPSS?,https://www.reddit.com/r/statistics/comments/8j6bqd/how_do_i_analyze_data_from_a_closed_open_question/,all_ads,2018-05-13 23:35:56,20 days 01:59:38.036584000,
"I will be graduating May 2018 with a B.A in statistics. The problem is I managed to finish a semester early and I been applying to jobs for entry level data analyst positions, operation analyst positions,  and entry level statistician for the DOD. The two reoccurring themes are I need a M.S or I don't have enough experience. I actually got accepted into my dream school for a M.S in Biostatistics. I was thrilled until I saw my financial aid package. I would be taking an unsubsidized loan of $37,167.00 while tuition and fees $37,580.00(plus I forgot to mention the estimated $12k for living expense). My current debt is as an undergrad is $0.00 (god bless the pell grant and state tag). Next week Im going to apply to another school where the debt will be around $24K...but obviously the last thing I want to do is go into debt but at the same time I would hate to go to a school I'm not too familiar with their job placement versus the school for $37k. Any recommendations on any jobs sites or fields I should apply for with just a B.A or should I swallow the pill and go to grad school and look at the debt as an investment in myself more than a trap. ",5,1526223109.0,8j26cf,False,"I will be graduating May 2018 with a B.A in statistics. The problem is I managed to finish a semester early and I been applying to jobs for entry level data analyst positions, operation analyst positions,  and entry level statistician for the DOD. The two reoccurring themes are I need a M.S or I don't have enough experience. I actually got accepted into my dream school for a M.S in Biostatistics. I was thrilled until I saw my financial aid package. I would be taking an unsubsidized loan of $37,167.00 while tuition and fees $37,580.00(plus I forgot to mention the estimated $12k for living expense). My current debt is as an undergrad is $0.00 (god bless the pell grant and state tag). Next week Im going to apply to another school where the debt will be around $24K...but obviously the last thing I want to do is go into debt but at the same time I would hate to go to a school I'm not too familiar with their job placement versus the school for $37k. Any recommendations on any jobs sites or fields I should apply for with just a B.A or should I swallow the pill and go to grad school and look at the debt as an investment in myself more than a trap. ",0,"I will be graduating May 2018 with a B.A in statistics. The problem is I managed to finish a semester early and I been applying to jobs for entry level data analyst positions, operation analyst positions,  and entry level statistician for the DOD. The two reoccurring themes are I need a M.S or I don't have enough experience. I actually got accepted into my dream school for a M.S in Biostatistics. I was thrilled until I saw my financial aid package. I would be taking an unsubsidized loan of $37,167.00 while tuition and fees $37,580.00(plus I forgot to mention the estimated $12k for living expense). My current debt is as an undergrad is $0.00 (god bless the pell grant and state tag). Next week Im going to apply to another school where the debt will be around $24K...but obviously the last thing I want to do is go into debt but at the same time I would hate to go to a school I'm not too familiar with their job placement versus the school for $37k. Any recommendations on any jobs sites or fields I should apply for with just a B.A or should I swallow the pill and go to grad school and look at the debt as an investment in myself more than a trap. ",8,statistics,54935,,B.A in Statistics need help deciding on what to do afterwards?,https://www.reddit.com/r/statistics/comments/8j26cf/ba_in_statistics_need_help_deciding_on_what_to_do/,all_ads,2018-05-13 10:51:49,20 days 14:43:45.036584000,
"How do you design a statistical test to place the burden of proof on the null hypothesis, rather than the alternative hypothesis? For example, if I'm faced with the task of proving that a random text is written by Shakespeare, then the trivial conclusion is that it was written by some random person we don't care about - finding a new Shakespearean play, on the other hand, requires a high burden of proof. This is the opposite of the problem confronted in most sciences, where the trivial conclusion is that your observations are no different from noise.

Normally you would plot your observation on a distribution and look for a high enough z score to say that something is different - to say it's the same, do you look for a z-score *below* a certain threshold?

EDIT: Sorry for beating around the bush: I am talking about author verification. To do this, I would count word frequencies (or n-grams, or whatever), then make two vectors corresponding to relative word frequencies for a set of words, one vector each for the unknown text and the works of the author in question. I can compare the two vectors using cosine similarity. I could construct a distribution by lumping the unknown text in with the author and doing a Monte Carlo simulation, but this gives me a distribution for my alternative hypothesis. I'm not sure what I do with that.",17,1526198144.0,8j000t,False,"How do you design a statistical test to place the burden of proof on the null hypothesis, rather than the alternative hypothesis? For example, if I'm faced with the task of proving that a random text is written by Shakespeare, then the trivial conclusion is that it was written by some random person we don't care about - finding a new Shakespearean play, on the other hand, requires a high burden of proof. This is the opposite of the problem confronted in most sciences, where the trivial conclusion is that your observations are no different from noise.

Normally you would plot your observation on a distribution and look for a high enough z score to say that something is different - to say it's the same, do you look for a z-score *below* a certain threshold?

EDIT: Sorry for beating around the bush: I am talking about author verification. To do this, I would count word frequencies (or n-grams, or whatever), then make two vectors corresponding to relative word frequencies for a set of words, one vector each for the unknown text and the works of the author in question. I can compare the two vectors using cosine similarity. I could construct a distribution by lumping the unknown text in with the author and doing a Monte Carlo simulation, but this gives me a distribution for my alternative hypothesis. I'm not sure what I do with that.",0,"How do you design a statistical test to place the burden of proof on the null hypothesis, rather than the alternative hypothesis? For example, if I'm faced with the task of proving that a random text is written by Shakespeare, then the trivial conclusion is that it was written by some random person we don't care about - finding a new Shakespearean play, on the other hand, requires a high burden of proof. This is the opposite of the problem confronted in most sciences, where the trivial conclusion is that your observations are no different from noise.

Normally you would plot your observation on a distribution and look for a high enough z score to say that something is different - to say it's the same, do you look for a z-score *below* a certain threshold?

EDIT: Sorry for beating around the bush: I am talking about author verification. To do this, I would count word frequencies (or n-grams, or whatever), then make two vectors corresponding to relative word frequencies for a set of words, one vector each for the unknown text and the works of the author in question. I can compare the two vectors using cosine similarity. I could construct a distribution by lumping the unknown text in with the author and doing a Monte Carlo simulation, but this gives me a distribution for my alternative hypothesis. I'm not sure what I do with that.",10,statistics,54935,,Switching the null and alternative hypothesis,https://www.reddit.com/r/statistics/comments/8j000t/switching_the_null_and_alternative_hypothesis/,all_ads,2018-05-13 03:55:44,20 days 21:39:50.036584000,
"Hi guys and gals, just wondering if somebody can help out.

my model is y_ij = beta0 + beta1 x_ij + U_i + W_i(t_ij)

where U_i ~ MVN(0, tau^2) W_i ~ MVN(0, sigma^2) 

Where U_i is random intercept and W_i is serial correlation. 

This is for longitudinal data just fyi.



The corvariance variance matrix is :

    R = [ sigma^2 sigma^2 * phi sigma^2 * phi^2 sigma^2 * phi^3,
          sigma^2*phi sigma^2  sigma^2*phi sigma^2*phi^2,
etc...
]

Sigma^2 is down the diagonal 

And the covariances are sigma^2 * phi^power where power is the lag.

Thanks in advance.
",1,1526208052.0,8j105o,False,"Hi guys and gals, just wondering if somebody can help out.

my model is y_ij = beta0 + beta1 x_ij + U_i + W_i(t_ij)

where U_i ~ MVN(0, tau^2) W_i ~ MVN(0, sigma^2) 

Where U_i is random intercept and W_i is serial correlation. 

This is for longitudinal data just fyi.



The corvariance variance matrix is :

    R = [ sigma^2 sigma^2 * phi sigma^2 * phi^2 sigma^2 * phi^3,
          sigma^2*phi sigma^2  sigma^2*phi sigma^2*phi^2,
etc...
]

Sigma^2 is down the diagonal 

And the covariances are sigma^2 * phi^power where power is the lag.

Thanks in advance.
",0,"Hi guys and gals, just wondering if somebody can help out.

my model is y_ij = beta0 + beta1 x_ij + U_i + W_i(t_ij)

where U_i ~ MVN(0, tau^2) W_i ~ MVN(0, sigma^2) 

Where U_i is random intercept and W_i is serial correlation. 

This is for longitudinal data just fyi.



The corvariance variance matrix is :

    R = [ sigma^2 sigma^2 * phi sigma^2 * phi^2 sigma^2 * phi^3,
          sigma^2*phi sigma^2  sigma^2*phi sigma^2*phi^2,
etc...
]

Sigma^2 is down the diagonal 

And the covariances are sigma^2 * phi^power where power is the lag.

Thanks in advance.
",5,statistics,54935,,Needs Help deriving variance linear mixed model with random intercept and ar error (no measurement error).,https://www.reddit.com/r/statistics/comments/8j105o/needs_help_deriving_variance_linear_mixed_model/,all_ads,2018-05-13 06:40:52,20 days 18:54:42.036584000,
"I was just wondering if there is any test I can do for fun to examine the election results of my country without any polling data? Our country in Malaysia has the same system as the UK election so the basic data that I can obtain are number of turnout, votes for each candidate, majority etc. Just thought of trying to learn some basic data analysis from a scratch. And I was hoping if there is some tests I can do with this kind of data. Thanks.",2,1526187870.0,8iyxep,False,"I was just wondering if there is any test I can do for fun to examine the election results of my country without any polling data? Our country in Malaysia has the same system as the UK election so the basic data that I can obtain are number of turnout, votes for each candidate, majority etc. Just thought of trying to learn some basic data analysis from a scratch. And I was hoping if there is some tests I can do with this kind of data. Thanks.",0,"I was just wondering if there is any test I can do for fun to examine the election results of my country without any polling data? Our country in Malaysia has the same system as the UK election so the basic data that I can obtain are number of turnout, votes for each candidate, majority etc. Just thought of trying to learn some basic data analysis from a scratch. And I was hoping if there is some tests I can do with this kind of data. Thanks.",13,statistics,54935,,Are there any test I can do to examine past election results to as a beginner try to learn data analysis?,https://www.reddit.com/r/statistics/comments/8iyxep/are_there_any_test_i_can_do_to_examine_past/,all_ads,2018-05-13 01:04:30,21 days 00:31:04.036584000,
"Hello.

Let's say I work at some marketplace startup, like Uber. And, suddenly, I notice a spike in Y=rides after day D1. By comparing each day after D1 to the same weekday 1, 2, 3 weeks before , there's a jump of like 20% in # of rides.

I have some independent variable that may help explain values of Y, like X1: number of passengers, X2: number of drivers, X3: reported app bugs and so on. And I wanna know which Xi is 'responsible' for the sudden change in Y.

How would I go about doing that?",42,1526165234.0,8iwh03,False,"Hello.

Let's say I work at some marketplace startup, like Uber. And, suddenly, I notice a spike in Y=rides after day D1. By comparing each day after D1 to the same weekday 1, 2, 3 weeks before , there's a jump of like 20% in # of rides.

I have some independent variable that may help explain values of Y, like X1: number of passengers, X2: number of drivers, X3: reported app bugs and so on. And I wanna know which Xi is 'responsible' for the sudden change in Y.

How would I go about doing that?",0,"Hello.

Let's say I work at some marketplace startup, like Uber. And, suddenly, I notice a spike in Y=rides after day D1. By comparing each day after D1 to the same weekday 1, 2, 3 weeks before , there's a jump of like 20% in # of rides.

I have some independent variable that may help explain values of Y, like X1: number of passengers, X2: number of drivers, X3: reported app bugs and so on. And I wanna know which Xi is 'responsible' for the sudden change in Y.

How would I go about doing that?",27,statistics,54935,,"How can I know which independent variable X1,X2,X3,... influenced the most the change in dependent variable Y?",https://www.reddit.com/r/statistics/comments/8iwh03/how_can_i_know_which_independent_variable_x1x2x3/,all_ads,2018-05-12 18:47:14,21 days 06:48:20.036584000,
,7,1526165297.0,8iwh81,False,,0,,25,statistics,54935,,Is there a way other than Fisher's exact test to see if two correlation coefficients are statistically different?,https://www.reddit.com/r/statistics/comments/8iwh81/is_there_a_way_other_than_fishers_exact_test_to/,all_ads,2018-05-12 18:48:17,21 days 06:47:17.036584000,
"I'm thinking of getting into data science, with possibly a PhD later in life. I appreciate the more theoretical side of statistics, not sure how much I would get of that in an MSc or data science program. I'm confident I can learn how to use statistical programs on my own as I am already doing, this makes me lean towards an MA in statistics. However, I haven't considered these until recently and don't know much about graduate statistics programs
",9,1526208326.0,8j111c,False,"I'm thinking of getting into data science, with possibly a PhD later in life. I appreciate the more theoretical side of statistics, not sure how much I would get of that in an MSc or data science program. I'm confident I can learn how to use statistical programs on my own as I am already doing, this makes me lean towards an MA in statistics. However, I haven't considered these until recently and don't know much about graduate statistics programs
",0,"I'm thinking of getting into data science, with possibly a PhD later in life. I appreciate the more theoretical side of statistics, not sure how much I would get of that in an MSc or data science program. I'm confident I can learn how to use statistical programs on my own as I am already doing, this makes me lean towards an MA in statistics. However, I haven't considered these until recently and don't know much about graduate statistics programs
",3,statistics,54935,,MA vs. MSc in statistics?,https://www.reddit.com/r/statistics/comments/8j111c/ma_vs_msc_in_statistics/,all_ads,2018-05-13 06:45:26,20 days 18:50:08.036584000,
"I've simplified the situation a bit, so let's say I have 24 tiles in a bag. Each tile has a matching partner, making 12 pairs of tiles. I also have 3 boxes that hold 8 tiles each.

What is the probability that when drawing one tile at a time from the bag and placing it into any box that isn't already full (if these conditions even matter), we will not end up with any matching tile pairs in any of the three boxes when all tiles have been placed?

I had one stats class years ago and I've tried throwing the numbers every which way, nothing seems to be working all that well. Is there a simpler version of this scenario we could use to sort of extrapolate to answer the one I just described?",9,1526179801.0,8iy2gr,False,"I've simplified the situation a bit, so let's say I have 24 tiles in a bag. Each tile has a matching partner, making 12 pairs of tiles. I also have 3 boxes that hold 8 tiles each.

What is the probability that when drawing one tile at a time from the bag and placing it into any box that isn't already full (if these conditions even matter), we will not end up with any matching tile pairs in any of the three boxes when all tiles have been placed?

I had one stats class years ago and I've tried throwing the numbers every which way, nothing seems to be working all that well. Is there a simpler version of this scenario we could use to sort of extrapolate to answer the one I just described?",0,"I've simplified the situation a bit, so let's say I have 24 tiles in a bag. Each tile has a matching partner, making 12 pairs of tiles. I also have 3 boxes that hold 8 tiles each.

What is the probability that when drawing one tile at a time from the bag and placing it into any box that isn't already full (if these conditions even matter), we will not end up with any matching tile pairs in any of the three boxes when all tiles have been placed?

I had one stats class years ago and I've tried throwing the numbers every which way, nothing seems to be working all that well. Is there a simpler version of this scenario we could use to sort of extrapolate to answer the one I just described?",7,statistics,54935,,"Haven't taken stats in years, but I'm babysitting my niece and she wants to know how likely the following scenario would be:",https://www.reddit.com/r/statistics/comments/8iy2gr/havent_taken_stats_in_years_but_im_babysitting_my/,all_ads,2018-05-12 22:50:01,21 days 02:45:33.036584000,
"Hi! I'm a first year statistics major student. What's should I do to prepare for stats program in graduate school? Other than good GPA, Should I go in research? ",7,1526171349.0,8ix4o4,False,"Hi! I'm a first year statistics major student. What's should I do to prepare for stats program in graduate school? Other than good GPA, Should I go in research? ",0,"Hi! I'm a first year statistics major student. What's should I do to prepare for stats program in graduate school? Other than good GPA, Should I go in research? ",11,statistics,54935,,Advice for undergraduate student for grad school!,https://www.reddit.com/r/statistics/comments/8ix4o4/advice_for_undergraduate_student_for_grad_school/,all_ads,2018-05-12 20:29:09,21 days 05:06:25.036584000,
"I’m using SPSS to analyse some lab data. I’m wanting to see if there’s a difference between genders for age, height and training load (it’s regarding a group of weight lifters).

What statistical test would I use to do this?
I need to find out mean, standard deviation and statistical significance for the above.

Thanks!",7,1526186448.0,8iys6z,False,"I’m using SPSS to analyse some lab data. I’m wanting to see if there’s a difference between genders for age, height and training load (it’s regarding a group of weight lifters).

What statistical test would I use to do this?
I need to find out mean, standard deviation and statistical significance for the above.

Thanks!",0,"I’m using SPSS to analyse some lab data. I’m wanting to see if there’s a difference between genders for age, height and training load (it’s regarding a group of weight lifters).

What statistical test would I use to do this?
I need to find out mean, standard deviation and statistical significance for the above.

Thanks!",2,statistics,54935,,What test would I use to see if there was a difference between genders for three different variables?,https://www.reddit.com/r/statistics/comments/8iys6z/what_test_would_i_use_to_see_if_there_was_a/,all_ads,2018-05-13 00:40:48,21 days 00:54:46.036584000,
What’s the odds to tie in Othello with a full board (each player has 32 pieces on the board each)? https://i.imgur.com/QIaVwRz.jpg,0,1526184734.0,8iylqd,False,What’s the odds to tie in Othello with a full board (each player has 32 pieces on the board each)? https://i.imgur.com/QIaVwRz.jpg,0,What’s the odds to tie in Othello with a full board (each player has 32 pieces on the board each)? https://i.imgur.com/QIaVwRz.jpg,1,statistics,54935,,Tie in Othello,https://www.reddit.com/r/statistics/comments/8iylqd/tie_in_othello/,all_ads,2018-05-13 00:12:14,21 days 01:23:20.036584000,
,11,1526180341.0,8iy4m0,False,,0,,1,statistics,54935,,"If there are two machines that are each programmed to pick a number from 1 to infinity, because the chances of the machines picking the same number are infinitely small, does that mean that it couldn't happen?",https://www.reddit.com/r/statistics/comments/8iy4m0/if_there_are_two_machines_that_are_each/,all_ads,2018-05-12 22:59:01,21 days 02:36:33.036584000,
"Hi all,
I'm having some trouble doing my analysis for an experiment I'm running.

The main problem is that i have 3x3 design, but actually I haven't done some of the comparisons, during data collectition. This is mine within_factor desing : 

A1 - B2

A1 - B3

A2 - B1

A2 - B3

A3 - B1

A3 - B2

So it is actually a 3x2 but no one of the levels of factor B is matched with all the level of factor A.

The result is that when i do an ANOVA i have a significant interaction always between factors even with random data.

It is worng conceptually? Do you know if it is possible to compute this on matlab?",1,1526175119.0,8ixjrx,False,"Hi all,
I'm having some trouble doing my analysis for an experiment I'm running.

The main problem is that i have 3x3 design, but actually I haven't done some of the comparisons, during data collectition. This is mine within_factor desing : 

A1 - B2

A1 - B3

A2 - B1

A2 - B3

A3 - B1

A3 - B2

So it is actually a 3x2 but no one of the levels of factor B is matched with all the level of factor A.

The result is that when i do an ANOVA i have a significant interaction always between factors even with random data.

It is worng conceptually? Do you know if it is possible to compute this on matlab?",0,"Hi all,
I'm having some trouble doing my analysis for an experiment I'm running.

The main problem is that i have 3x3 design, but actually I haven't done some of the comparisons, during data collectition. This is mine within_factor desing : 

A1 - B2

A1 - B3

A2 - B1

A2 - B3

A3 - B1

A3 - B2

So it is actually a 3x2 but no one of the levels of factor B is matched with all the level of factor A.

The result is that when i do an ANOVA i have a significant interaction always between factors even with random data.

It is worng conceptually? Do you know if it is possible to compute this on matlab?",1,statistics,54935,,2 ways ANOVA with unbalanced design factor,https://www.reddit.com/r/statistics/comments/8ixjrx/2_ways_anova_with_unbalanced_design_factor/,all_ads,2018-05-12 21:31:59,21 days 04:03:35.036584000,
"Solved

The answer my professor provided in class is not matching up to my answer.

**Q: An inspector inspects a large truckload of potatoes to determine the proportion p in the shipment with major defects. If there is clear evidence that this proportion is less than .10, she will accept the shipment. She tests the hypothesis Ho: p=.10 versus Ha: p<.10. To do so, she selects a random sample of 150 potatoes from the 3000 on the truck. Only 8 potatoes sampled are found to have major defects.**

So what I did was:

P hat = 8/50 = .16

z = .16-.10/Sqroot .10(1-.10)/150

the answer provided to us is -1.91 but I do not get this as an answer.",2,1526173085.0,8ixbi1,False,"Solved

The answer my professor provided in class is not matching up to my answer.

**Q: An inspector inspects a large truckload of potatoes to determine the proportion p in the shipment with major defects. If there is clear evidence that this proportion is less than .10, she will accept the shipment. She tests the hypothesis Ho: p=.10 versus Ha: p<.10. To do so, she selects a random sample of 150 potatoes from the 3000 on the truck. Only 8 potatoes sampled are found to have major defects.**

So what I did was:

P hat = 8/50 = .16

z = .16-.10/Sqroot .10(1-.10)/150

the answer provided to us is -1.91 but I do not get this as an answer.",0,"Solved

The answer my professor provided in class is not matching up to my answer.

**Q: An inspector inspects a large truckload of potatoes to determine the proportion p in the shipment with major defects. If there is clear evidence that this proportion is less than .10, she will accept the shipment. She tests the hypothesis Ho: p=.10 versus Ha: p<.10. To do so, she selects a random sample of 150 potatoes from the 3000 on the truck. Only 8 potatoes sampled are found to have major defects.**

So what I did was:

P hat = 8/50 = .16

z = .16-.10/Sqroot .10(1-.10)/150

the answer provided to us is -1.91 but I do not get this as an answer.",0,statistics,54935,,Calculating a value for the z statistic in a hypothesis test?,https://www.reddit.com/r/statistics/comments/8ixbi1/calculating_a_value_for_the_z_statistic_in_a/,all_ads,2018-05-12 20:58:05,21 days 04:37:29.036584000,
"The label on a brand of tonic water claims the concentration of quinine inside their product is 54 ppm.  A student used fluorescence spectrometer to analyse 10 bottles of this brand of tonic water and the results are as follows (in ppm):  54.0, 55.1, 53.8, 54.2, 52.1, 54.2, 55.0, 55.8, 55.1, 55.3. If 0.01 level of significance is appropriate, determine whether to accept or reject the claim of this tonic water manufacturer.",3,1526184163.0,8iyjhb,False,"The label on a brand of tonic water claims the concentration of quinine inside their product is 54 ppm.  A student used fluorescence spectrometer to analyse 10 bottles of this brand of tonic water and the results are as follows (in ppm):  54.0, 55.1, 53.8, 54.2, 52.1, 54.2, 55.0, 55.8, 55.1, 55.3. If 0.01 level of significance is appropriate, determine whether to accept or reject the claim of this tonic water manufacturer.",0,"The label on a brand of tonic water claims the concentration of quinine inside their product is 54 ppm.  A student used fluorescence spectrometer to analyse 10 bottles of this brand of tonic water and the results are as follows (in ppm):  54.0, 55.1, 53.8, 54.2, 52.1, 54.2, 55.0, 55.8, 55.1, 55.3. If 0.01 level of significance is appropriate, determine whether to accept or reject the claim of this tonic water manufacturer.",0,statistics,54935,,Can anyone solve this question about level of significance?,https://www.reddit.com/r/statistics/comments/8iyjhb/can_anyone_solve_this_question_about_level_of/,all_ads,2018-05-13 00:02:43,21 days 01:32:51.036584000,
"EDIT: Resolved by u/red_concrete.

DV: SVO (Social Value Orientation) [dichotomous: prosocial/proself]

IV: SDO (Social Dominance Orientation) [dichotomous: high/low]

I use SPSS and I have generated a generalized linear model (GLM) using a binary logistic regression where 'Prosocial' is the response category, 'Proself' is the reference category and the sample size is N = 108. According to the Categorical Variable Information there are in total 84 prosocials, 24 proselfs, 84 low scorers in social dominance orientation (SDO), and 24 high scorers in SDO.

However, the odds ratio is 6.000 for [SDO=1] (i.e. low scores in social dominance orientation), indicating that individuals scoring low in SDO have 6 times higher odds to have a proself orientation than those who score high, 95% CI [2.19, 16,42], p < .001. 

I ran a test with the actual vs. predicted SVO based on SDO scores and found that the model predicted 77.8% correct. However, the predictor model only predicted prosocial orientations exactly correct (i.e. 84/84, 77.8%) and the remaining proselfs (22.2%) were predicted by the model to be zero (i.e. 0/24).

I feel like the odds ratio is wrong, or that I have interpreted it wrong. If there are more prosocials and low scorers (SDO) than proselfs and high scorers (SDO) in the data, why would it predict a proself orientation? I would love to get any inputs. This is my first time doing GLMs and I am submitting my dissertation in three days. 

I hope this is all clear. If not, please let me know.
Thanks for your help!",21,1526108902.0,8is1mn,False,"EDIT: Resolved by u/red_concrete.

DV: SVO (Social Value Orientation) [dichotomous: prosocial/proself]

IV: SDO (Social Dominance Orientation) [dichotomous: high/low]

I use SPSS and I have generated a generalized linear model (GLM) using a binary logistic regression where 'Prosocial' is the response category, 'Proself' is the reference category and the sample size is N = 108. According to the Categorical Variable Information there are in total 84 prosocials, 24 proselfs, 84 low scorers in social dominance orientation (SDO), and 24 high scorers in SDO.

However, the odds ratio is 6.000 for [SDO=1] (i.e. low scores in social dominance orientation), indicating that individuals scoring low in SDO have 6 times higher odds to have a proself orientation than those who score high, 95% CI [2.19, 16,42], p < .001. 

I ran a test with the actual vs. predicted SVO based on SDO scores and found that the model predicted 77.8% correct. However, the predictor model only predicted prosocial orientations exactly correct (i.e. 84/84, 77.8%) and the remaining proselfs (22.2%) were predicted by the model to be zero (i.e. 0/24).

I feel like the odds ratio is wrong, or that I have interpreted it wrong. If there are more prosocials and low scorers (SDO) than proselfs and high scorers (SDO) in the data, why would it predict a proself orientation? I would love to get any inputs. This is my first time doing GLMs and I am submitting my dissertation in three days. 

I hope this is all clear. If not, please let me know.
Thanks for your help!",0,"EDIT: Resolved by u/red_concrete.

DV: SVO (Social Value Orientation) [dichotomous: prosocial/proself]

IV: SDO (Social Dominance Orientation) [dichotomous: high/low]

I use SPSS and I have generated a generalized linear model (GLM) using a binary logistic regression where 'Prosocial' is the response category, 'Proself' is the reference category and the sample size is N = 108. According to the Categorical Variable Information there are in total 84 prosocials, 24 proselfs, 84 low scorers in social dominance orientation (SDO), and 24 high scorers in SDO.

However, the odds ratio is 6.000 for [SDO=1] (i.e. low scores in social dominance orientation), indicating that individuals scoring low in SDO have 6 times higher odds to have a proself orientation than those who score high, 95% CI [2.19, 16,42], p < .001. 

I ran a test with the actual vs. predicted SVO based on SDO scores and found that the model predicted 77.8% correct. However, the predictor model only predicted prosocial orientations exactly correct (i.e. 84/84, 77.8%) and the remaining proselfs (22.2%) were predicted by the model to be zero (i.e. 0/24).

I feel like the odds ratio is wrong, or that I have interpreted it wrong. If there are more prosocials and low scorers (SDO) than proselfs and high scorers (SDO) in the data, why would it predict a proself orientation? I would love to get any inputs. This is my first time doing GLMs and I am submitting my dissertation in three days. 

I hope this is all clear. If not, please let me know.
Thanks for your help!",11,statistics,54935,,Interpreting Odds Ratio in a Binary Logistic Model (GLM),https://www.reddit.com/r/statistics/comments/8is1mn/interpreting_odds_ratio_in_a_binary_logistic/,all_ads,2018-05-12 03:08:22,21 days 22:27:12.036584000,
"The strong likelihood principle states that if the likelihood functions for two different experiments are identical, then the inference one should make from each experiment should be the same, given identical prior beliefs.  The strong likelihood principle says that *how* that data was collected is largely immaterial; for instance, if two different experiments follow different stopping rules, but obtain the same results anyway, then inference from the two experiments ought to be identical.

The SLP follows by acceptance of the strong sufficiency principle, and the strong conditionality principle.  It is this second condition, and a crude interpretation of it, which I want to ask about: essentially what the conditionality principle states is that only that which did happen matters; that which did not happen, even if it could have, does not statistically matter.

I do not know if I fully understand what this does—and does not—mean, and my misgivings arise from Monty Hall-esque examples, and variants which have different counterfactuals.

Consider first the classical Monty Hall problem, where you are on a game show where you have to choose from among 3 doors which might hide a sports car.  One door does; the other two hide an anti-prize, typically a donkey.  The host, Monty Hall, you know will choose to reveal one of the other doors to you, one that does not have the car behind it.  He does this reveal, and then you are asked if you would like to switch or stay with your door.

To maximize your probability of winning in this case, you should switch, because the probability that the door you chose has the car is 1/3; Monty has revealed no new information to you about the contents of your door v. the others in aggregate, because you already knew that there was a non-car door among the others.  The reveal then induced no new information about your door, though it did ""collapse"" the probability of the two other doors now into one.

This can be shown by a simple frequentist-style simulation, but also from a Bayesian formulation: let X be a categorical variable taking values d^(i), i = 1, ..., N, where N is the number of doors that are in the game, and to say ""X = d^(i)"" means that the i^(th) door hides the car.

A simple Bayesian prior, with no further insight into which door hides the car, is equal likelihood, which is to say p(X = d^(i)) = 1/N, for all i in 1:N.  A door is revealed to us according to Monty's restrictions: must not be ours, and must not hide the car.  WLOG, let us have chosen the first door; let the k^(th) door hide the car (k may equal 1); and let Y be a categorical variable representing the revealed door, and the j^(th) door be the one revealed, so Y = d^(j), j not equal to 1 or k.  For the sake of argument, Monty chose from the restricted subset of doors uniformly.

The likelihood function for this new data—the reveal of the j^(th) door, and that the door hid a donkey—is:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/(N–1), for i = 1,

> = 0, for i = j,

> = 1/(N–2), otherwise.

The integrated/summed prior+likelihood is thus 1/(N–1), and the posterior probability that the i^(th) door hides the car is:

> P(X = d^(i)) = ...

> = 1/N, for i = 1,

> = 0, for i = j,

> = (N–1)/{ N(N–2) }, otherwise.

For the N = 3 case, these evaluate to 1/3, 0, and 2/3, respectively.

------------------

However, consider a variant where you know that the host randomly chose a door number from a hat.  He could have chosen any door, regardless of whether it was yours or whether it had the car; if he chooses your door, or chooses the one with the car, you simply lose and leave the game show.  However, you find out that the one he randomly drew, and then revealed, was not your own, and also still had a donkey behind it.

So we find ourselves in a very similar scenario: the same type of door was revealed to us, and we have no reason to have different prior beliefs that before.  However, our inference must be different.  Consider that the same formalization as above; the likelihood function is now instead:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/N, for i = 1,

> = 0, for i = j,

> = 1/N, otherwise.

This is a fundamentally different likelihood function, and so our inference is different (after seeing one such door revealed, we still consider all remaining doors—including our own—equally likely, and so in the N = 3 case, our probability our door has the car is 1/2, and switching offers no benefit).

This does not violate the SLP, because under the SLP inference may differ when the likelihood functions differ.  But, it's not clear *why* the likelihood functions differ.  All that seems to differ are the counterfactuals in the case: different things, in these two variants, could have happened, but didn't.

------------------

To help drive home the point, consider a second variant from the classic problem.  In this one, the host uniformly randomly chooses a door among those that are *not yours*, though he may end up choosing the one with the car if it is out there.  We still find ourselves in the scenario where a door that is not ours, and that does not hide the car, is revealed.

The likelihood function here is identical to the first variant, to scaling:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/(N–1), for i = 1,

> = 0, for i = j,

> = 1/(N–1), otherwise.

Thus the inference is the same in first variant.  Or, consider a variant where the host may have chosen from any door that did not have the car; he could have chosen your door.  Again, if we find ourselves in the scenario where a door that was not ours was revealed, and it did not have the car, then the likelihood function is the same as this recently shown one.  Thus, the variants all have the same posterior probabilities, and the likelihood functions contain the same information.

So when at most one restriction is placed on Monty's door reveal—he may only choose doors which are not yours, he may only choose doors which hide donkeys, or he may choose any door at all—those don't seem to affect inference.  But when both restrictions are applied at the same time, our inference changes.

In light of the strong conditionality principle, what is it about these counterfactuals that, when considered together, get us to different results with apparently the same door reveal?  What is the nature of the information obtained from the reveal?",11,1526089060.0,8ipmwp,False,"The strong likelihood principle states that if the likelihood functions for two different experiments are identical, then the inference one should make from each experiment should be the same, given identical prior beliefs.  The strong likelihood principle says that *how* that data was collected is largely immaterial; for instance, if two different experiments follow different stopping rules, but obtain the same results anyway, then inference from the two experiments ought to be identical.

The SLP follows by acceptance of the strong sufficiency principle, and the strong conditionality principle.  It is this second condition, and a crude interpretation of it, which I want to ask about: essentially what the conditionality principle states is that only that which did happen matters; that which did not happen, even if it could have, does not statistically matter.

I do not know if I fully understand what this does—and does not—mean, and my misgivings arise from Monty Hall-esque examples, and variants which have different counterfactuals.

Consider first the classical Monty Hall problem, where you are on a game show where you have to choose from among 3 doors which might hide a sports car.  One door does; the other two hide an anti-prize, typically a donkey.  The host, Monty Hall, you know will choose to reveal one of the other doors to you, one that does not have the car behind it.  He does this reveal, and then you are asked if you would like to switch or stay with your door.

To maximize your probability of winning in this case, you should switch, because the probability that the door you chose has the car is 1/3; Monty has revealed no new information to you about the contents of your door v. the others in aggregate, because you already knew that there was a non-car door among the others.  The reveal then induced no new information about your door, though it did ""collapse"" the probability of the two other doors now into one.

This can be shown by a simple frequentist-style simulation, but also from a Bayesian formulation: let X be a categorical variable taking values d^(i), i = 1, ..., N, where N is the number of doors that are in the game, and to say ""X = d^(i)"" means that the i^(th) door hides the car.

A simple Bayesian prior, with no further insight into which door hides the car, is equal likelihood, which is to say p(X = d^(i)) = 1/N, for all i in 1:N.  A door is revealed to us according to Monty's restrictions: must not be ours, and must not hide the car.  WLOG, let us have chosen the first door; let the k^(th) door hide the car (k may equal 1); and let Y be a categorical variable representing the revealed door, and the j^(th) door be the one revealed, so Y = d^(j), j not equal to 1 or k.  For the sake of argument, Monty chose from the restricted subset of doors uniformly.

The likelihood function for this new data—the reveal of the j^(th) door, and that the door hid a donkey—is:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/(N–1), for i = 1,

> = 0, for i = j,

> = 1/(N–2), otherwise.

The integrated/summed prior+likelihood is thus 1/(N–1), and the posterior probability that the i^(th) door hides the car is:

> P(X = d^(i)) = ...

> = 1/N, for i = 1,

> = 0, for i = j,

> = (N–1)/{ N(N–2) }, otherwise.

For the N = 3 case, these evaluate to 1/3, 0, and 2/3, respectively.

------------------

However, consider a variant where you know that the host randomly chose a door number from a hat.  He could have chosen any door, regardless of whether it was yours or whether it had the car; if he chooses your door, or chooses the one with the car, you simply lose and leave the game show.  However, you find out that the one he randomly drew, and then revealed, was not your own, and also still had a donkey behind it.

So we find ourselves in a very similar scenario: the same type of door was revealed to us, and we have no reason to have different prior beliefs that before.  However, our inference must be different.  Consider that the same formalization as above; the likelihood function is now instead:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/N, for i = 1,

> = 0, for i = j,

> = 1/N, otherwise.

This is a fundamentally different likelihood function, and so our inference is different (after seeing one such door revealed, we still consider all remaining doors—including our own—equally likely, and so in the N = 3 case, our probability our door has the car is 1/2, and switching offers no benefit).

This does not violate the SLP, because under the SLP inference may differ when the likelihood functions differ.  But, it's not clear *why* the likelihood functions differ.  All that seems to differ are the counterfactuals in the case: different things, in these two variants, could have happened, but didn't.

------------------

To help drive home the point, consider a second variant from the classic problem.  In this one, the host uniformly randomly chooses a door among those that are *not yours*, though he may end up choosing the one with the car if it is out there.  We still find ourselves in the scenario where a door that is not ours, and that does not hide the car, is revealed.

The likelihood function here is identical to the first variant, to scaling:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/(N–1), for i = 1,

> = 0, for i = j,

> = 1/(N–1), otherwise.

Thus the inference is the same in first variant.  Or, consider a variant where the host may have chosen from any door that did not have the car; he could have chosen your door.  Again, if we find ourselves in the scenario where a door that was not ours was revealed, and it did not have the car, then the likelihood function is the same as this recently shown one.  Thus, the variants all have the same posterior probabilities, and the likelihood functions contain the same information.

So when at most one restriction is placed on Monty's door reveal—he may only choose doors which are not yours, he may only choose doors which hide donkeys, or he may choose any door at all—those don't seem to affect inference.  But when both restrictions are applied at the same time, our inference changes.

In light of the strong conditionality principle, what is it about these counterfactuals that, when considered together, get us to different results with apparently the same door reveal?  What is the nature of the information obtained from the reveal?",0,"The strong likelihood principle states that if the likelihood functions for two different experiments are identical, then the inference one should make from each experiment should be the same, given identical prior beliefs.  The strong likelihood principle says that *how* that data was collected is largely immaterial; for instance, if two different experiments follow different stopping rules, but obtain the same results anyway, then inference from the two experiments ought to be identical.

The SLP follows by acceptance of the strong sufficiency principle, and the strong conditionality principle.  It is this second condition, and a crude interpretation of it, which I want to ask about: essentially what the conditionality principle states is that only that which did happen matters; that which did not happen, even if it could have, does not statistically matter.

I do not know if I fully understand what this does—and does not—mean, and my misgivings arise from Monty Hall-esque examples, and variants which have different counterfactuals.

Consider first the classical Monty Hall problem, where you are on a game show where you have to choose from among 3 doors which might hide a sports car.  One door does; the other two hide an anti-prize, typically a donkey.  The host, Monty Hall, you know will choose to reveal one of the other doors to you, one that does not have the car behind it.  He does this reveal, and then you are asked if you would like to switch or stay with your door.

To maximize your probability of winning in this case, you should switch, because the probability that the door you chose has the car is 1/3; Monty has revealed no new information to you about the contents of your door v. the others in aggregate, because you already knew that there was a non-car door among the others.  The reveal then induced no new information about your door, though it did ""collapse"" the probability of the two other doors now into one.

This can be shown by a simple frequentist-style simulation, but also from a Bayesian formulation: let X be a categorical variable taking values d^(i), i = 1, ..., N, where N is the number of doors that are in the game, and to say ""X = d^(i)"" means that the i^(th) door hides the car.

A simple Bayesian prior, with no further insight into which door hides the car, is equal likelihood, which is to say p(X = d^(i)) = 1/N, for all i in 1:N.  A door is revealed to us according to Monty's restrictions: must not be ours, and must not hide the car.  WLOG, let us have chosen the first door; let the k^(th) door hide the car (k may equal 1); and let Y be a categorical variable representing the revealed door, and the j^(th) door be the one revealed, so Y = d^(j), j not equal to 1 or k.  For the sake of argument, Monty chose from the restricted subset of doors uniformly.

The likelihood function for this new data—the reveal of the j^(th) door, and that the door hid a donkey—is:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/(N–1), for i = 1,

> = 0, for i = j,

> = 1/(N–2), otherwise.

The integrated/summed prior+likelihood is thus 1/(N–1), and the posterior probability that the i^(th) door hides the car is:

> P(X = d^(i)) = ...

> = 1/N, for i = 1,

> = 0, for i = j,

> = (N–1)/{ N(N–2) }, otherwise.

For the N = 3 case, these evaluate to 1/3, 0, and 2/3, respectively.

------------------

However, consider a variant where you know that the host randomly chose a door number from a hat.  He could have chosen any door, regardless of whether it was yours or whether it had the car; if he chooses your door, or chooses the one with the car, you simply lose and leave the game show.  However, you find out that the one he randomly drew, and then revealed, was not your own, and also still had a donkey behind it.

So we find ourselves in a very similar scenario: the same type of door was revealed to us, and we have no reason to have different prior beliefs that before.  However, our inference must be different.  Consider that the same formalization as above; the likelihood function is now instead:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/N, for i = 1,

> = 0, for i = j,

> = 1/N, otherwise.

This is a fundamentally different likelihood function, and so our inference is different (after seeing one such door revealed, we still consider all remaining doors—including our own—equally likely, and so in the N = 3 case, our probability our door has the car is 1/2, and switching offers no benefit).

This does not violate the SLP, because under the SLP inference may differ when the likelihood functions differ.  But, it's not clear *why* the likelihood functions differ.  All that seems to differ are the counterfactuals in the case: different things, in these two variants, could have happened, but didn't.

------------------

To help drive home the point, consider a second variant from the classic problem.  In this one, the host uniformly randomly chooses a door among those that are *not yours*, though he may end up choosing the one with the car if it is out there.  We still find ourselves in the scenario where a door that is not ours, and that does not hide the car, is revealed.

The likelihood function here is identical to the first variant, to scaling:

> L(i) = P(Y = d^(j) & X != d^(j)) = ...

> = 1/(N–1), for i = 1,

> = 0, for i = j,

> = 1/(N–1), otherwise.

Thus the inference is the same in first variant.  Or, consider a variant where the host may have chosen from any door that did not have the car; he could have chosen your door.  Again, if we find ourselves in the scenario where a door that was not ours was revealed, and it did not have the car, then the likelihood function is the same as this recently shown one.  Thus, the variants all have the same posterior probabilities, and the likelihood functions contain the same information.

So when at most one restriction is placed on Monty's door reveal—he may only choose doors which are not yours, he may only choose doors which hide donkeys, or he may choose any door at all—those don't seem to affect inference.  But when both restrictions are applied at the same time, our inference changes.

In light of the strong conditionality principle, what is it about these counterfactuals that, when considered together, get us to different results with apparently the same door reveal?  What is the nature of the information obtained from the reveal?",17,statistics,54935,,What are the qualities of counterfactuals that can influence a likelihood function?,https://www.reddit.com/r/statistics/comments/8ipmwp/what_are_the_qualities_of_counterfactuals_that/,all_ads,2018-05-11 21:37:40,22 days 03:57:54.036584000,
"I really enjoy stats and film, and wondering if there's any way to combine them. Maybe it'd be fun to do data analytics for Netflix/Hulu/etc., not sure if that's something that stats majors are qualified/trained for. Currently a college freshman, any advice is appreciated.",9,1526093982.0,8iq9il,False,"I really enjoy stats and film, and wondering if there's any way to combine them. Maybe it'd be fun to do data analytics for Netflix/Hulu/etc., not sure if that's something that stats majors are qualified/trained for. Currently a college freshman, any advice is appreciated.",0,"I really enjoy stats and film, and wondering if there's any way to combine them. Maybe it'd be fun to do data analytics for Netflix/Hulu/etc., not sure if that's something that stats majors are qualified/trained for. Currently a college freshman, any advice is appreciated.",7,statistics,54935,,What jobs are there for people interested in statistics and entertainment?,https://www.reddit.com/r/statistics/comments/8iq9il/what_jobs_are_there_for_people_interested_in/,all_ads,2018-05-11 22:59:42,22 days 02:35:52.036584000,
[https://stackoverflow.com/questions/50144597/if\-we\-combine\-one\-trainable\-parameters\-with\-a\-non\-trainable\-parameter\-is\-the\-or/50270587?noredirect=1#comment87587179\_50270587](https://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or/50270587?noredirect=1#comment87587179_50270587),0,1526113413.0,8ishnp,False,[https://stackoverflow.com/questions/50144597/if\-we\-combine\-one\-trainable\-parameters\-with\-a\-non\-trainable\-parameter\-is\-the\-or/50270587?noredirect=1#comment87587179\_50270587](https://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or/50270587?noredirect=1#comment87587179_50270587),0,[https://stackoverflow.com/questions/50144597/if\-we\-combine\-one\-trainable\-parameters\-with\-a\-non\-trainable\-parameter\-is\-the\-or/50270587?noredirect=1#comment87587179\_50270587](https://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or/50270587?noredirect=1#comment87587179_50270587),2,statistics,54935,,"If we combine one trainable parameters with a non-trainable parameter, is the original trainable param trainable?",https://www.reddit.com/r/statistics/comments/8ishnp/if_we_combine_one_trainable_parameters_with_a/,all_ads,2018-05-12 04:23:33,21 days 21:12:01.036584000,
edit: Thank you all for your answers around this. I am still a noob and trying to learn as much as I can. ,44,1526073512.0,8ino4s,False,edit: Thank you all for your answers around this. I am still a noob and trying to learn as much as I can. ,0,edit: Thank you all for your answers around this. I am still a noob and trying to learn as much as I can. ,12,statistics,54935,,"If the median and mean are equal for normal distributions and we use median for non-normal distributions, why don’t we just always use the median?",https://www.reddit.com/r/statistics/comments/8ino4s/if_the_median_and_mean_are_equal_for_normal/,all_ads,2018-05-11 17:18:32,22 days 08:17:02.036584000,
"Hello. I'm working on a project where it would be helpful to segment the data according to ""elbow"" points - where the trend direction takes a sudden shift. 

I've come up with a useful measure of *finding* an elbow, where you define an elbow point as the maximum-distance vector from the line segment connecting the start and end points.

In the following example, the color represents the passage of time, flowing from light to dark. The dotted red line represents the line segment connecting the start and end point.

[Example 1](https://imgur.com/a/7d5Yxjb)

[Example 2](https://imgur.com/a/cgxvTox)

Now, my given data set [could have multiple elbows](https://imgur.com/a/t87hmKc). I'm thinking that an iterative or divisive approach would suffice, continuing to split the data along these ""elbow points"" until I reach a line segment without any good elbow points.

The question is how to determine the ""**stopping point**"" of this iterative approach -- when to decide that an elbow point [is not ""strong"" enough to consist of a split](https://imgur.com/a/chnLZlR).

This is further exacerbated by the fact that my data is measured in discrete resolutions. Instead of a continuous line, you will often see patterns [like this](https://imgur.com/a/ZNQUVn8) where the data appears to ""jump"", but in reality this is simply a one-unit increase and not significant at all. (I have ground knowledge that this is a smooth process - the true precision would display a smooth piecewise-linear trend).

I'm open to any and all ideas! Thanks.",0,1526084979.0,8ip3mb,False,"Hello. I'm working on a project where it would be helpful to segment the data according to ""elbow"" points - where the trend direction takes a sudden shift. 

I've come up with a useful measure of *finding* an elbow, where you define an elbow point as the maximum-distance vector from the line segment connecting the start and end points.

In the following example, the color represents the passage of time, flowing from light to dark. The dotted red line represents the line segment connecting the start and end point.

[Example 1](https://imgur.com/a/7d5Yxjb)

[Example 2](https://imgur.com/a/cgxvTox)

Now, my given data set [could have multiple elbows](https://imgur.com/a/t87hmKc). I'm thinking that an iterative or divisive approach would suffice, continuing to split the data along these ""elbow points"" until I reach a line segment without any good elbow points.

The question is how to determine the ""**stopping point**"" of this iterative approach -- when to decide that an elbow point [is not ""strong"" enough to consist of a split](https://imgur.com/a/chnLZlR).

This is further exacerbated by the fact that my data is measured in discrete resolutions. Instead of a continuous line, you will often see patterns [like this](https://imgur.com/a/ZNQUVn8) where the data appears to ""jump"", but in reality this is simply a one-unit increase and not significant at all. (I have ground knowledge that this is a smooth process - the true precision would display a smooth piecewise-linear trend).

I'm open to any and all ideas! Thanks.",0,"Hello. I'm working on a project where it would be helpful to segment the data according to ""elbow"" points - where the trend direction takes a sudden shift. 

I've come up with a useful measure of *finding* an elbow, where you define an elbow point as the maximum-distance vector from the line segment connecting the start and end points.

In the following example, the color represents the passage of time, flowing from light to dark. The dotted red line represents the line segment connecting the start and end point.

[Example 1](https://imgur.com/a/7d5Yxjb)

[Example 2](https://imgur.com/a/cgxvTox)

Now, my given data set [could have multiple elbows](https://imgur.com/a/t87hmKc). I'm thinking that an iterative or divisive approach would suffice, continuing to split the data along these ""elbow points"" until I reach a line segment without any good elbow points.

The question is how to determine the ""**stopping point**"" of this iterative approach -- when to decide that an elbow point [is not ""strong"" enough to consist of a split](https://imgur.com/a/chnLZlR).

This is further exacerbated by the fact that my data is measured in discrete resolutions. Instead of a continuous line, you will often see patterns [like this](https://imgur.com/a/ZNQUVn8) where the data appears to ""jump"", but in reality this is simply a one-unit increase and not significant at all. (I have ground knowledge that this is a smooth process - the true precision would display a smooth piecewise-linear trend).

I'm open to any and all ideas! Thanks.",3,statistics,54935,,"Identifying the number of ""elbows"" in a time-indexed dataset",https://www.reddit.com/r/statistics/comments/8ip3mb/identifying_the_number_of_elbows_in_a_timeindexed/,all_ads,2018-05-11 20:29:39,22 days 05:05:55.036584000,
"So, two part advice post here, figured it would be better to keep it in one submission.

I just received my master's degree in statistics, and right now, I'm working a summer internship for a baseball data collection agency. I chose this over a couple other ""real"" job offers because I love baseball (I did my master's thesis on baseball statistics) and wanted to see if I'd enjoy working within the industry. I'm enjoying the work so far, but admittedly, most of it is data collection, watching games and recording data from what we see, and not as much on the analysis side (though this should be changing soon). Still, I want to keep my skills sharp, and I'm really curious to see real world examples of data in use, so I'm wondering if anyone has any good book recommendations along these lines, perhaps something with how model building and algorithms work. All I've done up to this point is coursework and I really want to see how data is actually put to use.

Also, what's a good way to do my own statistical work? Nothing complex at the moment, but just good ways to dabble, again, to keep my skills sharp, perhaps learn how to build simple models. I would get Minitab if it wasn't so expensive, but otherwise, I have Excel, and I can download any free or cheap software.

Thanks for the help, everyone.
-Nick",11,1526043437.0,8il8nc,False,"So, two part advice post here, figured it would be better to keep it in one submission.

I just received my master's degree in statistics, and right now, I'm working a summer internship for a baseball data collection agency. I chose this over a couple other ""real"" job offers because I love baseball (I did my master's thesis on baseball statistics) and wanted to see if I'd enjoy working within the industry. I'm enjoying the work so far, but admittedly, most of it is data collection, watching games and recording data from what we see, and not as much on the analysis side (though this should be changing soon). Still, I want to keep my skills sharp, and I'm really curious to see real world examples of data in use, so I'm wondering if anyone has any good book recommendations along these lines, perhaps something with how model building and algorithms work. All I've done up to this point is coursework and I really want to see how data is actually put to use.

Also, what's a good way to do my own statistical work? Nothing complex at the moment, but just good ways to dabble, again, to keep my skills sharp, perhaps learn how to build simple models. I would get Minitab if it wasn't so expensive, but otherwise, I have Excel, and I can download any free or cheap software.

Thanks for the help, everyone.
-Nick",0,"So, two part advice post here, figured it would be better to keep it in one submission.

I just received my master's degree in statistics, and right now, I'm working a summer internship for a baseball data collection agency. I chose this over a couple other ""real"" job offers because I love baseball (I did my master's thesis on baseball statistics) and wanted to see if I'd enjoy working within the industry. I'm enjoying the work so far, but admittedly, most of it is data collection, watching games and recording data from what we see, and not as much on the analysis side (though this should be changing soon). Still, I want to keep my skills sharp, and I'm really curious to see real world examples of data in use, so I'm wondering if anyone has any good book recommendations along these lines, perhaps something with how model building and algorithms work. All I've done up to this point is coursework and I really want to see how data is actually put to use.

Also, what's a good way to do my own statistical work? Nothing complex at the moment, but just good ways to dabble, again, to keep my skills sharp, perhaps learn how to build simple models. I would get Minitab if it wasn't so expensive, but otherwise, I have Excel, and I can download any free or cheap software.

Thanks for the help, everyone.
-Nick",22,statistics,54935,,Good books on real world statistical analysis with some math included? And good ways to dabble with stats work on my own time?,https://www.reddit.com/r/statistics/comments/8il8nc/good_books_on_real_world_statistical_analysis/,all_ads,2018-05-11 08:57:17,22 days 16:38:17.036584000,
"I see a lot of posts asking for advice on strategies to become a Data Scientist. I wrote an article aimed to my younger self when I was just starting out. Hope you find it useful

https://www.linkedin.com/pulse/how-land-your-first-data-science-job-hakim-khan/",15,1525984399.0,8ieikb,False,"I see a lot of posts asking for advice on strategies to become a Data Scientist. I wrote an article aimed to my younger self when I was just starting out. Hope you find it useful

https://www.linkedin.com/pulse/how-land-your-first-data-science-job-hakim-khan/",0,"I see a lot of posts asking for advice on strategies to become a Data Scientist. I wrote an article aimed to my younger self when I was just starting out. Hope you find it useful

https://www.linkedin.com/pulse/how-land-your-first-data-science-job-hakim-khan/",45,statistics,54935,,Getting hired as a Data Scientist,https://www.reddit.com/r/statistics/comments/8ieikb/getting_hired_as_a_data_scientist/,all_ads,2018-05-10 16:33:19,23 days 09:02:15.036584000,
"Distribution isn't symmetrical \(i.e mean is smaller than median \- but not by that much\)

How can I go about making sense of the information I have?",6,1526032602.0,8ik7uw,False,"Distribution isn't symmetrical \(i.e mean is smaller than median \- but not by that much\)

How can I go about making sense of the information I have?",0,"Distribution isn't symmetrical \(i.e mean is smaller than median \- but not by that much\)

How can I go about making sense of the information I have?",3,statistics,54935,,"Question: What information can I know if I have the standard deviation, mean, median, and one data point? (Distribution isn't symmetrical (i.e mean is smaller than median[but not by that much]))",https://www.reddit.com/r/statistics/comments/8ik7uw/question_what_information_can_i_know_if_i_have/,all_ads,2018-05-11 05:56:42,22 days 19:38:52.036584000,
"Apologies, this is pretty off topic, but I figured you folks would probably know.

How could I determine how impartial a polling organisation is ? I'm thinking specifically of the Angus Reid institute in Canada",2,1526023114.0,8ij7lx,False,"Apologies, this is pretty off topic, but I figured you folks would probably know.

How could I determine how impartial a polling organisation is ? I'm thinking specifically of the Angus Reid institute in Canada",0,"Apologies, this is pretty off topic, but I figured you folks would probably know.

How could I determine how impartial a polling organisation is ? I'm thinking specifically of the Angus Reid institute in Canada",4,statistics,54935,,Question: how do I rate the reliability / integrity of a polling organisation?,https://www.reddit.com/r/statistics/comments/8ij7lx/question_how_do_i_rate_the_reliability_integrity/,all_ads,2018-05-11 03:18:34,22 days 22:17:00.036584000,
"In the mixed effects model that I'm running, my data has three levels, and I'm building a model with random slopes for fixed predictors all at one level (with correlation allowed between the random slope and intercept). 

I see a perfect negative correlation between the random slope and intercept for a one variable; and a perfect positive correlation between the random slope and intercept for another variable. For my third predictor I see a 'normal' correlation of 0.78. 

As far as I am aware this happens due to overfitting of the model. But my question is about the changing distribution of the variance components: the variance of the random intercepts and slopes (at the same level of the predictor) increase for the model with the negative correlation. Why does this happen?   

I guess my question is that how do the variance components change from a model which does not allow a correlation to a model which allows a correlation between the random slope and intercept?

Edit: The models without correlations has a variance of 0 for the random slope, but the models with correlation (-1) has a positive variance for the random slope.",1,1526003860.0,8iguao,False,"In the mixed effects model that I'm running, my data has three levels, and I'm building a model with random slopes for fixed predictors all at one level (with correlation allowed between the random slope and intercept). 

I see a perfect negative correlation between the random slope and intercept for a one variable; and a perfect positive correlation between the random slope and intercept for another variable. For my third predictor I see a 'normal' correlation of 0.78. 

As far as I am aware this happens due to overfitting of the model. But my question is about the changing distribution of the variance components: the variance of the random intercepts and slopes (at the same level of the predictor) increase for the model with the negative correlation. Why does this happen?   

I guess my question is that how do the variance components change from a model which does not allow a correlation to a model which allows a correlation between the random slope and intercept?

Edit: The models without correlations has a variance of 0 for the random slope, but the models with correlation (-1) has a positive variance for the random slope.",0,"In the mixed effects model that I'm running, my data has three levels, and I'm building a model with random slopes for fixed predictors all at one level (with correlation allowed between the random slope and intercept). 

I see a perfect negative correlation between the random slope and intercept for a one variable; and a perfect positive correlation between the random slope and intercept for another variable. For my third predictor I see a 'normal' correlation of 0.78. 

As far as I am aware this happens due to overfitting of the model. But my question is about the changing distribution of the variance components: the variance of the random intercepts and slopes (at the same level of the predictor) increase for the model with the negative correlation. Why does this happen?   

I guess my question is that how do the variance components change from a model which does not allow a correlation to a model which allows a correlation between the random slope and intercept?

Edit: The models without correlations has a variance of 0 for the random slope, but the models with correlation (-1) has a positive variance for the random slope.",9,statistics,54935,,Perfect positive/negative correlation between random intercept and random slope in mixed effects model,https://www.reddit.com/r/statistics/comments/8iguao/perfect_positivenegative_correlation_between/,all_ads,2018-05-10 21:57:40,23 days 03:37:54.036584000,
"I'm looking for a description of making a right choice out of several options.

This was something my statistics professor talked about in a lecture years ago, he used an example on how to pick a person to commit to. If I remember correctly you were to estimate how many serious life partners you think you could date in your lifetime. Divide this number by e, which gives you a new number let's say 4. Now the person you should commit to is the first person who comes along and who is better than the last 4 people.

I've been searching for a better description on this explanation, but I haven't found anything. I need this to make an important decision soon.",4,1526006814.0,8ih88l,False,"I'm looking for a description of making a right choice out of several options.

This was something my statistics professor talked about in a lecture years ago, he used an example on how to pick a person to commit to. If I remember correctly you were to estimate how many serious life partners you think you could date in your lifetime. Divide this number by e, which gives you a new number let's say 4. Now the person you should commit to is the first person who comes along and who is better than the last 4 people.

I've been searching for a better description on this explanation, but I haven't found anything. I need this to make an important decision soon.",0,"I'm looking for a description of making a right choice out of several options.

This was something my statistics professor talked about in a lecture years ago, he used an example on how to pick a person to commit to. If I remember correctly you were to estimate how many serious life partners you think you could date in your lifetime. Divide this number by e, which gives you a new number let's say 4. Now the person you should commit to is the first person who comes along and who is better than the last 4 people.

I've been searching for a better description on this explanation, but I haven't found anything. I need this to make an important decision soon.",5,statistics,54935,,Making the right choice of many. Divide by e.,https://www.reddit.com/r/statistics/comments/8ih88l/making_the_right_choice_of_many_divide_by_e/,all_ads,2018-05-10 22:46:54,23 days 02:48:40.036584000,
"I feel like I'm in this position where I'm just moving too slowly. Like there's just so much to learn but I'm just not sure if I'll be employable enough by the time I finish my university education and come out to work.

I've also just finished a Python programming class, and next semester my school will be going on to R and 2 obscure languages (I think SPSS or something along those lines). I initially wanted to do an internship too but unfortunately, most companies just aren't looking for year 1 university students so that's out of the equation (for this year).

Anyway, it would be great if I could get some professional advice from people who have advanced in this field.

1) what language to learn? I know python is definitely a must. How about R and SQL? Any other recommendations (JavaScript, C++ etc)?

2) what are some ways to build up on my resume? From what I've researched, it seems kaggle and creating a blog to showcase my skillsets would be good.

If it helps, I'm thinking of getting a Master's in statistics / data science in the future. I think there isn't any room to climb in this industry without at least a Master's. **If you think otherwise, do share too. I'm very open to perspectives.**

Any kind of advice would be appreciated, thanks.",27,1525977144.0,8idvjn,False,"I feel like I'm in this position where I'm just moving too slowly. Like there's just so much to learn but I'm just not sure if I'll be employable enough by the time I finish my university education and come out to work.

I've also just finished a Python programming class, and next semester my school will be going on to R and 2 obscure languages (I think SPSS or something along those lines). I initially wanted to do an internship too but unfortunately, most companies just aren't looking for year 1 university students so that's out of the equation (for this year).

Anyway, it would be great if I could get some professional advice from people who have advanced in this field.

1) what language to learn? I know python is definitely a must. How about R and SQL? Any other recommendations (JavaScript, C++ etc)?

2) what are some ways to build up on my resume? From what I've researched, it seems kaggle and creating a blog to showcase my skillsets would be good.

If it helps, I'm thinking of getting a Master's in statistics / data science in the future. I think there isn't any room to climb in this industry without at least a Master's. **If you think otherwise, do share too. I'm very open to perspectives.**

Any kind of advice would be appreciated, thanks.",0,"I feel like I'm in this position where I'm just moving too slowly. Like there's just so much to learn but I'm just not sure if I'll be employable enough by the time I finish my university education and come out to work.

I've also just finished a Python programming class, and next semester my school will be going on to R and 2 obscure languages (I think SPSS or something along those lines). I initially wanted to do an internship too but unfortunately, most companies just aren't looking for year 1 university students so that's out of the equation (for this year).

Anyway, it would be great if I could get some professional advice from people who have advanced in this field.

1) what language to learn? I know python is definitely a must. How about R and SQL? Any other recommendations (JavaScript, C++ etc)?

2) what are some ways to build up on my resume? From what I've researched, it seems kaggle and creating a blog to showcase my skillsets would be good.

If it helps, I'm thinking of getting a Master's in statistics / data science in the future. I think there isn't any room to climb in this industry without at least a Master's. **If you think otherwise, do share too. I'm very open to perspectives.**

Any kind of advice would be appreciated, thanks.",13,statistics,54935,,What advice would you give to statistics / data science undergraduates?,https://www.reddit.com/r/statistics/comments/8idvjn/what_advice_would_you_give_to_statistics_data/,all_ads,2018-05-10 14:32:24,23 days 11:03:10.036584000,
"I'll be starting my graduate degree this Fall and while I have experience with the Computer Science side of research, I don't have much in the way of Math(specifically Statistics). 

While doing research with the CS department at my school I noticed that lots of the lesser-known ideas that had potential to get big in the future came from recent papers rather than classes offered. Maybe this is because of the niches that exist within fields, but it was interesting and I'd like to see if it holds true for my graduate degree before getting started! Are there any common go-to places for papers on Statistics?

edit: Title should be without ""the"". :( I was originally gonna say ""... in the area"" but decided against it, haha. Current interests are in Data Science and and Statistics side of Machine Learning.",7,1525991285.0,8ifadq,False,"I'll be starting my graduate degree this Fall and while I have experience with the Computer Science side of research, I don't have much in the way of Math(specifically Statistics). 

While doing research with the CS department at my school I noticed that lots of the lesser-known ideas that had potential to get big in the future came from recent papers rather than classes offered. Maybe this is because of the niches that exist within fields, but it was interesting and I'd like to see if it holds true for my graduate degree before getting started! Are there any common go-to places for papers on Statistics?

edit: Title should be without ""the"". :( I was originally gonna say ""... in the area"" but decided against it, haha. Current interests are in Data Science and and Statistics side of Machine Learning.",0,"I'll be starting my graduate degree this Fall and while I have experience with the Computer Science side of research, I don't have much in the way of Math(specifically Statistics). 

While doing research with the CS department at my school I noticed that lots of the lesser-known ideas that had potential to get big in the future came from recent papers rather than classes offered. Maybe this is because of the niches that exist within fields, but it was interesting and I'd like to see if it holds true for my graduate degree before getting started! Are there any common go-to places for papers on Statistics?

edit: Title should be without ""the"". :( I was originally gonna say ""... in the area"" but decided against it, haha. Current interests are in Data Science and and Statistics side of Machine Learning.",6,statistics,54935,,Sources for papers in the Statistics?,https://www.reddit.com/r/statistics/comments/8ifadq/sources_for_papers_in_the_statistics/,all_ads,2018-05-10 18:28:05,23 days 07:07:29.036584000,
"I am applying to a job similar to this one:
https://www.sixflagsjobs.com/job/agawam/marketing-surveyor-and-brand-ambassador-seasonal/1047/6874463?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic

I see the words:
Following proper sampling methodology and surveying at all times


Curious to know how stats related this would be? I'm kind of afraid this job is very watered down in terms of statistics (Look at the age requirement) and what other other stat entry opportunities I could look into? Any advice would be much appreciated. 
",2,1526006536.0,8ih6zn,False,"I am applying to a job similar to this one:
https://www.sixflagsjobs.com/job/agawam/marketing-surveyor-and-brand-ambassador-seasonal/1047/6874463?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic

I see the words:
Following proper sampling methodology and surveying at all times


Curious to know how stats related this would be? I'm kind of afraid this job is very watered down in terms of statistics (Look at the age requirement) and what other other stat entry opportunities I could look into? Any advice would be much appreciated. 
",0,"I am applying to a job similar to this one:
https://www.sixflagsjobs.com/job/agawam/marketing-surveyor-and-brand-ambassador-seasonal/1047/6874463?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic

I see the words:
Following proper sampling methodology and surveying at all times


Curious to know how stats related this would be? I'm kind of afraid this job is very watered down in terms of statistics (Look at the age requirement) and what other other stat entry opportunities I could look into? Any advice would be much appreciated. 
",2,statistics,54935,,What can you glean from this job description and is this a nice first start to enter a statistics/data related job?,https://www.reddit.com/r/statistics/comments/8ih6zn/what_can_you_glean_from_this_job_description_and/,all_ads,2018-05-10 22:42:16,23 days 02:53:18.036584000,
I have a stats degree but my programming skills are weak. I'm afraid that as time goes I will lose my stat knowledge and skills from doing these unrelated jobs. ,14,1525948181.0,8ibm98,False,I have a stats degree but my programming skills are weak. I'm afraid that as time goes I will lose my stat knowledge and skills from doing these unrelated jobs. ,0,I have a stats degree but my programming skills are weak. I'm afraid that as time goes I will lose my stat knowledge and skills from doing these unrelated jobs. ,23,statistics,54935,,How do I hone my statistics and programming skills while working in an unrelated job?,https://www.reddit.com/r/statistics/comments/8ibm98/how_do_i_hone_my_statistics_and_programming/,all_ads,2018-05-10 06:29:41,23 days 19:05:53.036584000,
"BSc Physics: completed  
MSc Statistics: in progress, will be writing dissertation over this summer and graduating December.

Both at top 20 UK universities. Circumstances didn't permit me to take any summer internships unfortunately.

When I read about the job market I get whole bunch of contradictory info. Some say a Masters degree is fine, others say you're not getting shit unless you have a PhD. I don't have any relevant work experience but I could throw up a few personal projects onto github.

So with all this said, where should I be realistically setting my sights? Can I get a good job right away, or will I have to wander in the internship desert for a while before reaching the promised land?",5,1526006578.0,8ih77t,False,"BSc Physics: completed  
MSc Statistics: in progress, will be writing dissertation over this summer and graduating December.

Both at top 20 UK universities. Circumstances didn't permit me to take any summer internships unfortunately.

When I read about the job market I get whole bunch of contradictory info. Some say a Masters degree is fine, others say you're not getting shit unless you have a PhD. I don't have any relevant work experience but I could throw up a few personal projects onto github.

So with all this said, where should I be realistically setting my sights? Can I get a good job right away, or will I have to wander in the internship desert for a while before reaching the promised land?",0,"BSc Physics: completed  
MSc Statistics: in progress, will be writing dissertation over this summer and graduating December.

Both at top 20 UK universities. Circumstances didn't permit me to take any summer internships unfortunately.

When I read about the job market I get whole bunch of contradictory info. Some say a Masters degree is fine, others say you're not getting shit unless you have a PhD. I don't have any relevant work experience but I could throw up a few personal projects onto github.

So with all this said, where should I be realistically setting my sights? Can I get a good job right away, or will I have to wander in the internship desert for a while before reaching the promised land?",0,statistics,54935,,"Given these qualifications, what should my expectations be in the data science job market as a fresh graduate?",https://www.reddit.com/r/statistics/comments/8ih77t/given_these_qualifications_what_should_my/,all_ads,2018-05-10 22:42:58,23 days 02:52:36.036584000,
"Quick background: Everything I know about statistics comes from applied stats and psych research methods classes. After graduation, I took linear algebra an calc classes to knock out pre-reqs for grad school. I'm sure I'll learn how it all fits together once I get to grad school, but in the mean time I'd like to start connecting the dots between linear algebra and statistics. 

Other than OLS regression, the go-to example in linear algebra textbooks, what statistical concepts should I revisit now that I've taken a bit of linear algebra?",19,1525925789.0,8i93gy,False,"Quick background: Everything I know about statistics comes from applied stats and psych research methods classes. After graduation, I took linear algebra an calc classes to knock out pre-reqs for grad school. I'm sure I'll learn how it all fits together once I get to grad school, but in the mean time I'd like to start connecting the dots between linear algebra and statistics. 

Other than OLS regression, the go-to example in linear algebra textbooks, what statistical concepts should I revisit now that I've taken a bit of linear algebra?",0,"Quick background: Everything I know about statistics comes from applied stats and psych research methods classes. After graduation, I took linear algebra an calc classes to knock out pre-reqs for grad school. I'm sure I'll learn how it all fits together once I get to grad school, but in the mean time I'd like to start connecting the dots between linear algebra and statistics. 

Other than OLS regression, the go-to example in linear algebra textbooks, what statistical concepts should I revisit now that I've taken a bit of linear algebra?",32,statistics,54935,,"Outside of OLS regression, what's the best way to connect my newfound linear algebra knowledge to statistics?",https://www.reddit.com/r/statistics/comments/8i93gy/outside_of_ols_regression_whats_the_best_way_to/,all_ads,2018-05-10 00:16:29,24 days 01:19:05.036584000,
"Hi all, I'm taking a course in regression this summer, but it's going to be very intensive, so I was thinking it might be a good idea to familiarize myself with the material beforehand so I don't fall behind.  Does anyone have recommendations for some simple books I could read or any webpages I could check out? I'm not looking for anything really thorough, I'm sure the course will teach me that. I just want a background and familiarity with the major concepts.  ",1,1525987836.0,8ievyy,False,"Hi all, I'm taking a course in regression this summer, but it's going to be very intensive, so I was thinking it might be a good idea to familiarize myself with the material beforehand so I don't fall behind.  Does anyone have recommendations for some simple books I could read or any webpages I could check out? I'm not looking for anything really thorough, I'm sure the course will teach me that. I just want a background and familiarity with the major concepts.  ",0,"Hi all, I'm taking a course in regression this summer, but it's going to be very intensive, so I was thinking it might be a good idea to familiarize myself with the material beforehand so I don't fall behind.  Does anyone have recommendations for some simple books I could read or any webpages I could check out? I'm not looking for anything really thorough, I'm sure the course will teach me that. I just want a background and familiarity with the major concepts.  ",1,statistics,54935,,"Taking an intensive regression course this summer, anyone have any recommendations for material to catch up on?",https://www.reddit.com/r/statistics/comments/8ievyy/taking_an_intensive_regression_course_this_summer/,all_ads,2018-05-10 17:30:36,23 days 08:04:58.036584000,
I have a BS applied mathematics and am looking for a masters in statistics ,11,1525957947.0,8icgjy,False,I have a BS applied mathematics and am looking for a masters in statistics ,0,I have a BS applied mathematics and am looking for a masters in statistics ,4,statistics,54935,,What is the best graduate school for statistics?,https://www.reddit.com/r/statistics/comments/8icgjy/what_is_the_best_graduate_school_for_statistics/,all_ads,2018-05-10 09:12:27,23 days 16:23:07.036584000,
"In this article, we show that the issue with polynomial regression is not over\-fitting, but numerical precision. Even if done right, numerical precision still remains an insurmountable challenge. We focus here on step\-wise polynomial regression, which is supposed to be more stable than the traditional model. In step\-wise regression, we estimate one coefficient at a time, using the classic least square technique. 

Even if the function to be estimated is very smooth, due to machine precision, only the first three or four coefficients can be accurately computed. With infinite precision, all coefficients would be correctly computed without over\-fitting. We first explore this problem from a mathematical point of view in the next section, then provide recommendations for practical model implementations in the last section. 

This is also a good read for professionals with a math background interested in learning more about data science, as we start with some simple math, then discuss how it relates to data science. Also, this is an original article, not something you will learn in college classes or data camps, and it even features the solution to a linear regression involving an infinite number of variables.

**Content of this article**:

1. Polynomial regression for Taylor series

* Stepwise polynomial regression: algorithm
* Convergence theorem

2.Application to Real Life Regression Models

* Recommendations for practical model implementation

Read the full article, [here](https://dsc.news/2G6qFo4).",1,1525987970.0,8iewhw,False,"In this article, we show that the issue with polynomial regression is not over\-fitting, but numerical precision. Even if done right, numerical precision still remains an insurmountable challenge. We focus here on step\-wise polynomial regression, which is supposed to be more stable than the traditional model. In step\-wise regression, we estimate one coefficient at a time, using the classic least square technique. 

Even if the function to be estimated is very smooth, due to machine precision, only the first three or four coefficients can be accurately computed. With infinite precision, all coefficients would be correctly computed without over\-fitting. We first explore this problem from a mathematical point of view in the next section, then provide recommendations for practical model implementations in the last section. 

This is also a good read for professionals with a math background interested in learning more about data science, as we start with some simple math, then discuss how it relates to data science. Also, this is an original article, not something you will learn in college classes or data camps, and it even features the solution to a linear regression involving an infinite number of variables.

**Content of this article**:

1. Polynomial regression for Taylor series

* Stepwise polynomial regression: algorithm
* Convergence theorem

2.Application to Real Life Regression Models

* Recommendations for practical model implementation

Read the full article, [here](https://dsc.news/2G6qFo4).",0,"In this article, we show that the issue with polynomial regression is not over\-fitting, but numerical precision. Even if done right, numerical precision still remains an insurmountable challenge. We focus here on step\-wise polynomial regression, which is supposed to be more stable than the traditional model. In step\-wise regression, we estimate one coefficient at a time, using the classic least square technique. 

Even if the function to be estimated is very smooth, due to machine precision, only the first three or four coefficients can be accurately computed. With infinite precision, all coefficients would be correctly computed without over\-fitting. We first explore this problem from a mathematical point of view in the next section, then provide recommendations for practical model implementations in the last section. 

This is also a good read for professionals with a math background interested in learning more about data science, as we start with some simple math, then discuss how it relates to data science. Also, this is an original article, not something you will learn in college classes or data camps, and it even features the solution to a linear regression involving an infinite number of variables.

**Content of this article**:

1. Polynomial regression for Taylor series

* Stepwise polynomial regression: algorithm
* Convergence theorem

2.Application to Real Life Regression Models

* Recommendations for practical model implementation

Read the full article, [here](https://dsc.news/2G6qFo4).",0,statistics,54935,,Deep Dive into Polynomial Regression and Overfitting,https://www.reddit.com/r/statistics/comments/8iewhw/deep_dive_into_polynomial_regression_and/,all_ads,2018-05-10 17:32:50,23 days 08:02:44.036584000,
"I’m slowly getting into data analytics and I think a huge portion of it is visualizing/modeling the data into something that the business/upper managment can understand and appreciate. 

Aside from using Excel, are there any that you guys could recommend?

Sidenote: i want to go beyond just data entry( which is what I am mostly doing at work at the moment) and have started to read about data analytics. I just find it so interesting working with tons of data and finding little things that correlate with each other. ",23,1525901548.0,8i5zuq,False,"I’m slowly getting into data analytics and I think a huge portion of it is visualizing/modeling the data into something that the business/upper managment can understand and appreciate. 

Aside from using Excel, are there any that you guys could recommend?

Sidenote: i want to go beyond just data entry( which is what I am mostly doing at work at the moment) and have started to read about data analytics. I just find it so interesting working with tons of data and finding little things that correlate with each other. ",0,"I’m slowly getting into data analytics and I think a huge portion of it is visualizing/modeling the data into something that the business/upper managment can understand and appreciate. 

Aside from using Excel, are there any that you guys could recommend?

Sidenote: i want to go beyond just data entry( which is what I am mostly doing at work at the moment) and have started to read about data analytics. I just find it so interesting working with tons of data and finding little things that correlate with each other. ",17,statistics,54935,,What are some good (free) programs I can use to model data?,https://www.reddit.com/r/statistics/comments/8i5zuq/what_are_some_good_free_programs_i_can_use_to/,all_ads,2018-05-09 17:32:28,24 days 08:03:06.036584000,
"Quick question -- I have a sample of more than 30 data values (n>30). I do not know the exact population variance.

Based on this information, when calculating a confidence interval, should I use a t test or z test?",6,1525955878.0,8icab6,False,"Quick question -- I have a sample of more than 30 data values (n>30). I do not know the exact population variance.

Based on this information, when calculating a confidence interval, should I use a t test or z test?",0,"Quick question -- I have a sample of more than 30 data values (n>30). I do not know the exact population variance.

Based on this information, when calculating a confidence interval, should I use a t test or z test?",0,statistics,54935,,Z test versus t test?,https://www.reddit.com/r/statistics/comments/8icab6/z_test_versus_t_test/,all_ads,2018-05-10 08:37:58,23 days 16:57:36.036584000,
"Back in 2001 or so, I was working towards an undergraduate social science degree and we had to conduct some research, put the data into SPSS, and run some ANOVA and T-Tests.  (I honestly can’t remember what those mean anymore).   I haven’t thought about SPSS since then and I went on to earn a non-social science graduate degree in an industry in which I now work.  

Fast forward to today, and during a work meeting it was announced that we’d begin working on a project with other offices in which we’d be collecting data, looking for correlations, etc.  A discussion ensued as to whether the data should be entered into Word versus Excel.  I had a momentary lapse in judgment and opened my big mouth about some program called SPSS that could do some amazing statistical analyses.  I was promptly assigned to “look into that” and get back to the group.  


So, here I am.  The Google tells me that SPSS is still a thing.  I have no idea if it is still the “go-to” (maybe it never was?) or whether there’s something better out there?  Sorry for being vague, I can’t really give more details than that at the moment.  Also, this is my first post on this sub, so please go easy on this newb if I have completely wasted everybody’s time.  Thanks.   ",21,1525932140.0,8i9vw0,False,"Back in 2001 or so, I was working towards an undergraduate social science degree and we had to conduct some research, put the data into SPSS, and run some ANOVA and T-Tests.  (I honestly can’t remember what those mean anymore).   I haven’t thought about SPSS since then and I went on to earn a non-social science graduate degree in an industry in which I now work.  

Fast forward to today, and during a work meeting it was announced that we’d begin working on a project with other offices in which we’d be collecting data, looking for correlations, etc.  A discussion ensued as to whether the data should be entered into Word versus Excel.  I had a momentary lapse in judgment and opened my big mouth about some program called SPSS that could do some amazing statistical analyses.  I was promptly assigned to “look into that” and get back to the group.  


So, here I am.  The Google tells me that SPSS is still a thing.  I have no idea if it is still the “go-to” (maybe it never was?) or whether there’s something better out there?  Sorry for being vague, I can’t really give more details than that at the moment.  Also, this is my first post on this sub, so please go easy on this newb if I have completely wasted everybody’s time.  Thanks.   ",0,"Back in 2001 or so, I was working towards an undergraduate social science degree and we had to conduct some research, put the data into SPSS, and run some ANOVA and T-Tests.  (I honestly can’t remember what those mean anymore).   I haven’t thought about SPSS since then and I went on to earn a non-social science graduate degree in an industry in which I now work.  

Fast forward to today, and during a work meeting it was announced that we’d begin working on a project with other offices in which we’d be collecting data, looking for correlations, etc.  A discussion ensued as to whether the data should be entered into Word versus Excel.  I had a momentary lapse in judgment and opened my big mouth about some program called SPSS that could do some amazing statistical analyses.  I was promptly assigned to “look into that” and get back to the group.  


So, here I am.  The Google tells me that SPSS is still a thing.  I have no idea if it is still the “go-to” (maybe it never was?) or whether there’s something better out there?  Sorry for being vague, I can’t really give more details than that at the moment.  Also, this is my first post on this sub, so please go easy on this newb if I have completely wasted everybody’s time.  Thanks.   ",3,statistics,54935,,Beginner question - is SPSS still the best tool for analyzing social science data?,https://www.reddit.com/r/statistics/comments/8i9vw0/beginner_question_is_spss_still_the_best_tool_for/,all_ads,2018-05-10 02:02:20,23 days 23:33:14.036584000,
"Hello,

I'm a graduate student studying agricultural economics, and I start my PhD courses in the fall. I decided to take the econometrics route for my PhD, which requires more statistics. Over the summer, I am thinking about taking a mathematical probability class through the math department before going on and taking ""Theory of Distribution"" in the fall. 

Here is the general topics covered in Mathematical Probability:

Probability spaces, discrete and continuous random variables, special distributions, joint distributions, expectations, law of large numbers, the central limit theorem. 

And the general topics covered in Theory of Distribution:

Brief introduction to probability theory; distributions and expectations of random variables, transformations of random variables and order statistics; generating functions and basic limit concepts. 

How useful will the mathematical probability course be for the stats course? ",11,1525948145.0,8ibm4d,False,"Hello,

I'm a graduate student studying agricultural economics, and I start my PhD courses in the fall. I decided to take the econometrics route for my PhD, which requires more statistics. Over the summer, I am thinking about taking a mathematical probability class through the math department before going on and taking ""Theory of Distribution"" in the fall. 

Here is the general topics covered in Mathematical Probability:

Probability spaces, discrete and continuous random variables, special distributions, joint distributions, expectations, law of large numbers, the central limit theorem. 

And the general topics covered in Theory of Distribution:

Brief introduction to probability theory; distributions and expectations of random variables, transformations of random variables and order statistics; generating functions and basic limit concepts. 

How useful will the mathematical probability course be for the stats course? ",0,"Hello,

I'm a graduate student studying agricultural economics, and I start my PhD courses in the fall. I decided to take the econometrics route for my PhD, which requires more statistics. Over the summer, I am thinking about taking a mathematical probability class through the math department before going on and taking ""Theory of Distribution"" in the fall. 

Here is the general topics covered in Mathematical Probability:

Probability spaces, discrete and continuous random variables, special distributions, joint distributions, expectations, law of large numbers, the central limit theorem. 

And the general topics covered in Theory of Distribution:

Brief introduction to probability theory; distributions and expectations of random variables, transformations of random variables and order statistics; generating functions and basic limit concepts. 

How useful will the mathematical probability course be for the stats course? ",1,statistics,54935,,Should I take Mathematical Probability over the summer to better prepare for my stats classes?,https://www.reddit.com/r/statistics/comments/8ibm4d/should_i_take_mathematical_probability_over_the/,all_ads,2018-05-10 06:29:05,23 days 19:06:29.036584000,
"Philosophically, what is your perception of psychological statistics? 

Professionally, what are some paths in the field of psychological research that mostly involves statistics? 

As it may be rare to have both a psychology and a statistics background, how integrative is this area of research? Can someone for example with a Biostatistics or Epidemiology background be involved? What would be your recommended path for someone interested in the ways psychological constructs and statistics overlap, specifically in regards to health and well-being (as opposed to educational outcomes)? ",7,1525923356.0,8i8s4m,False,"Philosophically, what is your perception of psychological statistics? 

Professionally, what are some paths in the field of psychological research that mostly involves statistics? 

As it may be rare to have both a psychology and a statistics background, how integrative is this area of research? Can someone for example with a Biostatistics or Epidemiology background be involved? What would be your recommended path for someone interested in the ways psychological constructs and statistics overlap, specifically in regards to health and well-being (as opposed to educational outcomes)? ",0,"Philosophically, what is your perception of psychological statistics? 

Professionally, what are some paths in the field of psychological research that mostly involves statistics? 

As it may be rare to have both a psychology and a statistics background, how integrative is this area of research? Can someone for example with a Biostatistics or Epidemiology background be involved? What would be your recommended path for someone interested in the ways psychological constructs and statistics overlap, specifically in regards to health and well-being (as opposed to educational outcomes)? ",3,statistics,54935,,Statistics in Behavioral Research,https://www.reddit.com/r/statistics/comments/8i8s4m/statistics_in_behavioral_research/,all_ads,2018-05-09 23:35:56,24 days 01:59:38.036584000,
"I understand all of the examples about the probability of something being EXACTLY  1.00000 (not 1.000001) is effectively zero, and therefore we have to evaluate continuous variables over a range (the integral). 

But, what does the value returned by the normal PDF actually mean? If I plug in NORM.DIST(0,0,0.4,FALSE) into excel it spits out 0.9973. What does that value mean, if anything? It's certainly not the probability of the value 0 occurring (but it would be if I used the binomial distribution, correct?",8,1525924941.0,8i8zjc,False,"I understand all of the examples about the probability of something being EXACTLY  1.00000 (not 1.000001) is effectively zero, and therefore we have to evaluate continuous variables over a range (the integral). 

But, what does the value returned by the normal PDF actually mean? If I plug in NORM.DIST(0,0,0.4,FALSE) into excel it spits out 0.9973. What does that value mean, if anything? It's certainly not the probability of the value 0 occurring (but it would be if I used the binomial distribution, correct?",0,"I understand all of the examples about the probability of something being EXACTLY  1.00000 (not 1.000001) is effectively zero, and therefore we have to evaluate continuous variables over a range (the integral). 

But, what does the value returned by the normal PDF actually mean? If I plug in NORM.DIST(0,0,0.4,FALSE) into excel it spits out 0.9973. What does that value mean, if anything? It's certainly not the probability of the value 0 occurring (but it would be if I used the binomial distribution, correct?",2,statistics,54935,,What does the numerical evaluation of the normal distribution curve mean for a given point?,https://www.reddit.com/r/statistics/comments/8i8zjc/what_does_the_numerical_evaluation_of_the_normal/,all_ads,2018-05-10 00:02:21,24 days 01:33:13.036584000,
"Currently pulling my hair out trying to reformat a Kaplan-Meier curve using the Chart Editor (sorry, very new to the program). I need to add essentially a footnote that allows me to display no. of patients at risk to look like [this](https://imgur.com/a/svKhKhG). However, I can't seem to create space underneath the plot to accommodate additional footnotes ([my graph](https://imgur.com/Xwytlxr) currently) without it shrinking or hiding the text. I'm essentially looking for a way to increase the white space below the graph itself. Also, is there any way to add additional lines of text in a single textbox? Any help would be VERY appreciated. Thank you!",3,1525923743.0,8i8u16,False,"Currently pulling my hair out trying to reformat a Kaplan-Meier curve using the Chart Editor (sorry, very new to the program). I need to add essentially a footnote that allows me to display no. of patients at risk to look like [this](https://imgur.com/a/svKhKhG). However, I can't seem to create space underneath the plot to accommodate additional footnotes ([my graph](https://imgur.com/Xwytlxr) currently) without it shrinking or hiding the text. I'm essentially looking for a way to increase the white space below the graph itself. Also, is there any way to add additional lines of text in a single textbox? Any help would be VERY appreciated. Thank you!",0,"Currently pulling my hair out trying to reformat a Kaplan-Meier curve using the Chart Editor (sorry, very new to the program). I need to add essentially a footnote that allows me to display no. of patients at risk to look like [this](https://imgur.com/a/svKhKhG). However, I can't seem to create space underneath the plot to accommodate additional footnotes ([my graph](https://imgur.com/Xwytlxr) currently) without it shrinking or hiding the text. I'm essentially looking for a way to increase the white space below the graph itself. Also, is there any way to add additional lines of text in a single textbox? Any help would be VERY appreciated. Thank you!",2,statistics,54935,,Need help with graph formatting in SPSS Chart Editor,https://www.reddit.com/r/statistics/comments/8i8u16/need_help_with_graph_formatting_in_spss_chart/,all_ads,2018-05-09 23:42:23,24 days 01:53:11.036584000,
"Hey All, quick question regarding hypothesis’. I’m taking a test this week and I’m having trouble identifying the two hypothesis’.  A question that tripped me up was: originally batteries had a lifetime of 100 hours, but better materials have led to a belief that the life expectancy has increased. A sample of 50 batteries showed an average life of 120 hours. 

Null: M>=100
Alt:   M<100

Did I do this correctly?",11,1525919507.0,8i8a7x,False,"Hey All, quick question regarding hypothesis’. I’m taking a test this week and I’m having trouble identifying the two hypothesis’.  A question that tripped me up was: originally batteries had a lifetime of 100 hours, but better materials have led to a belief that the life expectancy has increased. A sample of 50 batteries showed an average life of 120 hours. 

Null: M>=100
Alt:   M<100

Did I do this correctly?",0,"Hey All, quick question regarding hypothesis’. I’m taking a test this week and I’m having trouble identifying the two hypothesis’.  A question that tripped me up was: originally batteries had a lifetime of 100 hours, but better materials have led to a belief that the life expectancy has increased. A sample of 50 batteries showed an average life of 120 hours. 

Null: M>=100
Alt:   M<100

Did I do this correctly?",2,statistics,54935,,Null vs Alternative Hypothesis,https://www.reddit.com/r/statistics/comments/8i8a7x/null_vs_alternative_hypothesis/,all_ads,2018-05-09 22:31:47,24 days 03:03:47.036584000,
"Confident in the performance of basic stats tests but not in the selection of the test itself. 

Bit of background for my tests- i am trying to compare bacterial recovery number across various methods. I have a pre-established expected recovery that i want to compare to my results to, in order to work out whether the recovery of bacteria with each method differs from this expected. 

E.g. i am expecting a recovery of 30 bacterials cells but one method i get a mean recovery of 10 cells then another 15 cells etc. 

Any help welcome and if you need more info let me know. 

Edit: more info now in the comments.",6,1525932707.0,8i9yak,False,"Confident in the performance of basic stats tests but not in the selection of the test itself. 

Bit of background for my tests- i am trying to compare bacterial recovery number across various methods. I have a pre-established expected recovery that i want to compare to my results to, in order to work out whether the recovery of bacteria with each method differs from this expected. 

E.g. i am expecting a recovery of 30 bacterials cells but one method i get a mean recovery of 10 cells then another 15 cells etc. 

Any help welcome and if you need more info let me know. 

Edit: more info now in the comments.",0,"Confident in the performance of basic stats tests but not in the selection of the test itself. 

Bit of background for my tests- i am trying to compare bacterial recovery number across various methods. I have a pre-established expected recovery that i want to compare to my results to, in order to work out whether the recovery of bacteria with each method differs from this expected. 

E.g. i am expecting a recovery of 30 bacterials cells but one method i get a mean recovery of 10 cells then another 15 cells etc. 

Any help welcome and if you need more info let me know. 

Edit: more info now in the comments.",1,statistics,54935,,"Research microbiologist here, is chi aquared the right test for me to use?",https://www.reddit.com/r/statistics/comments/8i9yak/research_microbiologist_here_is_chi_aquared_the/,all_ads,2018-05-10 02:11:47,23 days 23:23:47.036584000,
Does anyone know how to interpret b0 for a poisson regression?  ,4,1525919714.0,8i8b5q,False,Does anyone know how to interpret b0 for a poisson regression?  ,0,Does anyone know how to interpret b0 for a poisson regression?  ,0,statistics,54935,,Question: B0 (B naught) for Poisson Regression?,https://www.reddit.com/r/statistics/comments/8i8b5q/question_b0_b_naught_for_poisson_regression/,all_ads,2018-05-09 22:35:14,24 days 03:00:20.036584000,
,4,1525936650.0,8iae2a,False,,0,,0,statistics,54935,,What is called the group of standard deviation range should be get the mean in my data? The highs and lows will not be included.,https://www.reddit.com/r/statistics/comments/8iae2a/what_is_called_the_group_of_standard_deviation/,all_ads,2018-05-10 03:17:30,23 days 22:18:04.036584000,
"After, performing the Hausman test I found no evidence of endogeneity.  I remember hearing some rule of thumb where you can double the standard error of the 2SLS results and see if the difference in coefficients between OLS and 2SLS is greater than 2x the S.E.  Is this the case or is there some other method to compare the two results to see if there is a large bias?",1,1525913728.0,8i7j10,False,"After, performing the Hausman test I found no evidence of endogeneity.  I remember hearing some rule of thumb where you can double the standard error of the 2SLS results and see if the difference in coefficients between OLS and 2SLS is greater than 2x the S.E.  Is this the case or is there some other method to compare the two results to see if there is a large bias?",0,"After, performing the Hausman test I found no evidence of endogeneity.  I remember hearing some rule of thumb where you can double the standard error of the 2SLS results and see if the difference in coefficients between OLS and 2SLS is greater than 2x the S.E.  Is this the case or is there some other method to compare the two results to see if there is a large bias?",1,statistics,54935,,Question about comparing 2SLS and OLS results,https://www.reddit.com/r/statistics/comments/8i7j10/question_about_comparing_2sls_and_ols_results/,all_ads,2018-05-09 20:55:28,24 days 04:40:06.036584000,
"I'm looking to create a tier scheme for a dynasty fantasy football spreadsheet I made for fun.  My knowledge of statistics is relatively basic, and I'm looking for advice/resources on a best methodoloy.  

Here is a screenshot of the graph: https://i.imgur.com/0ThuOiG.png

I've looked into k-mean, SSE, and Gaussian mixture model techniques, but haven't really gotten a good enough grasp of any to move forward.  

Any advice on what kind of method would work best for what I want?  Any places to find resources on that method?



Thanks,

--FIGHTMILK--",2,1525910680.0,8i75ja,False,"I'm looking to create a tier scheme for a dynasty fantasy football spreadsheet I made for fun.  My knowledge of statistics is relatively basic, and I'm looking for advice/resources on a best methodoloy.  

Here is a screenshot of the graph: https://i.imgur.com/0ThuOiG.png

I've looked into k-mean, SSE, and Gaussian mixture model techniques, but haven't really gotten a good enough grasp of any to move forward.  

Any advice on what kind of method would work best for what I want?  Any places to find resources on that method?



Thanks,

--FIGHTMILK--",0,"I'm looking to create a tier scheme for a dynasty fantasy football spreadsheet I made for fun.  My knowledge of statistics is relatively basic, and I'm looking for advice/resources on a best methodoloy.  

Here is a screenshot of the graph: https://i.imgur.com/0ThuOiG.png

I've looked into k-mean, SSE, and Gaussian mixture model techniques, but haven't really gotten a good enough grasp of any to move forward.  

Any advice on what kind of method would work best for what I want?  Any places to find resources on that method?



Thanks,

--FIGHTMILK--",1,statistics,54935,,Tiering (clusters) on curve scatter plot.,https://www.reddit.com/r/statistics/comments/8i75ja/tiering_clusters_on_curve_scatter_plot/,all_ads,2018-05-09 20:04:40,24 days 05:30:54.036584000,
"Hi guys, I'm a Computer Engineering student at Purdue and I'm trying to graduate early but my advisor told me that I would need to take a Statistics course (Calculus based) over this summer 2018 so I can take another class in the Fall 2018.

I'm not sure if this is the correct place to ask, but he literally said I could take it online at any accredited college and was wondering if anyone knew anywhere to take a Statistics calculus based course online?",5,1525846442.0,8i0tv3,False,"Hi guys, I'm a Computer Engineering student at Purdue and I'm trying to graduate early but my advisor told me that I would need to take a Statistics course (Calculus based) over this summer 2018 so I can take another class in the Fall 2018.

I'm not sure if this is the correct place to ask, but he literally said I could take it online at any accredited college and was wondering if anyone knew anywhere to take a Statistics calculus based course online?",0,"Hi guys, I'm a Computer Engineering student at Purdue and I'm trying to graduate early but my advisor told me that I would need to take a Statistics course (Calculus based) over this summer 2018 so I can take another class in the Fall 2018.

I'm not sure if this is the correct place to ask, but he literally said I could take it online at any accredited college and was wondering if anyone knew anywhere to take a Statistics calculus based course online?",26,statistics,54935,,Online Intro to Statistics (Calculus based) course over summer?,https://www.reddit.com/r/statistics/comments/8i0tv3/online_intro_to_statistics_calculus_based_course/,all_ads,2018-05-09 02:14:02,24 days 23:21:32.036584000,
"HI!

I am in the process of analysing a randomized block design field trial, where we investigated to impact of 3 different sowing dates in 3 different strains of Millet, on the yield, germination rate and thousand kernel weight of the harvested grain.

I am doing a mixed effects model, where i so far have assigned the sowing date (early, mid or late), and accession (three strains of millet) as independant variables. Year and Block are considered random effects, and the response variables are yield, thousand kernel weight and germination rate.

To make for a good (significant) comparison of the strains and the sowing dates between the two years, i imagine that it would be good to include things such as accumulated growing degree days and precipitation during the growing season, which of course are different between years but also between the sowing date treatments.

I have calculated accumulated growing degree-days for the different treatments, and i have acces to precipitation data as well, but my question is if this is to be included in the model as a covariable, independant variable or a random effect.

Furthermore i am concerned that, as the calculation of growing degree-days are dependant on the sowing date (and harvest date), if there is a problem with having a random effect or a covariate dependant on a variable in the model.

I hope you understand my issue.

I will gladly provide a sample of my data, but i can't find anywhere to attach a file or a picture here.",1,1525897314.0,8i5iuw,False,"HI!

I am in the process of analysing a randomized block design field trial, where we investigated to impact of 3 different sowing dates in 3 different strains of Millet, on the yield, germination rate and thousand kernel weight of the harvested grain.

I am doing a mixed effects model, where i so far have assigned the sowing date (early, mid or late), and accession (three strains of millet) as independant variables. Year and Block are considered random effects, and the response variables are yield, thousand kernel weight and germination rate.

To make for a good (significant) comparison of the strains and the sowing dates between the two years, i imagine that it would be good to include things such as accumulated growing degree days and precipitation during the growing season, which of course are different between years but also between the sowing date treatments.

I have calculated accumulated growing degree-days for the different treatments, and i have acces to precipitation data as well, but my question is if this is to be included in the model as a covariable, independant variable or a random effect.

Furthermore i am concerned that, as the calculation of growing degree-days are dependant on the sowing date (and harvest date), if there is a problem with having a random effect or a covariate dependant on a variable in the model.

I hope you understand my issue.

I will gladly provide a sample of my data, but i can't find anywhere to attach a file or a picture here.",0,"HI!

I am in the process of analysing a randomized block design field trial, where we investigated to impact of 3 different sowing dates in 3 different strains of Millet, on the yield, germination rate and thousand kernel weight of the harvested grain.

I am doing a mixed effects model, where i so far have assigned the sowing date (early, mid or late), and accession (three strains of millet) as independant variables. Year and Block are considered random effects, and the response variables are yield, thousand kernel weight and germination rate.

To make for a good (significant) comparison of the strains and the sowing dates between the two years, i imagine that it would be good to include things such as accumulated growing degree days and precipitation during the growing season, which of course are different between years but also between the sowing date treatments.

I have calculated accumulated growing degree-days for the different treatments, and i have acces to precipitation data as well, but my question is if this is to be included in the model as a covariable, independant variable or a random effect.

Furthermore i am concerned that, as the calculation of growing degree-days are dependant on the sowing date (and harvest date), if there is a problem with having a random effect or a covariate dependant on a variable in the model.

I hope you understand my issue.

I will gladly provide a sample of my data, but i can't find anywhere to attach a file or a picture here.",1,statistics,54935,,Using GDD and other agrometeorological data to allow for better comparison of response variables between years in randomized block trials.,https://www.reddit.com/r/statistics/comments/8i5iuw/using_gdd_and_other_agrometeorological_data_to/,all_ads,2018-05-09 16:21:54,24 days 09:13:40.036584000,
"Hey! For my dissertation I am looking to perform some statistical tests and although I have some background in econometrics, I am by no means an expert. One question is really bugging me and if anyone can point me in the right direction, this would be much appreciated!

I'd like to test the volatility of capital flows by regressing their coefficient of variation on a series of time dummies to see whether they have become more or less volatile during these periods (using a panel data model, with fixed country effects). This approach has already been performed in numerous papers, and I feel confident that I can make this work.

However, rather than regression on years, I would make periods correspond with the so called Quantitative Easing programme enacted by the Federal Reserve, but these programmes (three in total) are unequal in length. The first period is Q4 2008 to Q3 2010, the second Q4 2010 to Q2 2011 and the third Q3 2012 to Q4 2014.

My intuition tells me that regressing with unequal periods might not give me the results I wanted, because longer periods might create distortion with regards to shorter ones? Basically the question is whether indicator dummies for unequal time periods are allowed (e.g. rather than having 4 different periods, each of two years, vary the length of those periods to capture certain events).


",0,1525877397.0,8i3x8h,False,"Hey! For my dissertation I am looking to perform some statistical tests and although I have some background in econometrics, I am by no means an expert. One question is really bugging me and if anyone can point me in the right direction, this would be much appreciated!

I'd like to test the volatility of capital flows by regressing their coefficient of variation on a series of time dummies to see whether they have become more or less volatile during these periods (using a panel data model, with fixed country effects). This approach has already been performed in numerous papers, and I feel confident that I can make this work.

However, rather than regression on years, I would make periods correspond with the so called Quantitative Easing programme enacted by the Federal Reserve, but these programmes (three in total) are unequal in length. The first period is Q4 2008 to Q3 2010, the second Q4 2010 to Q2 2011 and the third Q3 2012 to Q4 2014.

My intuition tells me that regressing with unequal periods might not give me the results I wanted, because longer periods might create distortion with regards to shorter ones? Basically the question is whether indicator dummies for unequal time periods are allowed (e.g. rather than having 4 different periods, each of two years, vary the length of those periods to capture certain events).


",0,"Hey! For my dissertation I am looking to perform some statistical tests and although I have some background in econometrics, I am by no means an expert. One question is really bugging me and if anyone can point me in the right direction, this would be much appreciated!

I'd like to test the volatility of capital flows by regressing their coefficient of variation on a series of time dummies to see whether they have become more or less volatile during these periods (using a panel data model, with fixed country effects). This approach has already been performed in numerous papers, and I feel confident that I can make this work.

However, rather than regression on years, I would make periods correspond with the so called Quantitative Easing programme enacted by the Federal Reserve, but these programmes (three in total) are unequal in length. The first period is Q4 2008 to Q3 2010, the second Q4 2010 to Q2 2011 and the third Q3 2012 to Q4 2014.

My intuition tells me that regressing with unequal periods might not give me the results I wanted, because longer periods might create distortion with regards to shorter ones? Basically the question is whether indicator dummies for unequal time periods are allowed (e.g. rather than having 4 different periods, each of two years, vary the length of those periods to capture certain events).


",1,statistics,54935,,Indicator variables over unequal periods?,https://www.reddit.com/r/statistics/comments/8i3x8h/indicator_variables_over_unequal_periods/,all_ads,2018-05-09 10:49:57,24 days 14:45:37.036584000,
"I want to see if there is a statistical correlation between weather and certain defects at my manufacturing plant using Minitab. These defects  relate to the painting process of the product which is sensitive to sudden temp and humidity conditions I've got two datasets:

First one containing the sample of units produced that day and quantity of defective items.
Second one containing weather conditions (min, max and avg temp, min-max and avg humidity)

What would be the beset way to find out a statistical correlation? Im confused if I should use Pearson or if a regression. [Here's a screenshot of what my raw data looks like. .](https://i.imgur.com/YnptPn1.jpg)

I'm a total noob to minitab so this is probably very basic but still couldn't get around to it.  I appreciate any help I can get. Thanks!",3,1525858452.0,8i26jk,False,"I want to see if there is a statistical correlation between weather and certain defects at my manufacturing plant using Minitab. These defects  relate to the painting process of the product which is sensitive to sudden temp and humidity conditions I've got two datasets:

First one containing the sample of units produced that day and quantity of defective items.
Second one containing weather conditions (min, max and avg temp, min-max and avg humidity)

What would be the beset way to find out a statistical correlation? Im confused if I should use Pearson or if a regression. [Here's a screenshot of what my raw data looks like. .](https://i.imgur.com/YnptPn1.jpg)

I'm a total noob to minitab so this is probably very basic but still couldn't get around to it.  I appreciate any help I can get. Thanks!",0,"I want to see if there is a statistical correlation between weather and certain defects at my manufacturing plant using Minitab. These defects  relate to the painting process of the product which is sensitive to sudden temp and humidity conditions I've got two datasets:

First one containing the sample of units produced that day and quantity of defective items.
Second one containing weather conditions (min, max and avg temp, min-max and avg humidity)

What would be the beset way to find out a statistical correlation? Im confused if I should use Pearson or if a regression. [Here's a screenshot of what my raw data looks like. .](https://i.imgur.com/YnptPn1.jpg)

I'm a total noob to minitab so this is probably very basic but still couldn't get around to it.  I appreciate any help I can get. Thanks!",2,statistics,54935,,Establish statistical correlation between weather and quality defects?,https://www.reddit.com/r/statistics/comments/8i26jk/establish_statistical_correlation_between_weather/,all_ads,2018-05-09 05:34:12,24 days 20:01:22.036584000,
"What do your job responsibilities consist of? 

I started working as one a few months ago and most of my job so far has been data entry. I’ve done a bit of data management and stuff and doing my own descriptive stats when I have free time, but I wanted to get an idea as I’m kind of feeling like I’m using 10% of my masters skills & not sure if that’s common of some entry level positions ",35,1525814139.0,8hwl5p,False,"What do your job responsibilities consist of? 

I started working as one a few months ago and most of my job so far has been data entry. I’ve done a bit of data management and stuff and doing my own descriptive stats when I have free time, but I wanted to get an idea as I’m kind of feeling like I’m using 10% of my masters skills & not sure if that’s common of some entry level positions ",0,"What do your job responsibilities consist of? 

I started working as one a few months ago and most of my job so far has been data entry. I’ve done a bit of data management and stuff and doing my own descriptive stats when I have free time, but I wanted to get an idea as I’m kind of feeling like I’m using 10% of my masters skills & not sure if that’s common of some entry level positions ",20,statistics,54935,,A question for data analysts?,https://www.reddit.com/r/statistics/comments/8hwl5p/a_question_for_data_analysts/,all_ads,2018-05-08 17:15:39,25 days 08:19:55.036584000,
"I think if you ask an academic statistician what the biggest advancements in stats in the past 25 years have been you’ll here the words “LASSO” “bootstrapping” and “R” (unironically). But if you ask an industry “data scientist” or someone in machine learning they can rattle off: classifiers, boosting, bagging, gradient descent, neural nets, image recognition and NLP as optimization problems, kernel methods, supervised learning, reinforcement learning, naive bayes, random forests. There’s a lot of research into these methods. 

But here’s the funny thing! If you ask a statistician or machine learning guy what books they should read first they’ll both say “Tibshirani” or a “intro to probability” book. The stats is more
Important than the programming...

What do you think? ",15,1525847595.0,8i0yx4,False,"I think if you ask an academic statistician what the biggest advancements in stats in the past 25 years have been you’ll here the words “LASSO” “bootstrapping” and “R” (unironically). But if you ask an industry “data scientist” or someone in machine learning they can rattle off: classifiers, boosting, bagging, gradient descent, neural nets, image recognition and NLP as optimization problems, kernel methods, supervised learning, reinforcement learning, naive bayes, random forests. There’s a lot of research into these methods. 

But here’s the funny thing! If you ask a statistician or machine learning guy what books they should read first they’ll both say “Tibshirani” or a “intro to probability” book. The stats is more
Important than the programming...

What do you think? ",0,"I think if you ask an academic statistician what the biggest advancements in stats in the past 25 years have been you’ll here the words “LASSO” “bootstrapping” and “R” (unironically). But if you ask an industry “data scientist” or someone in machine learning they can rattle off: classifiers, boosting, bagging, gradient descent, neural nets, image recognition and NLP as optimization problems, kernel methods, supervised learning, reinforcement learning, naive bayes, random forests. There’s a lot of research into these methods. 

But here’s the funny thing! If you ask a statistician or machine learning guy what books they should read first they’ll both say “Tibshirani” or a “intro to probability” book. The stats is more
Important than the programming...

What do you think? ",1,statistics,54935,,Is Machine Learning the new generation of statistics?,https://www.reddit.com/r/statistics/comments/8i0yx4/is_machine_learning_the_new_generation_of/,all_ads,2018-05-09 02:33:15,24 days 23:02:19.036584000,
"My data is individual customers of a subscription product, specifically looking at how many times they send a business email per month.  I want to determine if the customers are seasonal so that we can offer them a more personalized experience.

So I'm taking each customers monthly data and fitting a TBATS model in R.   I know this model uses ARMA errors, but the determination of seasonality is still based off of autocorrelation, correct?  And then what is left over becomes the errors?

Anyone see a problem with using this method to basically create an indicator variable of ""customer is seasonal?"" (Y/N)?

Help is really appreciated!",1,1525848494.0,8i12md,False,"My data is individual customers of a subscription product, specifically looking at how many times they send a business email per month.  I want to determine if the customers are seasonal so that we can offer them a more personalized experience.

So I'm taking each customers monthly data and fitting a TBATS model in R.   I know this model uses ARMA errors, but the determination of seasonality is still based off of autocorrelation, correct?  And then what is left over becomes the errors?

Anyone see a problem with using this method to basically create an indicator variable of ""customer is seasonal?"" (Y/N)?

Help is really appreciated!",0,"My data is individual customers of a subscription product, specifically looking at how many times they send a business email per month.  I want to determine if the customers are seasonal so that we can offer them a more personalized experience.

So I'm taking each customers monthly data and fitting a TBATS model in R.   I know this model uses ARMA errors, but the determination of seasonality is still based off of autocorrelation, correct?  And then what is left over becomes the errors?

Anyone see a problem with using this method to basically create an indicator variable of ""customer is seasonal?"" (Y/N)?

Help is really appreciated!",1,statistics,54935,,TBATS help,https://www.reddit.com/r/statistics/comments/8i12md/tbats_help/,all_ads,2018-05-09 02:48:14,24 days 22:47:20.036584000,
,10,1525889426.0,8i4tph,False,,0,,0,statistics,54935,,"Hi Reddit Stats, I have 18 test consist of 10, 12, 15, 10, 11, 16, 20, 12, 10, 44, 12, 11, 12, 13, 10, 12, 12,10, & 16. How do I know that is the real test? Should I just use average method? Or there are more professional and efficient way of doing it?",https://www.reddit.com/r/statistics/comments/8i4tph/hi_reddit_stats_i_have_18_test_consist_of_10_12/,all_ads,2018-05-09 14:10:26,24 days 11:25:08.036584000,
"Hello r/statistics,
I hope you can help me with a little question which came up during my studies today:

So given the Central Limit theorem, if I take 2 samples and calculate the sample means, could I predict the chance that two random sample means have a certain distance to each other?
For example I have a populations with mu=100 and a Var=36, and I take two samples two times. Could I calculate, if I do not know the original mu and var the chance of the two sample means for example taking the values 101 and 95?

My reasoning is that you could estimate the real variance out of the sample variances? Is pooling the variance correct in this case?

How precise would this be? Is it possible to calculate the chance of the mean of the two sample means to be the real mean out of this?

Thanks for your help!",1,1525828192.0,8hye46,False,"Hello r/statistics,
I hope you can help me with a little question which came up during my studies today:

So given the Central Limit theorem, if I take 2 samples and calculate the sample means, could I predict the chance that two random sample means have a certain distance to each other?
For example I have a populations with mu=100 and a Var=36, and I take two samples two times. Could I calculate, if I do not know the original mu and var the chance of the two sample means for example taking the values 101 and 95?

My reasoning is that you could estimate the real variance out of the sample variances? Is pooling the variance correct in this case?

How precise would this be? Is it possible to calculate the chance of the mean of the two sample means to be the real mean out of this?

Thanks for your help!",0,"Hello r/statistics,
I hope you can help me with a little question which came up during my studies today:

So given the Central Limit theorem, if I take 2 samples and calculate the sample means, could I predict the chance that two random sample means have a certain distance to each other?
For example I have a populations with mu=100 and a Var=36, and I take two samples two times. Could I calculate, if I do not know the original mu and var the chance of the two sample means for example taking the values 101 and 95?

My reasoning is that you could estimate the real variance out of the sample variances? Is pooling the variance correct in this case?

How precise would this be? Is it possible to calculate the chance of the mean of the two sample means to be the real mean out of this?

Thanks for your help!",2,statistics,54935,,Predicting how likely two sample means are by the central limit theorem?,https://www.reddit.com/r/statistics/comments/8hye46/predicting_how_likely_two_sample_means_are_by_the/,all_ads,2018-05-08 21:09:52,25 days 04:25:42.036584000,
"So I am currently going into my Senior year of undergrad and I don't necessarily have the best GPA (2.8), but I have a few solid internship positions from the past working with various data scientists.  I was wondering if this will greatly hinder my chances coming out of college and I realize that I need to bust my ass to do better.  The truth is I'm not the best at Statistics and I know a ton of other students who understand concepts a lot faster than I do, but I absolutely love this field.  I'm just conflicted as I'm starting to think about my future, but the reality is that I'm not sure I set myself up for the best success and it's beginning to become a little worrisome for me.",28,1525762042.0,8hrr04,False,"So I am currently going into my Senior year of undergrad and I don't necessarily have the best GPA (2.8), but I have a few solid internship positions from the past working with various data scientists.  I was wondering if this will greatly hinder my chances coming out of college and I realize that I need to bust my ass to do better.  The truth is I'm not the best at Statistics and I know a ton of other students who understand concepts a lot faster than I do, but I absolutely love this field.  I'm just conflicted as I'm starting to think about my future, but the reality is that I'm not sure I set myself up for the best success and it's beginning to become a little worrisome for me.",0,"So I am currently going into my Senior year of undergrad and I don't necessarily have the best GPA (2.8), but I have a few solid internship positions from the past working with various data scientists.  I was wondering if this will greatly hinder my chances coming out of college and I realize that I need to bust my ass to do better.  The truth is I'm not the best at Statistics and I know a ton of other students who understand concepts a lot faster than I do, but I absolutely love this field.  I'm just conflicted as I'm starting to think about my future, but the reality is that I'm not sure I set myself up for the best success and it's beginning to become a little worrisome for me.",28,statistics,54935,,Not the best GPA and worried about my future.,https://www.reddit.com/r/statistics/comments/8hrr04/not_the_best_gpa_and_worried_about_my_future/,all_ads,2018-05-08 02:47:22,25 days 22:48:12.036584000,
"My results are these:

*Pearson Chi-Square = 8,061; DF = 1; P-Value = 0,005
Likelihood Ratio Chi-Square = 12,524; DF = 1; P-Value = 0,000

* NOTE * 1 cells with expected counts less than 5


Fisher’s exact test: P-Value =  0,0024429*

How can I interpret them?

My study is trying to show that students who got scholarships in a specific high school have a higher high school graduation rates and also higher college enrollment rates than those who did not get the scholarship in the same school.

McNemar’s Test

 Estimated
Difference        95% CI           P
   -0,0889  (-0,2399; 0,0622)  0,280

FYI, this is not for homework. I am just trying to understand the theory from a class.",0,1525823689.0,8hxs3h,False,"My results are these:

*Pearson Chi-Square = 8,061; DF = 1; P-Value = 0,005
Likelihood Ratio Chi-Square = 12,524; DF = 1; P-Value = 0,000

* NOTE * 1 cells with expected counts less than 5


Fisher’s exact test: P-Value =  0,0024429*

How can I interpret them?

My study is trying to show that students who got scholarships in a specific high school have a higher high school graduation rates and also higher college enrollment rates than those who did not get the scholarship in the same school.

McNemar’s Test

 Estimated
Difference        95% CI           P
   -0,0889  (-0,2399; 0,0622)  0,280

FYI, this is not for homework. I am just trying to understand the theory from a class.",0,"My results are these:

*Pearson Chi-Square = 8,061; DF = 1; P-Value = 0,005
Likelihood Ratio Chi-Square = 12,524; DF = 1; P-Value = 0,000

* NOTE * 1 cells with expected counts less than 5


Fisher’s exact test: P-Value =  0,0024429*

How can I interpret them?

My study is trying to show that students who got scholarships in a specific high school have a higher high school graduation rates and also higher college enrollment rates than those who did not get the scholarship in the same school.

McNemar’s Test

 Estimated
Difference        95% CI           P
   -0,0889  (-0,2399; 0,0622)  0,280

FYI, this is not for homework. I am just trying to understand the theory from a class.",1,statistics,54935,,Need some help with Statistics,https://www.reddit.com/r/statistics/comments/8hxs3h/need_some_help_with_statistics/,all_ads,2018-05-08 19:54:49,25 days 05:40:45.036584000,
"I'm interested in the effects of shift type on the workers' stress. In my study, I'm following the stress during a three-week period during which the participants have varying patterns of morning, day, or evening shifts and days off. Estimating the effects of particular shift types on stress should be adjusted for individual variance in stress levels.

Dependent variable: Stress: a dichotomous variable (markedly high stress or not).

Independent variable: Shift type: categorical - morning, day, or evening. The groups are not independent because almost every worker has every type of shifts in the data, and stress levels probably show inter-individual variability.

I've 20 subjects. Not all of them work all kinds of shifts.

Research questions: is there a difference in workers' stress between shift types? Is the stress affected by individual factors, and to what extent? Is some particular shift type clearly more stressful than others?

I'm thinking shift type is a fixed factor and subject is random.

What kind of test would best suit this purpose?
I'm using SPSS and am getting to know R better.
",2,1525806768.0,8hvuvq,False,"I'm interested in the effects of shift type on the workers' stress. In my study, I'm following the stress during a three-week period during which the participants have varying patterns of morning, day, or evening shifts and days off. Estimating the effects of particular shift types on stress should be adjusted for individual variance in stress levels.

Dependent variable: Stress: a dichotomous variable (markedly high stress or not).

Independent variable: Shift type: categorical - morning, day, or evening. The groups are not independent because almost every worker has every type of shifts in the data, and stress levels probably show inter-individual variability.

I've 20 subjects. Not all of them work all kinds of shifts.

Research questions: is there a difference in workers' stress between shift types? Is the stress affected by individual factors, and to what extent? Is some particular shift type clearly more stressful than others?

I'm thinking shift type is a fixed factor and subject is random.

What kind of test would best suit this purpose?
I'm using SPSS and am getting to know R better.
",0,"I'm interested in the effects of shift type on the workers' stress. In my study, I'm following the stress during a three-week period during which the participants have varying patterns of morning, day, or evening shifts and days off. Estimating the effects of particular shift types on stress should be adjusted for individual variance in stress levels.

Dependent variable: Stress: a dichotomous variable (markedly high stress or not).

Independent variable: Shift type: categorical - morning, day, or evening. The groups are not independent because almost every worker has every type of shifts in the data, and stress levels probably show inter-individual variability.

I've 20 subjects. Not all of them work all kinds of shifts.

Research questions: is there a difference in workers' stress between shift types? Is the stress affected by individual factors, and to what extent? Is some particular shift type clearly more stressful than others?

I'm thinking shift type is a fixed factor and subject is random.

What kind of test would best suit this purpose?
I'm using SPSS and am getting to know R better.
",2,statistics,54935,,Stress levels in work shifts,https://www.reddit.com/r/statistics/comments/8hvuvq/stress_levels_in_work_shifts/,all_ads,2018-05-08 15:12:48,25 days 10:22:46.036584000,
"I just got done with ""Naked Statistics"" and it left me wanting more! 
The book did a great job covering central limit theorem and regression. I'm in search of a book that will dive into more advanced subjects. That said, I took cal and linear algebra over 10 years ago. I forgot most of it and gets lost in books that explain through math. I want a book like naked statistics that will explain important concepts on a high level .",3,1525775990.0,8htc5e,False,"I just got done with ""Naked Statistics"" and it left me wanting more! 
The book did a great job covering central limit theorem and regression. I'm in search of a book that will dive into more advanced subjects. That said, I took cal and linear algebra over 10 years ago. I forgot most of it and gets lost in books that explain through math. I want a book like naked statistics that will explain important concepts on a high level .",0,"I just got done with ""Naked Statistics"" and it left me wanting more! 
The book did a great job covering central limit theorem and regression. I'm in search of a book that will dive into more advanced subjects. That said, I took cal and linear algebra over 10 years ago. I forgot most of it and gets lost in books that explain through math. I want a book like naked statistics that will explain important concepts on a high level .",4,statistics,54935,,"Good Follow up book to ""Naked Statistics""?",https://www.reddit.com/r/statistics/comments/8htc5e/good_follow_up_book_to_naked_statistics/,all_ads,2018-05-08 06:39:50,25 days 18:55:44.036584000,
"Why do people even use SAS if we have R, stata, tableau, etc....it seems very ancient and inefficient ",20,1525772884.0,8hszo7,False,"Why do people even use SAS if we have R, stata, tableau, etc....it seems very ancient and inefficient ",0,"Why do people even use SAS if we have R, stata, tableau, etc....it seems very ancient and inefficient ",3,statistics,54935,,Why is SAS still used?,https://www.reddit.com/r/statistics/comments/8hszo7/why_is_sas_still_used/,all_ads,2018-05-08 05:48:04,25 days 19:47:30.036584000,
,16,1525743045.0,8hpadb,False,,0,,12,statistics,54935,,Why do we use n-1 instead of n for a sample's variance ?,https://www.reddit.com/r/statistics/comments/8hpadb/why_do_we_use_n1_instead_of_n_for_a_samples/,all_ads,2018-05-07 21:30:45,26 days 04:04:49.036584000,
"Suppose we have 100 components and 90 observations. I can understand why the 90th component would be compromised (afaik there wouldn't be any variance left to account for), but wouldn't components 1-89 still be sensible?",14,1525750745.0,8hqaqn,False,"Suppose we have 100 components and 90 observations. I can understand why the 90th component would be compromised (afaik there wouldn't be any variance left to account for), but wouldn't components 1-89 still be sensible?",0,"Suppose we have 100 components and 90 observations. I can understand why the 90th component would be compromised (afaik there wouldn't be any variance left to account for), but wouldn't components 1-89 still be sensible?",8,statistics,54935,,"When doing PCA on n dimensions with p < n observations, why are components less than p compromised?",https://www.reddit.com/r/statistics/comments/8hqaqn/when_doing_pca_on_n_dimensions_with_p_n/,all_ads,2018-05-07 23:39:05,26 days 01:56:29.036584000,
"I am doing the returns of education on earnings.  I have education (number of years of edu) as my endogenous variable and I am using Father Education as an instrument.  
I am using SAS and I ran a 2sls.  I used the first stage to test for endogeneity by grabbing the residuals and adding it to the original model (Hausman Test).  The coefficient on the first stage residual ended up not being significant.  Does this mean that I should just use OLS?  Are there other methods to test/identify endogeneity?",0,1525780725.0,8httzg,False,"I am doing the returns of education on earnings.  I have education (number of years of edu) as my endogenous variable and I am using Father Education as an instrument.  
I am using SAS and I ran a 2sls.  I used the first stage to test for endogeneity by grabbing the residuals and adding it to the original model (Hausman Test).  The coefficient on the first stage residual ended up not being significant.  Does this mean that I should just use OLS?  Are there other methods to test/identify endogeneity?",0,"I am doing the returns of education on earnings.  I have education (number of years of edu) as my endogenous variable and I am using Father Education as an instrument.  
I am using SAS and I ran a 2sls.  I used the first stage to test for endogeneity by grabbing the residuals and adding it to the original model (Hausman Test).  The coefficient on the first stage residual ended up not being significant.  Does this mean that I should just use OLS?  Are there other methods to test/identify endogeneity?",1,statistics,54935,,Questions about Endogeneity and IVs,https://www.reddit.com/r/statistics/comments/8httzg/questions_about_endogeneity_and_ivs/,all_ads,2018-05-08 07:58:45,25 days 17:36:49.036584000,
"Hi everyone, I'm taking my second stats course and we are using this textbook: 

 Stochastic Modeling and Mathematical Statistics, J.Samaniego   

Since I notice exams are much more different than exercises in the book, do you know where I can find other exercises with solutions?

Specifically, topics are:  
-. Moment generating functions  
-. Multivariate models
-. Transformation Theory  
-. Order Statistics  
-. Chebychev's Inequality, Convergence of Distribution function and CLT  
-. Statistical estimation (fixed sample theory and asymptotic theory)  
-. Interval estimation  
-. Bayesian estimation  
-. Hypothesis testing  
-. Linear regression  
-. Distribution theory in linear regression  
-. Measures of Goodness of Fit  

 Thank you all in advance :)",6,1525722654.0,8hmw6h,False,"Hi everyone, I'm taking my second stats course and we are using this textbook: 

 Stochastic Modeling and Mathematical Statistics, J.Samaniego   

Since I notice exams are much more different than exercises in the book, do you know where I can find other exercises with solutions?

Specifically, topics are:  
-. Moment generating functions  
-. Multivariate models
-. Transformation Theory  
-. Order Statistics  
-. Chebychev's Inequality, Convergence of Distribution function and CLT  
-. Statistical estimation (fixed sample theory and asymptotic theory)  
-. Interval estimation  
-. Bayesian estimation  
-. Hypothesis testing  
-. Linear regression  
-. Distribution theory in linear regression  
-. Measures of Goodness of Fit  

 Thank you all in advance :)",0,"Hi everyone, I'm taking my second stats course and we are using this textbook: 

 Stochastic Modeling and Mathematical Statistics, J.Samaniego   

Since I notice exams are much more different than exercises in the book, do you know where I can find other exercises with solutions?

Specifically, topics are:  
-. Moment generating functions  
-. Multivariate models
-. Transformation Theory  
-. Order Statistics  
-. Chebychev's Inequality, Convergence of Distribution function and CLT  
-. Statistical estimation (fixed sample theory and asymptotic theory)  
-. Interval estimation  
-. Bayesian estimation  
-. Hypothesis testing  
-. Linear regression  
-. Distribution theory in linear regression  
-. Measures of Goodness of Fit  

 Thank you all in advance :)",18,statistics,54935,,I need a suggestion on where to find exercises,https://www.reddit.com/r/statistics/comments/8hmw6h/i_need_a_suggestion_on_where_to_find_exercises/,all_ads,2018-05-07 15:50:54,26 days 09:44:40.036584000,
"So in my science class our overall grade is dependent on your percentile. Top 15% students get an A while the next 15% get A- and so on and so forth. 

Our grading is based on 4 exams which each equally contribute to our final grade in class 
 On test 1: I scored 90 while the average was 61 and a sd of 15, giving a z score of 1.93
Test 2: I scored 96 with an average of 75 and standard deviation of 15. z score of 1.4

Test 3 I got 73 and average was 62 and sd of 13.8 giving a z score of 0.797

Test 4 I got 70 average was 50 sd of 15 giving me a z score of 1.3 

Can I average my z scores (1.3+0.797+1.4+1.93)= to get 1.36 or would my overall z score for the class be different because z scores/ percentiles can’t be average?


Thank u!!",6,1525774937.0,8ht80h,False,"So in my science class our overall grade is dependent on your percentile. Top 15% students get an A while the next 15% get A- and so on and so forth. 

Our grading is based on 4 exams which each equally contribute to our final grade in class 
 On test 1: I scored 90 while the average was 61 and a sd of 15, giving a z score of 1.93
Test 2: I scored 96 with an average of 75 and standard deviation of 15. z score of 1.4

Test 3 I got 73 and average was 62 and sd of 13.8 giving a z score of 0.797

Test 4 I got 70 average was 50 sd of 15 giving me a z score of 1.3 

Can I average my z scores (1.3+0.797+1.4+1.93)= to get 1.36 or would my overall z score for the class be different because z scores/ percentiles can’t be average?


Thank u!!",0,"So in my science class our overall grade is dependent on your percentile. Top 15% students get an A while the next 15% get A- and so on and so forth. 

Our grading is based on 4 exams which each equally contribute to our final grade in class 
 On test 1: I scored 90 while the average was 61 and a sd of 15, giving a z score of 1.93
Test 2: I scored 96 with an average of 75 and standard deviation of 15. z score of 1.4

Test 3 I got 73 and average was 62 and sd of 13.8 giving a z score of 0.797

Test 4 I got 70 average was 50 sd of 15 giving me a z score of 1.3 

Can I average my z scores (1.3+0.797+1.4+1.93)= to get 1.36 or would my overall z score for the class be different because z scores/ percentiles can’t be average?


Thank u!!",0,statistics,54935,,Urgent!! Can you average percentiles/ z scores?,https://www.reddit.com/r/statistics/comments/8ht80h/urgent_can_you_average_percentiles_z_scores/,all_ads,2018-05-08 06:22:17,25 days 19:13:17.036584000,
"So I'm a stats grad student but I know absolutely zero, nada, zilch about time series analysis. For a side project, I'm trying to forecast flu diagnoses and admissions at a single hospital over the season and I have access to a whole bunch of time series of counts of flu diagnoses from the CDC (broken down every which way by state and region) and Australia. That said, I don't really know what approach to start with here. The hts package seems like the natural choice since the data have some hierarchical character, but I'll have to read up on it more. What about something ARIMA-flavored? Stupid sexy Gaussian processes? Is there anything out there I'm missing that would be a natural fit for this kind of problem?",6,1525706836.0,8hlpv5,False,"So I'm a stats grad student but I know absolutely zero, nada, zilch about time series analysis. For a side project, I'm trying to forecast flu diagnoses and admissions at a single hospital over the season and I have access to a whole bunch of time series of counts of flu diagnoses from the CDC (broken down every which way by state and region) and Australia. That said, I don't really know what approach to start with here. The hts package seems like the natural choice since the data have some hierarchical character, but I'll have to read up on it more. What about something ARIMA-flavored? Stupid sexy Gaussian processes? Is there anything out there I'm missing that would be a natural fit for this kind of problem?",0,"So I'm a stats grad student but I know absolutely zero, nada, zilch about time series analysis. For a side project, I'm trying to forecast flu diagnoses and admissions at a single hospital over the season and I have access to a whole bunch of time series of counts of flu diagnoses from the CDC (broken down every which way by state and region) and Australia. That said, I don't really know what approach to start with here. The hts package seems like the natural choice since the data have some hierarchical character, but I'll have to read up on it more. What about something ARIMA-flavored? Stupid sexy Gaussian processes? Is there anything out there I'm missing that would be a natural fit for this kind of problem?",10,statistics,54935,,Forecasting a single time series with multiple input time series?,https://www.reddit.com/r/statistics/comments/8hlpv5/forecasting_a_single_time_series_with_multiple/,all_ads,2018-05-07 11:27:16,26 days 14:08:18.036584000,
"Hello there!


I don't know if this is the right subreddit for asking such questions, but I need help with my probit analysis.

I'm doing a master in biology and I need to analyse some results from acute toxicity tests on Daphnia magna. At the lab where I wokred we used a program to calculate EC50 using probits and I seem to have lost the part of my data, where I had the EC50's and CI's for my tests (3 tests). Since I left the lab my mentor had a fallout with the lab leader and I can't get permission to get inside and redo the analyses one more time.

Would a soul here be so kind and help me calculate them or be able to put them into a probit analysis program? I'm not a biostatistician or have much to do with toxicology, so I'm really not that well versed on this subject. This is just part of my thesis, but I need it badly.
10 concentrations  + control. That is all that I need analysed.

Would anyone be willing to help me? 
",11,1525727325.0,8hncqy,False,"Hello there!


I don't know if this is the right subreddit for asking such questions, but I need help with my probit analysis.

I'm doing a master in biology and I need to analyse some results from acute toxicity tests on Daphnia magna. At the lab where I wokred we used a program to calculate EC50 using probits and I seem to have lost the part of my data, where I had the EC50's and CI's for my tests (3 tests). Since I left the lab my mentor had a fallout with the lab leader and I can't get permission to get inside and redo the analyses one more time.

Would a soul here be so kind and help me calculate them or be able to put them into a probit analysis program? I'm not a biostatistician or have much to do with toxicology, so I'm really not that well versed on this subject. This is just part of my thesis, but I need it badly.
10 concentrations  + control. That is all that I need analysed.

Would anyone be willing to help me? 
",0,"Hello there!


I don't know if this is the right subreddit for asking such questions, but I need help with my probit analysis.

I'm doing a master in biology and I need to analyse some results from acute toxicity tests on Daphnia magna. At the lab where I wokred we used a program to calculate EC50 using probits and I seem to have lost the part of my data, where I had the EC50's and CI's for my tests (3 tests). Since I left the lab my mentor had a fallout with the lab leader and I can't get permission to get inside and redo the analyses one more time.

Would a soul here be so kind and help me calculate them or be able to put them into a probit analysis program? I'm not a biostatistician or have much to do with toxicology, so I'm really not that well versed on this subject. This is just part of my thesis, but I need it badly.
10 concentrations  + control. That is all that I need analysed.

Would anyone be willing to help me? 
",4,statistics,54935,,Help with PROBIT analysis,https://www.reddit.com/r/statistics/comments/8hncqy/help_with_probit_analysis/,all_ads,2018-05-07 17:08:45,26 days 08:26:49.036584000,
"Dear r/statistics, 

I am currently writing my bachelor thesis on the correlation between social and economic indicators and the sustainability of a country. 

I am using the stats program R and I have already found the indicators I want to work with, like life expectancy, child labour, and literacy rates as social indicators and economic indicators like GNI, GDP per capita, and unemployment rates. 

I also have operationalised 'Sustainability' on indicators like PM2.5 (air pollution), the national, renewable electricity output and an index called EPI (Environmental Performance Index). 

All the data is longitudinal (from 1997-2017) and sourced mostly from the Worldbank data archive. The sample consists of 48 randomly selected countries. 

**My question is now about the methodology:**

How could I best analyse the data to see whether there is a relation between the social and economic indicators and the operationalised sustainability? 

So far my idea was using a Multiple Regression Analysis, though I'm not sure if there might be a better, more suitable way to do this. 

Thank you so much for your help! 

**Tl;dr:** I want to compare longitudinal data sets of various countries to find out whether there is a correlation. What methods would be best to use here? Thank you very much!",12,1525742942.0,8hp9we,False,"Dear r/statistics, 

I am currently writing my bachelor thesis on the correlation between social and economic indicators and the sustainability of a country. 

I am using the stats program R and I have already found the indicators I want to work with, like life expectancy, child labour, and literacy rates as social indicators and economic indicators like GNI, GDP per capita, and unemployment rates. 

I also have operationalised 'Sustainability' on indicators like PM2.5 (air pollution), the national, renewable electricity output and an index called EPI (Environmental Performance Index). 

All the data is longitudinal (from 1997-2017) and sourced mostly from the Worldbank data archive. The sample consists of 48 randomly selected countries. 

**My question is now about the methodology:**

How could I best analyse the data to see whether there is a relation between the social and economic indicators and the operationalised sustainability? 

So far my idea was using a Multiple Regression Analysis, though I'm not sure if there might be a better, more suitable way to do this. 

Thank you so much for your help! 

**Tl;dr:** I want to compare longitudinal data sets of various countries to find out whether there is a correlation. What methods would be best to use here? Thank you very much!",0,"Dear r/statistics, 

I am currently writing my bachelor thesis on the correlation between social and economic indicators and the sustainability of a country. 

I am using the stats program R and I have already found the indicators I want to work with, like life expectancy, child labour, and literacy rates as social indicators and economic indicators like GNI, GDP per capita, and unemployment rates. 

I also have operationalised 'Sustainability' on indicators like PM2.5 (air pollution), the national, renewable electricity output and an index called EPI (Environmental Performance Index). 

All the data is longitudinal (from 1997-2017) and sourced mostly from the Worldbank data archive. The sample consists of 48 randomly selected countries. 

**My question is now about the methodology:**

How could I best analyse the data to see whether there is a relation between the social and economic indicators and the operationalised sustainability? 

So far my idea was using a Multiple Regression Analysis, though I'm not sure if there might be a better, more suitable way to do this. 

Thank you so much for your help! 

**Tl;dr:** I want to compare longitudinal data sets of various countries to find out whether there is a correlation. What methods would be best to use here? Thank you very much!",1,statistics,54935,,Bachelor Thesis - Some Help with Statistics,https://www.reddit.com/r/statistics/comments/8hp9we/bachelor_thesis_some_help_with_statistics/,all_ads,2018-05-07 21:29:02,26 days 04:06:32.036584000,
"This is not a post about which is better, I just want to get more insight about them and how the industry is adopting each one ",40,1525681632.0,8hjjq8,False,"This is not a post about which is better, I just want to get more insight about them and how the industry is adopting each one ",0,"This is not a post about which is better, I just want to get more insight about them and how the industry is adopting each one ",20,statistics,54935,,"R, Python or Julia?",https://www.reddit.com/r/statistics/comments/8hjjq8/r_python_or_julia/,all_ads,2018-05-07 04:27:12,26 days 21:08:22.036584000,
"I have a study that was done with three groups. 

Control: Healthy patients. No intervention

Sick patients: Pre-treatment 

Sick patients: Post-treatment

I know if this was a comparison between the measured values in the sick patients alone, a paired t-test must be done to see if the outcome improves with treatment. However, I also want to examine if the sick pre-treatment group is indeed different from the healthy controls and if the sick post-treatment group is similar to the healthy controls. Would I simply perform a one-way ANOVA and then use any appropriate post-hoc analysis to detect specific between group differences? Thanks for any guidance.",7,1525734216.0,8ho5k4,False,"I have a study that was done with three groups. 

Control: Healthy patients. No intervention

Sick patients: Pre-treatment 

Sick patients: Post-treatment

I know if this was a comparison between the measured values in the sick patients alone, a paired t-test must be done to see if the outcome improves with treatment. However, I also want to examine if the sick pre-treatment group is indeed different from the healthy controls and if the sick post-treatment group is similar to the healthy controls. Would I simply perform a one-way ANOVA and then use any appropriate post-hoc analysis to detect specific between group differences? Thanks for any guidance.",0,"I have a study that was done with three groups. 

Control: Healthy patients. No intervention

Sick patients: Pre-treatment 

Sick patients: Post-treatment

I know if this was a comparison between the measured values in the sick patients alone, a paired t-test must be done to see if the outcome improves with treatment. However, I also want to examine if the sick pre-treatment group is indeed different from the healthy controls and if the sick post-treatment group is similar to the healthy controls. Would I simply perform a one-way ANOVA and then use any appropriate post-hoc analysis to detect specific between group differences? Thanks for any guidance.",1,statistics,54935,,Doing a paired t-test with a control group that does not undergo treatment?,https://www.reddit.com/r/statistics/comments/8ho5k4/doing_a_paired_ttest_with_a_control_group_that/,all_ads,2018-05-07 19:03:36,26 days 06:31:58.036584000,
"It shouldn't be based on machine learning or more complicated methods, as the amount of data nor the granularity of data support it.

I am predicting future crime based on current one. The predictions don't have to be super accurate, but they have to be based on something other than ""naive"" or ""greedy"" guesswork.

**Some details about the dataset:**  

Crimes have their dates of occurrence, latitude and longitude, type and location (can be used to determine economical significance). I also have weather forecast data for the year dataset is in and information on schools and funding which both can be used as influencing factors (I guess coefficients) when predicting data, but mainly I rely on position and the type of crime here.

**Outcome** of the prediction should of course be a crime with its location and type, or at least an area. As I said, doesn't have to be super accurate, but it does have to predict something.",7,1525720110.0,8hmo9y,False,"It shouldn't be based on machine learning or more complicated methods, as the amount of data nor the granularity of data support it.

I am predicting future crime based on current one. The predictions don't have to be super accurate, but they have to be based on something other than ""naive"" or ""greedy"" guesswork.

**Some details about the dataset:**  

Crimes have their dates of occurrence, latitude and longitude, type and location (can be used to determine economical significance). I also have weather forecast data for the year dataset is in and information on schools and funding which both can be used as influencing factors (I guess coefficients) when predicting data, but mainly I rely on position and the type of crime here.

**Outcome** of the prediction should of course be a crime with its location and type, or at least an area. As I said, doesn't have to be super accurate, but it does have to predict something.",0,"It shouldn't be based on machine learning or more complicated methods, as the amount of data nor the granularity of data support it.

I am predicting future crime based on current one. The predictions don't have to be super accurate, but they have to be based on something other than ""naive"" or ""greedy"" guesswork.

**Some details about the dataset:**  

Crimes have their dates of occurrence, latitude and longitude, type and location (can be used to determine economical significance). I also have weather forecast data for the year dataset is in and information on schools and funding which both can be used as influencing factors (I guess coefficients) when predicting data, but mainly I rely on position and the type of crime here.

**Outcome** of the prediction should of course be a crime with its location and type, or at least an area. As I said, doesn't have to be super accurate, but it does have to predict something.",3,statistics,54935,,"What are some ways to predict future data based on current one, relevant to my dataset (crime data)?",https://www.reddit.com/r/statistics/comments/8hmo9y/what_are_some_ways_to_predict_future_data_based/,all_ads,2018-05-07 15:08:30,26 days 10:27:04.036584000,
So I am an aspiring statistician and I am trying to learn a programming language this summer. I know a little SAS already but I am definitely not proficient. I have heard that Python is beginning to dominate SAS in stats so I was wondering if I should quit learning SAS and work on Python. ,22,1525679019.0,8hja9l,False,So I am an aspiring statistician and I am trying to learn a programming language this summer. I know a little SAS already but I am definitely not proficient. I have heard that Python is beginning to dominate SAS in stats so I was wondering if I should quit learning SAS and work on Python. ,0,So I am an aspiring statistician and I am trying to learn a programming language this summer. I know a little SAS already but I am definitely not proficient. I have heard that Python is beginning to dominate SAS in stats so I was wondering if I should quit learning SAS and work on Python. ,7,statistics,54935,,Python or SAS?,https://www.reddit.com/r/statistics/comments/8hja9l/python_or_sas/,all_ads,2018-05-07 03:43:39,26 days 21:51:55.036584000,
"Im doing a study on safety mechanisms and one of the research questions asks : has your job provided trining in workplace health and safety?

Responses include yes, no, unsure

Now i wanna see if specific job fields had higer yesses, which test do i run?",12,1525663179.0,8hhjfr,False,"Im doing a study on safety mechanisms and one of the research questions asks : has your job provided trining in workplace health and safety?

Responses include yes, no, unsure

Now i wanna see if specific job fields had higer yesses, which test do i run?",0,"Im doing a study on safety mechanisms and one of the research questions asks : has your job provided trining in workplace health and safety?

Responses include yes, no, unsure

Now i wanna see if specific job fields had higer yesses, which test do i run?",11,statistics,54935,,I need help selecting a test to run,https://www.reddit.com/r/statistics/comments/8hhjfr/i_need_help_selecting_a_test_to_run/,all_ads,2018-05-06 23:19:39,27 days 02:15:55.036584000,
"I would have assumed that heart rate was distributed as Poisson since it is a discrete count ratio relative to scale of measurement. Also, I would assume that active and resting heart rate are distributed differently. Any literature on the topic?",8,1525647923.0,8hfrb5,False,"I would have assumed that heart rate was distributed as Poisson since it is a discrete count ratio relative to scale of measurement. Also, I would assume that active and resting heart rate are distributed differently. Any literature on the topic?",0,"I would have assumed that heart rate was distributed as Poisson since it is a discrete count ratio relative to scale of measurement. Also, I would assume that active and resting heart rate are distributed differently. Any literature on the topic?",24,statistics,54935,,Do active and resting heart rate have known distributions?,https://www.reddit.com/r/statistics/comments/8hfrb5/do_active_and_resting_heart_rate_have_known/,all_ads,2018-05-06 19:05:23,27 days 06:30:11.036584000,
"I am quite new to Bayesian statistics and have come across a problem I don't know how to approach. I have a dataset comprising of reaction times, amongst other information, over time, taken hourly, with half the participants consuming a particular substance and the other not, i.e. the control. I'm not too sure how best to compare the reaction times between the two groups using a bayesian approach.

Two approaches I have considered are:
1. Simply comparing the Bayseian Factors for the data at each hour.
2. Using the Bayesian Estimation r library to compare the models at each hour

Would anyone be able to offer any opinions as to how I should go about this?",2,1525694918.0,8hkt0j,False,"I am quite new to Bayesian statistics and have come across a problem I don't know how to approach. I have a dataset comprising of reaction times, amongst other information, over time, taken hourly, with half the participants consuming a particular substance and the other not, i.e. the control. I'm not too sure how best to compare the reaction times between the two groups using a bayesian approach.

Two approaches I have considered are:
1. Simply comparing the Bayseian Factors for the data at each hour.
2. Using the Bayesian Estimation r library to compare the models at each hour

Would anyone be able to offer any opinions as to how I should go about this?",0,"I am quite new to Bayesian statistics and have come across a problem I don't know how to approach. I have a dataset comprising of reaction times, amongst other information, over time, taken hourly, with half the participants consuming a particular substance and the other not, i.e. the control. I'm not too sure how best to compare the reaction times between the two groups using a bayesian approach.

Two approaches I have considered are:
1. Simply comparing the Bayseian Factors for the data at each hour.
2. Using the Bayesian Estimation r library to compare the models at each hour

Would anyone be able to offer any opinions as to how I should go about this?",2,statistics,54935,,Bayesian approach to comparing means over time?,https://www.reddit.com/r/statistics/comments/8hkt0j/bayesian_approach_to_comparing_means_over_time/,all_ads,2018-05-07 08:08:38,26 days 17:26:56.036584000,
"I used ""compare means"" command to compare means and sd of my 6 categories. How can I find p value for those?",3,1525707193.0,8hlqt5,False,"I used ""compare means"" command to compare means and sd of my 6 categories. How can I find p value for those?",0,"I used ""compare means"" command to compare means and sd of my 6 categories. How can I find p value for those?",0,statistics,54935,,SPSS: Find p-value when comparing means and SD,https://www.reddit.com/r/statistics/comments/8hlqt5/spss_find_pvalue_when_comparing_means_and_sd/,all_ads,2018-05-07 11:33:13,26 days 14:02:21.036584000,
"Hello,

I posted the other day about crimes and communities data by Redmond—read about the variables here [here](http://archive.ics.uci.edu/ml/datasets/communities+and+crime+unnormalized) and get [the text file here](http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt). 

here is my code:

    install.packages(""leaps"")
    library(leaps)
    cacd <-read.csv(""/Users/thomasdevine/Desktop/cac.txt"",header=F, na.strings=""?"")
    colnames(cacd)=c(""communityname"",""state"",""countyCode"",""communityCode"",""fold"",""population"",""householdsize"",""racepctblack"",""racePctWhite"",""racePctAsian"",""racePctHisp"",""agePct12t21"",""agePct12t29"",""agePct16t24"",""agePct65up"",""numbUrban"",""pctUrban"",""medIncome"",""pctWWage"",""pctWFarmSelf"",""pctWInvInc"",""pctWSocSec"",""pctWPubAsst"",""pctWRetire"",""medFamInc"",""perCapInc"",""whitePerCap"",""blackPerCap"",""indianPerCap"",""AsianPerCap"",""OtherPerCap"",""HispPerCap"",""NumUnderPov"",""PctPopUnderPov"",""PctLess9thGrade"",""PctNotHSGrad"",""PctBSorMore"",""PctUnemployed"",""PctEmploy"",""PctEmplManu"",""PctEmplProfServ"",""PctOccupManu"",""PctOccupMgmtProf"",""MalePctDivorce"",""MalePctNevMarr"",""FemalePctDiv"",""TotalPctDiv"",""PersPerFam"",""PctFam2Par"",""PctKids2Par"",""PctYoungKids2Par"",""PctTeen2Par"",""PctWorkMomYoungKids"",""PctWorkMom"",""NumKidsBornNeverMar"",""PctKidsBornNeverMar"",""NumImmig"",""PctImmigRecent"",""PctImmigRec5"",""PctImmigRec8"",""PctImmigRec10"",""PctRecentImmig"",""PctRecImmig5"",""PctRecImmig8"",""PctRecImmig10"",""PctSpeakEnglOnly"",""PctNotSpeakEnglWell"",""PctLargHouseFam"",""PctLargHouseOccup"",""PersPerOccupHous"",""PersPerOwnOccHous"",""PersPerRentOccHous"",""PctPersOwnOccup"",""PctPersDenseHous"",""PctHousLess3BR"",""MedNumBR"",""HousVacant"",""PctHousOccup"",""PctHousOwnOcc"",""PctVacantBoarded"",""PctVacMore6Mos"",""MedYrHousBuilt"",""PctHousNoPhone"",""PctWOFullPlumb"",""OwnOccLowQuart"",""OwnOccMedVal"",""OwnOccHiQuart"",""OwnOccQrange"",""RentLowQ"",""RentMedian"",""RentHighQ"",""RentQrange"",""MedRent"",""MedRentPctHousInc"",""MedOwnCostPctInc"",""MedOwnCostPctIncNoMtg"",""NumInShelters"",""NumStreet"",""PctForeignBorn"",""PctBornSameState"",""PctSameHouse85"",""PctSameCity85"",""PctSameState85"",""LemasSwornFT"",""LemasSwFTPerPop"",""LemasSwFTFieldOps"",""LemasSwFTFieldPerPop"",""LemasTotalReq"",""LemasTotReqPerPop"",""PolicReqPerOffic"",""PolicPerPop"",""RacialMatchCommPol"",""PctPolicWhite"",""PctPolicBlack"",""PctPolicHisp"",""PctPolicAsian"",""PctPolicMinor"",""OfficAssgnDrugUnits"",""NumKindsDrugsSeiz"",""PolicAveOTWorked"",""LandArea"",""PopDens"",""PctUsePubTrans"",""PolicCars"",""PolicOperBudg"",""LemasPctPolicOnPatr"",""LemasGangUnitDeploy"",""LemasPctOfficDrugUn"",""PolicBudgPerPop"",""murders"",""murdPerPop"",""rapes"",""rapesPerPop"",""robberies"",""robbbPerPop"",""assaults"",""assaultPerPop"",""burglaries"",""burglPerPop"",""larcenies"",""larcPerPop"",""autoTheft"",""autoTheftPerPop"",""arsons"",""arsonsPerPop"",""ViolentCrimesPerPop"",""nonViolPerPop"")
    v=cacd$ViolentCrimesPerPop
    xa=cacd$PctKids2Par
    xb=cacd$PctFam2Par 
    xc= cacd$RentMedian 
    xd=(cacd$medIncome)^(-.235014)
    xe=cacd$PctKidsBornNeverMar 
    xf=(cacd$population)^(-.6166634)
    xg= cacd$racePctAsian 
    xh=cacd$PctEmploy 
    xi= cacd$medFamInc 
    xj= cacd$PctUnemployed 
    xk= cacd$pctWPubAsst 
    xl= cacd$racepctblack
    xm=cacd$racePctWhite 
    xn= cacd$PctPopUnderPov
    xo= cacd$PctVacMore6Mos
    xp= cacd$PctWOFullPlumb 
    xq=cacd$RentLowQ
    xr=cacd$OwnOccMedVal 
    xs=cacd$MedOwnCostPctInc 
    xt=cacd$PolicPerPop
    
    reg.subs<-regsubsets(v ~ xa + xb + xc + xd + xe + xf + xg + xh + xi + xj + xk + xl+ xm + xn + xo + xp + xq + xr + xs + xt,data=cacd,nbest=3,nvmax=10)

    sub8.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh)
    sub8.2<-lm(v~xa+xf+xm+xo+xi+xs+xq+xd)
    sub8.3<-lm(v~xa+xf+xm+xo+xi+xq+xe+xh)
    sub9.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xd+xh)
    sub9.2<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh+xe)
    sub9.3<-lm(v~xa+xf+xm+xo+xi+xs+xq+xj+xd)
    sub10.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh+xe+xd)
    sub10.2<-lm(v~xa+xf+xm+xo+xi+xt+xs+xq+xj+xd)
    sub10.3<-lm(v~xa+xf+xm+xo+xi+xs+xq+xj+xd+xn)
    AIC(sub8.1)
    AIC(sub8.2)
    AIC(sub8.3)
    AIC(sub9.1)
    AIC(sub9.2)
    AIC(sub9.3)
    AIC(sub10.1)
    AIC(sub10.2)
    AIC(sub10.3)
    BIC(sub8.1)
    BIC(sub8.2)
    BIC(sub8.3)
    BIC(sub9.1)
    BIC(sub9.2)
    BIC(sub9.3)
    BIC(sub10.1)
    BIC(sub10.2)
    BIC(sub10.3)
 Note how sub 10.2 is so much smaller in its BIC and AIC. Isn’t this the best model then? I’d argue for a low trade-off <.01 in adj.R^2 is nothing give how great our model improves. I guess this is disregarding the chicken and egg part of what I’m prediction (violent crime with these variables). Thoughts?",4,1525703657.0,8hlhph,False,"Hello,

I posted the other day about crimes and communities data by Redmond—read about the variables here [here](http://archive.ics.uci.edu/ml/datasets/communities+and+crime+unnormalized) and get [the text file here](http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt). 

here is my code:

    install.packages(""leaps"")
    library(leaps)
    cacd <-read.csv(""/Users/thomasdevine/Desktop/cac.txt"",header=F, na.strings=""?"")
    colnames(cacd)=c(""communityname"",""state"",""countyCode"",""communityCode"",""fold"",""population"",""householdsize"",""racepctblack"",""racePctWhite"",""racePctAsian"",""racePctHisp"",""agePct12t21"",""agePct12t29"",""agePct16t24"",""agePct65up"",""numbUrban"",""pctUrban"",""medIncome"",""pctWWage"",""pctWFarmSelf"",""pctWInvInc"",""pctWSocSec"",""pctWPubAsst"",""pctWRetire"",""medFamInc"",""perCapInc"",""whitePerCap"",""blackPerCap"",""indianPerCap"",""AsianPerCap"",""OtherPerCap"",""HispPerCap"",""NumUnderPov"",""PctPopUnderPov"",""PctLess9thGrade"",""PctNotHSGrad"",""PctBSorMore"",""PctUnemployed"",""PctEmploy"",""PctEmplManu"",""PctEmplProfServ"",""PctOccupManu"",""PctOccupMgmtProf"",""MalePctDivorce"",""MalePctNevMarr"",""FemalePctDiv"",""TotalPctDiv"",""PersPerFam"",""PctFam2Par"",""PctKids2Par"",""PctYoungKids2Par"",""PctTeen2Par"",""PctWorkMomYoungKids"",""PctWorkMom"",""NumKidsBornNeverMar"",""PctKidsBornNeverMar"",""NumImmig"",""PctImmigRecent"",""PctImmigRec5"",""PctImmigRec8"",""PctImmigRec10"",""PctRecentImmig"",""PctRecImmig5"",""PctRecImmig8"",""PctRecImmig10"",""PctSpeakEnglOnly"",""PctNotSpeakEnglWell"",""PctLargHouseFam"",""PctLargHouseOccup"",""PersPerOccupHous"",""PersPerOwnOccHous"",""PersPerRentOccHous"",""PctPersOwnOccup"",""PctPersDenseHous"",""PctHousLess3BR"",""MedNumBR"",""HousVacant"",""PctHousOccup"",""PctHousOwnOcc"",""PctVacantBoarded"",""PctVacMore6Mos"",""MedYrHousBuilt"",""PctHousNoPhone"",""PctWOFullPlumb"",""OwnOccLowQuart"",""OwnOccMedVal"",""OwnOccHiQuart"",""OwnOccQrange"",""RentLowQ"",""RentMedian"",""RentHighQ"",""RentQrange"",""MedRent"",""MedRentPctHousInc"",""MedOwnCostPctInc"",""MedOwnCostPctIncNoMtg"",""NumInShelters"",""NumStreet"",""PctForeignBorn"",""PctBornSameState"",""PctSameHouse85"",""PctSameCity85"",""PctSameState85"",""LemasSwornFT"",""LemasSwFTPerPop"",""LemasSwFTFieldOps"",""LemasSwFTFieldPerPop"",""LemasTotalReq"",""LemasTotReqPerPop"",""PolicReqPerOffic"",""PolicPerPop"",""RacialMatchCommPol"",""PctPolicWhite"",""PctPolicBlack"",""PctPolicHisp"",""PctPolicAsian"",""PctPolicMinor"",""OfficAssgnDrugUnits"",""NumKindsDrugsSeiz"",""PolicAveOTWorked"",""LandArea"",""PopDens"",""PctUsePubTrans"",""PolicCars"",""PolicOperBudg"",""LemasPctPolicOnPatr"",""LemasGangUnitDeploy"",""LemasPctOfficDrugUn"",""PolicBudgPerPop"",""murders"",""murdPerPop"",""rapes"",""rapesPerPop"",""robberies"",""robbbPerPop"",""assaults"",""assaultPerPop"",""burglaries"",""burglPerPop"",""larcenies"",""larcPerPop"",""autoTheft"",""autoTheftPerPop"",""arsons"",""arsonsPerPop"",""ViolentCrimesPerPop"",""nonViolPerPop"")
    v=cacd$ViolentCrimesPerPop
    xa=cacd$PctKids2Par
    xb=cacd$PctFam2Par 
    xc= cacd$RentMedian 
    xd=(cacd$medIncome)^(-.235014)
    xe=cacd$PctKidsBornNeverMar 
    xf=(cacd$population)^(-.6166634)
    xg= cacd$racePctAsian 
    xh=cacd$PctEmploy 
    xi= cacd$medFamInc 
    xj= cacd$PctUnemployed 
    xk= cacd$pctWPubAsst 
    xl= cacd$racepctblack
    xm=cacd$racePctWhite 
    xn= cacd$PctPopUnderPov
    xo= cacd$PctVacMore6Mos
    xp= cacd$PctWOFullPlumb 
    xq=cacd$RentLowQ
    xr=cacd$OwnOccMedVal 
    xs=cacd$MedOwnCostPctInc 
    xt=cacd$PolicPerPop
    
    reg.subs<-regsubsets(v ~ xa + xb + xc + xd + xe + xf + xg + xh + xi + xj + xk + xl+ xm + xn + xo + xp + xq + xr + xs + xt,data=cacd,nbest=3,nvmax=10)

    sub8.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh)
    sub8.2<-lm(v~xa+xf+xm+xo+xi+xs+xq+xd)
    sub8.3<-lm(v~xa+xf+xm+xo+xi+xq+xe+xh)
    sub9.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xd+xh)
    sub9.2<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh+xe)
    sub9.3<-lm(v~xa+xf+xm+xo+xi+xs+xq+xj+xd)
    sub10.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh+xe+xd)
    sub10.2<-lm(v~xa+xf+xm+xo+xi+xt+xs+xq+xj+xd)
    sub10.3<-lm(v~xa+xf+xm+xo+xi+xs+xq+xj+xd+xn)
    AIC(sub8.1)
    AIC(sub8.2)
    AIC(sub8.3)
    AIC(sub9.1)
    AIC(sub9.2)
    AIC(sub9.3)
    AIC(sub10.1)
    AIC(sub10.2)
    AIC(sub10.3)
    BIC(sub8.1)
    BIC(sub8.2)
    BIC(sub8.3)
    BIC(sub9.1)
    BIC(sub9.2)
    BIC(sub9.3)
    BIC(sub10.1)
    BIC(sub10.2)
    BIC(sub10.3)
 Note how sub 10.2 is so much smaller in its BIC and AIC. Isn’t this the best model then? I’d argue for a low trade-off <.01 in adj.R^2 is nothing give how great our model improves. I guess this is disregarding the chicken and egg part of what I’m prediction (violent crime with these variables). Thoughts?",0,"Hello,

I posted the other day about crimes and communities data by Redmond—read about the variables here [here](http://archive.ics.uci.edu/ml/datasets/communities+and+crime+unnormalized) and get [the text file here](http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt). 

here is my code:

    install.packages(""leaps"")
    library(leaps)
    cacd <-read.csv(""/Users/thomasdevine/Desktop/cac.txt"",header=F, na.strings=""?"")
    colnames(cacd)=c(""communityname"",""state"",""countyCode"",""communityCode"",""fold"",""population"",""householdsize"",""racepctblack"",""racePctWhite"",""racePctAsian"",""racePctHisp"",""agePct12t21"",""agePct12t29"",""agePct16t24"",""agePct65up"",""numbUrban"",""pctUrban"",""medIncome"",""pctWWage"",""pctWFarmSelf"",""pctWInvInc"",""pctWSocSec"",""pctWPubAsst"",""pctWRetire"",""medFamInc"",""perCapInc"",""whitePerCap"",""blackPerCap"",""indianPerCap"",""AsianPerCap"",""OtherPerCap"",""HispPerCap"",""NumUnderPov"",""PctPopUnderPov"",""PctLess9thGrade"",""PctNotHSGrad"",""PctBSorMore"",""PctUnemployed"",""PctEmploy"",""PctEmplManu"",""PctEmplProfServ"",""PctOccupManu"",""PctOccupMgmtProf"",""MalePctDivorce"",""MalePctNevMarr"",""FemalePctDiv"",""TotalPctDiv"",""PersPerFam"",""PctFam2Par"",""PctKids2Par"",""PctYoungKids2Par"",""PctTeen2Par"",""PctWorkMomYoungKids"",""PctWorkMom"",""NumKidsBornNeverMar"",""PctKidsBornNeverMar"",""NumImmig"",""PctImmigRecent"",""PctImmigRec5"",""PctImmigRec8"",""PctImmigRec10"",""PctRecentImmig"",""PctRecImmig5"",""PctRecImmig8"",""PctRecImmig10"",""PctSpeakEnglOnly"",""PctNotSpeakEnglWell"",""PctLargHouseFam"",""PctLargHouseOccup"",""PersPerOccupHous"",""PersPerOwnOccHous"",""PersPerRentOccHous"",""PctPersOwnOccup"",""PctPersDenseHous"",""PctHousLess3BR"",""MedNumBR"",""HousVacant"",""PctHousOccup"",""PctHousOwnOcc"",""PctVacantBoarded"",""PctVacMore6Mos"",""MedYrHousBuilt"",""PctHousNoPhone"",""PctWOFullPlumb"",""OwnOccLowQuart"",""OwnOccMedVal"",""OwnOccHiQuart"",""OwnOccQrange"",""RentLowQ"",""RentMedian"",""RentHighQ"",""RentQrange"",""MedRent"",""MedRentPctHousInc"",""MedOwnCostPctInc"",""MedOwnCostPctIncNoMtg"",""NumInShelters"",""NumStreet"",""PctForeignBorn"",""PctBornSameState"",""PctSameHouse85"",""PctSameCity85"",""PctSameState85"",""LemasSwornFT"",""LemasSwFTPerPop"",""LemasSwFTFieldOps"",""LemasSwFTFieldPerPop"",""LemasTotalReq"",""LemasTotReqPerPop"",""PolicReqPerOffic"",""PolicPerPop"",""RacialMatchCommPol"",""PctPolicWhite"",""PctPolicBlack"",""PctPolicHisp"",""PctPolicAsian"",""PctPolicMinor"",""OfficAssgnDrugUnits"",""NumKindsDrugsSeiz"",""PolicAveOTWorked"",""LandArea"",""PopDens"",""PctUsePubTrans"",""PolicCars"",""PolicOperBudg"",""LemasPctPolicOnPatr"",""LemasGangUnitDeploy"",""LemasPctOfficDrugUn"",""PolicBudgPerPop"",""murders"",""murdPerPop"",""rapes"",""rapesPerPop"",""robberies"",""robbbPerPop"",""assaults"",""assaultPerPop"",""burglaries"",""burglPerPop"",""larcenies"",""larcPerPop"",""autoTheft"",""autoTheftPerPop"",""arsons"",""arsonsPerPop"",""ViolentCrimesPerPop"",""nonViolPerPop"")
    v=cacd$ViolentCrimesPerPop
    xa=cacd$PctKids2Par
    xb=cacd$PctFam2Par 
    xc= cacd$RentMedian 
    xd=(cacd$medIncome)^(-.235014)
    xe=cacd$PctKidsBornNeverMar 
    xf=(cacd$population)^(-.6166634)
    xg= cacd$racePctAsian 
    xh=cacd$PctEmploy 
    xi= cacd$medFamInc 
    xj= cacd$PctUnemployed 
    xk= cacd$pctWPubAsst 
    xl= cacd$racepctblack
    xm=cacd$racePctWhite 
    xn= cacd$PctPopUnderPov
    xo= cacd$PctVacMore6Mos
    xp= cacd$PctWOFullPlumb 
    xq=cacd$RentLowQ
    xr=cacd$OwnOccMedVal 
    xs=cacd$MedOwnCostPctInc 
    xt=cacd$PolicPerPop
    
    reg.subs<-regsubsets(v ~ xa + xb + xc + xd + xe + xf + xg + xh + xi + xj + xk + xl+ xm + xn + xo + xp + xq + xr + xs + xt,data=cacd,nbest=3,nvmax=10)

    sub8.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh)
    sub8.2<-lm(v~xa+xf+xm+xo+xi+xs+xq+xd)
    sub8.3<-lm(v~xa+xf+xm+xo+xi+xq+xe+xh)
    sub9.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xd+xh)
    sub9.2<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh+xe)
    sub9.3<-lm(v~xa+xf+xm+xo+xi+xs+xq+xj+xd)
    sub10.1<-lm(v~xa+xf+xm+xo+xi+xs+xq+xh+xe+xd)
    sub10.2<-lm(v~xa+xf+xm+xo+xi+xt+xs+xq+xj+xd)
    sub10.3<-lm(v~xa+xf+xm+xo+xi+xs+xq+xj+xd+xn)
    AIC(sub8.1)
    AIC(sub8.2)
    AIC(sub8.3)
    AIC(sub9.1)
    AIC(sub9.2)
    AIC(sub9.3)
    AIC(sub10.1)
    AIC(sub10.2)
    AIC(sub10.3)
    BIC(sub8.1)
    BIC(sub8.2)
    BIC(sub8.3)
    BIC(sub9.1)
    BIC(sub9.2)
    BIC(sub9.3)
    BIC(sub10.1)
    BIC(sub10.2)
    BIC(sub10.3)
 Note how sub 10.2 is so much smaller in its BIC and AIC. Isn’t this the best model then? I’d argue for a low trade-off <.01 in adj.R^2 is nothing give how great our model improves. I guess this is disregarding the chicken and egg part of what I’m prediction (violent crime with these variables). Thoughts?",1,statistics,54935,,"regsubsets gives me a model that I don't agree with being ""better"" (because of AIC & BIC)",https://www.reddit.com/r/statistics/comments/8hlhph/regsubsets_gives_me_a_model_that_i_dont_agree/,all_ads,2018-05-07 10:34:17,26 days 15:01:17.036584000,
"For my AP Final Project we have to formulate and conduct a statistical question, and design the study to answer the question. Is ""Are Males more attracted to a “Mystery booth,” than Females are? In which, we have a booth with only the label ""Mystery Booth"", and we would take data based off of who comes over based on gender. What are some possible explanatory, and response variables that could occur? 
",2,1525696487.0,8hkxtr,False,"For my AP Final Project we have to formulate and conduct a statistical question, and design the study to answer the question. Is ""Are Males more attracted to a “Mystery booth,” than Females are? In which, we have a booth with only the label ""Mystery Booth"", and we would take data based off of who comes over based on gender. What are some possible explanatory, and response variables that could occur? 
",0,"For my AP Final Project we have to formulate and conduct a statistical question, and design the study to answer the question. Is ""Are Males more attracted to a “Mystery booth,” than Females are? In which, we have a booth with only the label ""Mystery Booth"", and we would take data based off of who comes over based on gender. What are some possible explanatory, and response variables that could occur? 
",1,statistics,54935,,Need Help For a Stats Project,https://www.reddit.com/r/statistics/comments/8hkxtr/need_help_for_a_stats_project/,all_ads,2018-05-07 08:34:47,26 days 17:00:47.036584000,
"When working a regression including years (i.e: 2000, 2001, 2002...2007) should the actual year be used or should it be 0,1,2....7? ",6,1525670718.0,8hieow,False,"When working a regression including years (i.e: 2000, 2001, 2002...2007) should the actual year be used or should it be 0,1,2....7? ",0,"When working a regression including years (i.e: 2000, 2001, 2002...2007) should the actual year be used or should it be 0,1,2....7? ",4,statistics,54935,,Year or number?,https://www.reddit.com/r/statistics/comments/8hieow/year_or_number/,all_ads,2018-05-07 01:25:18,27 days 00:10:16.036584000,
Should you test for it in every analysis or only when you suspect it might be present?,4,1525662236.0,8hhfeu,False,Should you test for it in every analysis or only when you suspect it might be present?,0,Should you test for it in every analysis or only when you suspect it might be present?,2,statistics,54935,,When should you test for multicollinearity?,https://www.reddit.com/r/statistics/comments/8hhfeu/when_should_you_test_for_multicollinearity/,all_ads,2018-05-06 23:03:56,27 days 02:31:38.036584000,
"I have two values that are being combined in one equation: Days Between Transactions (d) and Transaction Value (v). The equation is simply v/d to determine average daily transaction amount. 

The problem is I need to have a confidence interval for this calculated v/d value. I found lots of documentation about doing this with subsets that are part of the same population, but not so much with unrelated populations. I've tried taking the bounds of the individual confidence intervals and calculating those out: 
||| v lower bound/d upper bound ||| v upper bound/d lower bound |||
But I'm not sure if this is the correct way to combine these. Bear in mind, I'm not doing upper bound/upper bound or lower bound/lower bound because the combined lower bound should probably be the lowest trans. value divided by the largest distance between a transaction, at least that was my thinking. 

Any help would be appreciated?",5,1525647616.0,8hfq20,False,"I have two values that are being combined in one equation: Days Between Transactions (d) and Transaction Value (v). The equation is simply v/d to determine average daily transaction amount. 

The problem is I need to have a confidence interval for this calculated v/d value. I found lots of documentation about doing this with subsets that are part of the same population, but not so much with unrelated populations. I've tried taking the bounds of the individual confidence intervals and calculating those out: 
||| v lower bound/d upper bound ||| v upper bound/d lower bound |||
But I'm not sure if this is the correct way to combine these. Bear in mind, I'm not doing upper bound/upper bound or lower bound/lower bound because the combined lower bound should probably be the lowest trans. value divided by the largest distance between a transaction, at least that was my thinking. 

Any help would be appreciated?",0,"I have two values that are being combined in one equation: Days Between Transactions (d) and Transaction Value (v). The equation is simply v/d to determine average daily transaction amount. 

The problem is I need to have a confidence interval for this calculated v/d value. I found lots of documentation about doing this with subsets that are part of the same population, but not so much with unrelated populations. I've tried taking the bounds of the individual confidence intervals and calculating those out: 
||| v lower bound/d upper bound ||| v upper bound/d lower bound |||
But I'm not sure if this is the correct way to combine these. Bear in mind, I'm not doing upper bound/upper bound or lower bound/lower bound because the combined lower bound should probably be the lowest trans. value divided by the largest distance between a transaction, at least that was my thinking. 

Any help would be appreciated?",4,statistics,54935,,Sanity Check - Combining SDs/Confidence Intervals,https://www.reddit.com/r/statistics/comments/8hfq20/sanity_check_combining_sdsconfidence_intervals/,all_ads,2018-05-06 19:00:16,27 days 06:35:18.036584000,
"Relative novice, be warned!

I’m investigating a within-subjects main effect of let’s say, “M”.

M has 5 levels (M1-M5). I’m doing paired samples t-tests to clarify my main effect.

M1-M2
M1-M3
M1-M4
M1-M5
M2-M3
M2-M4
M2-M5
M3-M4
M3-M5
M4-M5

My supervisors says that I probably need to correct for Bonferroni... does anyone know how to do this specifically in SPSS, and for paired-samples t-tests??

Thank you!
",10,1525656512.0,8hgr0u,False,"Relative novice, be warned!

I’m investigating a within-subjects main effect of let’s say, “M”.

M has 5 levels (M1-M5). I’m doing paired samples t-tests to clarify my main effect.

M1-M2
M1-M3
M1-M4
M1-M5
M2-M3
M2-M4
M2-M5
M3-M4
M3-M5
M4-M5

My supervisors says that I probably need to correct for Bonferroni... does anyone know how to do this specifically in SPSS, and for paired-samples t-tests??

Thank you!
",0,"Relative novice, be warned!

I’m investigating a within-subjects main effect of let’s say, “M”.

M has 5 levels (M1-M5). I’m doing paired samples t-tests to clarify my main effect.

M1-M2
M1-M3
M1-M4
M1-M5
M2-M3
M2-M4
M2-M5
M3-M4
M3-M5
M4-M5

My supervisors says that I probably need to correct for Bonferroni... does anyone know how to do this specifically in SPSS, and for paired-samples t-tests??

Thank you!
",1,statistics,54935,,Bonferroni corrections SPSS - pls help!,https://www.reddit.com/r/statistics/comments/8hgr0u/bonferroni_corrections_spss_pls_help/,all_ads,2018-05-06 21:28:32,27 days 04:07:02.036584000,
"I was looking into applications of the random forest classifier, and stumbled upon this post:

https://partyondata.com/2011/09/21/betting-on-ufc-fights-a-statistical-data-analysis/

What’s really interesting was that the analyst was able to extract the individual strong classifiers out of the random forest and make individual insights using the predictors. I’ve not written such a model from scratch but using statistical packages (e.g. Scikit-Learn) the classifier doesn’t output insights such as these showing how the various predictors interact. Does anyone know how that can be done? 

E.g. Given a binary classification problem, train a random forest classifier using a dataset with 500 attributes; Attribute 25 and 145 were identified as a particularly strong classifier with x% error",9,1525598566.0,8hbwuh,False,"I was looking into applications of the random forest classifier, and stumbled upon this post:

https://partyondata.com/2011/09/21/betting-on-ufc-fights-a-statistical-data-analysis/

What’s really interesting was that the analyst was able to extract the individual strong classifiers out of the random forest and make individual insights using the predictors. I’ve not written such a model from scratch but using statistical packages (e.g. Scikit-Learn) the classifier doesn’t output insights such as these showing how the various predictors interact. Does anyone know how that can be done? 

E.g. Given a binary classification problem, train a random forest classifier using a dataset with 500 attributes; Attribute 25 and 145 were identified as a particularly strong classifier with x% error",0,"I was looking into applications of the random forest classifier, and stumbled upon this post:

https://partyondata.com/2011/09/21/betting-on-ufc-fights-a-statistical-data-analysis/

What’s really interesting was that the analyst was able to extract the individual strong classifiers out of the random forest and make individual insights using the predictors. I’ve not written such a model from scratch but using statistical packages (e.g. Scikit-Learn) the classifier doesn’t output insights such as these showing how the various predictors interact. Does anyone know how that can be done? 

E.g. Given a binary classification problem, train a random forest classifier using a dataset with 500 attributes; Attribute 25 and 145 were identified as a particularly strong classifier with x% error",18,statistics,54935,,How can we select the strongest decision tree classifiers in a random forest?,https://www.reddit.com/r/statistics/comments/8hbwuh/how_can_we_select_the_strongest_decision_tree/,all_ads,2018-05-06 05:22:46,27 days 20:12:48.036584000,
"Figured it would be good for college to have one that can run code for 5GB of data, any suggestions? Or suggestions for specifications ?",5,1525663357.0,8hhk7g,False,"Figured it would be good for college to have one that can run code for 5GB of data, any suggestions? Or suggestions for specifications ?",0,"Figured it would be good for college to have one that can run code for 5GB of data, any suggestions? Or suggestions for specifications ?",0,statistics,54935,,Looking for a laptop that’ll handle complex datasets,https://www.reddit.com/r/statistics/comments/8hhk7g/looking_for_a_laptop_thatll_handle_complex/,all_ads,2018-05-06 23:22:37,27 days 02:12:57.036584000,
I have one variance of ethnicity listed and one variance of disease prevalence. So i need to calculate the correlance between those two while adjusting things like BMI etc..,6,1525636368.0,8heq21,False,I have one variance of ethnicity listed and one variance of disease prevalence. So i need to calculate the correlance between those two while adjusting things like BMI etc..,0,I have one variance of ethnicity listed and one variance of disease prevalence. So i need to calculate the correlance between those two while adjusting things like BMI etc..,1,statistics,54935,,SPSS: How to find correlance of two variables while adjusting with multiple variances?,https://www.reddit.com/r/statistics/comments/8heq21/spss_how_to_find_correlance_of_two_variables/,all_ads,2018-05-06 15:52:48,27 days 09:42:46.036584000,
"Long story short, my job hunt is not going well. Can't commit to doing apps 24/7 due to my M.S. program being a major time waster. I am looking for something over the summer to pay the bills and maybe after graduation as well (I'm finishing in December). What kind of jobs should I look for that I could get hired in within 1-2 months?",6,1525576365.0,8h9o90,False,"Long story short, my job hunt is not going well. Can't commit to doing apps 24/7 due to my M.S. program being a major time waster. I am looking for something over the summer to pay the bills and maybe after graduation as well (I'm finishing in December). What kind of jobs should I look for that I could get hired in within 1-2 months?",0,"Long story short, my job hunt is not going well. Can't commit to doing apps 24/7 due to my M.S. program being a major time waster. I am looking for something over the summer to pay the bills and maybe after graduation as well (I'm finishing in December). What kind of jobs should I look for that I could get hired in within 1-2 months?",17,statistics,54935,,How to find short-term statistics-related work?,https://www.reddit.com/r/statistics/comments/8h9o90/how_to_find_shortterm_statisticsrelated_work/,all_ads,2018-05-05 23:12:45,28 days 02:22:49.036584000,
"Pretty much the title. Just curious if anyone knows \(maybe teachers/professors out there?\) of any textbooks \(Elementary Statistics\) that use the TI\-nspire CX as their calculator of choice. I usually just see the TI\-83, TI\-84, and TI\-89.",7,1525641079.0,8hf3bc,False,"Pretty much the title. Just curious if anyone knows \(maybe teachers/professors out there?\) of any textbooks \(Elementary Statistics\) that use the TI\-nspire CX as their calculator of choice. I usually just see the TI\-83, TI\-84, and TI\-89.",0,"Pretty much the title. Just curious if anyone knows \(maybe teachers/professors out there?\) of any textbooks \(Elementary Statistics\) that use the TI\-nspire CX as their calculator of choice. I usually just see the TI\-83, TI\-84, and TI\-89.",0,statistics,54935,,"Any college textbooks that use the TI-nspire CX for their ""technology"" walkthroughs?",https://www.reddit.com/r/statistics/comments/8hf3bc/any_college_textbooks_that_use_the_tinspire_cx/,all_ads,2018-05-06 17:11:19,27 days 08:24:15.036584000,
"Last night I had a #showerthought: Does it really matter when a movie is released? When I woke up this morning, the question was still on my mind, so I decided to mess around and see what I could find.

**THE CONCEPT**

I wanted to look at a couple different ways to portray release date and see how they affected the success of the movie. To do this, I first had to define a measurement of movie success. Seeing as it is the producing company who decides the release date of a movie, I decided to use the most important metric to them: $$$. However, it didn't make sense to use simply the revenue gained by the movie, because raw revenue numbers have an innate bias towards bigger movies with bigger budgets. For example, a movie with a production budget of $200 million that grosses $700 million would be considered extremely successful. However, so would a movie with a production budget of $20 million that grosses $70 million. Raw revenue number would show the second movie as being equally successful as a movie with a production budget of $200 million the grosses $250 million, which makes no sense. As such, I decided that my metric for success should be total revenue divided by production budget, thus scaling all of the movies to an equal scale.

The three portrayals of release date that I chose were week of the year, month of the year, and day of the week. 

**THE DATA**

At first, I used data scraped from [boxofficemojo.com](http://www.boxofficemojo.com), but then I realized that it only included data for movies that had grossed over $200 million. Seeing as that excludes a lot of the less successful movies (an important exclusion, since one of my variables is supposed to measure success), I had to switch. I ended up using data from the [TMDB 5,000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata/data), which is data in csv format regarding 5,000 movies. It included all of the values I needed, which were title, revenue, budget, and release date.

**THE DATA WRANGLING**

First, I calculated the necessary values. Revenue weighted by budget was easy enough, and I called it percent return. With my original data this name made a lot more sense. However, only as I typed this up did I realize that it is nonsensical. I'm too lazy to go back and fix it, so it will stay that way.

R deals with dates really well, so in order to display them on a graph, all I had to do was make a new column equal to the release date minus January 1 of the release year. I called this column, logically, days_since_jan_1.

To get week of the year, I subset the rows with day values in the correct range and set the correct week number. I applied this to all 52 weeks using a for loop. The 365th and 366th day (leap years) were thrown in with the 52nd week.

Month of the year was easy, I just pulled a substring out of the date value.

For day of the week, I converted the release date to a POSIXlt object and used $wday to record the numerical day of the week. Sunday is 0, Monday is 1, Tuesday is 2, etc.

**THE DISPLAY**

Now that I had the necessary values, I displayed them in a variety of formats. First, I created data frames of weekly, monthly, and daily medians and displayed them in scatterplots. These were the results: [Weekly](https://imgur.com/vrLmT1g), [monthly](https://imgur.com/2A3xIFT), and [daily](https://imgur.com/jOOiB0b). Remember, the days of the week are 0=Sunday, 1=Monday, 2=Tuesday, etc.

However, since these graphs are automatically scaled, any small difference can look significant. For some context, I graphed them as box plots, too. [Weekly](https://imgur.com/3nRXjlQ), [monthly](https://imgur.com/LvAw6nD), and [daily](https://imgur.com/uv6J9Lw).

**STATISTICAL SIGNIFICANCE**

There's no real way to know if all of this work means anything until statistical significance tests are applied. I chose to use a one-factor T Test to compare the mean of each week/month/day to the population mean.

*Monthly Results*

I'm going to start broad and work my way in. With an alpha of 0.05, the months with a statistically significant difference from the population mean were:

* January (lower)
* March (lower)
* September (lower)
* October (lower)
* June (higher)
* November (higher)

When the alpha was lowered to 0.01, the months with a statistically significant difference from the population mean were:

* September (lower)
* June (higher)

*Conclusion:* Movies are most profitable when released in the early summer and towards the end of the year, and least profitable when released at the beginning of the year and at the beginning of the school year.

*Weekly Results*

With an alpha of 0.05, the weeks with a statistically significant difference from the population mean were:

* January 1-7 (lower)
* February 19-25 (lower)
* March 25-31 (lower)
* April 1-7 (lower)
* August 19-25 (lower)
* August 26-September 1 (lower)
* September 9-15 (lower)
* October 7-13 (lower)
* May 20-26 (higher)
* June 17-23 (higher)
* June 24-30 (higher)
* November 4-10 (higher)
* November 11-17 (higher)
* December 9-15 (higher)

When the alpha was lowered to 0.01, the weeks with a statistically significant difference from the population mean were:

* August 19-25 (lower)
* August 26-September 1 (lower)
* September 9-15 (lower)
* June 24-30 (higher)
* December 9-15 (higher)

*Conclusion:* Movies are most profitable when released between late May and late June and between November and early December and least profitable when released in early January, between mid February and early April, and between mid August and mid September.

*Daily Results*

With an alpha of 0.05, the days of the week with a statistically significant difference from the population mean were:

* Friday (lower)
* Saturday (lower)
* Wednesday (higher)
* Thursday (higher)

When the alpha was lowered to 0.01, the weeks with a statistically significant difference from the population mean were:

* Friday (lower)
* Saturday (lower)
* Wednesday (higher)

*Conclusion:* Movies are most profitable when released on Wednesdays and least profitable when released on Fridays and Saturdays.

**APPLICATION**

The days this year that are the best candidates for release dates are Wednesday, June 27 and Wednesday, December 12.",0,1525606135.0,8hcm2o,False,"Last night I had a #showerthought: Does it really matter when a movie is released? When I woke up this morning, the question was still on my mind, so I decided to mess around and see what I could find.

**THE CONCEPT**

I wanted to look at a couple different ways to portray release date and see how they affected the success of the movie. To do this, I first had to define a measurement of movie success. Seeing as it is the producing company who decides the release date of a movie, I decided to use the most important metric to them: $$$. However, it didn't make sense to use simply the revenue gained by the movie, because raw revenue numbers have an innate bias towards bigger movies with bigger budgets. For example, a movie with a production budget of $200 million that grosses $700 million would be considered extremely successful. However, so would a movie with a production budget of $20 million that grosses $70 million. Raw revenue number would show the second movie as being equally successful as a movie with a production budget of $200 million the grosses $250 million, which makes no sense. As such, I decided that my metric for success should be total revenue divided by production budget, thus scaling all of the movies to an equal scale.

The three portrayals of release date that I chose were week of the year, month of the year, and day of the week. 

**THE DATA**

At first, I used data scraped from [boxofficemojo.com](http://www.boxofficemojo.com), but then I realized that it only included data for movies that had grossed over $200 million. Seeing as that excludes a lot of the less successful movies (an important exclusion, since one of my variables is supposed to measure success), I had to switch. I ended up using data from the [TMDB 5,000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata/data), which is data in csv format regarding 5,000 movies. It included all of the values I needed, which were title, revenue, budget, and release date.

**THE DATA WRANGLING**

First, I calculated the necessary values. Revenue weighted by budget was easy enough, and I called it percent return. With my original data this name made a lot more sense. However, only as I typed this up did I realize that it is nonsensical. I'm too lazy to go back and fix it, so it will stay that way.

R deals with dates really well, so in order to display them on a graph, all I had to do was make a new column equal to the release date minus January 1 of the release year. I called this column, logically, days_since_jan_1.

To get week of the year, I subset the rows with day values in the correct range and set the correct week number. I applied this to all 52 weeks using a for loop. The 365th and 366th day (leap years) were thrown in with the 52nd week.

Month of the year was easy, I just pulled a substring out of the date value.

For day of the week, I converted the release date to a POSIXlt object and used $wday to record the numerical day of the week. Sunday is 0, Monday is 1, Tuesday is 2, etc.

**THE DISPLAY**

Now that I had the necessary values, I displayed them in a variety of formats. First, I created data frames of weekly, monthly, and daily medians and displayed them in scatterplots. These were the results: [Weekly](https://imgur.com/vrLmT1g), [monthly](https://imgur.com/2A3xIFT), and [daily](https://imgur.com/jOOiB0b). Remember, the days of the week are 0=Sunday, 1=Monday, 2=Tuesday, etc.

However, since these graphs are automatically scaled, any small difference can look significant. For some context, I graphed them as box plots, too. [Weekly](https://imgur.com/3nRXjlQ), [monthly](https://imgur.com/LvAw6nD), and [daily](https://imgur.com/uv6J9Lw).

**STATISTICAL SIGNIFICANCE**

There's no real way to know if all of this work means anything until statistical significance tests are applied. I chose to use a one-factor T Test to compare the mean of each week/month/day to the population mean.

*Monthly Results*

I'm going to start broad and work my way in. With an alpha of 0.05, the months with a statistically significant difference from the population mean were:

* January (lower)
* March (lower)
* September (lower)
* October (lower)
* June (higher)
* November (higher)

When the alpha was lowered to 0.01, the months with a statistically significant difference from the population mean were:

* September (lower)
* June (higher)

*Conclusion:* Movies are most profitable when released in the early summer and towards the end of the year, and least profitable when released at the beginning of the year and at the beginning of the school year.

*Weekly Results*

With an alpha of 0.05, the weeks with a statistically significant difference from the population mean were:

* January 1-7 (lower)
* February 19-25 (lower)
* March 25-31 (lower)
* April 1-7 (lower)
* August 19-25 (lower)
* August 26-September 1 (lower)
* September 9-15 (lower)
* October 7-13 (lower)
* May 20-26 (higher)
* June 17-23 (higher)
* June 24-30 (higher)
* November 4-10 (higher)
* November 11-17 (higher)
* December 9-15 (higher)

When the alpha was lowered to 0.01, the weeks with a statistically significant difference from the population mean were:

* August 19-25 (lower)
* August 26-September 1 (lower)
* September 9-15 (lower)
* June 24-30 (higher)
* December 9-15 (higher)

*Conclusion:* Movies are most profitable when released between late May and late June and between November and early December and least profitable when released in early January, between mid February and early April, and between mid August and mid September.

*Daily Results*

With an alpha of 0.05, the days of the week with a statistically significant difference from the population mean were:

* Friday (lower)
* Saturday (lower)
* Wednesday (higher)
* Thursday (higher)

When the alpha was lowered to 0.01, the weeks with a statistically significant difference from the population mean were:

* Friday (lower)
* Saturday (lower)
* Wednesday (higher)

*Conclusion:* Movies are most profitable when released on Wednesdays and least profitable when released on Fridays and Saturdays.

**APPLICATION**

The days this year that are the best candidates for release dates are Wednesday, June 27 and Wednesday, December 12.",0,"Last night I had a #showerthought: Does it really matter when a movie is released? When I woke up this morning, the question was still on my mind, so I decided to mess around and see what I could find.

**THE CONCEPT**

I wanted to look at a couple different ways to portray release date and see how they affected the success of the movie. To do this, I first had to define a measurement of movie success. Seeing as it is the producing company who decides the release date of a movie, I decided to use the most important metric to them: $$$. However, it didn't make sense to use simply the revenue gained by the movie, because raw revenue numbers have an innate bias towards bigger movies with bigger budgets. For example, a movie with a production budget of $200 million that grosses $700 million would be considered extremely successful. However, so would a movie with a production budget of $20 million that grosses $70 million. Raw revenue number would show the second movie as being equally successful as a movie with a production budget of $200 million the grosses $250 million, which makes no sense. As such, I decided that my metric for success should be total revenue divided by production budget, thus scaling all of the movies to an equal scale.

The three portrayals of release date that I chose were week of the year, month of the year, and day of the week. 

**THE DATA**

At first, I used data scraped from [boxofficemojo.com](http://www.boxofficemojo.com), but then I realized that it only included data for movies that had grossed over $200 million. Seeing as that excludes a lot of the less successful movies (an important exclusion, since one of my variables is supposed to measure success), I had to switch. I ended up using data from the [TMDB 5,000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata/data), which is data in csv format regarding 5,000 movies. It included all of the values I needed, which were title, revenue, budget, and release date.

**THE DATA WRANGLING**

First, I calculated the necessary values. Revenue weighted by budget was easy enough, and I called it percent return. With my original data this name made a lot more sense. However, only as I typed this up did I realize that it is nonsensical. I'm too lazy to go back and fix it, so it will stay that way.

R deals with dates really well, so in order to display them on a graph, all I had to do was make a new column equal to the release date minus January 1 of the release year. I called this column, logically, days_since_jan_1.

To get week of the year, I subset the rows with day values in the correct range and set the correct week number. I applied this to all 52 weeks using a for loop. The 365th and 366th day (leap years) were thrown in with the 52nd week.

Month of the year was easy, I just pulled a substring out of the date value.

For day of the week, I converted the release date to a POSIXlt object and used $wday to record the numerical day of the week. Sunday is 0, Monday is 1, Tuesday is 2, etc.

**THE DISPLAY**

Now that I had the necessary values, I displayed them in a variety of formats. First, I created data frames of weekly, monthly, and daily medians and displayed them in scatterplots. These were the results: [Weekly](https://imgur.com/vrLmT1g), [monthly](https://imgur.com/2A3xIFT), and [daily](https://imgur.com/jOOiB0b). Remember, the days of the week are 0=Sunday, 1=Monday, 2=Tuesday, etc.

However, since these graphs are automatically scaled, any small difference can look significant. For some context, I graphed them as box plots, too. [Weekly](https://imgur.com/3nRXjlQ), [monthly](https://imgur.com/LvAw6nD), and [daily](https://imgur.com/uv6J9Lw).

**STATISTICAL SIGNIFICANCE**

There's no real way to know if all of this work means anything until statistical significance tests are applied. I chose to use a one-factor T Test to compare the mean of each week/month/day to the population mean.

*Monthly Results*

I'm going to start broad and work my way in. With an alpha of 0.05, the months with a statistically significant difference from the population mean were:

* January (lower)
* March (lower)
* September (lower)
* October (lower)
* June (higher)
* November (higher)

When the alpha was lowered to 0.01, the months with a statistically significant difference from the population mean were:

* September (lower)
* June (higher)

*Conclusion:* Movies are most profitable when released in the early summer and towards the end of the year, and least profitable when released at the beginning of the year and at the beginning of the school year.

*Weekly Results*

With an alpha of 0.05, the weeks with a statistically significant difference from the population mean were:

* January 1-7 (lower)
* February 19-25 (lower)
* March 25-31 (lower)
* April 1-7 (lower)
* August 19-25 (lower)
* August 26-September 1 (lower)
* September 9-15 (lower)
* October 7-13 (lower)
* May 20-26 (higher)
* June 17-23 (higher)
* June 24-30 (higher)
* November 4-10 (higher)
* November 11-17 (higher)
* December 9-15 (higher)

When the alpha was lowered to 0.01, the weeks with a statistically significant difference from the population mean were:

* August 19-25 (lower)
* August 26-September 1 (lower)
* September 9-15 (lower)
* June 24-30 (higher)
* December 9-15 (higher)

*Conclusion:* Movies are most profitable when released between late May and late June and between November and early December and least profitable when released in early January, between mid February and early April, and between mid August and mid September.

*Daily Results*

With an alpha of 0.05, the days of the week with a statistically significant difference from the population mean were:

* Friday (lower)
* Saturday (lower)
* Wednesday (higher)
* Thursday (higher)

When the alpha was lowered to 0.01, the weeks with a statistically significant difference from the population mean were:

* Friday (lower)
* Saturday (lower)
* Wednesday (higher)

*Conclusion:* Movies are most profitable when released on Wednesdays and least profitable when released on Fridays and Saturdays.

**APPLICATION**

The days this year that are the best candidates for release dates are Wednesday, June 27 and Wednesday, December 12.",1,statistics,54935,,A Statistical Analysis of Movie Releases Using R,https://www.reddit.com/r/statistics/comments/8hcm2o/a_statistical_analysis_of_movie_releases_using_r/,all_ads,2018-05-06 07:28:55,27 days 18:06:39.036584000,
"Hello there, we've carried out a survey as a school project. We interviewed 273 people. The topic was renewable energy. Now we face a problem: we need to analyse data (15 questions) possibly also based on gender, age, location, etc, but we don't know any programme that could help us, and we don't want to count and calculate everything manually 😅. Does anybody have suggestions?",24,1525540666.0,8h6gcf,False,"Hello there, we've carried out a survey as a school project. We interviewed 273 people. The topic was renewable energy. Now we face a problem: we need to analyse data (15 questions) possibly also based on gender, age, location, etc, but we don't know any programme that could help us, and we don't want to count and calculate everything manually 😅. Does anybody have suggestions?",0,"Hello there, we've carried out a survey as a school project. We interviewed 273 people. The topic was renewable energy. Now we face a problem: we need to analyse data (15 questions) possibly also based on gender, age, location, etc, but we don't know any programme that could help us, and we don't want to count and calculate everything manually 😅. Does anybody have suggestions?",23,statistics,54935,,Free/low cost software to analyse data?,https://www.reddit.com/r/statistics/comments/8h6gcf/freelow_cost_software_to_analyse_data/,all_ads,2018-05-05 13:17:46,28 days 12:17:48.036584000,
"Hey folks, I am a former academic who has used tools like STATA and SPSS for years. I have moved on to data analytics in the business world and beginning to dabble in r/RStudio. Does anyone recommend [DataCamp.com](https://DataCamp.com) for online learning of R? Thanks!",7,1525565332.0,8h8gry,False,"Hey folks, I am a former academic who has used tools like STATA and SPSS for years. I have moved on to data analytics in the business world and beginning to dabble in r/RStudio. Does anyone recommend [DataCamp.com](https://DataCamp.com) for online learning of R? Thanks!",0,"Hey folks, I am a former academic who has used tools like STATA and SPSS for years. I have moved on to data analytics in the business world and beginning to dabble in r/RStudio. Does anyone recommend [DataCamp.com](https://DataCamp.com) for online learning of R? Thanks!",2,statistics,54935,,DataCamp.com,https://www.reddit.com/r/statistics/comments/8h8gry/datacampcom/,all_ads,2018-05-05 20:08:52,28 days 05:26:42.036584000,
"I struggle with convergence when I use the standardized: \(Xt\-Xmean\)/Xstd, formula.

Can I instead use: \(Xt\-Xmean\)/\(Xmin\-Xmax\), and are there anyone who has done this previously that i cite in my paper?

Thanks :\)",0,1525550584.0,8h73qu,False,"I struggle with convergence when I use the standardized: \(Xt\-Xmean\)/Xstd, formula.

Can I instead use: \(Xt\-Xmean\)/\(Xmin\-Xmax\), and are there anyone who has done this previously that i cite in my paper?

Thanks :\)",0,"I struggle with convergence when I use the standardized: \(Xt\-Xmean\)/Xstd, formula.

Can I instead use: \(Xt\-Xmean\)/\(Xmin\-Xmax\), and are there anyone who has done this previously that i cite in my paper?

Thanks :\)",5,statistics,54935,,"Is it common to use mean normalization for dependent variables in the GARCH(1,1)-model?",https://www.reddit.com/r/statistics/comments/8h73qu/is_it_common_to_use_mean_normalization_for/,all_ads,2018-05-05 16:03:04,28 days 09:32:30.036584000,
"Hi, we are investigating the psychology of money. What does money mean to you? Why do you want it? How are these related to your personality traits and other social beliefs. Please help us explore these research questions by participating in this interesting study. Please click or copy and paste the link into you web browser to complete the study. Thanks! https://uic.ca1.qualtrics.com/jfe/form/SV_bJgTLvUsM2YAT5P.
",2,1525594895.0,8hbl1k,False,"Hi, we are investigating the psychology of money. What does money mean to you? Why do you want it? How are these related to your personality traits and other social beliefs. Please help us explore these research questions by participating in this interesting study. Please click or copy and paste the link into you web browser to complete the study. Thanks! https://uic.ca1.qualtrics.com/jfe/form/SV_bJgTLvUsM2YAT5P.
",0,"Hi, we are investigating the psychology of money. What does money mean to you? Why do you want it? How are these related to your personality traits and other social beliefs. Please help us explore these research questions by participating in this interesting study. Please click or copy and paste the link into you web browser to complete the study. Thanks! https://uic.ca1.qualtrics.com/jfe/form/SV_bJgTLvUsM2YAT5P.
",0,statistics,54935,,"[Academic] The psychology of money! money! money! (Only US residents, +18)",https://www.reddit.com/r/statistics/comments/8hbl1k/academic_the_psychology_of_money_money_money_only/,all_ads,2018-05-06 04:21:35,27 days 21:13:59.036584000,
"For example, let's say I run a clinical trial comparing drug A with drug B and notice that patients treated with drug A survived longer.  However, when I look at the underlying patient data, I noticed that patients treated with drug A were also statistically younger (p < 0.001).  I would say that our data demonstrate that Drug A results in increased overall survival; however, patient age may also be a ________""

THanks. ",2,1525560270.0,8h7xvs,False,"For example, let's say I run a clinical trial comparing drug A with drug B and notice that patients treated with drug A survived longer.  However, when I look at the underlying patient data, I noticed that patients treated with drug A were also statistically younger (p < 0.001).  I would say that our data demonstrate that Drug A results in increased overall survival; however, patient age may also be a ________""

THanks. ",0,"For example, let's say I run a clinical trial comparing drug A with drug B and notice that patients treated with drug A survived longer.  However, when I look at the underlying patient data, I noticed that patients treated with drug A were also statistically younger (p < 0.001).  I would say that our data demonstrate that Drug A results in increased overall survival; however, patient age may also be a ________""

THanks. ",0,statistics,54935,,Easy question: what do you call an alternative variable/factor the influences an outcome?,https://www.reddit.com/r/statistics/comments/8h7xvs/easy_question_what_do_you_call_an_alternative/,all_ads,2018-05-05 18:44:30,28 days 06:51:04.036584000,
"I just starting learning about Bayesian analysis, and it's fascinating.  To me at least, it conceptually makes more sense to me than a frequentist approach. 

That being said, if we assume some populations parameters are distributed according to a prior distribution, are there ever situations where that prior population's parameters are themselves distributed according to another distribution? And what if that ""sub-prior"" distribution has itself a prior? If there was an infinite number of priors, would the distribution asymptotically collapse to a certain distribution? 

For example, imagine a normal distribution with parameters (u0, s0), and u0~N(u1,s1), u1~N(u2,s2), so on so forth. ",17,1525477623.0,8h08c0,False,"I just starting learning about Bayesian analysis, and it's fascinating.  To me at least, it conceptually makes more sense to me than a frequentist approach. 

That being said, if we assume some populations parameters are distributed according to a prior distribution, are there ever situations where that prior population's parameters are themselves distributed according to another distribution? And what if that ""sub-prior"" distribution has itself a prior? If there was an infinite number of priors, would the distribution asymptotically collapse to a certain distribution? 

For example, imagine a normal distribution with parameters (u0, s0), and u0~N(u1,s1), u1~N(u2,s2), so on so forth. ",0,"I just starting learning about Bayesian analysis, and it's fascinating.  To me at least, it conceptually makes more sense to me than a frequentist approach. 

That being said, if we assume some populations parameters are distributed according to a prior distribution, are there ever situations where that prior population's parameters are themselves distributed according to another distribution? And what if that ""sub-prior"" distribution has itself a prior? If there was an infinite number of priors, would the distribution asymptotically collapse to a certain distribution? 

For example, imagine a normal distribution with parameters (u0, s0), and u0~N(u1,s1), u1~N(u2,s2), so on so forth. ",50,statistics,54935,,"So if Bayesian analysis assumes that the parameters of a population are not fixed, but distributed according to some kind of prior distribution, can the prior's parameters also have a distribution?",https://www.reddit.com/r/statistics/comments/8h08c0/so_if_bayesian_analysis_assumes_that_the/,all_ads,2018-05-04 19:47:03,29 days 05:48:31.036584000,
[Please see the CrossValidated post with the relevant equations.](https://stats.stackexchange.com/questions/344545/finding-the-mle-for-covariance-matrix-in-random-effects-model),5,1525512973.0,8h4b93,False,[Please see the CrossValidated post with the relevant equations.](https://stats.stackexchange.com/questions/344545/finding-the-mle-for-covariance-matrix-in-random-effects-model),0,[Please see the CrossValidated post with the relevant equations.](https://stats.stackexchange.com/questions/344545/finding-the-mle-for-covariance-matrix-in-random-effects-model),7,statistics,54935,,Trying to find the MLE for covariance matrix in a random effects regression mdel,https://www.reddit.com/r/statistics/comments/8h4b93/trying_to_find_the_mle_for_covariance_matrix_in_a/,all_ads,2018-05-05 05:36:13,28 days 19:59:21.036584000,
In DPMOs it represents a 76% improvement (1010.1 to 244.7 DPMOs) but it just doesnt seem that way when you do the sigma level. Am I missing something?,6,1525555241.0,8h7hbz,False,In DPMOs it represents a 76% improvement (1010.1 to 244.7 DPMOs) but it just doesnt seem that way when you do the sigma level. Am I missing something?,0,In DPMOs it represents a 76% improvement (1010.1 to 244.7 DPMOs) but it just doesnt seem that way when you do the sigma level. Am I missing something?,0,statistics,54935,,Did my first black belt project. Is 4.6 to 5sigma level improvement significant?,https://www.reddit.com/r/statistics/comments/8h7hbz/did_my_first_black_belt_project_is_46_to_5sigma/,all_ads,2018-05-05 17:20:41,28 days 08:14:53.036584000,
Is there any online demo for a MaxDiff survey that shows the results immediately? I'm just looking to help explain some folks at my company how MaxDiff works and some aren't quite getting it. I thought a live example might help them understand how their answers are turned into a ranking.... Thanks,3,1525465399.0,8gytki,False,Is there any online demo for a MaxDiff survey that shows the results immediately? I'm just looking to help explain some folks at my company how MaxDiff works and some aren't quite getting it. I thought a live example might help them understand how their answers are turned into a ranking.... Thanks,0,Is there any online demo for a MaxDiff survey that shows the results immediately? I'm just looking to help explain some folks at my company how MaxDiff works and some aren't quite getting it. I thought a live example might help them understand how their answers are turned into a ranking.... Thanks,4,statistics,54935,,How to run a live MaxDiff demo to show how it works?,https://www.reddit.com/r/statistics/comments/8gytki/how_to_run_a_live_maxdiff_demo_to_show_how_it/,all_ads,2018-05-04 16:23:19,29 days 09:12:15.036584000,
"Hi guys, first time in this sub.

I'm running a very basic stat analysis for a paper on tax progressivity and I need help to know how to proceed with this.

I had a few Stat and Econometrics classes, however I forgot a lot of the basics needed for an analysis like this, and I was hoping you could help me clear this up.

I need to compare tax revenue and GDP through various years for a country based on the method given by [this paper](https://iuj.repo.nii.ac.jp/?action=repository_action_common_download&item_id=410&item_no=1&attribute_id=22&file_no=1). I already have the two indicators in an excel table, and I managed to do that proportional standard index results. Nonetheless, I want to know if I need to filter the data somehow for the analysis to be relevant.

I heard that the HP filter was an alternative, but I also found a paper advising people not to use it, but I'm afraid just listing the raw numbers won't have much statistical validity.

My question is, for a very basic set of yearly data like this, what do I need to do?

Thanks",4,1525476072.0,8h01g3,False,"Hi guys, first time in this sub.

I'm running a very basic stat analysis for a paper on tax progressivity and I need help to know how to proceed with this.

I had a few Stat and Econometrics classes, however I forgot a lot of the basics needed for an analysis like this, and I was hoping you could help me clear this up.

I need to compare tax revenue and GDP through various years for a country based on the method given by [this paper](https://iuj.repo.nii.ac.jp/?action=repository_action_common_download&item_id=410&item_no=1&attribute_id=22&file_no=1). I already have the two indicators in an excel table, and I managed to do that proportional standard index results. Nonetheless, I want to know if I need to filter the data somehow for the analysis to be relevant.

I heard that the HP filter was an alternative, but I also found a paper advising people not to use it, but I'm afraid just listing the raw numbers won't have much statistical validity.

My question is, for a very basic set of yearly data like this, what do I need to do?

Thanks",0,"Hi guys, first time in this sub.

I'm running a very basic stat analysis for a paper on tax progressivity and I need help to know how to proceed with this.

I had a few Stat and Econometrics classes, however I forgot a lot of the basics needed for an analysis like this, and I was hoping you could help me clear this up.

I need to compare tax revenue and GDP through various years for a country based on the method given by [this paper](https://iuj.repo.nii.ac.jp/?action=repository_action_common_download&item_id=410&item_no=1&attribute_id=22&file_no=1). I already have the two indicators in an excel table, and I managed to do that proportional standard index results. Nonetheless, I want to know if I need to filter the data somehow for the analysis to be relevant.

I heard that the HP filter was an alternative, but I also found a paper advising people not to use it, but I'm afraid just listing the raw numbers won't have much statistical validity.

My question is, for a very basic set of yearly data like this, what do I need to do?

Thanks",3,statistics,54935,,Need help for a uni paper,https://www.reddit.com/r/statistics/comments/8h01g3/need_help_for_a_uni_paper/,all_ads,2018-05-04 19:21:12,29 days 06:14:22.036584000,
"Hi Stats :)  I need some assistance (and I'm not even sure if this is possible). I work for an ecommerce company. They recently updated the look and feel of their website. My boss wants to know what impact the updates have had on sales conversions. It wasn't possible to A/B test the changes with a control group.  The only ""control"" data that I have is the conversion rates and sales figures from users of the old website. Is there anything I can do here? ",4,1525444873.0,8gx8sx,False,"Hi Stats :)  I need some assistance (and I'm not even sure if this is possible). I work for an ecommerce company. They recently updated the look and feel of their website. My boss wants to know what impact the updates have had on sales conversions. It wasn't possible to A/B test the changes with a control group.  The only ""control"" data that I have is the conversion rates and sales figures from users of the old website. Is there anything I can do here? ",0,"Hi Stats :)  I need some assistance (and I'm not even sure if this is possible). I work for an ecommerce company. They recently updated the look and feel of their website. My boss wants to know what impact the updates have had on sales conversions. It wasn't possible to A/B test the changes with a control group.  The only ""control"" data that I have is the conversion rates and sales figures from users of the old website. Is there anything I can do here? ",7,statistics,54935,,Help with statistical test to evaluate online sales conversions?,https://www.reddit.com/r/statistics/comments/8gx8sx/help_with_statistical_test_to_evaluate_online/,all_ads,2018-05-04 10:41:13,29 days 14:54:21.036584000,
"I am setting up a spreadsheet to use for outgoing part inspection in a manufacturing environment. I'd like to get some help understanding how to deal with combining multiple measurements and calculating an overall statistical result.

&nbsp;

My end goal is a spreadsheet we can use to continue making measurements until we are at some specified confidence level (e.g. 99%) that a given lot of parts is at or below some threshold level of bad parts (for example, say 1%).

&nbsp;


Where I'm getting hung up is how to go from calculating mean and standard deviation, etc. for a single measurement to an overall ""this is a good part"" measurement, which is what I really care about. For simplicity's sake you can assume the measurement distributions are normal, samples are random, and defects of one type don't correlate with defects of another type.

&nbsp;



For example:

&nbsp;

Lot size of N parts.

On a random subset of those parts:

Measure dimension A (conforming parts are within some range)

Measure dimension B (conforming parts are within some range)

Measure parameter C (conforming parts must be on one side of a numeric threshold... less than or greater than the threshold)

Continue making measurements until:

&nbsp;

   \**** Magic Happens ****

&nbsp;

**A:** spreadsheet says this lot is good to go, 99% confident we are only shipping 1% or less bad product.

**B:** spreadsheet says there is a quality problem, this lot of parts has too high of a defect rate

&nbsp;

Can someone either explain how to do this, how to reach the end goal in a more effective way, or link to some appropriate reference? Thanks! :)",1,1525454504.0,8gxxj2,False,"I am setting up a spreadsheet to use for outgoing part inspection in a manufacturing environment. I'd like to get some help understanding how to deal with combining multiple measurements and calculating an overall statistical result.

&nbsp;

My end goal is a spreadsheet we can use to continue making measurements until we are at some specified confidence level (e.g. 99%) that a given lot of parts is at or below some threshold level of bad parts (for example, say 1%).

&nbsp;


Where I'm getting hung up is how to go from calculating mean and standard deviation, etc. for a single measurement to an overall ""this is a good part"" measurement, which is what I really care about. For simplicity's sake you can assume the measurement distributions are normal, samples are random, and defects of one type don't correlate with defects of another type.

&nbsp;



For example:

&nbsp;

Lot size of N parts.

On a random subset of those parts:

Measure dimension A (conforming parts are within some range)

Measure dimension B (conforming parts are within some range)

Measure parameter C (conforming parts must be on one side of a numeric threshold... less than or greater than the threshold)

Continue making measurements until:

&nbsp;

   \**** Magic Happens ****

&nbsp;

**A:** spreadsheet says this lot is good to go, 99% confident we are only shipping 1% or less bad product.

**B:** spreadsheet says there is a quality problem, this lot of parts has too high of a defect rate

&nbsp;

Can someone either explain how to do this, how to reach the end goal in a more effective way, or link to some appropriate reference? Thanks! :)",0,"I am setting up a spreadsheet to use for outgoing part inspection in a manufacturing environment. I'd like to get some help understanding how to deal with combining multiple measurements and calculating an overall statistical result.

&nbsp;

My end goal is a spreadsheet we can use to continue making measurements until we are at some specified confidence level (e.g. 99%) that a given lot of parts is at or below some threshold level of bad parts (for example, say 1%).

&nbsp;


Where I'm getting hung up is how to go from calculating mean and standard deviation, etc. for a single measurement to an overall ""this is a good part"" measurement, which is what I really care about. For simplicity's sake you can assume the measurement distributions are normal, samples are random, and defects of one type don't correlate with defects of another type.

&nbsp;



For example:

&nbsp;

Lot size of N parts.

On a random subset of those parts:

Measure dimension A (conforming parts are within some range)

Measure dimension B (conforming parts are within some range)

Measure parameter C (conforming parts must be on one side of a numeric threshold... less than or greater than the threshold)

Continue making measurements until:

&nbsp;

   \**** Magic Happens ****

&nbsp;

**A:** spreadsheet says this lot is good to go, 99% confident we are only shipping 1% or less bad product.

**B:** spreadsheet says there is a quality problem, this lot of parts has too high of a defect rate

&nbsp;

Can someone either explain how to do this, how to reach the end goal in a more effective way, or link to some appropriate reference? Thanks! :)",3,statistics,54935,,Combining statistical measurements in manufacturing.,https://www.reddit.com/r/statistics/comments/8gxxj2/combining_statistical_measurements_in/,all_ads,2018-05-04 13:21:44,29 days 12:13:50.036584000,
"Hello everyone, I was attempting to run a poisson regression analysis on this data: https://imgur.com/a/HEcDzZL. (This is a sample of the weather data and respective hospital admissions on a given day)
What would be the best way to do this? Which categorical variables would you use to predict the outcome of Hospital Admissions.

I am hoping to run a Regression analysis and want to have the most useful regression for it, but haven't been able to do an effective Poisson Regression. Thanks for reading and your assistance. ",8,1525470113.0,8gzbq7,False,"Hello everyone, I was attempting to run a poisson regression analysis on this data: https://imgur.com/a/HEcDzZL. (This is a sample of the weather data and respective hospital admissions on a given day)
What would be the best way to do this? Which categorical variables would you use to predict the outcome of Hospital Admissions.

I am hoping to run a Regression analysis and want to have the most useful regression for it, but haven't been able to do an effective Poisson Regression. Thanks for reading and your assistance. ",0,"Hello everyone, I was attempting to run a poisson regression analysis on this data: https://imgur.com/a/HEcDzZL. (This is a sample of the weather data and respective hospital admissions on a given day)
What would be the best way to do this? Which categorical variables would you use to predict the outcome of Hospital Admissions.

I am hoping to run a Regression analysis and want to have the most useful regression for it, but haven't been able to do an effective Poisson Regression. Thanks for reading and your assistance. ",1,statistics,54935,,Running Poisson Regression,https://www.reddit.com/r/statistics/comments/8gzbq7/running_poisson_regression/,all_ads,2018-05-04 17:41:53,29 days 07:53:41.036584000,
"I am working with hypothesis testing. Finding original claim, null, and alternative. I have not come across a question that was worded this way and I'm confused on how to figure out what it is saying. 
""...claim that the mean weight of fish A is different from the mean weight of fish B."" 

I'm not sure how to interpret just 'different'. 

It's either greater than or less than but I'm not sure how I'm suppose to be able to figure out which one from it just saying different. 
If someone could help me out that would be great!  ",2,1525470052.0,8gzbhl,False,"I am working with hypothesis testing. Finding original claim, null, and alternative. I have not come across a question that was worded this way and I'm confused on how to figure out what it is saying. 
""...claim that the mean weight of fish A is different from the mean weight of fish B."" 

I'm not sure how to interpret just 'different'. 

It's either greater than or less than but I'm not sure how I'm suppose to be able to figure out which one from it just saying different. 
If someone could help me out that would be great!  ",0,"I am working with hypothesis testing. Finding original claim, null, and alternative. I have not come across a question that was worded this way and I'm confused on how to figure out what it is saying. 
""...claim that the mean weight of fish A is different from the mean weight of fish B."" 

I'm not sure how to interpret just 'different'. 

It's either greater than or less than but I'm not sure how I'm suppose to be able to figure out which one from it just saying different. 
If someone could help me out that would be great!  ",1,statistics,54935,,Statistics wording help!!,https://www.reddit.com/r/statistics/comments/8gzbhl/statistics_wording_help/,all_ads,2018-05-04 17:40:52,29 days 07:54:42.036584000,
"I've been playing around with regression models for ordinal data using the clm() function in package ""ordinal"". The outcome variable I'm working with is on a seven point scale, and I have one categorical (6 bins) and one continuous predictor. I'm using the logit link function. 

The results are puzzling. The model consistently predicts values that are 1-2 points less than the observed values, in every bin. I plotted the average of the actual values vs. the average of the predicted values, and it looks like the [same graph shifted downwards](https://imgur.com/a/l4qKOYU). 

It does this with or without the continuous variable. It does a much better job of predicting the relationship between the outcome variable and continuous variable. 

Can anyone think of a reason for this? I'm stumped. ",5,1525420831.0,8gv03i,False,"I've been playing around with regression models for ordinal data using the clm() function in package ""ordinal"". The outcome variable I'm working with is on a seven point scale, and I have one categorical (6 bins) and one continuous predictor. I'm using the logit link function. 

The results are puzzling. The model consistently predicts values that are 1-2 points less than the observed values, in every bin. I plotted the average of the actual values vs. the average of the predicted values, and it looks like the [same graph shifted downwards](https://imgur.com/a/l4qKOYU). 

It does this with or without the continuous variable. It does a much better job of predicting the relationship between the outcome variable and continuous variable. 

Can anyone think of a reason for this? I'm stumped. ",0,"I've been playing around with regression models for ordinal data using the clm() function in package ""ordinal"". The outcome variable I'm working with is on a seven point scale, and I have one categorical (6 bins) and one continuous predictor. I'm using the logit link function. 

The results are puzzling. The model consistently predicts values that are 1-2 points less than the observed values, in every bin. I plotted the average of the actual values vs. the average of the predicted values, and it looks like the [same graph shifted downwards](https://imgur.com/a/l4qKOYU). 

It does this with or without the continuous variable. It does a much better job of predicting the relationship between the outcome variable and continuous variable. 

Can anyone think of a reason for this? I'm stumped. ",10,statistics,54935,,Can you think of any reason why an ordinal regression would consistently under-predict values?,https://www.reddit.com/r/statistics/comments/8gv03i/can_you_think_of_any_reason_why_an_ordinal/,all_ads,2018-05-04 04:00:31,29 days 21:35:03.036584000,
"Hi all,

So I have some pairwise count data (values 0 - 5 for both factors) with roughly 30 independent replicates.

What I want to ask is:

1. Is there a difference between the chance of getting a zero between the two groups?

2. Do the two groups have different means.

I am lucky enough to be working with data that are substantially different between the two groups, so I am fairly certain I will be getting a significant result between the means no matter how I look at it.  Further, I think it is unlikely there will be a significant difference between the chance of getting a zero based on the two groups (6/28 zeros vs 11/28 zeros).  My current, simple plan is to use a non-parametric Wilcox Ranked Sum test, though of course this ignores the possibility of two different effects.  It is also my understanding that if the two groups have different distributions, significant results may not actually be because of a difference between their means.

The data being pairwise is throwing me off when I try reading up on this.  I am way into the deep-end here (compared to my normal understanding of stats), so I would greatly appreciate any help.",6,1525425335.0,8gvhbm,False,"Hi all,

So I have some pairwise count data (values 0 - 5 for both factors) with roughly 30 independent replicates.

What I want to ask is:

1. Is there a difference between the chance of getting a zero between the two groups?

2. Do the two groups have different means.

I am lucky enough to be working with data that are substantially different between the two groups, so I am fairly certain I will be getting a significant result between the means no matter how I look at it.  Further, I think it is unlikely there will be a significant difference between the chance of getting a zero based on the two groups (6/28 zeros vs 11/28 zeros).  My current, simple plan is to use a non-parametric Wilcox Ranked Sum test, though of course this ignores the possibility of two different effects.  It is also my understanding that if the two groups have different distributions, significant results may not actually be because of a difference between their means.

The data being pairwise is throwing me off when I try reading up on this.  I am way into the deep-end here (compared to my normal understanding of stats), so I would greatly appreciate any help.",0,"Hi all,

So I have some pairwise count data (values 0 - 5 for both factors) with roughly 30 independent replicates.

What I want to ask is:

1. Is there a difference between the chance of getting a zero between the two groups?

2. Do the two groups have different means.

I am lucky enough to be working with data that are substantially different between the two groups, so I am fairly certain I will be getting a significant result between the means no matter how I look at it.  Further, I think it is unlikely there will be a significant difference between the chance of getting a zero based on the two groups (6/28 zeros vs 11/28 zeros).  My current, simple plan is to use a non-parametric Wilcox Ranked Sum test, though of course this ignores the possibility of two different effects.  It is also my understanding that if the two groups have different distributions, significant results may not actually be because of a difference between their means.

The data being pairwise is throwing me off when I try reading up on this.  I am way into the deep-end here (compared to my normal understanding of stats), so I would greatly appreciate any help.",7,statistics,54935,,Help with statistical design for zero inflated analysis?,https://www.reddit.com/r/statistics/comments/8gvhbm/help_with_statistical_design_for_zero_inflated/,all_ads,2018-05-04 05:15:35,29 days 20:19:59.036584000,
"IV1 is a dummy coded dichotomous variable; IV2 is also a dummy coded dichotomous variable.

Suppose I run a regression with IV1, IV2 and IV1 x IV2 interaction as predictors. If I get a significant interaction, is it correct to say that IV1 and IV2 interact *in general*? Or, is it the case that because the interaction term only applies to a single cell (i.e., IV1 = 1 and IV2 = 1), its interpretation is much less general than saying the two variables interact in general?

---

Now, suppose IV1 is a three level variable that is also dummy coded; IV2 is still a two level dummy coded variable. If I get the following interaction: IV1 (level 3 vs. 1) x IV2, is the interpretation the same as if I imagine IV1 is dichotomous and only reflects level 3 vs. level 1? That is, the effect of IV1 level 3 vs. level 1 depends on the level of IV2?",11,1525409170.0,8gtmt2,False,"IV1 is a dummy coded dichotomous variable; IV2 is also a dummy coded dichotomous variable.

Suppose I run a regression with IV1, IV2 and IV1 x IV2 interaction as predictors. If I get a significant interaction, is it correct to say that IV1 and IV2 interact *in general*? Or, is it the case that because the interaction term only applies to a single cell (i.e., IV1 = 1 and IV2 = 1), its interpretation is much less general than saying the two variables interact in general?

---

Now, suppose IV1 is a three level variable that is also dummy coded; IV2 is still a two level dummy coded variable. If I get the following interaction: IV1 (level 3 vs. 1) x IV2, is the interpretation the same as if I imagine IV1 is dichotomous and only reflects level 3 vs. level 1? That is, the effect of IV1 level 3 vs. level 1 depends on the level of IV2?",0,"IV1 is a dummy coded dichotomous variable; IV2 is also a dummy coded dichotomous variable.

Suppose I run a regression with IV1, IV2 and IV1 x IV2 interaction as predictors. If I get a significant interaction, is it correct to say that IV1 and IV2 interact *in general*? Or, is it the case that because the interaction term only applies to a single cell (i.e., IV1 = 1 and IV2 = 1), its interpretation is much less general than saying the two variables interact in general?

---

Now, suppose IV1 is a three level variable that is also dummy coded; IV2 is still a two level dummy coded variable. If I get the following interaction: IV1 (level 3 vs. 1) x IV2, is the interpretation the same as if I imagine IV1 is dichotomous and only reflects level 3 vs. level 1? That is, the effect of IV1 level 3 vs. level 1 depends on the level of IV2?",3,statistics,54935,,Interpreting interactions between two dummy coded variables.,https://www.reddit.com/r/statistics/comments/8gtmt2/interpreting_interactions_between_two_dummy_coded/,all_ads,2018-05-04 00:46:10,30 days 00:49:24.036584000,
"In my Stata class I have to do a research project. I decided to look at the effect of women's empowerment on GDP per capita. I decided to use m/f sex ratio at birth, m/f GNI per capita ratio, m/f secondary education ratio, m/f legislator ratio, and m/f business executive ratio.

I dropped all the countries without data for each category leaving me with 105 countries and then I recategorised my independent variables into quartiles (GNI per capita ratio, m/f legislator ratio, and m/f business executive ratio) and halves (m/f sex ratio, m/f education ratio).

Now my question: I am trying to run a multiple regression using this categorical data. Would it make sense to recode it all into dummy variables (but then I lose a lot of accuracy of the data) or just use it as it is?

Many thanks for any help",6,1525380878.0,8gq555,False,"In my Stata class I have to do a research project. I decided to look at the effect of women's empowerment on GDP per capita. I decided to use m/f sex ratio at birth, m/f GNI per capita ratio, m/f secondary education ratio, m/f legislator ratio, and m/f business executive ratio.

I dropped all the countries without data for each category leaving me with 105 countries and then I recategorised my independent variables into quartiles (GNI per capita ratio, m/f legislator ratio, and m/f business executive ratio) and halves (m/f sex ratio, m/f education ratio).

Now my question: I am trying to run a multiple regression using this categorical data. Would it make sense to recode it all into dummy variables (but then I lose a lot of accuracy of the data) or just use it as it is?

Many thanks for any help",0,"In my Stata class I have to do a research project. I decided to look at the effect of women's empowerment on GDP per capita. I decided to use m/f sex ratio at birth, m/f GNI per capita ratio, m/f secondary education ratio, m/f legislator ratio, and m/f business executive ratio.

I dropped all the countries without data for each category leaving me with 105 countries and then I recategorised my independent variables into quartiles (GNI per capita ratio, m/f legislator ratio, and m/f business executive ratio) and halves (m/f sex ratio, m/f education ratio).

Now my question: I am trying to run a multiple regression using this categorical data. Would it make sense to recode it all into dummy variables (but then I lose a lot of accuracy of the data) or just use it as it is?

Many thanks for any help",2,statistics,54935,,Running Multiple Regression using Categorical Variables?,https://www.reddit.com/r/statistics/comments/8gq555/running_multiple_regression_using_categorical/,all_ads,2018-05-03 16:54:38,30 days 08:40:56.036584000,
" I have asked this other places, but no answers yet...

My software \(Oxmetrics\) won't allow for my independent variables to be both in the mean equation and the variance equation at the same time. However, they are allowed when ran separately. Is this okei?

I use two explanatory variables, where both of which are added as their current value and lagged. Meaning; four in total.

That is: running the GARCH\-model twice, one with the independent variables in the mean equation and one with the independent variables in the variance equation.

Thanks!",3,1525385042.0,8gqln0,False," I have asked this other places, but no answers yet...

My software \(Oxmetrics\) won't allow for my independent variables to be both in the mean equation and the variance equation at the same time. However, they are allowed when ran separately. Is this okei?

I use two explanatory variables, where both of which are added as their current value and lagged. Meaning; four in total.

That is: running the GARCH\-model twice, one with the independent variables in the mean equation and one with the independent variables in the variance equation.

Thanks!",0," I have asked this other places, but no answers yet...

My software \(Oxmetrics\) won't allow for my independent variables to be both in the mean equation and the variance equation at the same time. However, they are allowed when ran separately. Is this okei?

I use two explanatory variables, where both of which are added as their current value and lagged. Meaning; four in total.

That is: running the GARCH\-model twice, one with the independent variables in the mean equation and one with the independent variables in the variance equation.

Thanks!",0,statistics,54935,,Can I estimate the mean equation and the variance equation separately?,https://www.reddit.com/r/statistics/comments/8gqln0/can_i_estimate_the_mean_equation_and_the_variance/,all_ads,2018-05-03 18:04:02,30 days 07:31:32.036584000,
Do you know the maths behind everything you use and interpret? ,16,1525301210.0,8ghy84,False,Do you know the maths behind everything you use and interpret? ,0,Do you know the maths behind everything you use and interpret? ,44,statistics,54935,,How deep should your mathematical understanding be of the models you interpret?,https://www.reddit.com/r/statistics/comments/8ghy84/how_deep_should_your_mathematical_understanding/,all_ads,2018-05-02 18:46:50,31 days 06:48:44.036584000,
"Hi all,

I'm sort of stuck. I'm interested in learning more about linear mixed models, but it seems like there are two kinds of articles out there about them - ones aimed at social science researchers who have little or no background in mathematics, and ones aimed at people who have a graduate level understanding of statistics. 

Does anyone know of any resources out there for someone in between there? I've taken multivariable calculus and am halfway through linear algebra. My previous stats classes were all geared toward the social sciences (aka very little math involved). ",2,1525330474.0,8glo5z,False,"Hi all,

I'm sort of stuck. I'm interested in learning more about linear mixed models, but it seems like there are two kinds of articles out there about them - ones aimed at social science researchers who have little or no background in mathematics, and ones aimed at people who have a graduate level understanding of statistics. 

Does anyone know of any resources out there for someone in between there? I've taken multivariable calculus and am halfway through linear algebra. My previous stats classes were all geared toward the social sciences (aka very little math involved). ",0,"Hi all,

I'm sort of stuck. I'm interested in learning more about linear mixed models, but it seems like there are two kinds of articles out there about them - ones aimed at social science researchers who have little or no background in mathematics, and ones aimed at people who have a graduate level understanding of statistics. 

Does anyone know of any resources out there for someone in between there? I've taken multivariable calculus and am halfway through linear algebra. My previous stats classes were all geared toward the social sciences (aka very little math involved). ",9,statistics,54935,,Accessible description of mixed models for someone who's taken calculus and a little bit of linear algebra?,https://www.reddit.com/r/statistics/comments/8glo5z/accessible_description_of_mixed_models_for/,all_ads,2018-05-03 02:54:34,30 days 22:41:00.036584000,
"*I cross-posted [this question to Cross Validated](https://stats.stackexchange.com/questions/344072/is-omitted-variable-bias-always-bad-what-are-the-implications-of-omitting-varia) if you'd prefer answering it there.*

Say I'm using multiple logistic regression to help caterers in a large city predict the probability invited adults will come to a wedding. Say I have a proprietary dataset of likely relevant predictor variables for each invited guest's traits, like `age`, `gender`, `marital status`, `how far away they live from the event site`, and whether the event is on a `Saturday`, and I have guests' attendance history to past weddings (`1 = they attended; 0 = they didn't`). 

Say the results show all of the coefficients are significant (`age`: younger people are more likely to attend; `gender`: women are more likely to attend; `marital status`: single people are more likely to attend; `how far away they live`: the closer you live, the more likely you'll attend; `Saturday`: people are more likely to attend Saturday weddings). 

Say I show those results to the caterers and teach them how to calculate the predicted probability of attendance given an invited guest's traits. However, the caterers unanimously proclaim they don't have access to all those data. They don't know the `age`, `gender`, or `marital status` of the guests but they do know `how far away they live` and whether the wedding is on a `Saturday`. 

The caterers ask me if I can re-run the model using only those variables they have information on, so that they can feasibly employ the predicted probability calculations with their own data. What are the implications of this? Is there a better strategy?

Usually, omitted-variable bias is a concern because leaving out relevant variables ""[results in the model attributing the effect of the missing variables to the estimated effects of the included variables](https://en.wikipedia.org/wiki/Omitted-variable_bias)"". But is that a bad thing here? 

For instance, I assume some of the omitted variables correlate, like `marital status` and `how fare they live away`, with married folk being more likely to live in the suburbs, farther away from the event locations in the downtown area of the city. Would the caterers' ability to control for `how far they live away` capture the effects of `marital status` too, then, so that all in all the reduced model will still be effective at predicting attendance?

",7,1525322616.0,8gkpzs,False,"*I cross-posted [this question to Cross Validated](https://stats.stackexchange.com/questions/344072/is-omitted-variable-bias-always-bad-what-are-the-implications-of-omitting-varia) if you'd prefer answering it there.*

Say I'm using multiple logistic regression to help caterers in a large city predict the probability invited adults will come to a wedding. Say I have a proprietary dataset of likely relevant predictor variables for each invited guest's traits, like `age`, `gender`, `marital status`, `how far away they live from the event site`, and whether the event is on a `Saturday`, and I have guests' attendance history to past weddings (`1 = they attended; 0 = they didn't`). 

Say the results show all of the coefficients are significant (`age`: younger people are more likely to attend; `gender`: women are more likely to attend; `marital status`: single people are more likely to attend; `how far away they live`: the closer you live, the more likely you'll attend; `Saturday`: people are more likely to attend Saturday weddings). 

Say I show those results to the caterers and teach them how to calculate the predicted probability of attendance given an invited guest's traits. However, the caterers unanimously proclaim they don't have access to all those data. They don't know the `age`, `gender`, or `marital status` of the guests but they do know `how far away they live` and whether the wedding is on a `Saturday`. 

The caterers ask me if I can re-run the model using only those variables they have information on, so that they can feasibly employ the predicted probability calculations with their own data. What are the implications of this? Is there a better strategy?

Usually, omitted-variable bias is a concern because leaving out relevant variables ""[results in the model attributing the effect of the missing variables to the estimated effects of the included variables](https://en.wikipedia.org/wiki/Omitted-variable_bias)"". But is that a bad thing here? 

For instance, I assume some of the omitted variables correlate, like `marital status` and `how fare they live away`, with married folk being more likely to live in the suburbs, farther away from the event locations in the downtown area of the city. Would the caterers' ability to control for `how far they live away` capture the effects of `marital status` too, then, so that all in all the reduced model will still be effective at predicting attendance?

",0,"*I cross-posted [this question to Cross Validated](https://stats.stackexchange.com/questions/344072/is-omitted-variable-bias-always-bad-what-are-the-implications-of-omitting-varia) if you'd prefer answering it there.*

Say I'm using multiple logistic regression to help caterers in a large city predict the probability invited adults will come to a wedding. Say I have a proprietary dataset of likely relevant predictor variables for each invited guest's traits, like `age`, `gender`, `marital status`, `how far away they live from the event site`, and whether the event is on a `Saturday`, and I have guests' attendance history to past weddings (`1 = they attended; 0 = they didn't`). 

Say the results show all of the coefficients are significant (`age`: younger people are more likely to attend; `gender`: women are more likely to attend; `marital status`: single people are more likely to attend; `how far away they live`: the closer you live, the more likely you'll attend; `Saturday`: people are more likely to attend Saturday weddings). 

Say I show those results to the caterers and teach them how to calculate the predicted probability of attendance given an invited guest's traits. However, the caterers unanimously proclaim they don't have access to all those data. They don't know the `age`, `gender`, or `marital status` of the guests but they do know `how far away they live` and whether the wedding is on a `Saturday`. 

The caterers ask me if I can re-run the model using only those variables they have information on, so that they can feasibly employ the predicted probability calculations with their own data. What are the implications of this? Is there a better strategy?

Usually, omitted-variable bias is a concern because leaving out relevant variables ""[results in the model attributing the effect of the missing variables to the estimated effects of the included variables](https://en.wikipedia.org/wiki/Omitted-variable_bias)"". But is that a bad thing here? 

For instance, I assume some of the omitted variables correlate, like `marital status` and `how fare they live away`, with married folk being more likely to live in the suburbs, farther away from the event locations in the downtown area of the city. Would the caterers' ability to control for `how far they live away` capture the effects of `marital status` too, then, so that all in all the reduced model will still be effective at predicting attendance?

",9,statistics,54935,,Is Omitted Variable Bias Always Bad? What are the implications of omitting variables from a regression that aren't easily obtained in the real-world?,https://www.reddit.com/r/statistics/comments/8gkpzs/is_omitted_variable_bias_always_bad_what_are_the/,all_ads,2018-05-03 00:43:36,31 days 00:51:58.036584000,
"Hello Reddit. I have a question regarding choosing what srtatistic test to do. I know how to solve for t-test and z-tests and for t-test and z-tests for groups but I am having trouble deciding which one to use. Can you help me clarify which test to use and when?

1. two portions T-test
2. two proportion Z-test
3. t-test
4. z-test",11,1525315640.0,8gjtro,False,"Hello Reddit. I have a question regarding choosing what srtatistic test to do. I know how to solve for t-test and z-tests and for t-test and z-tests for groups but I am having trouble deciding which one to use. Can you help me clarify which test to use and when?

1. two portions T-test
2. two proportion Z-test
3. t-test
4. z-test",0,"Hello Reddit. I have a question regarding choosing what srtatistic test to do. I know how to solve for t-test and z-tests and for t-test and z-tests for groups but I am having trouble deciding which one to use. Can you help me clarify which test to use and when?

1. two portions T-test
2. two proportion Z-test
3. t-test
4. z-test",8,statistics,54935,,How do you know when to use a t-test or a z-test?,https://www.reddit.com/r/statistics/comments/8gjtro/how_do_you_know_when_to_use_a_ttest_or_a_ztest/,all_ads,2018-05-02 22:47:20,31 days 02:48:14.036584000,
"Hi guys, I'm here as a new beginner in Statistics, and need some advices where to start, what to do. I want a job as a **sports data analyst**(I'm heavily influenced by the movie **Moneyball**). There's lots of things I have to deal with to achieve my job, but of course **Statistics** is one of the most important. My question is to all people who use and enjoy Statistics, will be great if have a job related to using Statistics, couldn't be better if it's sports related.

**Question**

* Where did you build the **foundation for Statistics**?
* Can you **recommend me to where to start**?
* I believe learn and study Statistics is very important, but I also think the concept I learn in Statistics always has to be used in in real life situation. **How did you applied knowledge of Statistics that you learned to the fields that you are interested in to improve your Statistics skills?**
",3,1525351668.0,8gnu0v,False,"Hi guys, I'm here as a new beginner in Statistics, and need some advices where to start, what to do. I want a job as a **sports data analyst**(I'm heavily influenced by the movie **Moneyball**). There's lots of things I have to deal with to achieve my job, but of course **Statistics** is one of the most important. My question is to all people who use and enjoy Statistics, will be great if have a job related to using Statistics, couldn't be better if it's sports related.

**Question**

* Where did you build the **foundation for Statistics**?
* Can you **recommend me to where to start**?
* I believe learn and study Statistics is very important, but I also think the concept I learn in Statistics always has to be used in in real life situation. **How did you applied knowledge of Statistics that you learned to the fields that you are interested in to improve your Statistics skills?**
",0,"Hi guys, I'm here as a new beginner in Statistics, and need some advices where to start, what to do. I want a job as a **sports data analyst**(I'm heavily influenced by the movie **Moneyball**). There's lots of things I have to deal with to achieve my job, but of course **Statistics** is one of the most important. My question is to all people who use and enjoy Statistics, will be great if have a job related to using Statistics, couldn't be better if it's sports related.

**Question**

* Where did you build the **foundation for Statistics**?
* Can you **recommend me to where to start**?
* I believe learn and study Statistics is very important, but I also think the concept I learn in Statistics always has to be used in in real life situation. **How did you applied knowledge of Statistics that you learned to the fields that you are interested in to improve your Statistics skills?**
",1,statistics,54935,,"A beginner in Statistics(Where to Start, What to do)",https://www.reddit.com/r/statistics/comments/8gnu0v/a_beginner_in_statisticswhere_to_start_what_to_do/,all_ads,2018-05-03 08:47:48,30 days 16:47:46.036584000,
"Hi, Tell me please, what books I can read about time series analysis? I is begginer, but I have some experience in data analisys.",3,1525350406.0,8gnq0s,False,"Hi, Tell me please, what books I can read about time series analysis? I is begginer, but I have some experience in data analisys.",0,"Hi, Tell me please, what books I can read about time series analysis? I is begginer, but I have some experience in data analisys.",1,statistics,54935,,"Hi, Tell me please, what books I can read about time series analysis? I is begginer, but я have some experience in data analisys.",https://www.reddit.com/r/statistics/comments/8gnq0s/hi_tell_me_please_what_books_i_can_read_about/,all_ads,2018-05-03 08:26:46,30 days 17:08:48.036584000,
"For right skewed data with unequal means variances that’s similar to Poisson distributions, is it fair to approximate with a negative binomial distribution?",5,1525328284.0,8glf14,False,"For right skewed data with unequal means variances that’s similar to Poisson distributions, is it fair to approximate with a negative binomial distribution?",0,"For right skewed data with unequal means variances that’s similar to Poisson distributions, is it fair to approximate with a negative binomial distribution?",3,statistics,54935,,Using negative binomial to approximate distributions,https://www.reddit.com/r/statistics/comments/8glf14/using_negative_binomial_to_approximate/,all_ads,2018-05-03 02:18:04,30 days 23:17:30.036584000,
I am doing a study for a class project that I have been pushing off for a while. I am going to be collecting data about how fast of internet speeds that people get. I need at least 30 to 50 people. All I ask of you is to go to fast.com and record only the numbers you get. for example I get 5.4 which is shit but I live out in the middle of nowhere. thank you for your time!,42,1525341361.0,8gmuf7,False,I am doing a study for a class project that I have been pushing off for a while. I am going to be collecting data about how fast of internet speeds that people get. I need at least 30 to 50 people. All I ask of you is to go to fast.com and record only the numbers you get. for example I get 5.4 which is shit but I live out in the middle of nowhere. thank you for your time!,0,I am doing a study for a class project that I have been pushing off for a while. I am going to be collecting data about how fast of internet speeds that people get. I need at least 30 to 50 people. All I ask of you is to go to fast.com and record only the numbers you get. for example I get 5.4 which is shit but I live out in the middle of nowhere. thank you for your time!,0,statistics,54935,,I'm doing a study,https://www.reddit.com/r/statistics/comments/8gmuf7/im_doing_a_study/,all_ads,2018-05-03 05:56:01,30 days 19:39:33.036584000,
"I am having some difficulty as to what to denote which is p1 and which is p2. I can't tell when a problem is asking which I should denote. Can anyone explain to me how I can tell which one I should be using for p1 and p2.

Here is an example:

Of 361 patients with no depression, 67 died.  Of the 89 patients with minor or major 
depression, 26 died. Among people who suffer from cardiac 
disease, are depressed patients more likely to die that non­
depressed ones?

How can I tell which proportion I should be assigning to p1 and p2? I'm not asking to solve this problem as I have already answered it, I'm just confused since I assign the wrong values to each variable resulting in me getting the wrong answer.
",2,1525316259.0,8gjwo2,False,"I am having some difficulty as to what to denote which is p1 and which is p2. I can't tell when a problem is asking which I should denote. Can anyone explain to me how I can tell which one I should be using for p1 and p2.

Here is an example:

Of 361 patients with no depression, 67 died.  Of the 89 patients with minor or major 
depression, 26 died. Among people who suffer from cardiac 
disease, are depressed patients more likely to die that non­
depressed ones?

How can I tell which proportion I should be assigning to p1 and p2? I'm not asking to solve this problem as I have already answered it, I'm just confused since I assign the wrong values to each variable resulting in me getting the wrong answer.
",0,"I am having some difficulty as to what to denote which is p1 and which is p2. I can't tell when a problem is asking which I should denote. Can anyone explain to me how I can tell which one I should be using for p1 and p2.

Here is an example:

Of 361 patients with no depression, 67 died.  Of the 89 patients with minor or major 
depression, 26 died. Among people who suffer from cardiac 
disease, are depressed patients more likely to die that non­
depressed ones?

How can I tell which proportion I should be assigning to p1 and p2? I'm not asking to solve this problem as I have already answered it, I'm just confused since I assign the wrong values to each variable resulting in me getting the wrong answer.
",2,statistics,54935,,Question about Two Proportion Z-Tests,https://www.reddit.com/r/statistics/comments/8gjwo2/question_about_two_proportion_ztests/,all_ads,2018-05-02 22:57:39,31 days 02:37:55.036584000,
"Hello,

It has been many moons since I took statistics. I am writing a paper about greenhouse gas emissions in the post-Soviet states and I want to determine the correlation between standard of living and greenhouse gas emissions. My data (units omitted): 

Country/	IHDI/   	Greenhouse gas emissions per 1 million citizens

Estonia/	0.788/	18.13756602

Latvia/	0.742/	6.166997591

Lithuania/	0.759/	6.405014629

Russia/	0.725/	15.39634883

Belarus/	0.745/	9.672011797

Moldova/	0.628/	3.287476156

Ukraine/	0.69/	        8.304087608

Georgia/	0.672/	3.408329388

Armenia/	0.674/	2.887051723

Azerbaijan/	0.659/	7.406670477

Kazakhstan/	0.714/	19.08233685

Kyrgyzstan/	0.582/	2.793288134

Tajikistan/	0.532/	1.248124657

Uzbekistan/	0.59/	7.803891754

My problem is that if I just graph them side-by-side to see if there's a visually obvious correlation (it's a political science class and that's about all the precision that's asked for), the GG emissions bars are so tall that the IHDI bars all appear the same size because IDHI is measured on a scale of 0-1.

I just need to know how related the two pieces of information are for the set of 14 countries. How would I calculate that?",1,1525327571.0,8glc0o,False,"Hello,

It has been many moons since I took statistics. I am writing a paper about greenhouse gas emissions in the post-Soviet states and I want to determine the correlation between standard of living and greenhouse gas emissions. My data (units omitted): 

Country/	IHDI/   	Greenhouse gas emissions per 1 million citizens

Estonia/	0.788/	18.13756602

Latvia/	0.742/	6.166997591

Lithuania/	0.759/	6.405014629

Russia/	0.725/	15.39634883

Belarus/	0.745/	9.672011797

Moldova/	0.628/	3.287476156

Ukraine/	0.69/	        8.304087608

Georgia/	0.672/	3.408329388

Armenia/	0.674/	2.887051723

Azerbaijan/	0.659/	7.406670477

Kazakhstan/	0.714/	19.08233685

Kyrgyzstan/	0.582/	2.793288134

Tajikistan/	0.532/	1.248124657

Uzbekistan/	0.59/	7.803891754

My problem is that if I just graph them side-by-side to see if there's a visually obvious correlation (it's a political science class and that's about all the precision that's asked for), the GG emissions bars are so tall that the IHDI bars all appear the same size because IDHI is measured on a scale of 0-1.

I just need to know how related the two pieces of information are for the set of 14 countries. How would I calculate that?",0,"Hello,

It has been many moons since I took statistics. I am writing a paper about greenhouse gas emissions in the post-Soviet states and I want to determine the correlation between standard of living and greenhouse gas emissions. My data (units omitted): 

Country/	IHDI/   	Greenhouse gas emissions per 1 million citizens

Estonia/	0.788/	18.13756602

Latvia/	0.742/	6.166997591

Lithuania/	0.759/	6.405014629

Russia/	0.725/	15.39634883

Belarus/	0.745/	9.672011797

Moldova/	0.628/	3.287476156

Ukraine/	0.69/	        8.304087608

Georgia/	0.672/	3.408329388

Armenia/	0.674/	2.887051723

Azerbaijan/	0.659/	7.406670477

Kazakhstan/	0.714/	19.08233685

Kyrgyzstan/	0.582/	2.793288134

Tajikistan/	0.532/	1.248124657

Uzbekistan/	0.59/	7.803891754

My problem is that if I just graph them side-by-side to see if there's a visually obvious correlation (it's a political science class and that's about all the precision that's asked for), the GG emissions bars are so tall that the IHDI bars all appear the same size because IDHI is measured on a scale of 0-1.

I just need to know how related the two pieces of information are for the set of 14 countries. How would I calculate that?",0,statistics,54935,,"Trying to determine correlation between two data sets, unsure what i'm doing",https://www.reddit.com/r/statistics/comments/8glc0o/trying_to_determine_correlation_between_two_data/,all_ads,2018-05-03 02:06:11,30 days 23:29:23.036584000,
"Predictor A is a continuous predictor.

Predictor B is a dummy coded categorical predictor with three levels.

I run a regression including PA, PB and PA*PB.

The results indicate a significant effect of PB (level 3 vs. level 1) and an interaction between PA and PB (level 2 vs. level 1).

I understand that so long as the interaction is in the model, the effect of PB (level 3 vs. level 1) is only a simple effect. Does it make sense to remove the interaction between PA and PB in order to interpret the main effect of PB (level 3 vs. level 1)? Or am I stuck only being able to talk about the simple effect of PB (3 vs 1) because another level of PB is involved in an interaction?",2,1525317502.0,8gk2cs,False,"Predictor A is a continuous predictor.

Predictor B is a dummy coded categorical predictor with three levels.

I run a regression including PA, PB and PA*PB.

The results indicate a significant effect of PB (level 3 vs. level 1) and an interaction between PA and PB (level 2 vs. level 1).

I understand that so long as the interaction is in the model, the effect of PB (level 3 vs. level 1) is only a simple effect. Does it make sense to remove the interaction between PA and PB in order to interpret the main effect of PB (level 3 vs. level 1)? Or am I stuck only being able to talk about the simple effect of PB (3 vs 1) because another level of PB is involved in an interaction?",0,"Predictor A is a continuous predictor.

Predictor B is a dummy coded categorical predictor with three levels.

I run a regression including PA, PB and PA*PB.

The results indicate a significant effect of PB (level 3 vs. level 1) and an interaction between PA and PB (level 2 vs. level 1).

I understand that so long as the interaction is in the model, the effect of PB (level 3 vs. level 1) is only a simple effect. Does it make sense to remove the interaction between PA and PB in order to interpret the main effect of PB (level 3 vs. level 1)? Or am I stuck only being able to talk about the simple effect of PB (3 vs 1) because another level of PB is involved in an interaction?",1,statistics,54935,,"Sensible to remove interaction term in this instance, to interpret a main effect?",https://www.reddit.com/r/statistics/comments/8gk2cs/sensible_to_remove_interaction_term_in_this/,all_ads,2018-05-02 23:18:22,31 days 02:17:12.036584000,
Can you use an f-test? It doesn't make sense that you could because you can't isolate the effect of B1 without X2... dY/X1=B1+B2X2. I'd appreciate any help!,0,1525322886.0,8gkr77,False,Can you use an f-test? It doesn't make sense that you could because you can't isolate the effect of B1 without X2... dY/X1=B1+B2X2. I'd appreciate any help!,0,Can you use an f-test? It doesn't make sense that you could because you can't isolate the effect of B1 without X2... dY/X1=B1+B2X2. I'd appreciate any help!,0,statistics,54935,,HELP: how to determine causality for continuous interaction terms?,https://www.reddit.com/r/statistics/comments/8gkr77/help_how_to_determine_causality_for_continuous/,all_ads,2018-05-03 00:48:06,31 days 00:47:28.036584000,
,2,1525308647.0,8giwt8,False,,0,,1,statistics,54935,,how can factor analysis be used in scale construction when developing a measure?,https://www.reddit.com/r/statistics/comments/8giwt8/how_can_factor_analysis_be_used_in_scale/,all_ads,2018-05-02 20:50:47,31 days 04:44:47.036584000,
"Hi everyone,

I'm currently a 2nd year statistics major and will be doing a ""data science"" internship this summer. The problem is, I actually haven't learned much statistics -- the only related classes I've taken are probability theory, an introductory R class, and a decent amount of math. 

Having said that, are there any good resources I could check out regarding simple but vital concepts like hypothesis testing, linear regression, etc. that would be useful to know for my internship? Also, if you have any general internship-related tips I'd appreciate those too!",14,1525253712.0,8gdpgd,False,"Hi everyone,

I'm currently a 2nd year statistics major and will be doing a ""data science"" internship this summer. The problem is, I actually haven't learned much statistics -- the only related classes I've taken are probability theory, an introductory R class, and a decent amount of math. 

Having said that, are there any good resources I could check out regarding simple but vital concepts like hypothesis testing, linear regression, etc. that would be useful to know for my internship? Also, if you have any general internship-related tips I'd appreciate those too!",0,"Hi everyone,

I'm currently a 2nd year statistics major and will be doing a ""data science"" internship this summer. The problem is, I actually haven't learned much statistics -- the only related classes I've taken are probability theory, an introductory R class, and a decent amount of math. 

Having said that, are there any good resources I could check out regarding simple but vital concepts like hypothesis testing, linear regression, etc. that would be useful to know for my internship? Also, if you have any general internship-related tips I'd appreciate those too!",14,statistics,54935,,"Looking for Good, Introductory Material regarding basic statistical concepts",https://www.reddit.com/r/statistics/comments/8gdpgd/looking_for_good_introductory_material_regarding/,all_ads,2018-05-02 05:35:12,31 days 20:00:22.036584000,
"I'd like to find a statistics textbook or other resource that really builds the fundamentals and understanding of them from the ground up with mathematical concepts and proofs. 

I have some background in statistics but not the level of intuitive understanding I'd like. I have a decent background in calculus/linnear algebra/discrete math and can find my way around an ODE if I have to.",6,1525268156.0,8gf2aq,False,"I'd like to find a statistics textbook or other resource that really builds the fundamentals and understanding of them from the ground up with mathematical concepts and proofs. 

I have some background in statistics but not the level of intuitive understanding I'd like. I have a decent background in calculus/linnear algebra/discrete math and can find my way around an ODE if I have to.",0,"I'd like to find a statistics textbook or other resource that really builds the fundamentals and understanding of them from the ground up with mathematical concepts and proofs. 

I have some background in statistics but not the level of intuitive understanding I'd like. I have a decent background in calculus/linnear algebra/discrete math and can find my way around an ODE if I have to.",3,statistics,54935,,Looking for a good fundamental statistics book,https://www.reddit.com/r/statistics/comments/8gf2aq/looking_for_a_good_fundamental_statistics_book/,all_ads,2018-05-02 09:35:56,31 days 15:59:38.036584000,
"Seems like a somewhat arbitrary criterion if it's not combined with anything else. Also I'm not sure what makes admissible estimators better, like sure they aren't dominated anywhere but that doesn't necessarily mean they're good. ",2,1525256590.0,8ge0qh,False,"Seems like a somewhat arbitrary criterion if it's not combined with anything else. Also I'm not sure what makes admissible estimators better, like sure they aren't dominated anywhere but that doesn't necessarily mean they're good. ",0,"Seems like a somewhat arbitrary criterion if it's not combined with anything else. Also I'm not sure what makes admissible estimators better, like sure they aren't dominated anywhere but that doesn't necessarily mean they're good. ",5,statistics,54935,,Why do people care if an estimator is admissible?,https://www.reddit.com/r/statistics/comments/8ge0qh/why_do_people_care_if_an_estimator_is_admissible/,all_ads,2018-05-02 06:23:10,31 days 19:12:24.036584000,
,6,1525247992.0,8gd3v2,False,,0,,5,statistics,54935,,"In factor analysis, is the correlation between factor 1 and factor 2 undefined?",https://www.reddit.com/r/statistics/comments/8gd3v2/in_factor_analysis_is_the_correlation_between/,all_ads,2018-05-02 03:59:52,31 days 21:35:42.036584000,
"I am trying to develop a Bayesian approach to lot acceptance testing for my workplace.  In lot acceptance testing, we take a sample of *n* parts from a larger population *N* (N >> n) produced that day, in order to perform testing and gain an understanding of the underlying manufacturing process.  Because testing takes time and resources, we have an interest in testing as few parts as needed.  We understand that tests we recently conducted can inform our present estimates, so a Bayesian approach seems natural.

The basic idea right now is that for each day, we can form a semi-empirical prior estimate for the manufacturing process' parameters using data from the previous days.  (For simplicity, we assume a normal process and use an N-INV-GAMMA prior for conjugacy.)  We presume that the present is less and less correlated with the past; that is, as time goes on, our *a priori* estimate for the present parameter values changes as well.  I've chosen to incorporate this intuitive understanding by using a running, weighted average of past test values, weighted by time from present.  So, parts made yesterday will lose weight in prior calculations as time moves on.

------------

I am not sure that this is a temporally coherent approach, and I don't know how to fix it, so I'd like some advice.  One issue I see is of prediction.  After testing parts ""today"", I can give a highest posterior density region for parts made ""tomorrow"", using the joint posterior density I have just generated.  If I include a linear time parameter in my model, such that I am essentially creating a Bayesian local regression system, my estimates of the future, *made today*, will even change as time goes on, generally becoming less certain.

But, I also know that, if I did not test any new parts tomorrow (i.e. did not collect new data), I would nonetheless tomorrow give a *different* highest posterior density region for tomorrow's manufacturing process, than the one I give today using today's posterior density.  That's because I know that tomorrow, I will ""shift"" the past values according to my weighted moving average, and so the posterior density changes tomorrow, even from the posterior density I obtained today.

I'm concerned about coherence questions that this brings up, and what value I ought to use for prediction.  [Goldstein's 1985 paper in Bayesian Statistics 2](https://www.uv.es/bernardo/BayesStatist2.pdf) is a bit over my head but I fear I have such a situation where today's estimate of tomorrow's value does not seem to match tomorrow's estimate of tomorrow's value, even with identical information.  What does it mean if my estimate of tomorrow, made today, does not match my prevision of tomorrow's estimate of tomorrow?

The more natural resolution at this point seems to be to say that today's estimate of tomorrow *should* match my prevision of tomorrow's estimate of tomorrow, and thus it's tomorrow's estimate of tomorrow that should be taken as today's prediction, but this is not obvious to me.  One hangup for instance is that tomorrow's prior density is different than any density otherwise calculated today, so I don't know what it even means to use tomorrow's prior for today's estimate of tomorrow.

Is that what I should do?  Is there a better way to model my intuition that process certainty should decreasing with time, than changing the prior like so, in order to alleviate my concerns about coherence?",9,1525221013.0,8g9rzb,False,"I am trying to develop a Bayesian approach to lot acceptance testing for my workplace.  In lot acceptance testing, we take a sample of *n* parts from a larger population *N* (N >> n) produced that day, in order to perform testing and gain an understanding of the underlying manufacturing process.  Because testing takes time and resources, we have an interest in testing as few parts as needed.  We understand that tests we recently conducted can inform our present estimates, so a Bayesian approach seems natural.

The basic idea right now is that for each day, we can form a semi-empirical prior estimate for the manufacturing process' parameters using data from the previous days.  (For simplicity, we assume a normal process and use an N-INV-GAMMA prior for conjugacy.)  We presume that the present is less and less correlated with the past; that is, as time goes on, our *a priori* estimate for the present parameter values changes as well.  I've chosen to incorporate this intuitive understanding by using a running, weighted average of past test values, weighted by time from present.  So, parts made yesterday will lose weight in prior calculations as time moves on.

------------

I am not sure that this is a temporally coherent approach, and I don't know how to fix it, so I'd like some advice.  One issue I see is of prediction.  After testing parts ""today"", I can give a highest posterior density region for parts made ""tomorrow"", using the joint posterior density I have just generated.  If I include a linear time parameter in my model, such that I am essentially creating a Bayesian local regression system, my estimates of the future, *made today*, will even change as time goes on, generally becoming less certain.

But, I also know that, if I did not test any new parts tomorrow (i.e. did not collect new data), I would nonetheless tomorrow give a *different* highest posterior density region for tomorrow's manufacturing process, than the one I give today using today's posterior density.  That's because I know that tomorrow, I will ""shift"" the past values according to my weighted moving average, and so the posterior density changes tomorrow, even from the posterior density I obtained today.

I'm concerned about coherence questions that this brings up, and what value I ought to use for prediction.  [Goldstein's 1985 paper in Bayesian Statistics 2](https://www.uv.es/bernardo/BayesStatist2.pdf) is a bit over my head but I fear I have such a situation where today's estimate of tomorrow's value does not seem to match tomorrow's estimate of tomorrow's value, even with identical information.  What does it mean if my estimate of tomorrow, made today, does not match my prevision of tomorrow's estimate of tomorrow?

The more natural resolution at this point seems to be to say that today's estimate of tomorrow *should* match my prevision of tomorrow's estimate of tomorrow, and thus it's tomorrow's estimate of tomorrow that should be taken as today's prediction, but this is not obvious to me.  One hangup for instance is that tomorrow's prior density is different than any density otherwise calculated today, so I don't know what it even means to use tomorrow's prior for today's estimate of tomorrow.

Is that what I should do?  Is there a better way to model my intuition that process certainty should decreasing with time, than changing the prior like so, in order to alleviate my concerns about coherence?",0,"I am trying to develop a Bayesian approach to lot acceptance testing for my workplace.  In lot acceptance testing, we take a sample of *n* parts from a larger population *N* (N >> n) produced that day, in order to perform testing and gain an understanding of the underlying manufacturing process.  Because testing takes time and resources, we have an interest in testing as few parts as needed.  We understand that tests we recently conducted can inform our present estimates, so a Bayesian approach seems natural.

The basic idea right now is that for each day, we can form a semi-empirical prior estimate for the manufacturing process' parameters using data from the previous days.  (For simplicity, we assume a normal process and use an N-INV-GAMMA prior for conjugacy.)  We presume that the present is less and less correlated with the past; that is, as time goes on, our *a priori* estimate for the present parameter values changes as well.  I've chosen to incorporate this intuitive understanding by using a running, weighted average of past test values, weighted by time from present.  So, parts made yesterday will lose weight in prior calculations as time moves on.

------------

I am not sure that this is a temporally coherent approach, and I don't know how to fix it, so I'd like some advice.  One issue I see is of prediction.  After testing parts ""today"", I can give a highest posterior density region for parts made ""tomorrow"", using the joint posterior density I have just generated.  If I include a linear time parameter in my model, such that I am essentially creating a Bayesian local regression system, my estimates of the future, *made today*, will even change as time goes on, generally becoming less certain.

But, I also know that, if I did not test any new parts tomorrow (i.e. did not collect new data), I would nonetheless tomorrow give a *different* highest posterior density region for tomorrow's manufacturing process, than the one I give today using today's posterior density.  That's because I know that tomorrow, I will ""shift"" the past values according to my weighted moving average, and so the posterior density changes tomorrow, even from the posterior density I obtained today.

I'm concerned about coherence questions that this brings up, and what value I ought to use for prediction.  [Goldstein's 1985 paper in Bayesian Statistics 2](https://www.uv.es/bernardo/BayesStatist2.pdf) is a bit over my head but I fear I have such a situation where today's estimate of tomorrow's value does not seem to match tomorrow's estimate of tomorrow's value, even with identical information.  What does it mean if my estimate of tomorrow, made today, does not match my prevision of tomorrow's estimate of tomorrow?

The more natural resolution at this point seems to be to say that today's estimate of tomorrow *should* match my prevision of tomorrow's estimate of tomorrow, and thus it's tomorrow's estimate of tomorrow that should be taken as today's prediction, but this is not obvious to me.  One hangup for instance is that tomorrow's prior density is different than any density otherwise calculated today, so I don't know what it even means to use tomorrow's prior for today's estimate of tomorrow.

Is that what I should do?  Is there a better way to model my intuition that process certainty should decreasing with time, than changing the prior like so, in order to alleviate my concerns about coherence?",14,statistics,54935,,Bayesian temporal coherence—how to model decreased certainty,https://www.reddit.com/r/statistics/comments/8g9rzb/bayesian_temporal_coherencehow_to_model_decreased/,all_ads,2018-05-01 20:30:13,32 days 05:05:21.036584000,
"Hi,

I'm attempting to understand why we divide by (n-1) in the formula for sample variance. I'm following along in this video: https://youtu.be/D1hgiAla3KI?t=110 until 1:51 when he substitutes the variance for Xbar as sigma^2/n. 

So, I go to look up why the variance of Xbar is sigma^2 /n. In a video by the same author I follow to this point (https://youtu.be/7mYDHbrLEQo?t=193) where he substitutes var(X1) for sigma^2. But wouldn't the variance of a single value require the use of the sample variance formula (and thus not equal sigma^2 )? I dont see why we can simply sub in sigma^2 for the variance of a single observation, especially when there are two different formulas for variance.

Sorry for the confusion question, I'm just really not wrapping my head around any of the proof out there for why we divide by (n-1). ",7,1525236977.0,8gbu7u,False,"Hi,

I'm attempting to understand why we divide by (n-1) in the formula for sample variance. I'm following along in this video: https://youtu.be/D1hgiAla3KI?t=110 until 1:51 when he substitutes the variance for Xbar as sigma^2/n. 

So, I go to look up why the variance of Xbar is sigma^2 /n. In a video by the same author I follow to this point (https://youtu.be/7mYDHbrLEQo?t=193) where he substitutes var(X1) for sigma^2. But wouldn't the variance of a single value require the use of the sample variance formula (and thus not equal sigma^2 )? I dont see why we can simply sub in sigma^2 for the variance of a single observation, especially when there are two different formulas for variance.

Sorry for the confusion question, I'm just really not wrapping my head around any of the proof out there for why we divide by (n-1). ",0,"Hi,

I'm attempting to understand why we divide by (n-1) in the formula for sample variance. I'm following along in this video: https://youtu.be/D1hgiAla3KI?t=110 until 1:51 when he substitutes the variance for Xbar as sigma^2/n. 

So, I go to look up why the variance of Xbar is sigma^2 /n. In a video by the same author I follow to this point (https://youtu.be/7mYDHbrLEQo?t=193) where he substitutes var(X1) for sigma^2. But wouldn't the variance of a single value require the use of the sample variance formula (and thus not equal sigma^2 )? I dont see why we can simply sub in sigma^2 for the variance of a single observation, especially when there are two different formulas for variance.

Sorry for the confusion question, I'm just really not wrapping my head around any of the proof out there for why we divide by (n-1). ",6,statistics,54935,,Sample variance confusion,https://www.reddit.com/r/statistics/comments/8gbu7u/sample_variance_confusion/,all_ads,2018-05-02 00:56:17,32 days 00:39:17.036584000,
"Im starting a PHD in statistics soon, and I have some spare time which I plan spend reading and getting to know some more fields inside statistics. I was wondering what do you consider important papers in each of your respected fields? 

",31,1525187809.0,8g6lhq,False,"Im starting a PHD in statistics soon, and I have some spare time which I plan spend reading and getting to know some more fields inside statistics. I was wondering what do you consider important papers in each of your respected fields? 

",0,"Im starting a PHD in statistics soon, and I have some spare time which I plan spend reading and getting to know some more fields inside statistics. I was wondering what do you consider important papers in each of your respected fields? 

",68,statistics,54935,,Important papers in statistics,https://www.reddit.com/r/statistics/comments/8g6lhq/important_papers_in_statistics/,all_ads,2018-05-01 11:16:49,32 days 14:18:45.036584000,
"I'm looking at fundamentals (income statement items) for a group of companies. The size of the sample is <30 (twenty-six) and I've taken a trimmed mean of 10% (because there are some huge outliers) so about 20 now. I looked at both statistics (mean and median) and they are close-ish but still some discrepancy. I've decided to take a weighted average 70% to the median and 30% to the mean. I'd just like to know what critiques there are against this measure. If you want to know why I chose those weights, it was a subjective judgment call based on how much I view their relevance - so not completely random but no hard science.",11,1525228644.0,8gare5,False,"I'm looking at fundamentals (income statement items) for a group of companies. The size of the sample is <30 (twenty-six) and I've taken a trimmed mean of 10% (because there are some huge outliers) so about 20 now. I looked at both statistics (mean and median) and they are close-ish but still some discrepancy. I've decided to take a weighted average 70% to the median and 30% to the mean. I'd just like to know what critiques there are against this measure. If you want to know why I chose those weights, it was a subjective judgment call based on how much I view their relevance - so not completely random but no hard science.",0,"I'm looking at fundamentals (income statement items) for a group of companies. The size of the sample is <30 (twenty-six) and I've taken a trimmed mean of 10% (because there are some huge outliers) so about 20 now. I looked at both statistics (mean and median) and they are close-ish but still some discrepancy. I've decided to take a weighted average 70% to the median and 30% to the mean. I'd just like to know what critiques there are against this measure. If you want to know why I chose those weights, it was a subjective judgment call based on how much I view their relevance - so not completely random but no hard science.",5,statistics,54935,,Average of the mean and median.,https://www.reddit.com/r/statistics/comments/8gare5/average_of_the_mean_and_median/,all_ads,2018-05-01 22:37:24,32 days 02:58:10.036584000,
I'm gonna graduate in a few months but I have a ton of free-time right now and I wouldn't mind making a few extra bucks.,7,1525224135.0,8ga6h2,False,I'm gonna graduate in a few months but I have a ton of free-time right now and I wouldn't mind making a few extra bucks.,0,I'm gonna graduate in a few months but I have a ton of free-time right now and I wouldn't mind making a few extra bucks.,4,statistics,54935,,Do you know of any online/remote jobs for Economics/Econometrics graduates?,https://www.reddit.com/r/statistics/comments/8ga6h2/do_you_know_of_any_onlineremote_jobs_for/,all_ads,2018-05-01 21:22:15,32 days 04:13:19.036584000,
I'm assuming it would be negative given the lack of any sort of relationship? ,0,1525234056.0,8gbh75,False,I'm assuming it would be negative given the lack of any sort of relationship? ,0,I'm assuming it would be negative given the lack of any sort of relationship? ,1,statistics,54935,,How will a scree plot look if factor analysis uses meaningless daya,https://www.reddit.com/r/statistics/comments/8gbh75/how_will_a_scree_plot_look_if_factor_analysis/,all_ads,2018-05-02 00:07:36,32 days 01:27:58.036584000,
"I am trying to write a paper dealing with instrumental variables.  My topic is the minimum wage in the U.S.  I understand that there is simultaneity bias between employment and the minimum wage, so I am trying to come up with an instrument for the minimum wage.  Does anyone have any ideas?  Also, is it feasible to do a cross-sectional study for this topic?  *Note:  I am also open to other suggestions for topics.  The key is that the paper must deal with IVs.
",4,1525232649.0,8gbahy,False,"I am trying to write a paper dealing with instrumental variables.  My topic is the minimum wage in the U.S.  I understand that there is simultaneity bias between employment and the minimum wage, so I am trying to come up with an instrument for the minimum wage.  Does anyone have any ideas?  Also, is it feasible to do a cross-sectional study for this topic?  *Note:  I am also open to other suggestions for topics.  The key is that the paper must deal with IVs.
",0,"I am trying to write a paper dealing with instrumental variables.  My topic is the minimum wage in the U.S.  I understand that there is simultaneity bias between employment and the minimum wage, so I am trying to come up with an instrument for the minimum wage.  Does anyone have any ideas?  Also, is it feasible to do a cross-sectional study for this topic?  *Note:  I am also open to other suggestions for topics.  The key is that the paper must deal with IVs.
",1,statistics,54935,,Paper dealing with Minimum Wage and Instrumental Variables,https://www.reddit.com/r/statistics/comments/8gbahy/paper_dealing_with_minimum_wage_and_instrumental/,all_ads,2018-05-01 23:44:09,32 days 01:51:25.036584000,
"I will be teaching intro stats this summer and I am looking for funny/interesting movies or TV shows clips that show statistics in the context of popular culture. 

Any ideas? Here is one example https://www.youtube.com/watch?v=_USeEzp_rrE   ""...you outlying piece of data!"" ",2,1525231881.0,8gb6hr,False,"I will be teaching intro stats this summer and I am looking for funny/interesting movies or TV shows clips that show statistics in the context of popular culture. 

Any ideas? Here is one example https://www.youtube.com/watch?v=_USeEzp_rrE   ""...you outlying piece of data!"" ",0,"I will be teaching intro stats this summer and I am looking for funny/interesting movies or TV shows clips that show statistics in the context of popular culture. 

Any ideas? Here is one example https://www.youtube.com/watch?v=_USeEzp_rrE   ""...you outlying piece of data!"" ",1,statistics,54935,,Statistics movie clips?,https://www.reddit.com/r/statistics/comments/8gb6hr/statistics_movie_clips/,all_ads,2018-05-01 23:31:21,32 days 02:04:13.036584000,
"I made a diagram to try and illustrate some thoughts that I have been working with.  
https://imgur.com/gallery/PVU2CyT

I believe there is equal merit in choosing either door and I tried to explain it in my diagram. Please take a look and correct me if I am dead wrong or give me some further ideas on how I could develop this idea. I appreciate your input as I am pretty much a laymen. laywomen? is there a female version of that word? ",111,1525254492.0,8gdsie,False,"I made a diagram to try and illustrate some thoughts that I have been working with.  
https://imgur.com/gallery/PVU2CyT

I believe there is equal merit in choosing either door and I tried to explain it in my diagram. Please take a look and correct me if I am dead wrong or give me some further ideas on how I could develop this idea. I appreciate your input as I am pretty much a laymen. laywomen? is there a female version of that word? ",0,"I made a diagram to try and illustrate some thoughts that I have been working with.  
https://imgur.com/gallery/PVU2CyT

I believe there is equal merit in choosing either door and I tried to explain it in my diagram. Please take a look and correct me if I am dead wrong or give me some further ideas on how I could develop this idea. I appreciate your input as I am pretty much a laymen. laywomen? is there a female version of that word? ",0,statistics,54935,,"Monty Hall Problem, a possible different approach?",https://www.reddit.com/r/statistics/comments/8gdsie/monty_hall_problem_a_possible_different_approach/,all_ads,2018-05-02 05:48:12,31 days 19:47:22.036584000,
,14,1525211008.0,8g8jk0,False,,0,,2,statistics,54935,,"Is it possible to break down R^2 into its component parts? e.g. if you have R^2 of 60% and three x variables, can you say 10% is from one, 20% from the other, 30% from the last?",https://www.reddit.com/r/statistics/comments/8g8jk0/is_it_possible_to_break_down_r2_into_its/,all_ads,2018-05-01 17:43:28,32 days 07:52:06.036584000,
"I have collected some data on a project that we did in my lab, and I am wondering if there is a good way to answer the question I have. To give a quick overview - participants were asked to look at a picture of someone who had interacted with paint, and were meant to score parts of the body (face, shoulder, arm, etc.) on 1-5 how much paint there was. In the end, we also have an average score for each participant for each picture (e.g. the average of all parts). I am interested in looking at if there are certain parts (face, shoulder...) that contribute more to these averages - e.g. if the average is a 4, but chest consistently has 1, and hands consistently have 4, hands are then on average more like the average. Basically, which areas are consistently closer to the mean, if that makes sense. 

Is there an analysis for this? I feel like I can't see the forest for the trees anymore on this project, anyone willing to give advice/bounce ideas is appreciated!",6,1525224359.0,8ga7i6,False,"I have collected some data on a project that we did in my lab, and I am wondering if there is a good way to answer the question I have. To give a quick overview - participants were asked to look at a picture of someone who had interacted with paint, and were meant to score parts of the body (face, shoulder, arm, etc.) on 1-5 how much paint there was. In the end, we also have an average score for each participant for each picture (e.g. the average of all parts). I am interested in looking at if there are certain parts (face, shoulder...) that contribute more to these averages - e.g. if the average is a 4, but chest consistently has 1, and hands consistently have 4, hands are then on average more like the average. Basically, which areas are consistently closer to the mean, if that makes sense. 

Is there an analysis for this? I feel like I can't see the forest for the trees anymore on this project, anyone willing to give advice/bounce ideas is appreciated!",0,"I have collected some data on a project that we did in my lab, and I am wondering if there is a good way to answer the question I have. To give a quick overview - participants were asked to look at a picture of someone who had interacted with paint, and were meant to score parts of the body (face, shoulder, arm, etc.) on 1-5 how much paint there was. In the end, we also have an average score for each participant for each picture (e.g. the average of all parts). I am interested in looking at if there are certain parts (face, shoulder...) that contribute more to these averages - e.g. if the average is a 4, but chest consistently has 1, and hands consistently have 4, hands are then on average more like the average. Basically, which areas are consistently closer to the mean, if that makes sense. 

Is there an analysis for this? I feel like I can't see the forest for the trees anymore on this project, anyone willing to give advice/bounce ideas is appreciated!",0,statistics,54935,,Looking for input on what analyses to run on these data.,https://www.reddit.com/r/statistics/comments/8ga7i6/looking_for_input_on_what_analyses_to_run_on/,all_ads,2018-05-01 21:25:59,32 days 04:09:35.036584000,
"Is it 2, or K+2?",0,1525232140.0,8gb7vm,False,"Is it 2, or K+2?",0,"Is it 2, or K+2?",0,statistics,54935,,How many dimensions are modeled by a multiple regression model?,https://www.reddit.com/r/statistics/comments/8gb7vm/how_many_dimensions_are_modeled_by_a_multiple/,all_ads,2018-05-01 23:35:40,32 days 01:59:54.036584000,
"Last semester, I took a grad level survey sampling course that was taught *horribly*... I don't think I came away learning much of anything.

I'd love to do some self study and was hoping for some recommendations on books or online resources that are accessible/readable and well suited for independent work. ",2,1525213131.0,8g8sdq,False,"Last semester, I took a grad level survey sampling course that was taught *horribly*... I don't think I came away learning much of anything.

I'd love to do some self study and was hoping for some recommendations on books or online resources that are accessible/readable and well suited for independent work. ",0,"Last semester, I took a grad level survey sampling course that was taught *horribly*... I don't think I came away learning much of anything.

I'd love to do some self study and was hoping for some recommendations on books or online resources that are accessible/readable and well suited for independent work. ",1,statistics,54935,,Accessible survey sampling (theory and/or methods) book recommendations or online resources,https://www.reddit.com/r/statistics/comments/8g8sdq/accessible_survey_sampling_theory_andor_methods/,all_ads,2018-05-01 18:18:51,32 days 07:16:43.036584000,
"In short, I have developed a survey aimed to measure trait-level factors (I'll leave out the details). I rationally devised 8 sub-scales with 5 items in each scale, and received feedback from colleagues about the items (e.g., wording, length, etc.). After preliminary testing, I ended up with 34 items (I cut one item from 6 of the sub-scales after reviewing my colleagues' feedback. So in order to verify that these items are measuring what I theorized, I need to conduct a Confirmatory Factor Analysis; however, I'm having difficultly figuring out how large of a sample I would need to conduct such an analysis. Generally, it seems like people say that you should collect 10 participants per item, so in my case 340 participants (i.e., 10 x 34), but the psychological literature seems to be a mixed bag of suggestions. Some say as long as you have 200 participants, some say as long as you have 300 participants, some say 5 per item, some say 10 or 20 per item. So is that it? There is no hard a fast rule, maybe just collect the 10 per item like I said, and then collect more if needed? If anyone has a resource/credible citation to go along with any suggestions, that would be unbelievable appreciated!

Thanks!",0,1525230747.0,8gb11g,False,"In short, I have developed a survey aimed to measure trait-level factors (I'll leave out the details). I rationally devised 8 sub-scales with 5 items in each scale, and received feedback from colleagues about the items (e.g., wording, length, etc.). After preliminary testing, I ended up with 34 items (I cut one item from 6 of the sub-scales after reviewing my colleagues' feedback. So in order to verify that these items are measuring what I theorized, I need to conduct a Confirmatory Factor Analysis; however, I'm having difficultly figuring out how large of a sample I would need to conduct such an analysis. Generally, it seems like people say that you should collect 10 participants per item, so in my case 340 participants (i.e., 10 x 34), but the psychological literature seems to be a mixed bag of suggestions. Some say as long as you have 200 participants, some say as long as you have 300 participants, some say 5 per item, some say 10 or 20 per item. So is that it? There is no hard a fast rule, maybe just collect the 10 per item like I said, and then collect more if needed? If anyone has a resource/credible citation to go along with any suggestions, that would be unbelievable appreciated!

Thanks!",0,"In short, I have developed a survey aimed to measure trait-level factors (I'll leave out the details). I rationally devised 8 sub-scales with 5 items in each scale, and received feedback from colleagues about the items (e.g., wording, length, etc.). After preliminary testing, I ended up with 34 items (I cut one item from 6 of the sub-scales after reviewing my colleagues' feedback. So in order to verify that these items are measuring what I theorized, I need to conduct a Confirmatory Factor Analysis; however, I'm having difficultly figuring out how large of a sample I would need to conduct such an analysis. Generally, it seems like people say that you should collect 10 participants per item, so in my case 340 participants (i.e., 10 x 34), but the psychological literature seems to be a mixed bag of suggestions. Some say as long as you have 200 participants, some say as long as you have 300 participants, some say 5 per item, some say 10 or 20 per item. So is that it? There is no hard a fast rule, maybe just collect the 10 per item like I said, and then collect more if needed? If anyone has a resource/credible citation to go along with any suggestions, that would be unbelievable appreciated!

Thanks!",0,statistics,54935,,How many participants are need for a Confirmatory Factor Analysis?,https://www.reddit.com/r/statistics/comments/8gb11g/how_many_participants_are_need_for_a_confirmatory/,all_ads,2018-05-01 23:12:27,32 days 02:23:07.036584000,
"Hi,

I am currently working on a database which requires me find a QTL and locate it on a chromosome using R. 
The database consists of information on Animal_ID, Sire, Dam, Sex, Days,Company, Slaughterweight, Intramuscular fat and information on 9 markers. 

Please note that the R/qtl packages don't work on the format of the database. 

I boiled it down to a general model without including the markers.
To fulfill the assumptions I did a log transformation on IMF.

LIMF ~ SEX + SWEIGHT + (1 | SIRE) + (1 | 
    DAM) + (1 | DAYS) + (1 | Company) + (1 | Company:SEX) +  MarkerX

In which MarkerX is the marker that is being tested.

Is this approach usable? Should I include all markers? Should I build a model per marker?
",1,1525208250.0,8g88si,False,"Hi,

I am currently working on a database which requires me find a QTL and locate it on a chromosome using R. 
The database consists of information on Animal_ID, Sire, Dam, Sex, Days,Company, Slaughterweight, Intramuscular fat and information on 9 markers. 

Please note that the R/qtl packages don't work on the format of the database. 

I boiled it down to a general model without including the markers.
To fulfill the assumptions I did a log transformation on IMF.

LIMF ~ SEX + SWEIGHT + (1 | SIRE) + (1 | 
    DAM) + (1 | DAYS) + (1 | Company) + (1 | Company:SEX) +  MarkerX

In which MarkerX is the marker that is being tested.

Is this approach usable? Should I include all markers? Should I build a model per marker?
",0,"Hi,

I am currently working on a database which requires me find a QTL and locate it on a chromosome using R. 
The database consists of information on Animal_ID, Sire, Dam, Sex, Days,Company, Slaughterweight, Intramuscular fat and information on 9 markers. 

Please note that the R/qtl packages don't work on the format of the database. 

I boiled it down to a general model without including the markers.
To fulfill the assumptions I did a log transformation on IMF.

LIMF ~ SEX + SWEIGHT + (1 | SIRE) + (1 | 
    DAM) + (1 | DAYS) + (1 | Company) + (1 | Company:SEX) +  MarkerX

In which MarkerX is the marker that is being tested.

Is this approach usable? Should I include all markers? Should I build a model per marker?
",1,statistics,54935,,QTL mapping,https://www.reddit.com/r/statistics/comments/8g88si/qtl_mapping/,all_ads,2018-05-01 16:57:30,32 days 08:38:04.036584000,
"I was considering using a GARCH\-model, but to my knowledge it only consider shock\-effects, and I want to look at how a period, of say a week or 14 days, with higher than usual trade volume affect the price\-volatility.

Would really love some help :\)

Thanks in advance",7,1525157903.0,8g3qbv,False,"I was considering using a GARCH\-model, but to my knowledge it only consider shock\-effects, and I want to look at how a period, of say a week or 14 days, with higher than usual trade volume affect the price\-volatility.

Would really love some help :\)

Thanks in advance",0,"I was considering using a GARCH\-model, but to my knowledge it only consider shock\-effects, and I want to look at how a period, of say a week or 14 days, with higher than usual trade volume affect the price\-volatility.

Would really love some help :\)

Thanks in advance",7,statistics,54935,,What is a good model for estimating how a period of high trade volume affect the volatility of an exchange rate?,https://www.reddit.com/r/statistics/comments/8g3qbv/what_is_a_good_model_for_estimating_how_a_period/,all_ads,2018-05-01 02:58:23,32 days 22:37:11.036584000,
"Is there a closed form for estimating confidence intervals for the mean when the data \~ Weibull?

I found a surprisingly  lack of information out there. Mostly just curious, all thoughts welcome. ",8,1525155429.0,8g3g1j,False,"Is there a closed form for estimating confidence intervals for the mean when the data \~ Weibull?

I found a surprisingly  lack of information out there. Mostly just curious, all thoughts welcome. ",0,"Is there a closed form for estimating confidence intervals for the mean when the data \~ Weibull?

I found a surprisingly  lack of information out there. Mostly just curious, all thoughts welcome. ",4,statistics,54935,,Is there a closed form for estimating confidence intervals for a mean when the data ~ Weibull?,https://www.reddit.com/r/statistics/comments/8g3g1j/is_there_a_closed_form_for_estimating_confidence/,all_ads,2018-05-01 02:17:09,32 days 23:18:25.036584000,
I need help to calculate degrees of freedom on a calculator if it’s unknown in a 2 sample T-test. I’m given both sample means and standard deviations. And the sample sizes are 13 and 19. The answer key says DF=19.44 I don’t know how to get that. ,7,1525177422.0,8g5sd1,False,I need help to calculate degrees of freedom on a calculator if it’s unknown in a 2 sample T-test. I’m given both sample means and standard deviations. And the sample sizes are 13 and 19. The answer key says DF=19.44 I don’t know how to get that. ,0,I need help to calculate degrees of freedom on a calculator if it’s unknown in a 2 sample T-test. I’m given both sample means and standard deviations. And the sample sizes are 13 and 19. The answer key says DF=19.44 I don’t know how to get that. ,1,statistics,54935,,How do I calculate degrees of freedom if it’s unknown?,https://www.reddit.com/r/statistics/comments/8g5sd1/how_do_i_calculate_degrees_of_freedom_if_its/,all_ads,2018-05-01 08:23:42,32 days 17:11:52.036584000,
"This is more semantics and logic than anything.  But models like least squared regression models and ARIMA models in time series all look to predict the mean of some data, given some variables.  

So basically all these models produce the conditional mean of the dependent variable?",10,1525131951.0,8g0exv,False,"This is more semantics and logic than anything.  But models like least squared regression models and ARIMA models in time series all look to predict the mean of some data, given some variables.  

So basically all these models produce the conditional mean of the dependent variable?",0,"This is more semantics and logic than anything.  But models like least squared regression models and ARIMA models in time series all look to predict the mean of some data, given some variables.  

So basically all these models produce the conditional mean of the dependent variable?",4,statistics,54935,,Most models describe the mean of a data series?,https://www.reddit.com/r/statistics/comments/8g0exv/most_models_describe_the_mean_of_a_data_series/,all_ads,2018-04-30 19:45:51,33 days 05:49:43.036584000,
"EDIT (MY MISTAKE ):

If I have 3 Independent variable (2 of them are within subject and one of them is between-subject)  and one dependent variable, what type of ANOVA this would be?

I am not sure if it is factorial ANOVA, can you call it between-subject factorial ANOVA ?
",7,1525105165.0,8fxxy9,False,"EDIT (MY MISTAKE ):

If I have 3 Independent variable (2 of them are within subject and one of them is between-subject)  and one dependent variable, what type of ANOVA this would be?

I am not sure if it is factorial ANOVA, can you call it between-subject factorial ANOVA ?
",0,"EDIT (MY MISTAKE ):

If I have 3 Independent variable (2 of them are within subject and one of them is between-subject)  and one dependent variable, what type of ANOVA this would be?

I am not sure if it is factorial ANOVA, can you call it between-subject factorial ANOVA ?
",11,statistics,54935,,What type of ANOVA is this?,https://www.reddit.com/r/statistics/comments/8fxxy9/what_type_of_anova_is_this/,all_ads,2018-04-30 12:19:25,33 days 13:16:09.036584000,
"It's a multiple regression study. R=0.82, adjusted r square is .003. For coefficients beta is -.062 and -.044 P <0.01 SIG is .142 and .298 P <0.01

I'm just confused. Haven't ran one of these in a while/ stats isn't a strong suit.",5,1525163051.0,8g4b6y,False,"It's a multiple regression study. R=0.82, adjusted r square is .003. For coefficients beta is -.062 and -.044 P <0.01 SIG is .142 and .298 P <0.01

I'm just confused. Haven't ran one of these in a while/ stats isn't a strong suit.",0,"It's a multiple regression study. R=0.82, adjusted r square is .003. For coefficients beta is -.062 and -.044 P <0.01 SIG is .142 and .298 P <0.01

I'm just confused. Haven't ran one of these in a while/ stats isn't a strong suit.",0,statistics,54935,,Would this be significant?,https://www.reddit.com/r/statistics/comments/8g4b6y/would_this_be_significant/,all_ads,2018-05-01 04:24:11,32 days 21:11:23.036584000,
"Dear statisticians,

I thought I understood PCA pretty well but after reading more about it on stat forums I'm getting more confused.

My understanding was that say if you have 10 variables (X1, ..., X10), going through the procedure of PCA you will get 10 eigenvalue/vectors. And those eigens will tell you which variables you can discard and still maintain a good portion of variance explained. So maybe in an example we want to retain x% of variance explained, where the value of x is a threshold chosen by the researcher. And PCA will tell the researcher to discard, perhaps X3 and X8, and the rest (X1,2,4,5,6,7,9,10) will provide x% of the original variance explained.

But after reading about it more, it sounds like PCA doesn't really ""discard"" variables. But rather create a new variable that is a linear combination of correlated variables.

Or perhaps this is also an incorrect understanding of PCA? Someone please help.

In addition, I would also like to know under what situation would a researcher choose PCA over FA, since both can be used for dimension reduction. I understand that FA can be used to measure unobservable variables. But under what circumstances would someone suspect there to be unobservable factors? Because theoretically can't you always argue there to be some underlying factors that can't be measured?

Thanks in advance.",41,1525065353.0,8fudnu,False,"Dear statisticians,

I thought I understood PCA pretty well but after reading more about it on stat forums I'm getting more confused.

My understanding was that say if you have 10 variables (X1, ..., X10), going through the procedure of PCA you will get 10 eigenvalue/vectors. And those eigens will tell you which variables you can discard and still maintain a good portion of variance explained. So maybe in an example we want to retain x% of variance explained, where the value of x is a threshold chosen by the researcher. And PCA will tell the researcher to discard, perhaps X3 and X8, and the rest (X1,2,4,5,6,7,9,10) will provide x% of the original variance explained.

But after reading about it more, it sounds like PCA doesn't really ""discard"" variables. But rather create a new variable that is a linear combination of correlated variables.

Or perhaps this is also an incorrect understanding of PCA? Someone please help.

In addition, I would also like to know under what situation would a researcher choose PCA over FA, since both can be used for dimension reduction. I understand that FA can be used to measure unobservable variables. But under what circumstances would someone suspect there to be unobservable factors? Because theoretically can't you always argue there to be some underlying factors that can't be measured?

Thanks in advance.",0,"Dear statisticians,

I thought I understood PCA pretty well but after reading more about it on stat forums I'm getting more confused.

My understanding was that say if you have 10 variables (X1, ..., X10), going through the procedure of PCA you will get 10 eigenvalue/vectors. And those eigens will tell you which variables you can discard and still maintain a good portion of variance explained. So maybe in an example we want to retain x% of variance explained, where the value of x is a threshold chosen by the researcher. And PCA will tell the researcher to discard, perhaps X3 and X8, and the rest (X1,2,4,5,6,7,9,10) will provide x% of the original variance explained.

But after reading about it more, it sounds like PCA doesn't really ""discard"" variables. But rather create a new variable that is a linear combination of correlated variables.

Or perhaps this is also an incorrect understanding of PCA? Someone please help.

In addition, I would also like to know under what situation would a researcher choose PCA over FA, since both can be used for dimension reduction. I understand that FA can be used to measure unobservable variables. But under what circumstances would someone suspect there to be unobservable factors? Because theoretically can't you always argue there to be some underlying factors that can't be measured?

Thanks in advance.",34,statistics,54935,,Can someone help me understand the idea behind PCA?,https://www.reddit.com/r/statistics/comments/8fudnu/can_someone_help_me_understand_the_idea_behind_pca/,all_ads,2018-04-30 01:15:53,34 days 00:19:41.036584000,
"I fitted a binary logit model with unbalanced data which were oversampled using SMOTE. This gave an excellent ROC curve but very poor adequacy - the zero hypothesis of adequacy was rejected by Hosmer-Lemeshow test and le Cessie – van Houwelingen – Copas – Hosmer unweighted sum of squares test. However, the logit model for the original data (without oversampling) had very good adequacy statistics (but mediocre classification properties).

Are there any ways to oversample data without ruining adequacy statistics for a binary logit/probit model? Thanks in advance.",0,1525111719.0,8fydnf,False,"I fitted a binary logit model with unbalanced data which were oversampled using SMOTE. This gave an excellent ROC curve but very poor adequacy - the zero hypothesis of adequacy was rejected by Hosmer-Lemeshow test and le Cessie – van Houwelingen – Copas – Hosmer unweighted sum of squares test. However, the logit model for the original data (without oversampling) had very good adequacy statistics (but mediocre classification properties).

Are there any ways to oversample data without ruining adequacy statistics for a binary logit/probit model? Thanks in advance.",0,"I fitted a binary logit model with unbalanced data which were oversampled using SMOTE. This gave an excellent ROC curve but very poor adequacy - the zero hypothesis of adequacy was rejected by Hosmer-Lemeshow test and le Cessie – van Houwelingen – Copas – Hosmer unweighted sum of squares test. However, the logit model for the original data (without oversampling) had very good adequacy statistics (but mediocre classification properties).

Are there any ways to oversample data without ruining adequacy statistics for a binary logit/probit model? Thanks in advance.",3,statistics,54935,,Adequacy of logit model with oversampled data,https://www.reddit.com/r/statistics/comments/8fydnf/adequacy_of_logit_model_with_oversampled_data/,all_ads,2018-04-30 14:08:39,33 days 11:26:55.036584000,
"I have a dataset of seasonally adjusted quarterly GDP growth figures for a country. Now, I want to check to structural breaks in the data due to tax increases. I have run tests ACF, PACF, kpss and adf on my numbers, and charted the time series in R [see images](https://imgur.com/a/l0IYQpZ). But my PACF and ACF figures look reverse? What does this tell me, and can someone please help me construct an ARMA from this? Any help is appreciated!",7,1525109625.0,8fy8bb,False,"I have a dataset of seasonally adjusted quarterly GDP growth figures for a country. Now, I want to check to structural breaks in the data due to tax increases. I have run tests ACF, PACF, kpss and adf on my numbers, and charted the time series in R [see images](https://imgur.com/a/l0IYQpZ). But my PACF and ACF figures look reverse? What does this tell me, and can someone please help me construct an ARMA from this? Any help is appreciated!",0,"I have a dataset of seasonally adjusted quarterly GDP growth figures for a country. Now, I want to check to structural breaks in the data due to tax increases. I have run tests ACF, PACF, kpss and adf on my numbers, and charted the time series in R [see images](https://imgur.com/a/l0IYQpZ). But my PACF and ACF figures look reverse? What does this tell me, and can someone please help me construct an ARMA from this? Any help is appreciated!",3,statistics,54935,,"I want to use an ARMA model, can you help me understand my statistics?",https://www.reddit.com/r/statistics/comments/8fy8bb/i_want_to_use_an_arma_model_can_you_help_me/,all_ads,2018-04-30 13:33:45,33 days 12:01:49.036584000,
"I'm running some RM MANOVAs and for assumptions, I can find info on RM ANOVAS or two-way MANOVAs but not for RM MANOVAs. I don't want to be testing assumptions that are irrelevant and also don't want to incorrectly test relevant assumptions. I am using Laerd Statistics for assistance but they do not include RM MANOVAs so I feel stuck. Any help would be appreciated! ",1,1525082805.0,8fw6a3,False,"I'm running some RM MANOVAs and for assumptions, I can find info on RM ANOVAS or two-way MANOVAs but not for RM MANOVAs. I don't want to be testing assumptions that are irrelevant and also don't want to incorrectly test relevant assumptions. I am using Laerd Statistics for assistance but they do not include RM MANOVAs so I feel stuck. Any help would be appreciated! ",0,"I'm running some RM MANOVAs and for assumptions, I can find info on RM ANOVAS or two-way MANOVAs but not for RM MANOVAs. I don't want to be testing assumptions that are irrelevant and also don't want to incorrectly test relevant assumptions. I am using Laerd Statistics for assistance but they do not include RM MANOVAs so I feel stuck. Any help would be appreciated! ",3,statistics,54935,,RM MANOVA assumptions,https://www.reddit.com/r/statistics/comments/8fw6a3/rm_manova_assumptions/,all_ads,2018-04-30 06:06:45,33 days 19:28:49.036584000,
"Hey, redditors. I am a student who's currently writing his Bachelor's thesis, and I need some help with my Fixed Effects model. 

Is there any way to take care of heteroskedasticity and autocorrelation in GRETL software? I found this discussion (https://www.statalist.org/forums/forum/general-stata-discussion/general/833393-how-to-correct-for-heteroscedasticity-and-autocorrelation-in-the-same-regression-command-in-a-fixed-effects-panel-data-model), yet it concerns STATA, and I probably can't do the same in GRETL.

Another way might be to simply use robust standard errors. However, this concerns only the heteroskedasticity, and still does not alleviate it altogether, if I'm not mistaken.

Thanks for your help.",1,1525101290.0,8fxord,False,"Hey, redditors. I am a student who's currently writing his Bachelor's thesis, and I need some help with my Fixed Effects model. 

Is there any way to take care of heteroskedasticity and autocorrelation in GRETL software? I found this discussion (https://www.statalist.org/forums/forum/general-stata-discussion/general/833393-how-to-correct-for-heteroscedasticity-and-autocorrelation-in-the-same-regression-command-in-a-fixed-effects-panel-data-model), yet it concerns STATA, and I probably can't do the same in GRETL.

Another way might be to simply use robust standard errors. However, this concerns only the heteroskedasticity, and still does not alleviate it altogether, if I'm not mistaken.

Thanks for your help.",0,"Hey, redditors. I am a student who's currently writing his Bachelor's thesis, and I need some help with my Fixed Effects model. 

Is there any way to take care of heteroskedasticity and autocorrelation in GRETL software? I found this discussion (https://www.statalist.org/forums/forum/general-stata-discussion/general/833393-how-to-correct-for-heteroscedasticity-and-autocorrelation-in-the-same-regression-command-in-a-fixed-effects-panel-data-model), yet it concerns STATA, and I probably can't do the same in GRETL.

Another way might be to simply use robust standard errors. However, this concerns only the heteroskedasticity, and still does not alleviate it altogether, if I'm not mistaken.

Thanks for your help.",1,statistics,54935,,Any users of GRETL? Need help with my Fixed Effects model,https://www.reddit.com/r/statistics/comments/8fxord/any_users_of_gretl_need_help_with_my_fixed/,all_ads,2018-04-30 11:14:50,33 days 14:20:44.036584000,
"Hello. Recently, I was working with the Poisson distribution to gain insight on hockey goalie average allowed goals per game.

In my findings, a goalie with an average of 2.6 goals allowed per game had a 25% chance of allowing exactly 2 goals in 1 game. Oddly enough, a goalie with an average of 2.9 had a 23% chance of allowing exactly 2 goals in 1 game. Why does the goalie with the higher average have a smaller chance of allowing 2 goals? I would think it would be the other way around.",10,1525052051.0,8fsuez,False,"Hello. Recently, I was working with the Poisson distribution to gain insight on hockey goalie average allowed goals per game.

In my findings, a goalie with an average of 2.6 goals allowed per game had a 25% chance of allowing exactly 2 goals in 1 game. Oddly enough, a goalie with an average of 2.9 had a 23% chance of allowing exactly 2 goals in 1 game. Why does the goalie with the higher average have a smaller chance of allowing 2 goals? I would think it would be the other way around.",0,"Hello. Recently, I was working with the Poisson distribution to gain insight on hockey goalie average allowed goals per game.

In my findings, a goalie with an average of 2.6 goals allowed per game had a 25% chance of allowing exactly 2 goals in 1 game. Oddly enough, a goalie with an average of 2.9 had a 23% chance of allowing exactly 2 goals in 1 game. Why does the goalie with the higher average have a smaller chance of allowing 2 goals? I would think it would be the other way around.",11,statistics,54935,,Poisson Distribution Confusion,https://www.reddit.com/r/statistics/comments/8fsuez/poisson_distribution_confusion/,all_ads,2018-04-29 21:34:11,34 days 04:01:23.036584000,
"hello friends ... I'm new here & I actually did spend a fair amount of time admiring all the work that's been put into your sub , very awesome !

I came searching for a answer or formula to prove/disprove a probablity/statistics situation.

in a nutshell -- to simplify this, I have 9 cards ... I choose one card per round and it's never the winning one ... math and logic would dictate about 10% of the time I SHOULD pick the (1) winning card, however this is a simulated environment (and simplified example) but hundreds of others say the same happens for them .... they never pick the ""right"" card out of the 9 

how can I test for actual probablity if I log my attempts / fails 
we would all be curious how bad these odds actually are !

thanks so much for reading! 

any info/help would be wonderful  :) ",3,1525113621.0,8fyima,False,"hello friends ... I'm new here & I actually did spend a fair amount of time admiring all the work that's been put into your sub , very awesome !

I came searching for a answer or formula to prove/disprove a probablity/statistics situation.

in a nutshell -- to simplify this, I have 9 cards ... I choose one card per round and it's never the winning one ... math and logic would dictate about 10% of the time I SHOULD pick the (1) winning card, however this is a simulated environment (and simplified example) but hundreds of others say the same happens for them .... they never pick the ""right"" card out of the 9 

how can I test for actual probablity if I log my attempts / fails 
we would all be curious how bad these odds actually are !

thanks so much for reading! 

any info/help would be wonderful  :) ",0,"hello friends ... I'm new here & I actually did spend a fair amount of time admiring all the work that's been put into your sub , very awesome !

I came searching for a answer or formula to prove/disprove a probablity/statistics situation.

in a nutshell -- to simplify this, I have 9 cards ... I choose one card per round and it's never the winning one ... math and logic would dictate about 10% of the time I SHOULD pick the (1) winning card, however this is a simulated environment (and simplified example) but hundreds of others say the same happens for them .... they never pick the ""right"" card out of the 9 

how can I test for actual probablity if I log my attempts / fails 
we would all be curious how bad these odds actually are !

thanks so much for reading! 

any info/help would be wonderful  :) ",0,statistics,54935,,little help please :),https://www.reddit.com/r/statistics/comments/8fyima/little_help_please/,all_ads,2018-04-30 14:40:21,33 days 10:55:13.036584000,
"The assumption is that after monty has opened a door, you are left with a 2/3 chance that the other door in that set is a 2/3 chance to be the prize. 
this is wrong. 
it is a very simple matter of recognising that 1/3 of the opened door has been removed from the situation. 
2/3 - 1/3 = 1/3. 
the statistical probability of that door is still 1/3 of the original choice because the third door is no longer part of that (doors you didn't decide). that door is now part of a different (doors that monty removed) 


in order to see the maths of what happens I tried this...

* initial choice 3 doors = 1
* each door = 0.33/1
* you pick a door  = 0.33/1
* other 2 doors = 0.66/1
* monty removes 1 door, = 0.33/1
* new choice 2 doors = 0.66
* each remaining door = 0.33/0.66

 I'm not trained in maths or statistics, but I've written it the best I could. 

Edit:
never mind, I was taking this as an isolated situation. after looking into this some more, I realise the  most important factor here is 'statistics'. it is statistically more probable that it will be in the other door, because out of 3 possible occasions, the car is going to be in the door that the player didn't choose more times. 
monty, by opening the door, has give you a bigger chance that the other door will be correct. I get it now. 

but in an isolated situation, statistics isn't helpful. people beat the statistics all the time.  

second edit: 
I made a diagram going further with my thoughts. you can view it here - https://www.reddit.com/r/statistics/comments/8gdsie/monty_hall_problem_a_possible_different_approach/",188,1525148588.0,8g2l17,False,"The assumption is that after monty has opened a door, you are left with a 2/3 chance that the other door in that set is a 2/3 chance to be the prize. 
this is wrong. 
it is a very simple matter of recognising that 1/3 of the opened door has been removed from the situation. 
2/3 - 1/3 = 1/3. 
the statistical probability of that door is still 1/3 of the original choice because the third door is no longer part of that (doors you didn't decide). that door is now part of a different (doors that monty removed) 


in order to see the maths of what happens I tried this...

* initial choice 3 doors = 1
* each door = 0.33/1
* you pick a door  = 0.33/1
* other 2 doors = 0.66/1
* monty removes 1 door, = 0.33/1
* new choice 2 doors = 0.66
* each remaining door = 0.33/0.66

 I'm not trained in maths or statistics, but I've written it the best I could. 

Edit:
never mind, I was taking this as an isolated situation. after looking into this some more, I realise the  most important factor here is 'statistics'. it is statistically more probable that it will be in the other door, because out of 3 possible occasions, the car is going to be in the door that the player didn't choose more times. 
monty, by opening the door, has give you a bigger chance that the other door will be correct. I get it now. 

but in an isolated situation, statistics isn't helpful. people beat the statistics all the time.  

second edit: 
I made a diagram going further with my thoughts. you can view it here - https://www.reddit.com/r/statistics/comments/8gdsie/monty_hall_problem_a_possible_different_approach/",0,"The assumption is that after monty has opened a door, you are left with a 2/3 chance that the other door in that set is a 2/3 chance to be the prize. 
this is wrong. 
it is a very simple matter of recognising that 1/3 of the opened door has been removed from the situation. 
2/3 - 1/3 = 1/3. 
the statistical probability of that door is still 1/3 of the original choice because the third door is no longer part of that (doors you didn't decide). that door is now part of a different (doors that monty removed) 


in order to see the maths of what happens I tried this...

* initial choice 3 doors = 1
* each door = 0.33/1
* you pick a door  = 0.33/1
* other 2 doors = 0.66/1
* monty removes 1 door, = 0.33/1
* new choice 2 doors = 0.66
* each remaining door = 0.33/0.66

 I'm not trained in maths or statistics, but I've written it the best I could. 

Edit:
never mind, I was taking this as an isolated situation. after looking into this some more, I realise the  most important factor here is 'statistics'. it is statistically more probable that it will be in the other door, because out of 3 possible occasions, the car is going to be in the door that the player didn't choose more times. 
monty, by opening the door, has give you a bigger chance that the other door will be correct. I get it now. 

but in an isolated situation, statistics isn't helpful. people beat the statistics all the time.  

second edit: 
I made a diagram going further with my thoughts. you can view it here - https://www.reddit.com/r/statistics/comments/8gdsie/monty_hall_problem_a_possible_different_approach/",0,statistics,54935,,The Monty Hall Paradox is a falsidical paradox.,https://www.reddit.com/r/statistics/comments/8g2l17/the_monty_hall_paradox_is_a_falsidical_paradox/,all_ads,2018-05-01 00:23:08,33 days 01:12:26.036584000,
This may be a dumb question but I can’t seem to find an answer. I also apologize if this has been asked before.,8,1525093083.0,8fx399,False,This may be a dumb question but I can’t seem to find an answer. I also apologize if this has been asked before.,0,This may be a dumb question but I can’t seem to find an answer. I also apologize if this has been asked before.,0,statistics,54935,,"What are the odds you win Rock, Paper, Scissors?",https://www.reddit.com/r/statistics/comments/8fx399/what_are_the_odds_you_win_rock_paper_scissors/,all_ads,2018-04-30 08:58:03,33 days 16:37:31.036584000,
"I have a sample that I split in half. I argue that the top subsample will be a convex that will be higher on the plot than the bottom subsample. I run 2 OLS like this:

y= a+ bx +cx^2 +other variables.

How will I compare them, only by comparing b and c  that show the convexity or should i also check the coeffiecients of the other variables. ( do i just assume in order to compare them that the other variables are the same??)  AND is there any software that it can plot both of them on the same graph?

Thank you in advance.",8,1525069804.0,8fuv50,False,"I have a sample that I split in half. I argue that the top subsample will be a convex that will be higher on the plot than the bottom subsample. I run 2 OLS like this:

y= a+ bx +cx^2 +other variables.

How will I compare them, only by comparing b and c  that show the convexity or should i also check the coeffiecients of the other variables. ( do i just assume in order to compare them that the other variables are the same??)  AND is there any software that it can plot both of them on the same graph?

Thank you in advance.",0,"I have a sample that I split in half. I argue that the top subsample will be a convex that will be higher on the plot than the bottom subsample. I run 2 OLS like this:

y= a+ bx +cx^2 +other variables.

How will I compare them, only by comparing b and c  that show the convexity or should i also check the coeffiecients of the other variables. ( do i just assume in order to compare them that the other variables are the same??)  AND is there any software that it can plot both of them on the same graph?

Thank you in advance.",1,statistics,54935,,Compare 2 OLS regressions.,https://www.reddit.com/r/statistics/comments/8fuv50/compare_2_ols_regressions/,all_ads,2018-04-30 02:30:04,33 days 23:05:30.036584000,
"When you are doing the adjusting analysis, does finding effect modification mean that you will have to first find effect of confounding first? So effect modification is a subset of confounding?

So before you know anything wrong with your subset, you first do analysis and you find an effect of confounding. After you find an effect of confounding, then you see, o, it's actually stratified by age, so there is effect for 50y/o+ but not for those <50y/o

also, could a variable be both a confounding variable and an effect modifier? 

Is that understanding correct?

Thanks!",7,1525013268.0,8fps51,False,"When you are doing the adjusting analysis, does finding effect modification mean that you will have to first find effect of confounding first? So effect modification is a subset of confounding?

So before you know anything wrong with your subset, you first do analysis and you find an effect of confounding. After you find an effect of confounding, then you see, o, it's actually stratified by age, so there is effect for 50y/o+ but not for those <50y/o

also, could a variable be both a confounding variable and an effect modifier? 

Is that understanding correct?

Thanks!",0,"When you are doing the adjusting analysis, does finding effect modification mean that you will have to first find effect of confounding first? So effect modification is a subset of confounding?

So before you know anything wrong with your subset, you first do analysis and you find an effect of confounding. After you find an effect of confounding, then you see, o, it's actually stratified by age, so there is effect for 50y/o+ but not for those <50y/o

also, could a variable be both a confounding variable and an effect modifier? 

Is that understanding correct?

Thanks!",11,statistics,54935,,Confounding v Effect Modification,https://www.reddit.com/r/statistics/comments/8fps51/confounding_v_effect_modification/,all_ads,2018-04-29 10:47:48,34 days 14:47:46.036584000,
"As I've started to become more and more familiar with Bayesian statistics, I've noticed how easy it is for people (including myself) to try to leverage frequentist concepts (with which many of us are most familiar) to help with understanding Bayesian inference and, therefore, causing a great deal of confusion. In trying to understand all this stuff more, I came across these two articles and they really helped me, so I thought I'd share them in case anyone else finds the topic interesting. 

Enjoy!

[Tuning your priors to the world](https://doi.org/10.1111/tops.12003) (Feldman, 2013)

[What are the ""true"" statistics of the environment?](https://doi.org/10.1111/cogs.12444) (Feldman, 2016)",1,1524957902.0,8fkk68,False,"As I've started to become more and more familiar with Bayesian statistics, I've noticed how easy it is for people (including myself) to try to leverage frequentist concepts (with which many of us are most familiar) to help with understanding Bayesian inference and, therefore, causing a great deal of confusion. In trying to understand all this stuff more, I came across these two articles and they really helped me, so I thought I'd share them in case anyone else finds the topic interesting. 

Enjoy!

[Tuning your priors to the world](https://doi.org/10.1111/tops.12003) (Feldman, 2013)

[What are the ""true"" statistics of the environment?](https://doi.org/10.1111/cogs.12444) (Feldman, 2016)",0,"As I've started to become more and more familiar with Bayesian statistics, I've noticed how easy it is for people (including myself) to try to leverage frequentist concepts (with which many of us are most familiar) to help with understanding Bayesian inference and, therefore, causing a great deal of confusion. In trying to understand all this stuff more, I came across these two articles and they really helped me, so I thought I'd share them in case anyone else finds the topic interesting. 

Enjoy!

[Tuning your priors to the world](https://doi.org/10.1111/tops.12003) (Feldman, 2013)

[What are the ""true"" statistics of the environment?](https://doi.org/10.1111/cogs.12444) (Feldman, 2016)",74,statistics,54935,,Some helpful articles that help to clarify the Bayesian definition of probability and inference.,https://www.reddit.com/r/statistics/comments/8fkk68/some_helpful_articles_that_help_to_clarify_the/,all_ads,2018-04-28 19:25:02,35 days 06:10:32.036584000,
"I am a current high school senior, set to graduate (valedictorian!) in less than a month. I took AP Statistics this year and I really, really enjoyed it! I'm not sure why but something about it just interested me.

I will be attending St. Olaf College this fall, and I was planning on majoring in biology on a pre-med track, but lately I've been thinking about going into the field of statistics. It seems like something I would be much more interested in. I have a lot of uncertainty about it though (well, about my future in general).

So, my basic questions:

1. What types of jobs are available to one who goes into statistics, and beyond a general overview, what are they really, truly like?

2. St. Olaf offers a statistics concentration (basically a minor), but not as a major. Would this be an issue if I were to decide to go into statistics? Perhaps I could be a math major with a stats concentration and that would be similar?


Regarding the second question; I chose to attend St. Olaf because they offered me a full ride, and I come from a family with basically no income and no savings. Transferring schools would no doubt be a mess logistics-wise and I'm sure I would not receive as much financial aid in doing so. Even so, would it be unavoidable?

Thank you very much in advance!",17,1524978377.0,8fmr5k,False,"I am a current high school senior, set to graduate (valedictorian!) in less than a month. I took AP Statistics this year and I really, really enjoyed it! I'm not sure why but something about it just interested me.

I will be attending St. Olaf College this fall, and I was planning on majoring in biology on a pre-med track, but lately I've been thinking about going into the field of statistics. It seems like something I would be much more interested in. I have a lot of uncertainty about it though (well, about my future in general).

So, my basic questions:

1. What types of jobs are available to one who goes into statistics, and beyond a general overview, what are they really, truly like?

2. St. Olaf offers a statistics concentration (basically a minor), but not as a major. Would this be an issue if I were to decide to go into statistics? Perhaps I could be a math major with a stats concentration and that would be similar?


Regarding the second question; I chose to attend St. Olaf because they offered me a full ride, and I come from a family with basically no income and no savings. Transferring schools would no doubt be a mess logistics-wise and I'm sure I would not receive as much financial aid in doing so. Even so, would it be unavoidable?

Thank you very much in advance!",0,"I am a current high school senior, set to graduate (valedictorian!) in less than a month. I took AP Statistics this year and I really, really enjoyed it! I'm not sure why but something about it just interested me.

I will be attending St. Olaf College this fall, and I was planning on majoring in biology on a pre-med track, but lately I've been thinking about going into the field of statistics. It seems like something I would be much more interested in. I have a lot of uncertainty about it though (well, about my future in general).

So, my basic questions:

1. What types of jobs are available to one who goes into statistics, and beyond a general overview, what are they really, truly like?

2. St. Olaf offers a statistics concentration (basically a minor), but not as a major. Would this be an issue if I were to decide to go into statistics? Perhaps I could be a math major with a stats concentration and that would be similar?


Regarding the second question; I chose to attend St. Olaf because they offered me a full ride, and I come from a family with basically no income and no savings. Transferring schools would no doubt be a mess logistics-wise and I'm sure I would not receive as much financial aid in doing so. Even so, would it be unavoidable?

Thank you very much in advance!",3,statistics,54935,,Going into statistics: a good idea?,https://www.reddit.com/r/statistics/comments/8fmr5k/going_into_statistics_a_good_idea/,all_ads,2018-04-29 01:06:17,35 days 00:29:17.036584000,
"I stumbled upon a Coursera syllabus that includes some topics (below) that I'd really like to get to know. I want to get a lot of experience in these topics to get better at analyzing data as well as getting some tangential stats background as I learn Machine Learning. Can anyone recommend some online resources/textbooks that will give me some practical experience with the below topics as well as some more in depth knowledge? Thanks!


Appetite Whetting: Bad Science

Hypothesis Testing

Significance Tests and P-Values
Example: Difference of Means

Deriving the Sampling Distribution

Shuffle Test for Significance

Comparing Classical and Resampling Methods

Bootstrap

Resampling Caveats

Outliers and Rank Transformation
Example: Chi-Squared Test

Bad Science Revisited: Publication Bias

Effect Size

Meta-analysis

Fraud and Benford's Law

Intuition for Benford's Law

Benford's Law Explained Visually

Multiple Hypothesis Testing: Bonferroni and Sidak Corrections

Multiple Hypothesis Testing: False Discovery Rate

Multiple Hypothesis Testing: Benjamini-

Hochberg Procedure

Big Data and Spurious Correlations

Spurious Correlations: Stock Price Example

How is Big Data Different?

Bayesian vs. Frequentist

Motivation for Bayesian Approaches

Bayes' Theorem

Applying Bayes' Theorem

Naive Bayes: Spam Filtering


Edit: Formatting",4,1525004210.0,8fp4oq,False,"I stumbled upon a Coursera syllabus that includes some topics (below) that I'd really like to get to know. I want to get a lot of experience in these topics to get better at analyzing data as well as getting some tangential stats background as I learn Machine Learning. Can anyone recommend some online resources/textbooks that will give me some practical experience with the below topics as well as some more in depth knowledge? Thanks!


Appetite Whetting: Bad Science

Hypothesis Testing

Significance Tests and P-Values
Example: Difference of Means

Deriving the Sampling Distribution

Shuffle Test for Significance

Comparing Classical and Resampling Methods

Bootstrap

Resampling Caveats

Outliers and Rank Transformation
Example: Chi-Squared Test

Bad Science Revisited: Publication Bias

Effect Size

Meta-analysis

Fraud and Benford's Law

Intuition for Benford's Law

Benford's Law Explained Visually

Multiple Hypothesis Testing: Bonferroni and Sidak Corrections

Multiple Hypothesis Testing: False Discovery Rate

Multiple Hypothesis Testing: Benjamini-

Hochberg Procedure

Big Data and Spurious Correlations

Spurious Correlations: Stock Price Example

How is Big Data Different?

Bayesian vs. Frequentist

Motivation for Bayesian Approaches

Bayes' Theorem

Applying Bayes' Theorem

Naive Bayes: Spam Filtering


Edit: Formatting",0,"I stumbled upon a Coursera syllabus that includes some topics (below) that I'd really like to get to know. I want to get a lot of experience in these topics to get better at analyzing data as well as getting some tangential stats background as I learn Machine Learning. Can anyone recommend some online resources/textbooks that will give me some practical experience with the below topics as well as some more in depth knowledge? Thanks!


Appetite Whetting: Bad Science

Hypothesis Testing

Significance Tests and P-Values
Example: Difference of Means

Deriving the Sampling Distribution

Shuffle Test for Significance

Comparing Classical and Resampling Methods

Bootstrap

Resampling Caveats

Outliers and Rank Transformation
Example: Chi-Squared Test

Bad Science Revisited: Publication Bias

Effect Size

Meta-analysis

Fraud and Benford's Law

Intuition for Benford's Law

Benford's Law Explained Visually

Multiple Hypothesis Testing: Bonferroni and Sidak Corrections

Multiple Hypothesis Testing: False Discovery Rate

Multiple Hypothesis Testing: Benjamini-

Hochberg Procedure

Big Data and Spurious Correlations

Spurious Correlations: Stock Price Example

How is Big Data Different?

Bayesian vs. Frequentist

Motivation for Bayesian Approaches

Bayes' Theorem

Applying Bayes' Theorem

Naive Bayes: Spam Filtering


Edit: Formatting",1,statistics,54935,,Books/Resources Similar to List Below?,https://www.reddit.com/r/statistics/comments/8fp4oq/booksresources_similar_to_list_below/,all_ads,2018-04-29 08:16:50,34 days 17:18:44.036584000,
"Sup Statistic Bros.

I've got two sets of data that looks like this: https://puu.sh/Ace3S/0a871f1655.png

I need to quantifiably compare them. How would one do this? Gracias amigos. ",9,1524977019.0,8fmm3r,False,"Sup Statistic Bros.

I've got two sets of data that looks like this: https://puu.sh/Ace3S/0a871f1655.png

I need to quantifiably compare them. How would one do this? Gracias amigos. ",0,"Sup Statistic Bros.

I've got two sets of data that looks like this: https://puu.sh/Ace3S/0a871f1655.png

I need to quantifiably compare them. How would one do this? Gracias amigos. ",1,statistics,54935,,Comparing two data sets,https://www.reddit.com/r/statistics/comments/8fmm3r/comparing_two_data_sets/,all_ads,2018-04-29 00:43:39,35 days 00:51:55.036584000,
"I run a regression, with sample size n=114, with probit and logit models. The classification test results give identical sensitivity, specificity and correctly classified values for both models. Is this likely due to the small sample size? thanks.",2,1524972079.0,8fm3cv,False,"I run a regression, with sample size n=114, with probit and logit models. The classification test results give identical sensitivity, specificity and correctly classified values for both models. Is this likely due to the small sample size? thanks.",0,"I run a regression, with sample size n=114, with probit and logit models. The classification test results give identical sensitivity, specificity and correctly classified values for both models. Is this likely due to the small sample size? thanks.",1,statistics,54935,,Quick question regarding probit and logit classification tests.,https://www.reddit.com/r/statistics/comments/8fm3cv/quick_question_regarding_probit_and_logit/,all_ads,2018-04-28 23:21:19,35 days 02:14:15.036584000,
"I've got a new project and some new data available to me, but for industry-specific reasons I can use only aggregate data. All of the statistics I know is based on observation-level data so I'm looking for references and techniques for aggregate data.

My degree is in math with a focus in prob and stats so I can read something (a bit) technical, but I'd prefer material geared towards applications since this is for work and not research.

Thanks!",6,1524970201.0,8flw34,False,"I've got a new project and some new data available to me, but for industry-specific reasons I can use only aggregate data. All of the statistics I know is based on observation-level data so I'm looking for references and techniques for aggregate data.

My degree is in math with a focus in prob and stats so I can read something (a bit) technical, but I'd prefer material geared towards applications since this is for work and not research.

Thanks!",0,"I've got a new project and some new data available to me, but for industry-specific reasons I can use only aggregate data. All of the statistics I know is based on observation-level data so I'm looking for references and techniques for aggregate data.

My degree is in math with a focus in prob and stats so I can read something (a bit) technical, but I'd prefer material geared towards applications since this is for work and not research.

Thanks!",1,statistics,54935,,References for analysis of aggregate data,https://www.reddit.com/r/statistics/comments/8flw34/references_for_analysis_of_aggregate_data/,all_ads,2018-04-28 22:50:01,35 days 02:45:33.036584000,
"Hi all! As many students taking ap stat will know, the ap test for statistics is coming up soon. My class just started reviewing, but there are two things I never understood from the beginning of that class: block design and matched pairs design. My teacher has tried explaining them to me, but I still don't understand them.
Could someone give me a rundown of those two designs? I really don't understand them and I feel lost. If you could also give examples that would be awesome as well. Thank you!",5,1524953370.0,8fk3od,False,"Hi all! As many students taking ap stat will know, the ap test for statistics is coming up soon. My class just started reviewing, but there are two things I never understood from the beginning of that class: block design and matched pairs design. My teacher has tried explaining them to me, but I still don't understand them.
Could someone give me a rundown of those two designs? I really don't understand them and I feel lost. If you could also give examples that would be awesome as well. Thank you!",0,"Hi all! As many students taking ap stat will know, the ap test for statistics is coming up soon. My class just started reviewing, but there are two things I never understood from the beginning of that class: block design and matched pairs design. My teacher has tried explaining them to me, but I still don't understand them.
Could someone give me a rundown of those two designs? I really don't understand them and I feel lost. If you could also give examples that would be awesome as well. Thank you!",2,statistics,54935,,Help With Understanding Block design and matched pairs,https://www.reddit.com/r/statistics/comments/8fk3od/help_with_understanding_block_design_and_matched/,all_ads,2018-04-28 18:09:30,35 days 07:26:04.036584000,
"Q: https://imgur.com/a/P7ZU4sN

Hi, I am a bit stuck with this question. I can't figure out what topic this question is on, so I don't really know what approach to use... 
Any hints?

Thanks in advance 
",3,1524975971.0,8fmi64,False,"Q: https://imgur.com/a/P7ZU4sN

Hi, I am a bit stuck with this question. I can't figure out what topic this question is on, so I don't really know what approach to use... 
Any hints?

Thanks in advance 
",0,"Q: https://imgur.com/a/P7ZU4sN

Hi, I am a bit stuck with this question. I can't figure out what topic this question is on, so I don't really know what approach to use... 
Any hints?

Thanks in advance 
",0,statistics,54935,,Statistics question,https://www.reddit.com/r/statistics/comments/8fmi64/statistics_question/,all_ads,2018-04-29 00:26:11,35 days 01:09:23.036584000,
"I have a law degree (and History BA) but do not work as a lawyer.  I work at an academic center at a large law school for a group of professors that do empirical research.  I have taught myself some Python which has helped with some basic stuff, data crunching and visualization, etc.

But I find the big missing piece in my knowledge is Statistics.  For example a professor mentioned wanting to do linear regressions and I had to look that up.  

Is anyone working in or familiar with fields with non-Statistics domain knowledge but where some statistics knowledge is necessary?  Such as legal empirical research or other types of empirical research in social sciences, accounting, etc.

Would I be able to self-study through MOOCs and gain enough statistics knowledge for such areas?  Should I look into actually doing some university coursework or even pursuing a Masters degree?

I do want to make this my career and get deeper into empirical research and possibly even data science.",8,1524888315.0,8felzj,False,"I have a law degree (and History BA) but do not work as a lawyer.  I work at an academic center at a large law school for a group of professors that do empirical research.  I have taught myself some Python which has helped with some basic stuff, data crunching and visualization, etc.

But I find the big missing piece in my knowledge is Statistics.  For example a professor mentioned wanting to do linear regressions and I had to look that up.  

Is anyone working in or familiar with fields with non-Statistics domain knowledge but where some statistics knowledge is necessary?  Such as legal empirical research or other types of empirical research in social sciences, accounting, etc.

Would I be able to self-study through MOOCs and gain enough statistics knowledge for such areas?  Should I look into actually doing some university coursework or even pursuing a Masters degree?

I do want to make this my career and get deeper into empirical research and possibly even data science.",0,"I have a law degree (and History BA) but do not work as a lawyer.  I work at an academic center at a large law school for a group of professors that do empirical research.  I have taught myself some Python which has helped with some basic stuff, data crunching and visualization, etc.

But I find the big missing piece in my knowledge is Statistics.  For example a professor mentioned wanting to do linear regressions and I had to look that up.  

Is anyone working in or familiar with fields with non-Statistics domain knowledge but where some statistics knowledge is necessary?  Such as legal empirical research or other types of empirical research in social sciences, accounting, etc.

Would I be able to self-study through MOOCs and gain enough statistics knowledge for such areas?  Should I look into actually doing some university coursework or even pursuing a Masters degree?

I do want to make this my career and get deeper into empirical research and possibly even data science.",24,statistics,54935,,Learning Statistics for Empirical Research in Law,https://www.reddit.com/r/statistics/comments/8felzj/learning_statistics_for_empirical_research_in_law/,all_ads,2018-04-28 00:05:15,36 days 01:30:19.036584000,
"Forgive me for the rant, but I’m kind of disappointed with all the cheating that is going on in my grad program. It kind of pisses me off that I sit there for hours and struggle with the material, while some other people just copy homework solutions from Chegg or other resources. It’s pretty obvious though and the professor is no fool. He notices that some people average 90-100 on homework, but then get 30s on exams. (Probably why homework is like 10% of our grade and exams are like 30% each). The low scores could be due to many factors: time constraints, bad day, different types of questions, bad teaching methods, etc…More likely than not, it’s because the person didn’t understand the material or was cheating. Last exam there was a 23 point curve, so the grades were pretty low. 

&nbsp;

Another thing I have issues with: for most classes we have to give a 15-20 minute talk on some topic in the course material, which I think is excellent because it helps students become more comfortable with presenting things to an audience, which is a great skill to have, especially in industry. But anyways, some of the presentations were either too short, or just unclear. Some people just copied things off the internet without understanding and just read off the slides. When the professor questioned them and asked them to clarify something, they just froze, didn’t respond, and just continued reading the slides. It was supposed to be a learning experience for the class, but I doubt anyone learned something from the presentations. There were a couple interesting presentations though.

&nbsp;

Let me just clarify that around half of my class of 23 students are Asian international students (mostly Chinese or Korean, but a couple of Indians as well). Some of them are cool and very intelligent, others, I’m not sure why they’re there. It could be a language barrier, but their English is ok, so I’m not sure. Maybe cheating is normal in their home country, idk. Maybe they’re just seeking the credential of the M.S. degree and are just doing it because it pays well, and cheating is the quickest solution that requires the least effort. Idk. Too many factors to consider. I leave it as an exercise for social science researchers to perform this experiment and test the hypotheses. 

&nbsp;

Oh well, at least I'll be more competitive than the cheaters when interviewing for jobs because I can actually talk about what I know/learned. Anyways, that’s my story, if anyone has faced something similar, please feel free to share.  
",73,1524863295.0,8fbi24,False,"Forgive me for the rant, but I’m kind of disappointed with all the cheating that is going on in my grad program. It kind of pisses me off that I sit there for hours and struggle with the material, while some other people just copy homework solutions from Chegg or other resources. It’s pretty obvious though and the professor is no fool. He notices that some people average 90-100 on homework, but then get 30s on exams. (Probably why homework is like 10% of our grade and exams are like 30% each). The low scores could be due to many factors: time constraints, bad day, different types of questions, bad teaching methods, etc…More likely than not, it’s because the person didn’t understand the material or was cheating. Last exam there was a 23 point curve, so the grades were pretty low. 

&nbsp;

Another thing I have issues with: for most classes we have to give a 15-20 minute talk on some topic in the course material, which I think is excellent because it helps students become more comfortable with presenting things to an audience, which is a great skill to have, especially in industry. But anyways, some of the presentations were either too short, or just unclear. Some people just copied things off the internet without understanding and just read off the slides. When the professor questioned them and asked them to clarify something, they just froze, didn’t respond, and just continued reading the slides. It was supposed to be a learning experience for the class, but I doubt anyone learned something from the presentations. There were a couple interesting presentations though.

&nbsp;

Let me just clarify that around half of my class of 23 students are Asian international students (mostly Chinese or Korean, but a couple of Indians as well). Some of them are cool and very intelligent, others, I’m not sure why they’re there. It could be a language barrier, but their English is ok, so I’m not sure. Maybe cheating is normal in their home country, idk. Maybe they’re just seeking the credential of the M.S. degree and are just doing it because it pays well, and cheating is the quickest solution that requires the least effort. Idk. Too many factors to consider. I leave it as an exercise for social science researchers to perform this experiment and test the hypotheses. 

&nbsp;

Oh well, at least I'll be more competitive than the cheaters when interviewing for jobs because I can actually talk about what I know/learned. Anyways, that’s my story, if anyone has faced something similar, please feel free to share.  
",0,"Forgive me for the rant, but I’m kind of disappointed with all the cheating that is going on in my grad program. It kind of pisses me off that I sit there for hours and struggle with the material, while some other people just copy homework solutions from Chegg or other resources. It’s pretty obvious though and the professor is no fool. He notices that some people average 90-100 on homework, but then get 30s on exams. (Probably why homework is like 10% of our grade and exams are like 30% each). The low scores could be due to many factors: time constraints, bad day, different types of questions, bad teaching methods, etc…More likely than not, it’s because the person didn’t understand the material or was cheating. Last exam there was a 23 point curve, so the grades were pretty low. 

&nbsp;

Another thing I have issues with: for most classes we have to give a 15-20 minute talk on some topic in the course material, which I think is excellent because it helps students become more comfortable with presenting things to an audience, which is a great skill to have, especially in industry. But anyways, some of the presentations were either too short, or just unclear. Some people just copied things off the internet without understanding and just read off the slides. When the professor questioned them and asked them to clarify something, they just froze, didn’t respond, and just continued reading the slides. It was supposed to be a learning experience for the class, but I doubt anyone learned something from the presentations. There were a couple interesting presentations though.

&nbsp;

Let me just clarify that around half of my class of 23 students are Asian international students (mostly Chinese or Korean, but a couple of Indians as well). Some of them are cool and very intelligent, others, I’m not sure why they’re there. It could be a language barrier, but their English is ok, so I’m not sure. Maybe cheating is normal in their home country, idk. Maybe they’re just seeking the credential of the M.S. degree and are just doing it because it pays well, and cheating is the quickest solution that requires the least effort. Idk. Too many factors to consider. I leave it as an exercise for social science researchers to perform this experiment and test the hypotheses. 

&nbsp;

Oh well, at least I'll be more competitive than the cheaters when interviewing for jobs because I can actually talk about what I know/learned. Anyways, that’s my story, if anyone has faced something similar, please feel free to share.  
",44,statistics,54935,,Dealing with cheating in grad school classes?,https://www.reddit.com/r/statistics/comments/8fbi24/dealing_with_cheating_in_grad_school_classes/,all_ads,2018-04-27 17:08:15,36 days 08:27:19.036584000,
"Fictitious data to help visualise: https://docs.google.com/spreadsheets/d/1CVoYzgcrjKWnGSdWGQ90GhpUqrE8-GZi7TXGmRpeNJw/edit?usp=sharing


I'm trying to calculate the desired sample size (using G*Power) 

This is a repeated measures.
Participants are exposed to both conditions: with Independent variable (IV) & without IV. 
I'm comparing conditions without IV vs. condition with IV
Based on the spreadsheet link above, it's 16 data points vs 16 data points.

Would I be correct to characterise this as ""Differences between two dependent means.""
With estimated effect size = 0.5, Power = 0.8, I get a recommend sample size of at least 34. 

Am I thinking this correctly? Thank you! ",0,1524919239.0,8fhor2,False,"Fictitious data to help visualise: https://docs.google.com/spreadsheets/d/1CVoYzgcrjKWnGSdWGQ90GhpUqrE8-GZi7TXGmRpeNJw/edit?usp=sharing


I'm trying to calculate the desired sample size (using G*Power) 

This is a repeated measures.
Participants are exposed to both conditions: with Independent variable (IV) & without IV. 
I'm comparing conditions without IV vs. condition with IV
Based on the spreadsheet link above, it's 16 data points vs 16 data points.

Would I be correct to characterise this as ""Differences between two dependent means.""
With estimated effect size = 0.5, Power = 0.8, I get a recommend sample size of at least 34. 

Am I thinking this correctly? Thank you! ",0,"Fictitious data to help visualise: https://docs.google.com/spreadsheets/d/1CVoYzgcrjKWnGSdWGQ90GhpUqrE8-GZi7TXGmRpeNJw/edit?usp=sharing


I'm trying to calculate the desired sample size (using G*Power) 

This is a repeated measures.
Participants are exposed to both conditions: with Independent variable (IV) & without IV. 
I'm comparing conditions without IV vs. condition with IV
Based on the spreadsheet link above, it's 16 data points vs 16 data points.

Would I be correct to characterise this as ""Differences between two dependent means.""
With estimated effect size = 0.5, Power = 0.8, I get a recommend sample size of at least 34. 

Am I thinking this correctly? Thank you! ",1,statistics,54935,,Statistical test choice verification,https://www.reddit.com/r/statistics/comments/8fhor2/statistical_test_choice_verification/,all_ads,2018-04-28 08:40:39,35 days 16:54:55.036584000,
"[add NLSQL to skype](https://join.skype.com/bot/536181e9-e94a-47f3-b387-549350f625c1)

[watch video of how it works](https://www.nlsql.com/blog/post/2/)

[see statistic graph example](https://www.nlsql.com/static/img/skype/pic_ToLYSW4c.png)
",0,1524879573.0,8fdj3l,False,"[add NLSQL to skype](https://join.skype.com/bot/536181e9-e94a-47f3-b387-549350f625c1)

[watch video of how it works](https://www.nlsql.com/blog/post/2/)

[see statistic graph example](https://www.nlsql.com/static/img/skype/pic_ToLYSW4c.png)
",0,"[add NLSQL to skype](https://join.skype.com/bot/536181e9-e94a-47f3-b387-549350f625c1)

[watch video of how it works](https://www.nlsql.com/blog/post/2/)

[see statistic graph example](https://www.nlsql.com/static/img/skype/pic_ToLYSW4c.png)
",2,statistics,54935,,Easy tool to get graph USA import/export statistics using just natural language NLSQL bot,https://www.reddit.com/r/statistics/comments/8fdj3l/easy_tool_to_get_graph_usa_importexport/,all_ads,2018-04-27 21:39:33,36 days 03:56:01.036584000,
"I think I might have been misled somewhere but the ACF/PACF plots only tell you if autocorrelation exists and has nothing to do with stationarity correct?

To detect if a time series data is not stationary, the best way is still to eye ball it using a plot of the data against time?   Or after you build the model, eye ball to see if the residuals are stationary?",2,1524888056.0,8feku7,False,"I think I might have been misled somewhere but the ACF/PACF plots only tell you if autocorrelation exists and has nothing to do with stationarity correct?

To detect if a time series data is not stationary, the best way is still to eye ball it using a plot of the data against time?   Or after you build the model, eye ball to see if the residuals are stationary?",0,"I think I might have been misled somewhere but the ACF/PACF plots only tell you if autocorrelation exists and has nothing to do with stationarity correct?

To detect if a time series data is not stationary, the best way is still to eye ball it using a plot of the data against time?   Or after you build the model, eye ball to see if the residuals are stationary?",1,statistics,54935,,Is ACF or PACF plot ever used to determine stationarity?,https://www.reddit.com/r/statistics/comments/8feku7/is_acf_or_pacf_plot_ever_used_to_determine/,all_ads,2018-04-28 00:00:56,36 days 01:34:38.036584000,
"OK, I am by no means a statistics expert, but I think I know how to do a Z test.  A Z test is used specifically in the case where you know the population mean; however, when I look at [the documentation](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.zscore.html) for calculating a Z score using the scipy library in python, it seems that it calculates the Z score for each entry in the array and uses the sample mean for the population mean.  This makes absolutely no sense to me.  If you want to test multiple samples each with a size of one, then ok, but you need to still input the *population* mean, not just use the sample mean.  Or else, shouldn't you be using a t-test?  Am I missing something?  Thanks! ",3,1524885228.0,8fe8lx,False,"OK, I am by no means a statistics expert, but I think I know how to do a Z test.  A Z test is used specifically in the case where you know the population mean; however, when I look at [the documentation](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.zscore.html) for calculating a Z score using the scipy library in python, it seems that it calculates the Z score for each entry in the array and uses the sample mean for the population mean.  This makes absolutely no sense to me.  If you want to test multiple samples each with a size of one, then ok, but you need to still input the *population* mean, not just use the sample mean.  Or else, shouldn't you be using a t-test?  Am I missing something?  Thanks! ",0,"OK, I am by no means a statistics expert, but I think I know how to do a Z test.  A Z test is used specifically in the case where you know the population mean; however, when I look at [the documentation](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.zscore.html) for calculating a Z score using the scipy library in python, it seems that it calculates the Z score for each entry in the array and uses the sample mean for the population mean.  This makes absolutely no sense to me.  If you want to test multiple samples each with a size of one, then ok, but you need to still input the *population* mean, not just use the sample mean.  Or else, shouldn't you be using a t-test?  Am I missing something?  Thanks! ",1,statistics,54935,,Z Score Using Scipy,https://www.reddit.com/r/statistics/comments/8fe8lx/z_score_using_scipy/,all_ads,2018-04-27 23:13:48,36 days 02:21:46.036584000,
Does ageism exist in statistics jobs the way it does for programmers?  I'm an aging programmer looking to switch careers.,7,1524856689.0,8fav4a,False,Does ageism exist in statistics jobs the way it does for programmers?  I'm an aging programmer looking to switch careers.,0,Does ageism exist in statistics jobs the way it does for programmers?  I'm an aging programmer looking to switch careers.,2,statistics,54935,,Ageism in Statistician Jobs?,https://www.reddit.com/r/statistics/comments/8fav4a/ageism_in_statistician_jobs/,all_ads,2018-04-27 15:18:09,36 days 10:17:25.036584000,
"Hey everyone,

I got accepted to grad school yesterday for an MS in applied statistics and analytics! You guys will probably be hearing from me a lot more frequently now. I will make sure to take all of your advice to heart as I get ready to start in the fall. 

Cheers all!",27,1524787173.0,8f3re6,False,"Hey everyone,

I got accepted to grad school yesterday for an MS in applied statistics and analytics! You guys will probably be hearing from me a lot more frequently now. I will make sure to take all of your advice to heart as I get ready to start in the fall. 

Cheers all!",0,"Hey everyone,

I got accepted to grad school yesterday for an MS in applied statistics and analytics! You guys will probably be hearing from me a lot more frequently now. I will make sure to take all of your advice to heart as I get ready to start in the fall. 

Cheers all!",118,statistics,54935,,I got in!,https://www.reddit.com/r/statistics/comments/8f3re6/i_got_in/,all_ads,2018-04-26 19:59:33,37 days 05:36:01.036584000,
"Link to study: [https://ieeexplore.ieee.org/document/7038939/](https://ieeexplore.ieee.org/document/7038939/)

\>To predict the adverse outcomes, we built upon the random forest models shown in \[4\]. Random forests \(RF\) are ensembles of tree\-based classifiers where each tree divides the values of predictor variables into regions of small variation in the response variable. The root node in a tree represents the initial partitioning and leaf nodes output the predicted response. As an ensemble approach, RF build many treebased classifiers and combine their results. To diversify the forest, training data are bootstrapped and random subsets of the predictors are allocated to each tree. The trees in the forest vote to provide predictions on new observations \[4\]. More details on RF can be found in \[17\].

\>In this paper, we build upon the previous RF models by adding four types of new predictors:

\>Pre\-existing conditions, Time Series Forecasts, GARCH, and Low Heart Rate Variability

Conclusion:

\>In particular, both 30 and 60 minute forecasts of SpO2&#37; were important, as well as every measure of SpO2&#37; volatility and median pulse volatility.

\>All of the pre\-existing conditions had a standardized importance below 0.5, indicating that other factors were more predictive of death.

\>Also consistent with our findings for death, prior conditions are among the least important predictors of kidney injury.",0,1524876984.0,8fd74z,False,"Link to study: [https://ieeexplore.ieee.org/document/7038939/](https://ieeexplore.ieee.org/document/7038939/)

\>To predict the adverse outcomes, we built upon the random forest models shown in \[4\]. Random forests \(RF\) are ensembles of tree\-based classifiers where each tree divides the values of predictor variables into regions of small variation in the response variable. The root node in a tree represents the initial partitioning and leaf nodes output the predicted response. As an ensemble approach, RF build many treebased classifiers and combine their results. To diversify the forest, training data are bootstrapped and random subsets of the predictors are allocated to each tree. The trees in the forest vote to provide predictions on new observations \[4\]. More details on RF can be found in \[17\].

\>In this paper, we build upon the previous RF models by adding four types of new predictors:

\>Pre\-existing conditions, Time Series Forecasts, GARCH, and Low Heart Rate Variability

Conclusion:

\>In particular, both 30 and 60 minute forecasts of SpO2&#37; were important, as well as every measure of SpO2&#37; volatility and median pulse volatility.

\>All of the pre\-existing conditions had a standardized importance below 0.5, indicating that other factors were more predictive of death.

\>Also consistent with our findings for death, prior conditions are among the least important predictors of kidney injury.",0,"Link to study: [https://ieeexplore.ieee.org/document/7038939/](https://ieeexplore.ieee.org/document/7038939/)

\>To predict the adverse outcomes, we built upon the random forest models shown in \[4\]. Random forests \(RF\) are ensembles of tree\-based classifiers where each tree divides the values of predictor variables into regions of small variation in the response variable. The root node in a tree represents the initial partitioning and leaf nodes output the predicted response. As an ensemble approach, RF build many treebased classifiers and combine their results. To diversify the forest, training data are bootstrapped and random subsets of the predictors are allocated to each tree. The trees in the forest vote to provide predictions on new observations \[4\]. More details on RF can be found in \[17\].

\>In this paper, we build upon the previous RF models by adding four types of new predictors:

\>Pre\-existing conditions, Time Series Forecasts, GARCH, and Low Heart Rate Variability

Conclusion:

\>In particular, both 30 and 60 minute forecasts of SpO2&#37; were important, as well as every measure of SpO2&#37; volatility and median pulse volatility.

\>All of the pre\-existing conditions had a standardized importance below 0.5, indicating that other factors were more predictive of death.

\>Also consistent with our findings for death, prior conditions are among the least important predictors of kidney injury.",1,statistics,54935,,Time series forecasts and volatility measures as predictors of post-surgical death and kidney injury,https://www.reddit.com/r/statistics/comments/8fd74z/time_series_forecasts_and_volatility_measures_as/,all_ads,2018-04-27 20:56:24,36 days 04:39:10.036584000,
"I know this a basic question, but I need to determine the relationship between 1 variable and a few others. I know I should use correlation for variables like ""age"". Then I have ""sex"", which is nominal. But what if I have ""exercise"" and the only values assigned to them is 1, 2 or 3. Where 1 - high, 2 - moderate, 3 - low?

Thanks.",2,1524871078.0,8fcfqy,False,"I know this a basic question, but I need to determine the relationship between 1 variable and a few others. I know I should use correlation for variables like ""age"". Then I have ""sex"", which is nominal. But what if I have ""exercise"" and the only values assigned to them is 1, 2 or 3. Where 1 - high, 2 - moderate, 3 - low?

Thanks.",0,"I know this a basic question, but I need to determine the relationship between 1 variable and a few others. I know I should use correlation for variables like ""age"". Then I have ""sex"", which is nominal. But what if I have ""exercise"" and the only values assigned to them is 1, 2 or 3. Where 1 - high, 2 - moderate, 3 - low?

Thanks.",1,statistics,54935,,"If frequency is assigned to a variable in a data set, e.g. Never, often, a lot, does it count as ordinal or nominal.",https://www.reddit.com/r/statistics/comments/8fcfqy/if_frequency_is_assigned_to_a_variable_in_a_data/,all_ads,2018-04-27 19:17:58,36 days 06:17:36.036584000,
"Pace stats are reference all the time in the baseball (i.e. Mike Trout is on pace for 47 homers, based on the 12 he has hit). I'm just wondering if anyone can help me with these pace formulas for basketball.

For instance, if a game is 40mins long and a player scores 2 points in the first 30s of the game, what would he/she be on pace for? Then again 5mins into the game, if this same player still had 2 points, what would they now be on pace for?

Obviously at the end of 4 quarters they are easier to calculate. If a players has 4 points after the first Q then they would be on pace for 16 points.
",5,1524865724.0,8fbruj,False,"Pace stats are reference all the time in the baseball (i.e. Mike Trout is on pace for 47 homers, based on the 12 he has hit). I'm just wondering if anyone can help me with these pace formulas for basketball.

For instance, if a game is 40mins long and a player scores 2 points in the first 30s of the game, what would he/she be on pace for? Then again 5mins into the game, if this same player still had 2 points, what would they now be on pace for?

Obviously at the end of 4 quarters they are easier to calculate. If a players has 4 points after the first Q then they would be on pace for 16 points.
",0,"Pace stats are reference all the time in the baseball (i.e. Mike Trout is on pace for 47 homers, based on the 12 he has hit). I'm just wondering if anyone can help me with these pace formulas for basketball.

For instance, if a game is 40mins long and a player scores 2 points in the first 30s of the game, what would he/she be on pace for? Then again 5mins into the game, if this same player still had 2 points, what would they now be on pace for?

Obviously at the end of 4 quarters they are easier to calculate. If a players has 4 points after the first Q then they would be on pace for 16 points.
",1,statistics,54935,,How do you projected stats based on the time remaining in a game? (basketball),https://www.reddit.com/r/statistics/comments/8fbruj/how_do_you_projected_stats_based_on_the_time/,all_ads,2018-04-27 17:48:44,36 days 07:46:50.036584000,
"I'm starting to become a bit... Frustrated. I'm starting to wonder why was I brought on if you (i.e. in general) have every intention of doing things the same way that you were doing...? The old way that has brought your company problems...? What am I here for?

Has this happened to anyone?",42,1524780821.0,8f2yef,False,"I'm starting to become a bit... Frustrated. I'm starting to wonder why was I brought on if you (i.e. in general) have every intention of doing things the same way that you were doing...? The old way that has brought your company problems...? What am I here for?

Has this happened to anyone?",0,"I'm starting to become a bit... Frustrated. I'm starting to wonder why was I brought on if you (i.e. in general) have every intention of doing things the same way that you were doing...? The old way that has brought your company problems...? What am I here for?

Has this happened to anyone?",44,statistics,54935,,"Hired for expertise, but no one wants to listen to expertise... Has this happened to you?",https://www.reddit.com/r/statistics/comments/8f2yef/hired_for_expertise_but_no_one_wants_to_listen_to/,all_ads,2018-04-26 18:13:41,37 days 07:21:53.036584000,
"I am looking for data to analyze using a moving average.  I am considering doing both a simple moving average, and exponential but I do not know what type of data is analyzed using a moving average.  Any suggestions?",6,1524821097.0,8f7wsw,False,"I am looking for data to analyze using a moving average.  I am considering doing both a simple moving average, and exponential but I do not know what type of data is analyzed using a moving average.  Any suggestions?",0,"I am looking for data to analyze using a moving average.  I am considering doing both a simple moving average, and exponential but I do not know what type of data is analyzed using a moving average.  Any suggestions?",3,statistics,54935,,Data for Moving Average Analysis,https://www.reddit.com/r/statistics/comments/8f7wsw/data_for_moving_average_analysis/,all_ads,2018-04-27 05:24:57,36 days 20:10:37.036584000,
"Before I entered undergrad, I really was interested in maths/stats and originally was going to do this. However, I saw the reputation Fisher at OSU has and decided to pursue business instead; I chose finance as I figured this would be the most quantitative. My undergrad experience will be over in SP19 and, although I loved the instructors and the content of the courses, I with there was more stats. I'd really like to advance to grad school and study something related to stats (PhD in Stats, Ops Research, Biostats, Applied Maths, etc.) and I believe I have great credentials to get into a top school (>3.9 GPA, 3 great internships, undergrad thesis) but I am stuck on the 'in-between'. My long-term goal is to do something research-based in industry that requires heavy statistical modeling (quantitative finance, biostats (my dad worked in pharmaceuticals as a statistician and the projects were really interesting)).

Should I go straight to grad school or get work experience? If work, would employers help fund a PhD program (I know some fund Masters so this is a potential option too)? When selecting a school is there any sources that help list the best programs for Stats & Biostats PhD's? 

On a side note, I know I will need to learn programming. I've only used Minitab and Tableau for my stats projects in undergrad but I know coding will be crucial. Any tips for how to begin? Which ones are common? Any ones with easy UI's? ",1,1524832802.0,8f917j,False,"Before I entered undergrad, I really was interested in maths/stats and originally was going to do this. However, I saw the reputation Fisher at OSU has and decided to pursue business instead; I chose finance as I figured this would be the most quantitative. My undergrad experience will be over in SP19 and, although I loved the instructors and the content of the courses, I with there was more stats. I'd really like to advance to grad school and study something related to stats (PhD in Stats, Ops Research, Biostats, Applied Maths, etc.) and I believe I have great credentials to get into a top school (>3.9 GPA, 3 great internships, undergrad thesis) but I am stuck on the 'in-between'. My long-term goal is to do something research-based in industry that requires heavy statistical modeling (quantitative finance, biostats (my dad worked in pharmaceuticals as a statistician and the projects were really interesting)).

Should I go straight to grad school or get work experience? If work, would employers help fund a PhD program (I know some fund Masters so this is a potential option too)? When selecting a school is there any sources that help list the best programs for Stats & Biostats PhD's? 

On a side note, I know I will need to learn programming. I've only used Minitab and Tableau for my stats projects in undergrad but I know coding will be crucial. Any tips for how to begin? Which ones are common? Any ones with easy UI's? ",0,"Before I entered undergrad, I really was interested in maths/stats and originally was going to do this. However, I saw the reputation Fisher at OSU has and decided to pursue business instead; I chose finance as I figured this would be the most quantitative. My undergrad experience will be over in SP19 and, although I loved the instructors and the content of the courses, I with there was more stats. I'd really like to advance to grad school and study something related to stats (PhD in Stats, Ops Research, Biostats, Applied Maths, etc.) and I believe I have great credentials to get into a top school (>3.9 GPA, 3 great internships, undergrad thesis) but I am stuck on the 'in-between'. My long-term goal is to do something research-based in industry that requires heavy statistical modeling (quantitative finance, biostats (my dad worked in pharmaceuticals as a statistician and the projects were really interesting)).

Should I go straight to grad school or get work experience? If work, would employers help fund a PhD program (I know some fund Masters so this is a potential option too)? When selecting a school is there any sources that help list the best programs for Stats & Biostats PhD's? 

On a side note, I know I will need to learn programming. I've only used Minitab and Tableau for my stats projects in undergrad but I know coding will be crucial. Any tips for how to begin? Which ones are common? Any ones with easy UI's? ",1,statistics,54935,,Wish to pursue a career in stats/stats-heavy work but don't know how to go about,https://www.reddit.com/r/statistics/comments/8f917j/wish_to_pursue_a_career_in_statsstatsheavy_work/,all_ads,2018-04-27 08:40:02,36 days 16:55:32.036584000,
"Not really asking for homework help, just want to check my concepts to see if this is correct. So currently I got 2 questions. Goes as follows.

Researcher was interested in the relationship between smoking and lung cancer. He randomly sampled 500 smokers and 700 non-smokers and observed whether they developed lung cancer.

(Table results -- not important in this context)

Which of the following ratios provide a good estimate of the odds of smoking among lung cancer patients within the population?

So, based on **phrasing** of the question, the participants were chosen based on exposure (whether they smoked), so it seems like a cohort study to me.

However, based on what my tutor mentioned in class, he said that when finding odds(A|B), if selection is done based on A, it is a case control study. If selection is done based on B, it is a cohort study. 

In this case, we wish to find odds (smoking | lung cancer), and selection was done based on smoking (i.e A). Hence, this should be a case control. And since for case control studies, odds do not work at all, hence none of the answers will be a good estimate. 

Is this understanding correct? Or should it be that the whole experiment is a cohort study, but in terms of calculations I should be doing it like a case control study?


**Secondly,**

What measures are applicable for estimation (i.e accurate) for case control and cohort studies?

As far as I know,

For case control: Odds ratio only

For cohort study: Risk, Odds, Risk Ratio, Odds Ratio

Is this correct?

Thanks in advance.",3,1524828438.0,8f8mtm,False,"Not really asking for homework help, just want to check my concepts to see if this is correct. So currently I got 2 questions. Goes as follows.

Researcher was interested in the relationship between smoking and lung cancer. He randomly sampled 500 smokers and 700 non-smokers and observed whether they developed lung cancer.

(Table results -- not important in this context)

Which of the following ratios provide a good estimate of the odds of smoking among lung cancer patients within the population?

So, based on **phrasing** of the question, the participants were chosen based on exposure (whether they smoked), so it seems like a cohort study to me.

However, based on what my tutor mentioned in class, he said that when finding odds(A|B), if selection is done based on A, it is a case control study. If selection is done based on B, it is a cohort study. 

In this case, we wish to find odds (smoking | lung cancer), and selection was done based on smoking (i.e A). Hence, this should be a case control. And since for case control studies, odds do not work at all, hence none of the answers will be a good estimate. 

Is this understanding correct? Or should it be that the whole experiment is a cohort study, but in terms of calculations I should be doing it like a case control study?


**Secondly,**

What measures are applicable for estimation (i.e accurate) for case control and cohort studies?

As far as I know,

For case control: Odds ratio only

For cohort study: Risk, Odds, Risk Ratio, Odds Ratio

Is this correct?

Thanks in advance.",0,"Not really asking for homework help, just want to check my concepts to see if this is correct. So currently I got 2 questions. Goes as follows.

Researcher was interested in the relationship between smoking and lung cancer. He randomly sampled 500 smokers and 700 non-smokers and observed whether they developed lung cancer.

(Table results -- not important in this context)

Which of the following ratios provide a good estimate of the odds of smoking among lung cancer patients within the population?

So, based on **phrasing** of the question, the participants were chosen based on exposure (whether they smoked), so it seems like a cohort study to me.

However, based on what my tutor mentioned in class, he said that when finding odds(A|B), if selection is done based on A, it is a case control study. If selection is done based on B, it is a cohort study. 

In this case, we wish to find odds (smoking | lung cancer), and selection was done based on smoking (i.e A). Hence, this should be a case control. And since for case control studies, odds do not work at all, hence none of the answers will be a good estimate. 

Is this understanding correct? Or should it be that the whole experiment is a cohort study, but in terms of calculations I should be doing it like a case control study?


**Secondly,**

What measures are applicable for estimation (i.e accurate) for case control and cohort studies?

As far as I know,

For case control: Odds ratio only

For cohort study: Risk, Odds, Risk Ratio, Odds Ratio

Is this correct?

Thanks in advance.",1,statistics,54935,,Odds ratio and its relevance in case control / cohort studies,https://www.reddit.com/r/statistics/comments/8f8mtm/odds_ratio_and_its_relevance_in_case_control/,all_ads,2018-04-27 07:27:18,36 days 18:08:16.036584000,
"Hello,

I'm learning all these cool models and stuff in my stats classes but developing  model from scratch \(given a goal\) is still very difficult.  The hardest part is figuring out what variables to use, whether or not I should turn certain variables into ratios \- basically how to set up the model/what model to use.  Many variables are related to each other, maybe even based on another variable.

In all my classes, we get questions like, build a linear regression model with the dependent variable given and use the following independent variables.  It's very straight forward.  Transformations/techniques are obvious \- it's limited to what you learned in the previous class or classes.

But in the real world, it's like \- here's a data set of all the data we have.  Our goal is to try to increase revenue.  Now build a model.

How do I get from easy classroom models to working on real world problems?  Where can I get more practice?",1,1524791579.0,8f4cnw,False,"Hello,

I'm learning all these cool models and stuff in my stats classes but developing  model from scratch \(given a goal\) is still very difficult.  The hardest part is figuring out what variables to use, whether or not I should turn certain variables into ratios \- basically how to set up the model/what model to use.  Many variables are related to each other, maybe even based on another variable.

In all my classes, we get questions like, build a linear regression model with the dependent variable given and use the following independent variables.  It's very straight forward.  Transformations/techniques are obvious \- it's limited to what you learned in the previous class or classes.

But in the real world, it's like \- here's a data set of all the data we have.  Our goal is to try to increase revenue.  Now build a model.

How do I get from easy classroom models to working on real world problems?  Where can I get more practice?",0,"Hello,

I'm learning all these cool models and stuff in my stats classes but developing  model from scratch \(given a goal\) is still very difficult.  The hardest part is figuring out what variables to use, whether or not I should turn certain variables into ratios \- basically how to set up the model/what model to use.  Many variables are related to each other, maybe even based on another variable.

In all my classes, we get questions like, build a linear regression model with the dependent variable given and use the following independent variables.  It's very straight forward.  Transformations/techniques are obvious \- it's limited to what you learned in the previous class or classes.

But in the real world, it's like \- here's a data set of all the data we have.  Our goal is to try to increase revenue.  Now build a model.

How do I get from easy classroom models to working on real world problems?  Where can I get more practice?",3,statistics,54935,,How to gain real world experience with data science?,https://www.reddit.com/r/statistics/comments/8f4cnw/how_to_gain_real_world_experience_with_data/,all_ads,2018-04-26 21:12:59,37 days 04:22:35.036584000,
"I've been struggling to find an online mathematical statistics course with video lectures to prepare myself for learning ML, and the majority of online statistics courses don't use much math (such as the Duke's [Statistics with R](https://www.coursera.org/specializations/statistics) course or Bekeley's [Statistics 21](https://archive.org/details/ucberkeley-webcast-PL02A86BB93BE23D65)). The only true mathematical statistics course with video lectures that could find was CMU's [36-705](https://www.youtube.com/watch?v=uonMnn7BLX0&list=PLcW8xNfZoh7eI7KSWneVWq-7wr8ffRtHF), but the video lectures' quality is quite poor.

However, today I accidentally covered a statistics course that partially met my criteria. It's the [Introduction to Statistics](https://02402.compute.dtu.dk/home) course from the Technical University of Denmark. What I like about this course is:

1) The textbook is free!

2) Video lectures are available (under the 'Podcast' tab of the course website)

3) Homework and solutions are available, as well as exams going back a few years (again, with solutions!)

4) Most formulas have mathematical derivations (though it might not be quite as rigorous as an standard mathematical statistics course e.g. calculus was not present much, if at all)

5) It combines both probability and statistics so someone who wants to refresh both topics or learn them both for e.g. machine learning could accomplish quickly in one course

6) R programming is used liberally in the book and the homework, which is great for those who want to learn the material through programming

I have taken the Duke's statistics courses on Coursera, but will use this course to strengthen my probability and stats knowledge before I embark on a real Machine Learning course (I'm looking at CMU's [10-701](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) by Tom Mitchell). Hope you guys find this course useful as I do!",9,1524740815.0,8ezj5q,False,"I've been struggling to find an online mathematical statistics course with video lectures to prepare myself for learning ML, and the majority of online statistics courses don't use much math (such as the Duke's [Statistics with R](https://www.coursera.org/specializations/statistics) course or Bekeley's [Statistics 21](https://archive.org/details/ucberkeley-webcast-PL02A86BB93BE23D65)). The only true mathematical statistics course with video lectures that could find was CMU's [36-705](https://www.youtube.com/watch?v=uonMnn7BLX0&list=PLcW8xNfZoh7eI7KSWneVWq-7wr8ffRtHF), but the video lectures' quality is quite poor.

However, today I accidentally covered a statistics course that partially met my criteria. It's the [Introduction to Statistics](https://02402.compute.dtu.dk/home) course from the Technical University of Denmark. What I like about this course is:

1) The textbook is free!

2) Video lectures are available (under the 'Podcast' tab of the course website)

3) Homework and solutions are available, as well as exams going back a few years (again, with solutions!)

4) Most formulas have mathematical derivations (though it might not be quite as rigorous as an standard mathematical statistics course e.g. calculus was not present much, if at all)

5) It combines both probability and statistics so someone who wants to refresh both topics or learn them both for e.g. machine learning could accomplish quickly in one course

6) R programming is used liberally in the book and the homework, which is great for those who want to learn the material through programming

I have taken the Duke's statistics courses on Coursera, but will use this course to strengthen my probability and stats knowledge before I embark on a real Machine Learning course (I'm looking at CMU's [10-701](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) by Tom Mitchell). Hope you guys find this course useful as I do!",0,"I've been struggling to find an online mathematical statistics course with video lectures to prepare myself for learning ML, and the majority of online statistics courses don't use much math (such as the Duke's [Statistics with R](https://www.coursera.org/specializations/statistics) course or Bekeley's [Statistics 21](https://archive.org/details/ucberkeley-webcast-PL02A86BB93BE23D65)). The only true mathematical statistics course with video lectures that could find was CMU's [36-705](https://www.youtube.com/watch?v=uonMnn7BLX0&list=PLcW8xNfZoh7eI7KSWneVWq-7wr8ffRtHF), but the video lectures' quality is quite poor.

However, today I accidentally covered a statistics course that partially met my criteria. It's the [Introduction to Statistics](https://02402.compute.dtu.dk/home) course from the Technical University of Denmark. What I like about this course is:

1) The textbook is free!

2) Video lectures are available (under the 'Podcast' tab of the course website)

3) Homework and solutions are available, as well as exams going back a few years (again, with solutions!)

4) Most formulas have mathematical derivations (though it might not be quite as rigorous as an standard mathematical statistics course e.g. calculus was not present much, if at all)

5) It combines both probability and statistics so someone who wants to refresh both topics or learn them both for e.g. machine learning could accomplish quickly in one course

6) R programming is used liberally in the book and the homework, which is great for those who want to learn the material through programming

I have taken the Duke's statistics courses on Coursera, but will use this course to strengthen my probability and stats knowledge before I embark on a real Machine Learning course (I'm looking at CMU's [10-701](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) by Tom Mitchell). Hope you guys find this course useful as I do!",46,statistics,54935,,Intermediate statistics course (with lecture videos and free textbook) from the Technical University of Denmark (DTU),https://www.reddit.com/r/statistics/comments/8ezj5q/intermediate_statistics_course_with_lecture/,all_ads,2018-04-26 07:06:55,37 days 18:28:39.036584000,
"I split my data into two sets, the test set and validation set. From the test set I have determined two ""best"" models and want to compare the models by running them through the validation set. I'm not completely sure what that means or how to do it. I am using the application Minitab if that helps. Any support is appreciated!",1,1524798691.0,8f5alb,False,"I split my data into two sets, the test set and validation set. From the test set I have determined two ""best"" models and want to compare the models by running them through the validation set. I'm not completely sure what that means or how to do it. I am using the application Minitab if that helps. Any support is appreciated!",0,"I split my data into two sets, the test set and validation set. From the test set I have determined two ""best"" models and want to compare the models by running them through the validation set. I'm not completely sure what that means or how to do it. I am using the application Minitab if that helps. Any support is appreciated!",2,statistics,54935,,Cross model validation in Minitab?,https://www.reddit.com/r/statistics/comments/8f5alb/cross_model_validation_in_minitab/,all_ads,2018-04-26 23:11:31,37 days 02:24:03.036584000,
"Hey. I'm currently considering continuing my education past my bachelor's degree (I have one year left in a Software Engineering degree with an Applied Statistics minor and a planned Geographic Information Systems minor).

My first question(s) has to do with masters degree experiences in statistics. What do you wish you'd learned before getting there? What were your backgrounds before applying? Did you get your degree at the same school you did your undergrad degree at? Did you end up focusing on a specific topic, or did you try to get a broad education? Do you feel it was worth it in the end? Did you go on to further education? What field(s) are you working in now? Any information about stats masters degrees that I can get will be helpful and immensely appreciated (whether it answers the above questions or not).

Don't feel obligated to answer the following questions, as the first set of questions are the ones I'm most interested in currently.

My second question. I've been reading some academic papers and reading through a couple of textbooks (*Statistics for Spatial Data* by Noel Cressie and *Statistics for Earth and Environmental Scientists*) and I've been really enjoying them. I, however, do not have the strongest or most thorough fundamental background as of yet. I will be taking Regression Analysis and Stochastic Processes before I graduate, but it'll be about a year before I have those under my belt. In the meantime, does anyone have any suggestions for reading and autodidactic learning? For reference, I have taken Calc I & II, Discrete Math, Linear, App Stats, Design of Experiments, Math Stats, and Non-Parametric Stats so far. I'll be trying to teach myself Multivar this summer, since that seems to be important in what I've been reading so far.

My third and last question is in regards to projects. I've been interested in attempting more personal projects. I'm currently not really sure how to start though. Does anyone have any suggestions as to what to attempt? I've done a bit of Kaggle participation but nothing grand. I'd prefer to work with environmental data if I can. I'll have a bunch of free time this summer to work on projects, so I'd like to have stuff lined up beforehand. If anyone would be willing to mentor or take questions, that would also be immensely appreciated.

Thanks for reading this essay of a post, thank you in advance for any help you give, and I apologize for what are probably slightly inane questions!",1,1524790877.0,8f497y,False,"Hey. I'm currently considering continuing my education past my bachelor's degree (I have one year left in a Software Engineering degree with an Applied Statistics minor and a planned Geographic Information Systems minor).

My first question(s) has to do with masters degree experiences in statistics. What do you wish you'd learned before getting there? What were your backgrounds before applying? Did you get your degree at the same school you did your undergrad degree at? Did you end up focusing on a specific topic, or did you try to get a broad education? Do you feel it was worth it in the end? Did you go on to further education? What field(s) are you working in now? Any information about stats masters degrees that I can get will be helpful and immensely appreciated (whether it answers the above questions or not).

Don't feel obligated to answer the following questions, as the first set of questions are the ones I'm most interested in currently.

My second question. I've been reading some academic papers and reading through a couple of textbooks (*Statistics for Spatial Data* by Noel Cressie and *Statistics for Earth and Environmental Scientists*) and I've been really enjoying them. I, however, do not have the strongest or most thorough fundamental background as of yet. I will be taking Regression Analysis and Stochastic Processes before I graduate, but it'll be about a year before I have those under my belt. In the meantime, does anyone have any suggestions for reading and autodidactic learning? For reference, I have taken Calc I & II, Discrete Math, Linear, App Stats, Design of Experiments, Math Stats, and Non-Parametric Stats so far. I'll be trying to teach myself Multivar this summer, since that seems to be important in what I've been reading so far.

My third and last question is in regards to projects. I've been interested in attempting more personal projects. I'm currently not really sure how to start though. Does anyone have any suggestions as to what to attempt? I've done a bit of Kaggle participation but nothing grand. I'd prefer to work with environmental data if I can. I'll have a bunch of free time this summer to work on projects, so I'd like to have stuff lined up beforehand. If anyone would be willing to mentor or take questions, that would also be immensely appreciated.

Thanks for reading this essay of a post, thank you in advance for any help you give, and I apologize for what are probably slightly inane questions!",0,"Hey. I'm currently considering continuing my education past my bachelor's degree (I have one year left in a Software Engineering degree with an Applied Statistics minor and a planned Geographic Information Systems minor).

My first question(s) has to do with masters degree experiences in statistics. What do you wish you'd learned before getting there? What were your backgrounds before applying? Did you get your degree at the same school you did your undergrad degree at? Did you end up focusing on a specific topic, or did you try to get a broad education? Do you feel it was worth it in the end? Did you go on to further education? What field(s) are you working in now? Any information about stats masters degrees that I can get will be helpful and immensely appreciated (whether it answers the above questions or not).

Don't feel obligated to answer the following questions, as the first set of questions are the ones I'm most interested in currently.

My second question. I've been reading some academic papers and reading through a couple of textbooks (*Statistics for Spatial Data* by Noel Cressie and *Statistics for Earth and Environmental Scientists*) and I've been really enjoying them. I, however, do not have the strongest or most thorough fundamental background as of yet. I will be taking Regression Analysis and Stochastic Processes before I graduate, but it'll be about a year before I have those under my belt. In the meantime, does anyone have any suggestions for reading and autodidactic learning? For reference, I have taken Calc I & II, Discrete Math, Linear, App Stats, Design of Experiments, Math Stats, and Non-Parametric Stats so far. I'll be trying to teach myself Multivar this summer, since that seems to be important in what I've been reading so far.

My third and last question is in regards to projects. I've been interested in attempting more personal projects. I'm currently not really sure how to start though. Does anyone have any suggestions as to what to attempt? I've done a bit of Kaggle participation but nothing grand. I'd prefer to work with environmental data if I can. I'll have a bunch of free time this summer to work on projects, so I'd like to have stuff lined up beforehand. If anyone would be willing to mentor or take questions, that would also be immensely appreciated.

Thanks for reading this essay of a post, thank you in advance for any help you give, and I apologize for what are probably slightly inane questions!",2,statistics,54935,,Masters Degree Experiences and Two Other Questions,https://www.reddit.com/r/statistics/comments/8f497y/masters_degree_experiences_and_two_other_questions/,all_ads,2018-04-26 21:01:17,37 days 04:34:17.036584000,
"Doing a presentation on the paradox, can't find anything regarding any common solutions.",1,1524797710.0,8f560h,False,"Doing a presentation on the paradox, can't find anything regarding any common solutions.",0,"Doing a presentation on the paradox, can't find anything regarding any common solutions.",1,statistics,54935,,Is there a solution to Berkson's paradox?,https://www.reddit.com/r/statistics/comments/8f560h/is_there_a_solution_to_berksons_paradox/,all_ads,2018-04-26 22:55:10,37 days 02:40:24.036584000,
"I've completed an intro to stats course (probability theory, CLT, hypothesis testing, simple regression etc) in my degree and I'm after a book that explains some of the more advanced topics without being too hard to get through for someone who is still fairly early in their statistical learning. Thanks!

EDIT: Unsure of what I want to focus on topic-wise, due to being quite early in my stats learning and I am familiar with basic calculus and linear algebra.",33,1524731683.0,8eykre,False,"I've completed an intro to stats course (probability theory, CLT, hypothesis testing, simple regression etc) in my degree and I'm after a book that explains some of the more advanced topics without being too hard to get through for someone who is still fairly early in their statistical learning. Thanks!

EDIT: Unsure of what I want to focus on topic-wise, due to being quite early in my stats learning and I am familiar with basic calculus and linear algebra.",0,"I've completed an intro to stats course (probability theory, CLT, hypothesis testing, simple regression etc) in my degree and I'm after a book that explains some of the more advanced topics without being too hard to get through for someone who is still fairly early in their statistical learning. Thanks!

EDIT: Unsure of what I want to focus on topic-wise, due to being quite early in my stats learning and I am familiar with basic calculus and linear algebra.",21,statistics,54935,,After a good intermediate-level stats book,https://www.reddit.com/r/statistics/comments/8eykre/after_a_good_intermediatelevel_stats_book/,all_ads,2018-04-26 04:34:43,37 days 21:00:51.036584000,
"Alright. So this is coming from someone that hasn't taken any calculus or statistics in about 8 years so bear with me. I've collected some survival data from various primary literature sources (i.e person x with y malignancy survived z days) and compiled them in a list. I want to make a kaplan meier curve based on this data and feel I have somewhat of a grasp. My main question is that since this data is an aggregate from multiple sources with different study periods, how do I go about including surviving members at the tail end of each study period (i.e study 1 was conducted from 2000-2015. Patient X was found to have Y malignancy discovered in 2014 and has been alive since completion of the study with survival time of 1 year). Hope i'm making sense. Would appreciate the help. 

Thanks!


",1,1524790570.0,8f47o8,False,"Alright. So this is coming from someone that hasn't taken any calculus or statistics in about 8 years so bear with me. I've collected some survival data from various primary literature sources (i.e person x with y malignancy survived z days) and compiled them in a list. I want to make a kaplan meier curve based on this data and feel I have somewhat of a grasp. My main question is that since this data is an aggregate from multiple sources with different study periods, how do I go about including surviving members at the tail end of each study period (i.e study 1 was conducted from 2000-2015. Patient X was found to have Y malignancy discovered in 2014 and has been alive since completion of the study with survival time of 1 year). Hope i'm making sense. Would appreciate the help. 

Thanks!


",0,"Alright. So this is coming from someone that hasn't taken any calculus or statistics in about 8 years so bear with me. I've collected some survival data from various primary literature sources (i.e person x with y malignancy survived z days) and compiled them in a list. I want to make a kaplan meier curve based on this data and feel I have somewhat of a grasp. My main question is that since this data is an aggregate from multiple sources with different study periods, how do I go about including surviving members at the tail end of each study period (i.e study 1 was conducted from 2000-2015. Patient X was found to have Y malignancy discovered in 2014 and has been alive since completion of the study with survival time of 1 year). Hope i'm making sense. Would appreciate the help. 

Thanks!


",1,statistics,54935,,Need help with Kaplan Meier,https://www.reddit.com/r/statistics/comments/8f47o8/need_help_with_kaplan_meier/,all_ads,2018-04-26 20:56:10,37 days 04:39:24.036584000,
"The paper in question is: [Transcendental meditation, mindfulness, and longevity: an experimental study with the elderly.](https://www.ncbi.nlm.nih.gov/pubmed/2693686)

[full text](https://www.researchgate.net/profile/Ellen_Langer2/publication/20461920_Transcendental_Meditation_mindfulness_and_longevity_An_experimental_study_with_the_elderly/links/541175430cf264cee28b34fd/Transcendental-Meditation-mindfulness-and-longevity-An-experimental-study-with-the-elderly.pdf)

.

The specific question is about this finding:

""*Specific pairwise comparisons indicated, as predicted, a higher survival rate for TM than for MR, Z(n = 39) = 3.28, p< .001, or NT, Z(n = 41) = 2.54,p< .01 (Fischer exact p = .035), with a similar trend also compared with MF, Z(n — 35) = 1.52, p < .10. The rate for MF was also higher than that for MR, Z(n = 36) = 1.67, p < .05. No other pairwise contrasts were significant.2*""


.

TM = Transcendental Meditation

MF = Mindfulness (not MBSR)

MR = mental relaxation (relaxation response)


.

My opponent asserts that:


""*Specific pairwise comparisons indicated, as predicted, a higher survival rate for TM than for MR, Z(n = 39) = 3.28, p< .001, or NT, Z(n = 41) = 2.54,p< .01 (Fischer exact p = .035),*"" means that ""there was no statistically significant difference in survival rate between TM and no treatment..""


I assert the paper says exactly the opposite.

Who is correct here?


.

[original thread](https://www.reddit.com/r/Mindfulness/comments/8cnt3a/first_ever_neuroimaging_study_of_people_in_the/dxkohgc/)",2,1524789046.0,8f40fm,False,"The paper in question is: [Transcendental meditation, mindfulness, and longevity: an experimental study with the elderly.](https://www.ncbi.nlm.nih.gov/pubmed/2693686)

[full text](https://www.researchgate.net/profile/Ellen_Langer2/publication/20461920_Transcendental_Meditation_mindfulness_and_longevity_An_experimental_study_with_the_elderly/links/541175430cf264cee28b34fd/Transcendental-Meditation-mindfulness-and-longevity-An-experimental-study-with-the-elderly.pdf)

.

The specific question is about this finding:

""*Specific pairwise comparisons indicated, as predicted, a higher survival rate for TM than for MR, Z(n = 39) = 3.28, p< .001, or NT, Z(n = 41) = 2.54,p< .01 (Fischer exact p = .035), with a similar trend also compared with MF, Z(n — 35) = 1.52, p < .10. The rate for MF was also higher than that for MR, Z(n = 36) = 1.67, p < .05. No other pairwise contrasts were significant.2*""


.

TM = Transcendental Meditation

MF = Mindfulness (not MBSR)

MR = mental relaxation (relaxation response)


.

My opponent asserts that:


""*Specific pairwise comparisons indicated, as predicted, a higher survival rate for TM than for MR, Z(n = 39) = 3.28, p< .001, or NT, Z(n = 41) = 2.54,p< .01 (Fischer exact p = .035),*"" means that ""there was no statistically significant difference in survival rate between TM and no treatment..""


I assert the paper says exactly the opposite.

Who is correct here?


.

[original thread](https://www.reddit.com/r/Mindfulness/comments/8cnt3a/first_ever_neuroimaging_study_of_people_in_the/dxkohgc/)",0,"The paper in question is: [Transcendental meditation, mindfulness, and longevity: an experimental study with the elderly.](https://www.ncbi.nlm.nih.gov/pubmed/2693686)

[full text](https://www.researchgate.net/profile/Ellen_Langer2/publication/20461920_Transcendental_Meditation_mindfulness_and_longevity_An_experimental_study_with_the_elderly/links/541175430cf264cee28b34fd/Transcendental-Meditation-mindfulness-and-longevity-An-experimental-study-with-the-elderly.pdf)

.

The specific question is about this finding:

""*Specific pairwise comparisons indicated, as predicted, a higher survival rate for TM than for MR, Z(n = 39) = 3.28, p< .001, or NT, Z(n = 41) = 2.54,p< .01 (Fischer exact p = .035), with a similar trend also compared with MF, Z(n — 35) = 1.52, p < .10. The rate for MF was also higher than that for MR, Z(n = 36) = 1.67, p < .05. No other pairwise contrasts were significant.2*""


.

TM = Transcendental Meditation

MF = Mindfulness (not MBSR)

MR = mental relaxation (relaxation response)


.

My opponent asserts that:


""*Specific pairwise comparisons indicated, as predicted, a higher survival rate for TM than for MR, Z(n = 39) = 3.28, p< .001, or NT, Z(n = 41) = 2.54,p< .01 (Fischer exact p = .035),*"" means that ""there was no statistically significant difference in survival rate between TM and no treatment..""


I assert the paper says exactly the opposite.

Who is correct here?


.

[original thread](https://www.reddit.com/r/Mindfulness/comments/8cnt3a/first_ever_neuroimaging_study_of_people_in_the/dxkohgc/)",1,statistics,54935,,I need help interpreting the statistics of a paper...,https://www.reddit.com/r/statistics/comments/8f40fm/i_need_help_interpreting_the_statistics_of_a_paper/,all_ads,2018-04-26 20:30:46,37 days 05:04:48.036584000,
"I have a set of data that has an R-squared of 0.70 for a linear equation. Most of the data falls either in the bottom left quadrant (negative, negative) or the top right (positive, positive).

But when I look at the bottom left quadrant, it has an R-squared of 0.63 and the top right quadrant has an R-squared of 0.27.

Is there an explanation for this?",2,1524780728.0,8f2xyy,False,"I have a set of data that has an R-squared of 0.70 for a linear equation. Most of the data falls either in the bottom left quadrant (negative, negative) or the top right (positive, positive).

But when I look at the bottom left quadrant, it has an R-squared of 0.63 and the top right quadrant has an R-squared of 0.27.

Is there an explanation for this?",0,"I have a set of data that has an R-squared of 0.70 for a linear equation. Most of the data falls either in the bottom left quadrant (negative, negative) or the top right (positive, positive).

But when I look at the bottom left quadrant, it has an R-squared of 0.63 and the top right quadrant has an R-squared of 0.27.

Is there an explanation for this?",1,statistics,54935,,"Total data set has high R-squared, but the two subsets have lower R-squareds.",https://www.reddit.com/r/statistics/comments/8f2xyy/total_data_set_has_high_rsquared_but_the_two/,all_ads,2018-04-26 18:12:08,37 days 07:23:26.036584000,
"So I'm a math major who focused a lot on pure math courses, which i really enjoyed for its beauty and mind-blowing results. I took probability theory and mathematical statistics recently, and it seems rather... ugly, messy and ""forced"" in its mathematics. In addition, I feel like it's very tough to get an intuitive grasp on things.

So my question is how can I learn to appreciate the beauty behind statistics? I know it must exist but I'm just not seeing. Also, how can I understand statistics in an intuitive manner?",11,1524746320.0,8f01i5,False,"So I'm a math major who focused a lot on pure math courses, which i really enjoyed for its beauty and mind-blowing results. I took probability theory and mathematical statistics recently, and it seems rather... ugly, messy and ""forced"" in its mathematics. In addition, I feel like it's very tough to get an intuitive grasp on things.

So my question is how can I learn to appreciate the beauty behind statistics? I know it must exist but I'm just not seeing. Also, how can I understand statistics in an intuitive manner?",0,"So I'm a math major who focused a lot on pure math courses, which i really enjoyed for its beauty and mind-blowing results. I took probability theory and mathematical statistics recently, and it seems rather... ugly, messy and ""forced"" in its mathematics. In addition, I feel like it's very tough to get an intuitive grasp on things.

So my question is how can I learn to appreciate the beauty behind statistics? I know it must exist but I'm just not seeing. Also, how can I understand statistics in an intuitive manner?",3,statistics,54935,,"As a pure math major, how can I learn to appreciate statistics?",https://www.reddit.com/r/statistics/comments/8f01i5/as_a_pure_math_major_how_can_i_learn_to/,all_ads,2018-04-26 08:38:40,37 days 16:56:54.036584000,
"Hello r/statistics,

My colleagues and I are working a project where we take engineering datasets and turn them into an output. Said colleagues have become infatuated with a rival company’s technology that uses Bayesian Networks to help deal with the sometimes-large uncertainties (or total absence) we may have in a given data point. To help get us back to an objective view of the world I volunteered to give a 30min talk on Bayesian Statistics to help get everyone to engage with the content rather than assuming Bayesian Networks are a silver bullet.

I’m having trouble setting up a simple example using some generic failure data from OREDA to show how a discrete view and a Bayesian view would differ.

Let’s assume I have this dataset:

• Our algorithm gives a vessel a probability of corrosion failure (POFc) an 80% probability within 10 years (87,600 hours)

• In OREDA, the generic vessel corrosion failure rate is 2.5 / 10^6 hours of operational time. Mean is 2.5, lower and upper bounds are 0.85 & 5.7 respectively

What is the discrete and Bayesian view on answering a question about the probability that the vessel will fail within 10 years (87,600 hours) from today.

Thanks",13,1524765629.0,8f1h78,False,"Hello r/statistics,

My colleagues and I are working a project where we take engineering datasets and turn them into an output. Said colleagues have become infatuated with a rival company’s technology that uses Bayesian Networks to help deal with the sometimes-large uncertainties (or total absence) we may have in a given data point. To help get us back to an objective view of the world I volunteered to give a 30min talk on Bayesian Statistics to help get everyone to engage with the content rather than assuming Bayesian Networks are a silver bullet.

I’m having trouble setting up a simple example using some generic failure data from OREDA to show how a discrete view and a Bayesian view would differ.

Let’s assume I have this dataset:

• Our algorithm gives a vessel a probability of corrosion failure (POFc) an 80% probability within 10 years (87,600 hours)

• In OREDA, the generic vessel corrosion failure rate is 2.5 / 10^6 hours of operational time. Mean is 2.5, lower and upper bounds are 0.85 & 5.7 respectively

What is the discrete and Bayesian view on answering a question about the probability that the vessel will fail within 10 years (87,600 hours) from today.

Thanks",1,"Hello r/statistics,

My colleagues and I are working a project where we take engineering datasets and turn them into an output. Said colleagues have become infatuated with a rival company’s technology that uses Bayesian Networks to help deal with the sometimes-large uncertainties (or total absence) we may have in a given data point. To help get us back to an objective view of the world I volunteered to give a 30min talk on Bayesian Statistics to help get everyone to engage with the content rather than assuming Bayesian Networks are a silver bullet.

I’m having trouble setting up a simple example using some generic failure data from OREDA to show how a discrete view and a Bayesian view would differ.

Let’s assume I have this dataset:

• Our algorithm gives a vessel a probability of corrosion failure (POFc) an 80% probability within 10 years (87,600 hours)

• In OREDA, the generic vessel corrosion failure rate is 2.5 / 10^6 hours of operational time. Mean is 2.5, lower and upper bounds are 0.85 & 5.7 respectively

What is the discrete and Bayesian view on answering a question about the probability that the vessel will fail within 10 years (87,600 hours) from today.

Thanks",0,statistics,54935,,Help setting up a Bayesian/Discrete example,https://www.reddit.com/r/statistics/comments/8f1h78/help_setting_up_a_bayesiandiscrete_example/,all_ads,2018-04-26 14:00:29,37 days 11:35:05.036584000,
"Can you please tell me if this understanding of confidence intervals is correct?




First, it makes no sense to say that the probability of the population mean being in any 95% CI (be it a CI that you have already calculated or a CI that can be calculated by a random sample not yet drawn) is 95% because the population mean is not a random variable thus doesn't have a probability associated with it. 



Secondly, given a 95% CI (say 10cm to 83cm as the interval for the length of a bamboo tree, for the sake of concreteness) it makes no sense to say that the probability of this calculated CI containing the population mean is 95%. If you know what the CI is then the CI either contains the population mean or it does not. 



Thirdly, the correct meaning of 95% is: the probability that any random sample will yield a CI that contains the true pop mean. This judgement regarding probability is made BEFORE the confidence interval is concretely determined because if the confidence interval is determined then we can only state that the population mean either is in the CI or it is not.",5,1524747025.0,8f03lt,False,"Can you please tell me if this understanding of confidence intervals is correct?




First, it makes no sense to say that the probability of the population mean being in any 95% CI (be it a CI that you have already calculated or a CI that can be calculated by a random sample not yet drawn) is 95% because the population mean is not a random variable thus doesn't have a probability associated with it. 



Secondly, given a 95% CI (say 10cm to 83cm as the interval for the length of a bamboo tree, for the sake of concreteness) it makes no sense to say that the probability of this calculated CI containing the population mean is 95%. If you know what the CI is then the CI either contains the population mean or it does not. 



Thirdly, the correct meaning of 95% is: the probability that any random sample will yield a CI that contains the true pop mean. This judgement regarding probability is made BEFORE the confidence interval is concretely determined because if the confidence interval is determined then we can only state that the population mean either is in the CI or it is not.",0,"Can you please tell me if this understanding of confidence intervals is correct?




First, it makes no sense to say that the probability of the population mean being in any 95% CI (be it a CI that you have already calculated or a CI that can be calculated by a random sample not yet drawn) is 95% because the population mean is not a random variable thus doesn't have a probability associated with it. 



Secondly, given a 95% CI (say 10cm to 83cm as the interval for the length of a bamboo tree, for the sake of concreteness) it makes no sense to say that the probability of this calculated CI containing the population mean is 95%. If you know what the CI is then the CI either contains the population mean or it does not. 



Thirdly, the correct meaning of 95% is: the probability that any random sample will yield a CI that contains the true pop mean. This judgement regarding probability is made BEFORE the confidence interval is concretely determined because if the confidence interval is determined then we can only state that the population mean either is in the CI or it is not.",2,statistics,54935,,Is this a good conceptualization of what the confidence interval means?,https://www.reddit.com/r/statistics/comments/8f03lt/is_this_a_good_conceptualization_of_what_the/,all_ads,2018-04-26 08:50:25,37 days 16:45:09.036584000,
"I have 2 measurements, Rate1 (before) and Rate2 (after). These give the values for the metabolic rates for about 100 participants. 50 of these participants ate food and the other 50 did not.

If I want to know whether the difference between Rate1 and Rate2 are dependent on whether food was eaten or not, how would you go about doing something like that? What throws me off is that I have 2 categories(ate or not) and I don't know how to plot this except perhaps a histogram with 2 variables.

Thanks",4,1524752142.0,8f0ieu,False,"I have 2 measurements, Rate1 (before) and Rate2 (after). These give the values for the metabolic rates for about 100 participants. 50 of these participants ate food and the other 50 did not.

If I want to know whether the difference between Rate1 and Rate2 are dependent on whether food was eaten or not, how would you go about doing something like that? What throws me off is that I have 2 categories(ate or not) and I don't know how to plot this except perhaps a histogram with 2 variables.

Thanks",0,"I have 2 measurements, Rate1 (before) and Rate2 (after). These give the values for the metabolic rates for about 100 participants. 50 of these participants ate food and the other 50 did not.

If I want to know whether the difference between Rate1 and Rate2 are dependent on whether food was eaten or not, how would you go about doing something like that? What throws me off is that I have 2 categories(ate or not) and I don't know how to plot this except perhaps a histogram with 2 variables.

Thanks",1,statistics,54935,,What statistical method should I use,https://www.reddit.com/r/statistics/comments/8f0ieu/what_statistical_method_should_i_use/,all_ads,2018-04-26 10:15:42,37 days 15:19:52.036584000,
"Hey everybody. I'm doing a study regarding a surgical procedure. In the data below it shows the number of primary procedures performed across the time interval and the number of revision procedures needed in the same year of the same time interval. 

Looking at the data, there seems to be such a large increase in the number of primary procedures, whereas the number of revision procedures isn't keeping up with the primary procedures. However, when I do the statistical analysis doing either a pearson correlation coefficient or spearman's rho calculator - there's no significance. Any suggestions on how to go about this?

https://imgur.com/a/tHWyM2r",51,1524727400.0,8ey3kh,False,"Hey everybody. I'm doing a study regarding a surgical procedure. In the data below it shows the number of primary procedures performed across the time interval and the number of revision procedures needed in the same year of the same time interval. 

Looking at the data, there seems to be such a large increase in the number of primary procedures, whereas the number of revision procedures isn't keeping up with the primary procedures. However, when I do the statistical analysis doing either a pearson correlation coefficient or spearman's rho calculator - there's no significance. Any suggestions on how to go about this?

https://imgur.com/a/tHWyM2r",0,"Hey everybody. I'm doing a study regarding a surgical procedure. In the data below it shows the number of primary procedures performed across the time interval and the number of revision procedures needed in the same year of the same time interval. 

Looking at the data, there seems to be such a large increase in the number of primary procedures, whereas the number of revision procedures isn't keeping up with the primary procedures. However, when I do the statistical analysis doing either a pearson correlation coefficient or spearman's rho calculator - there's no significance. Any suggestions on how to go about this?

https://imgur.com/a/tHWyM2r",3,statistics,54935,,With these values how can there be no significance?,https://www.reddit.com/r/statistics/comments/8ey3kh/with_these_values_how_can_there_be_no_significance/,all_ads,2018-04-26 03:23:20,37 days 22:12:14.036584000,
"Is the following statement true?

""The confidence interval is just telling you how confident you can be that the error rate found in the sample is consistent with the error rate in the population.  Therefore as your confidence interval increases, the sample size will increase to provide the additional assurance that the error rate determined in the sample is representative of the error rate in the overall population.  You can increase your confidence interval which will increase your sample size, but this will only mean that you can be more confident that the error rate provided by the sample is also the same error rate in the population.  In other words, it likely won't affect your actual error rate if that is the error rate in the population.  You could say that you are 95% confident that the 3% error rate in the original sample is representative of the number of errors in the overall population.  Changing your confidence interval will just make you 99% confident that 3% is the true error rate.""
",29,1524713885.0,8ewezb,False,"Is the following statement true?

""The confidence interval is just telling you how confident you can be that the error rate found in the sample is consistent with the error rate in the population.  Therefore as your confidence interval increases, the sample size will increase to provide the additional assurance that the error rate determined in the sample is representative of the error rate in the overall population.  You can increase your confidence interval which will increase your sample size, but this will only mean that you can be more confident that the error rate provided by the sample is also the same error rate in the population.  In other words, it likely won't affect your actual error rate if that is the error rate in the population.  You could say that you are 95% confident that the 3% error rate in the original sample is representative of the number of errors in the overall population.  Changing your confidence interval will just make you 99% confident that 3% is the true error rate.""
",0,"Is the following statement true?

""The confidence interval is just telling you how confident you can be that the error rate found in the sample is consistent with the error rate in the population.  Therefore as your confidence interval increases, the sample size will increase to provide the additional assurance that the error rate determined in the sample is representative of the error rate in the overall population.  You can increase your confidence interval which will increase your sample size, but this will only mean that you can be more confident that the error rate provided by the sample is also the same error rate in the population.  In other words, it likely won't affect your actual error rate if that is the error rate in the population.  You could say that you are 95% confident that the 3% error rate in the original sample is representative of the number of errors in the overall population.  Changing your confidence interval will just make you 99% confident that 3% is the true error rate.""
",5,statistics,54935,,Am I interpreting confidence intervals correctly?,https://www.reddit.com/r/statistics/comments/8ewezb/am_i_interpreting_confidence_intervals_correctly/,all_ads,2018-04-25 23:38:05,38 days 01:57:29.036584000,
"I understand that my question is probably very basic for everyone here but I'm not the greatest at math and can only get so far trying to come up with statistics for my situation. 

There is a raffle where anyone can participate as many times as they want either with their own names or with made up names. Although you can insert as many entries you want with your own name, you can only be picked once. If your name is pulled again it is thrown out. You can apply with fake names and still be able to win and obviously if your name is picked multiple times, you can win multiple times since you are using multiple names. In the end, there will be 20 winners.

Now that the background info is introduced, my question is how many entries would a person need realistically to win at least twice? If anyone here would help me figure out a proper formula I would be very grateful. 

Example: if there are 200 names entered in the lottery you'd need 10 names entered to win at least once. Would that be correct? That would be very over simplistic but in basic statistics would that be how it is? 

And now if there are 200 names entered but there are around 800 entries all together. There are 15 names making 620 entries out of the 800, how would the chances of winning at least twice look like? How many entries would you need to do with your own name or with fake names?

It is based off of a real raffle that I am trying to win with a couple of friends and I would feel a lot more at ease knowing through statistics how much I need to prepare rather than a guesstimate. If any of you would help me make up a formula I would be glad doing the rest of the work myself. Writing out to you guys helped me out a lot understanding many parts of the its' math but I'm still not that great at making formulas so I would be very very grateful if someone could help. 

Thank you",6,1524710284.0,8evy90,False,"I understand that my question is probably very basic for everyone here but I'm not the greatest at math and can only get so far trying to come up with statistics for my situation. 

There is a raffle where anyone can participate as many times as they want either with their own names or with made up names. Although you can insert as many entries you want with your own name, you can only be picked once. If your name is pulled again it is thrown out. You can apply with fake names and still be able to win and obviously if your name is picked multiple times, you can win multiple times since you are using multiple names. In the end, there will be 20 winners.

Now that the background info is introduced, my question is how many entries would a person need realistically to win at least twice? If anyone here would help me figure out a proper formula I would be very grateful. 

Example: if there are 200 names entered in the lottery you'd need 10 names entered to win at least once. Would that be correct? That would be very over simplistic but in basic statistics would that be how it is? 

And now if there are 200 names entered but there are around 800 entries all together. There are 15 names making 620 entries out of the 800, how would the chances of winning at least twice look like? How many entries would you need to do with your own name or with fake names?

It is based off of a real raffle that I am trying to win with a couple of friends and I would feel a lot more at ease knowing through statistics how much I need to prepare rather than a guesstimate. If any of you would help me make up a formula I would be glad doing the rest of the work myself. Writing out to you guys helped me out a lot understanding many parts of the its' math but I'm still not that great at making formulas so I would be very very grateful if someone could help. 

Thank you",0,"I understand that my question is probably very basic for everyone here but I'm not the greatest at math and can only get so far trying to come up with statistics for my situation. 

There is a raffle where anyone can participate as many times as they want either with their own names or with made up names. Although you can insert as many entries you want with your own name, you can only be picked once. If your name is pulled again it is thrown out. You can apply with fake names and still be able to win and obviously if your name is picked multiple times, you can win multiple times since you are using multiple names. In the end, there will be 20 winners.

Now that the background info is introduced, my question is how many entries would a person need realistically to win at least twice? If anyone here would help me figure out a proper formula I would be very grateful. 

Example: if there are 200 names entered in the lottery you'd need 10 names entered to win at least once. Would that be correct? That would be very over simplistic but in basic statistics would that be how it is? 

And now if there are 200 names entered but there are around 800 entries all together. There are 15 names making 620 entries out of the 800, how would the chances of winning at least twice look like? How many entries would you need to do with your own name or with fake names?

It is based off of a real raffle that I am trying to win with a couple of friends and I would feel a lot more at ease knowing through statistics how much I need to prepare rather than a guesstimate. If any of you would help me make up a formula I would be glad doing the rest of the work myself. Writing out to you guys helped me out a lot understanding many parts of the its' math but I'm still not that great at making formulas so I would be very very grateful if someone could help. 

Thank you",6,statistics,54935,,Raffle picking statistics help needed!,https://www.reddit.com/r/statistics/comments/8evy90/raffle_picking_statistics_help_needed/,all_ads,2018-04-25 22:38:04,38 days 02:57:30.036584000,
"Im trying to test a treatment effect in a multinomial logit model (from a choice modelling survey). More specifically, I want to see if a numerical GHG emissions value label on food products could induce a change in dietary habits of meat consumption.
Would it be correct to use the following model to test for a change in meat consumption behavior?

*Y(Beef) = B1(Z(Price))+B2(T)+B3(P)+B4(T)(P)+u(X)+a(group fixed effects)+o(time fixed effects)+e(error term)*

*Y(Chicken) = B1(Z(Price))+B2(T)+B3(P)+B4(T)(P)+u(X)+a(group fixed effects)+o(time fixed effects)+e(error term)*

{...}

with time T=[0,1], treatment P=[0,1] and X=vector of characteristics
I am unfamiliar with both MNL and difference in differences models....please help!",3,1524715530.0,8ewmly,False,"Im trying to test a treatment effect in a multinomial logit model (from a choice modelling survey). More specifically, I want to see if a numerical GHG emissions value label on food products could induce a change in dietary habits of meat consumption.
Would it be correct to use the following model to test for a change in meat consumption behavior?

*Y(Beef) = B1(Z(Price))+B2(T)+B3(P)+B4(T)(P)+u(X)+a(group fixed effects)+o(time fixed effects)+e(error term)*

*Y(Chicken) = B1(Z(Price))+B2(T)+B3(P)+B4(T)(P)+u(X)+a(group fixed effects)+o(time fixed effects)+e(error term)*

{...}

with time T=[0,1], treatment P=[0,1] and X=vector of characteristics
I am unfamiliar with both MNL and difference in differences models....please help!",0,"Im trying to test a treatment effect in a multinomial logit model (from a choice modelling survey). More specifically, I want to see if a numerical GHG emissions value label on food products could induce a change in dietary habits of meat consumption.
Would it be correct to use the following model to test for a change in meat consumption behavior?

*Y(Beef) = B1(Z(Price))+B2(T)+B3(P)+B4(T)(P)+u(X)+a(group fixed effects)+o(time fixed effects)+e(error term)*

*Y(Chicken) = B1(Z(Price))+B2(T)+B3(P)+B4(T)(P)+u(X)+a(group fixed effects)+o(time fixed effects)+e(error term)*

{...}

with time T=[0,1], treatment P=[0,1] and X=vector of characteristics
I am unfamiliar with both MNL and difference in differences models....please help!",1,statistics,54935,,Difference-in-differences with multinomial logit regression,https://www.reddit.com/r/statistics/comments/8ewmly/differenceindifferences_with_multinomial_logit/,all_ads,2018-04-26 00:05:30,38 days 01:30:04.036584000,
"This is a shot in a dark since geometric morphometrics is probably a niche discipline here, but I figured it might be worth it. My question is really in the title of the post. If you have used the R package geomorph, then it uses prcomp() which is an R function for running a PCA. Since it is a package developed by researchers who specialize in GM, I put trust in the statistics part of it. However, what I don't really understand is why a PCA (in that package, and other packages concerned with GM) would treat each X, Y and Z coordinates of each landmark as separate variables, since they are highly associated. ",1,1524711903.0,8ew5ue,False,"This is a shot in a dark since geometric morphometrics is probably a niche discipline here, but I figured it might be worth it. My question is really in the title of the post. If you have used the R package geomorph, then it uses prcomp() which is an R function for running a PCA. Since it is a package developed by researchers who specialize in GM, I put trust in the statistics part of it. However, what I don't really understand is why a PCA (in that package, and other packages concerned with GM) would treat each X, Y and Z coordinates of each landmark as separate variables, since they are highly associated. ",0,"This is a shot in a dark since geometric morphometrics is probably a niche discipline here, but I figured it might be worth it. My question is really in the title of the post. If you have used the R package geomorph, then it uses prcomp() which is an R function for running a PCA. Since it is a package developed by researchers who specialize in GM, I put trust in the statistics part of it. However, what I don't really understand is why a PCA (in that package, and other packages concerned with GM) would treat each X, Y and Z coordinates of each landmark as separate variables, since they are highly associated. ",1,statistics,54935,,"Is anyone here familiar with geometric morphometrics and principal component analysis, and could shed some light on why a PCA would treat the coordinates of landmarks as separate units or values, instead of keeping them together? The Xs and Ys (or Zs assuming 3D) are after all associated.",https://www.reddit.com/r/statistics/comments/8ew5ue/is_anyone_here_familiar_with_geometric/,all_ads,2018-04-25 23:05:03,38 days 02:30:31.036584000,
"Hello, I was wondering if someone could explain to me which variables are dependent in the 2009 document. I already know what variables are dependent in the 2014 document because they say variable A based on variable B + variable C. They dont say this in the 2009 document. An example of a dependent variable in the 2014 document is SPSDINT. Again I am looking for a way to tell which variables are independent vs dependent in the 2009 document.

2009: https://drive.google.com/file/d/1gQZMgYpyP7MzK5HuZgYgrhEWzM76pRuE/view?usp=sharing

2014: https://drive.google.com/open?id=1F0fFPaNI8YU-T33aamSoyyMQGXLQPLiX",5,1524746779.0,8f02v0,False,"Hello, I was wondering if someone could explain to me which variables are dependent in the 2009 document. I already know what variables are dependent in the 2014 document because they say variable A based on variable B + variable C. They dont say this in the 2009 document. An example of a dependent variable in the 2014 document is SPSDINT. Again I am looking for a way to tell which variables are independent vs dependent in the 2009 document.

2009: https://drive.google.com/file/d/1gQZMgYpyP7MzK5HuZgYgrhEWzM76pRuE/view?usp=sharing

2014: https://drive.google.com/open?id=1F0fFPaNI8YU-T33aamSoyyMQGXLQPLiX",0,"Hello, I was wondering if someone could explain to me which variables are dependent in the 2009 document. I already know what variables are dependent in the 2014 document because they say variable A based on variable B + variable C. They dont say this in the 2009 document. An example of a dependent variable in the 2014 document is SPSDINT. Again I am looking for a way to tell which variables are independent vs dependent in the 2009 document.

2009: https://drive.google.com/file/d/1gQZMgYpyP7MzK5HuZgYgrhEWzM76pRuE/view?usp=sharing

2014: https://drive.google.com/open?id=1F0fFPaNI8YU-T33aamSoyyMQGXLQPLiX",0,statistics,54935,,"[Stats 2nd Year] Dependent vs. Independent variables ; Please help, real quick",https://www.reddit.com/r/statistics/comments/8f02v0/stats_2nd_year_dependent_vs_independent_variables/,all_ads,2018-04-26 08:46:19,37 days 16:49:15.036584000,
"Hello,

I'm an undergraduate student and in my course of Data analysis and visualization, we have a topic on General linear model(GLM) which essentially connects ANOVA and Linear regression to GLM. 

I'm looking for references for the same and would be happy if someone could give introductory pointers regarding the same

Thanks ",6,1524689448.0,8etaoi,False,"Hello,

I'm an undergraduate student and in my course of Data analysis and visualization, we have a topic on General linear model(GLM) which essentially connects ANOVA and Linear regression to GLM. 

I'm looking for references for the same and would be happy if someone could give introductory pointers regarding the same

Thanks ",0,"Hello,

I'm an undergraduate student and in my course of Data analysis and visualization, we have a topic on General linear model(GLM) which essentially connects ANOVA and Linear regression to GLM. 

I'm looking for references for the same and would be happy if someone could give introductory pointers regarding the same

Thanks ",2,statistics,54935,,Looking for references on General Linear Model,https://www.reddit.com/r/statistics/comments/8etaoi/looking_for_references_on_general_linear_model/,all_ads,2018-04-25 16:50:48,38 days 08:44:46.036584000,
"Let's say the dependent variable is exam results. I have an explanatory variable of work ethic taken from a questionnaire and it's on a 5 point scale (work quite hard, very hard etc.). If I make dummy variables with the lowest level of work ethic the base group for all dummies, how do I interpret the dummies? Surely the dummy which is furthest from the base group is likely to be the most significant?",2,1524702126.0,8euutp,False,"Let's say the dependent variable is exam results. I have an explanatory variable of work ethic taken from a questionnaire and it's on a 5 point scale (work quite hard, very hard etc.). If I make dummy variables with the lowest level of work ethic the base group for all dummies, how do I interpret the dummies? Surely the dummy which is furthest from the base group is likely to be the most significant?",0,"Let's say the dependent variable is exam results. I have an explanatory variable of work ethic taken from a questionnaire and it's on a 5 point scale (work quite hard, very hard etc.). If I make dummy variables with the lowest level of work ethic the base group for all dummies, how do I interpret the dummies? Surely the dummy which is furthest from the base group is likely to be the most significant?",1,statistics,54935,,Interpreting dummy variables,https://www.reddit.com/r/statistics/comments/8euutp/interpreting_dummy_variables/,all_ads,2018-04-25 20:22:06,38 days 05:13:28.036584000,
"Hi r/statistics,

I have a training set on which I'm running Quinlan's ID3/C4.5 classifiers in R, and the binary outcome variable has a ~50/50 split on yes/no. Now my testing set on the other hand has an 80-85/15-20 (no/yes) split, so when I try testing my model on the set, I get a HUGE number of false negatives as a result, and with the problem I'm trying to solve, I really want to get Type II error out of the way.

Without disturbing the split of my test set, how can I tweak my model to overcome this discrepancy?

Interestingly, this error also arose when my model was trained on a different set with an 80-20 split and tested on a set with a 90-10 split. (still trying to get experienced enough in these things, sorry!)",2,1524700912.0,8euoxw,False,"Hi r/statistics,

I have a training set on which I'm running Quinlan's ID3/C4.5 classifiers in R, and the binary outcome variable has a ~50/50 split on yes/no. Now my testing set on the other hand has an 80-85/15-20 (no/yes) split, so when I try testing my model on the set, I get a HUGE number of false negatives as a result, and with the problem I'm trying to solve, I really want to get Type II error out of the way.

Without disturbing the split of my test set, how can I tweak my model to overcome this discrepancy?

Interestingly, this error also arose when my model was trained on a different set with an 80-20 split and tested on a set with a 90-10 split. (still trying to get experienced enough in these things, sorry!)",0,"Hi r/statistics,

I have a training set on which I'm running Quinlan's ID3/C4.5 classifiers in R, and the binary outcome variable has a ~50/50 split on yes/no. Now my testing set on the other hand has an 80-85/15-20 (no/yes) split, so when I try testing my model on the set, I get a HUGE number of false negatives as a result, and with the problem I'm trying to solve, I really want to get Type II error out of the way.

Without disturbing the split of my test set, how can I tweak my model to overcome this discrepancy?

Interestingly, this error also arose when my model was trained on a different set with an 80-20 split and tested on a set with a 90-10 split. (still trying to get experienced enough in these things, sorry!)",1,statistics,54935,,Training on an imbalanced dataset vs. testing on one,https://www.reddit.com/r/statistics/comments/8euoxw/training_on_an_imbalanced_dataset_vs_testing_on/,all_ads,2018-04-25 20:01:52,38 days 05:33:42.036584000,
"I don't know how to properly word this, so I cannot google it. In short, I ran a model with 4 groups. The model output is presented via a table but I want to summarize the results in text using a format like group 1 < group 2 = group 3... The issue is group 4 is equal to group 3 but greater than group 2. That is the key difference I want to convey to the reader group 2 < group 4 but group group 3 = group 4. Any suggestions on how to clearly convey this to the reader? ",1,1524693835.0,8etsgh,False,"I don't know how to properly word this, so I cannot google it. In short, I ran a model with 4 groups. The model output is presented via a table but I want to summarize the results in text using a format like group 1 < group 2 = group 3... The issue is group 4 is equal to group 3 but greater than group 2. That is the key difference I want to convey to the reader group 2 < group 4 but group group 3 = group 4. Any suggestions on how to clearly convey this to the reader? ",0,"I don't know how to properly word this, so I cannot google it. In short, I ran a model with 4 groups. The model output is presented via a table but I want to summarize the results in text using a format like group 1 < group 2 = group 3... The issue is group 4 is equal to group 3 but greater than group 2. That is the key difference I want to convey to the reader group 2 < group 4 but group group 3 = group 4. Any suggestions on how to clearly convey this to the reader? ",1,statistics,54935,,Summarizing group differences in text write up using < = >,https://www.reddit.com/r/statistics/comments/8etsgh/summarizing_group_differences_in_text_write_up/,all_ads,2018-04-25 18:03:55,38 days 07:31:39.036584000,
"I am specifically looking for something that is good for someone that is trying to learn on their own, hopefully with problems and exercises for the reader to work on. It doesn't have to be a book per se, could be a web-book sort of things as well. The level is irrelevant, could be an intro or an advanced course.


Time Series is, unfortunately, the one area of statistics that I never had any experience with, and I would like to improve that part.",13,1524611323.0,8el4bo,False,"I am specifically looking for something that is good for someone that is trying to learn on their own, hopefully with problems and exercises for the reader to work on. It doesn't have to be a book per se, could be a web-book sort of things as well. The level is irrelevant, could be an intro or an advanced course.


Time Series is, unfortunately, the one area of statistics that I never had any experience with, and I would like to improve that part.",0,"I am specifically looking for something that is good for someone that is trying to learn on their own, hopefully with problems and exercises for the reader to work on. It doesn't have to be a book per se, could be a web-book sort of things as well. The level is irrelevant, could be an intro or an advanced course.


Time Series is, unfortunately, the one area of statistics that I never had any experience with, and I would like to improve that part.",54,statistics,54935,,Looking for a book recommendation on Time Series analysis with Python.,https://www.reddit.com/r/statistics/comments/8el4bo/looking_for_a_book_recommendation_on_time_series/,all_ads,2018-04-24 19:08:43,39 days 06:26:51.036584000,
^,3,1524677768.0,8es9xo,False,^,0,^,0,statistics,54935,,How do you find the critical t-value in SPSS ?,https://www.reddit.com/r/statistics/comments/8es9xo/how_do_you_find_the_critical_tvalue_in_spss/,all_ads,2018-04-25 13:36:08,38 days 11:59:26.036584000,
"For a single sample t test where we compare a sample mean to a known population mean, we are asking the question: ""how many standard deviations of the sampling distribution is the observed mean from the population mean?"", where the standard deviation of the sampling distribution is known as the ""standard error"" and can actually be measured as stdev/sqrt\(n\-1\).

If we took an infinite number of samples from this population, the standard deviation of all of those samples would be the standard error... and so when we see how many standard deviations the two means are from each other, we can use an understanding of the probabilities of a normal distribution to compare them since the sampling distribution is normal per the central limit theorem.

Is that correct?",15,1524621507.0,8emgya,False,"For a single sample t test where we compare a sample mean to a known population mean, we are asking the question: ""how many standard deviations of the sampling distribution is the observed mean from the population mean?"", where the standard deviation of the sampling distribution is known as the ""standard error"" and can actually be measured as stdev/sqrt\(n\-1\).

If we took an infinite number of samples from this population, the standard deviation of all of those samples would be the standard error... and so when we see how many standard deviations the two means are from each other, we can use an understanding of the probabilities of a normal distribution to compare them since the sampling distribution is normal per the central limit theorem.

Is that correct?",0,"For a single sample t test where we compare a sample mean to a known population mean, we are asking the question: ""how many standard deviations of the sampling distribution is the observed mean from the population mean?"", where the standard deviation of the sampling distribution is known as the ""standard error"" and can actually be measured as stdev/sqrt\(n\-1\).

If we took an infinite number of samples from this population, the standard deviation of all of those samples would be the standard error... and so when we see how many standard deviations the two means are from each other, we can use an understanding of the probabilities of a normal distribution to compare them since the sampling distribution is normal per the central limit theorem.

Is that correct?",13,statistics,54935,,"I think I finally understand the T-test, am I right?",https://www.reddit.com/r/statistics/comments/8emgya/i_think_i_finally_understand_the_ttest_am_i_right/,all_ads,2018-04-24 21:58:27,39 days 03:37:07.036584000,
"I'm getting my Masters in Political Science later this month (en passant) and I've been thinking about getting a job in Statistics. I was wondering if anyone else had made a move from a non-Statistics degree into Statistics, and could speak to the challenges of finding a job in the field. 

I minored in political methodology, so I've taken 6 graduate level poli-sci/applied statistics courses, and I've also taken two courses from the Statistics department at my uni on Real Analysis/the mathematical foundations of Stats, so I think I could do the job of a data scientist. But, I'm not sure how much prospective employers will care if I don't actually have the appropriate degree. ",22,1524609076.0,8ektxf,False,"I'm getting my Masters in Political Science later this month (en passant) and I've been thinking about getting a job in Statistics. I was wondering if anyone else had made a move from a non-Statistics degree into Statistics, and could speak to the challenges of finding a job in the field. 

I minored in political methodology, so I've taken 6 graduate level poli-sci/applied statistics courses, and I've also taken two courses from the Statistics department at my uni on Real Analysis/the mathematical foundations of Stats, so I think I could do the job of a data scientist. But, I'm not sure how much prospective employers will care if I don't actually have the appropriate degree. ",0,"I'm getting my Masters in Political Science later this month (en passant) and I've been thinking about getting a job in Statistics. I was wondering if anyone else had made a move from a non-Statistics degree into Statistics, and could speak to the challenges of finding a job in the field. 

I minored in political methodology, so I've taken 6 graduate level poli-sci/applied statistics courses, and I've also taken two courses from the Statistics department at my uni on Real Analysis/the mathematical foundations of Stats, so I think I could do the job of a data scientist. But, I'm not sure how much prospective employers will care if I don't actually have the appropriate degree. ",12,statistics,54935,,Statistics Jobs as Non-Statistician,https://www.reddit.com/r/statistics/comments/8ektxf/statistics_jobs_as_nonstatistician/,all_ads,2018-04-24 18:31:16,39 days 07:04:18.036584000,
"So I'm working on a homework problem where I'm asked to determine if employment status (employed, not employed, or never employed) is related to income level (high, medium, low). The data set is in two columns: Job (which contains employment status) and income (which contains income level).

So how would I determine if there are relationships here? My first thought was correlation, but creating a correlation matrix with non numeric values seems like it won't work. ",5,1524636241.0,8eoe7y,False,"So I'm working on a homework problem where I'm asked to determine if employment status (employed, not employed, or never employed) is related to income level (high, medium, low). The data set is in two columns: Job (which contains employment status) and income (which contains income level).

So how would I determine if there are relationships here? My first thought was correlation, but creating a correlation matrix with non numeric values seems like it won't work. ",0,"So I'm working on a homework problem where I'm asked to determine if employment status (employed, not employed, or never employed) is related to income level (high, medium, low). The data set is in two columns: Job (which contains employment status) and income (which contains income level).

So how would I determine if there are relationships here? My first thought was correlation, but creating a correlation matrix with non numeric values seems like it won't work. ",2,statistics,54935,,How would you determine if things are related?,https://www.reddit.com/r/statistics/comments/8eoe7y/how_would_you_determine_if_things_are_related/,all_ads,2018-04-25 02:04:01,38 days 23:31:33.036584000,
"https://www.reddit.com/r/soccer/comments/8eiv4s/whats_still_possible_and_likely_in_the_premier/?utm_source=reddit-android

Also if someone can link some other how-to's about things like that. I love sports and love stats about teams and their chances of doing something specific.

Thanks.",2,1524627575.0,8enad1,False,"https://www.reddit.com/r/soccer/comments/8eiv4s/whats_still_possible_and_likely_in_the_premier/?utm_source=reddit-android

Also if someone can link some other how-to's about things like that. I love sports and love stats about teams and their chances of doing something specific.

Thanks.",0,"https://www.reddit.com/r/soccer/comments/8eiv4s/whats_still_possible_and_likely_in_the_premier/?utm_source=reddit-android

Also if someone can link some other how-to's about things like that. I love sports and love stats about teams and their chances of doing something specific.

Thanks.",2,statistics,54935,,New to statistics - how to create graphs like this.,https://www.reddit.com/r/statistics/comments/8enad1/new_to_statistics_how_to_create_graphs_like_this/,all_ads,2018-04-24 23:39:35,39 days 01:55:59.036584000,
"Im at a loss here. The teacher sent me an assignment due next week and im backed up like crazy with other classes. Im likely going to fail stats, but no sense in giving up. I'd rather have a d then an F for my GPA[i also did the math and it wont affect my financial aid if i fail, and this is CC so retaking is dirt cheap]. So does anyone just...want to help. Pm me if you do and let me know if these posts are allowed.

Im not asking you to cheat for me. I do want to learn the material. ",3,1524636802.0,8eoglr,False,"Im at a loss here. The teacher sent me an assignment due next week and im backed up like crazy with other classes. Im likely going to fail stats, but no sense in giving up. I'd rather have a d then an F for my GPA[i also did the math and it wont affect my financial aid if i fail, and this is CC so retaking is dirt cheap]. So does anyone just...want to help. Pm me if you do and let me know if these posts are allowed.

Im not asking you to cheat for me. I do want to learn the material. ",0,"Im at a loss here. The teacher sent me an assignment due next week and im backed up like crazy with other classes. Im likely going to fail stats, but no sense in giving up. I'd rather have a d then an F for my GPA[i also did the math and it wont affect my financial aid if i fail, and this is CC so retaking is dirt cheap]. So does anyone just...want to help. Pm me if you do and let me know if these posts are allowed.

Im not asking you to cheat for me. I do want to learn the material. ",0,statistics,54935,,Homework help,https://www.reddit.com/r/statistics/comments/8eoglr/homework_help/,all_ads,2018-04-25 02:13:22,38 days 23:22:12.036584000,
"https://imgur.com/a/WqTZ5pc

Thanks for your help!",5,1524633854.0,8eo3u7,False,"https://imgur.com/a/WqTZ5pc

Thanks for your help!",0,"https://imgur.com/a/WqTZ5pc

Thanks for your help!",0,statistics,54935,,What were the steps used to get from the top (equation) to the bottom (equation)?,https://www.reddit.com/r/statistics/comments/8eo3u7/what_were_the_steps_used_to_get_from_the_top/,all_ads,2018-04-25 01:24:14,39 days 00:11:20.036584000,
"https://imgur.com/a/MxTQByu

Actual article for reference: http://sci-hub.tw/10.2106/JBJS.16.01332

I'm trying to do a trends study, and was told to calculate the incidence for each of the years, and then look to find the R-squared from the incidence. My data is the following:

n = 2016

Years--number in each year
-2005--72
-2006--134
-2007--138
-2008--195
-2009--202
-2010--228
-2011--220
-2012--191
-2013--254
-2014--387",7,1524619500.0,8em75s,False,"https://imgur.com/a/MxTQByu

Actual article for reference: http://sci-hub.tw/10.2106/JBJS.16.01332

I'm trying to do a trends study, and was told to calculate the incidence for each of the years, and then look to find the R-squared from the incidence. My data is the following:

n = 2016

Years--number in each year
-2005--72
-2006--134
-2007--138
-2008--195
-2009--202
-2010--228
-2011--220
-2012--191
-2013--254
-2014--387",0,"https://imgur.com/a/MxTQByu

Actual article for reference: http://sci-hub.tw/10.2106/JBJS.16.01332

I'm trying to do a trends study, and was told to calculate the incidence for each of the years, and then look to find the R-squared from the incidence. My data is the following:

n = 2016

Years--number in each year
-2005--72
-2006--134
-2007--138
-2008--195
-2009--202
-2010--228
-2011--220
-2012--191
-2013--254
-2014--387",2,statistics,54935,,How did they calculate the incidence in this study?,https://www.reddit.com/r/statistics/comments/8em75s/how_did_they_calculate_the_incidence_in_this_study/,all_ads,2018-04-24 21:25:00,39 days 04:10:34.036584000,
https://lucklab.ucdavis.edu/blog/2018/4/19/why-i-lost-faith-in-p-values,4,1524632542.0,8enxu3,False,https://lucklab.ucdavis.edu/blog/2018/4/19/why-i-lost-faith-in-p-values,0,https://lucklab.ucdavis.edu/blog/2018/4/19/why-i-lost-faith-in-p-values,1,statistics,54935,,Why I've lost faith in p values,https://www.reddit.com/r/statistics/comments/8enxu3/why_ive_lost_faith_in_p_values/,all_ads,2018-04-25 01:02:22,39 days 00:33:12.036584000,
"Hey there,

I am looking to find some way to construct confidence intervals for the estimated learning rate parameter in a standard Q-learning reinforcement model. 

I used MLE, with 100 random starts in a bounded interval to estimate two parameters, one of which can be considered as a nuisance parameter. The estimate with the highest likelihood from all the starts is retained. The problem I am seeing is that the likelihood function is not constructed from i.i.d. samples. They are time dependent. Therefore I'm assuming asymptotic behaviour cannot be expected. I'm also pretty sure this rules out bootstrapping as well. 

For a few reasons I was forced to use MLE instead of a bayesian approach, which I realize would probably be more suited to this situation. Please correct me if I am wrong, and thank you very much!!

Info on the Q-model: https://en.wikipedia.org/wiki/Q-learning",0,1524628745.0,8enfx8,False,"Hey there,

I am looking to find some way to construct confidence intervals for the estimated learning rate parameter in a standard Q-learning reinforcement model. 

I used MLE, with 100 random starts in a bounded interval to estimate two parameters, one of which can be considered as a nuisance parameter. The estimate with the highest likelihood from all the starts is retained. The problem I am seeing is that the likelihood function is not constructed from i.i.d. samples. They are time dependent. Therefore I'm assuming asymptotic behaviour cannot be expected. I'm also pretty sure this rules out bootstrapping as well. 

For a few reasons I was forced to use MLE instead of a bayesian approach, which I realize would probably be more suited to this situation. Please correct me if I am wrong, and thank you very much!!

Info on the Q-model: https://en.wikipedia.org/wiki/Q-learning",0,"Hey there,

I am looking to find some way to construct confidence intervals for the estimated learning rate parameter in a standard Q-learning reinforcement model. 

I used MLE, with 100 random starts in a bounded interval to estimate two parameters, one of which can be considered as a nuisance parameter. The estimate with the highest likelihood from all the starts is retained. The problem I am seeing is that the likelihood function is not constructed from i.i.d. samples. They are time dependent. Therefore I'm assuming asymptotic behaviour cannot be expected. I'm also pretty sure this rules out bootstrapping as well. 

For a few reasons I was forced to use MLE instead of a bayesian approach, which I realize would probably be more suited to this situation. Please correct me if I am wrong, and thank you very much!!

Info on the Q-model: https://en.wikipedia.org/wiki/Q-learning",1,statistics,54935,,Confidence Intervals for Maximum Likelihood Estimate in Reinforcement Model,https://www.reddit.com/r/statistics/comments/8enfx8/confidence_intervals_for_maximum_likelihood/,all_ads,2018-04-24 23:59:05,39 days 01:36:29.036584000,
"HiHi

I posted recently in r/statistics. I posted in the hope that statiticians would confirm something about statistics. Sadly, my last post was downvoted into oblivion and filled with high-level misdirection before anyone had a chance to explain things to me in plain English. [(old post for those who are interested)](https://www.reddit.com/r/statistics/comments/8cya0o/statistics_in_psychiatry/)

Round 2
Almost all of what pschiatrists claim to know about mental illness stems from psychiatric studies. However, befor a psychiatric study can be done, one would assume that the test-subjects with the illness relevant must be found. And if psychiatrists have not discovered the illness yet. One would assume the study cannot begin, because such a study would be statistically fraudulent. Which brings me to my question. In statistical terms, how inalienably correct would a lawyer be, were a lawyer to state, that one cannot make causal connections between data gleaned from test-subjects and a mental illness unless one has a medically discovered whether the test-subjects have the mental illness?

That is my question. It has to do with causality. Luckily for me, I'm final year law. The law has given me a good grasp of causality and behaviour. No need to hit me up with the high level stuff. I have a Mech Eng degree too, so, if we could keep things in English (I work for a living) that would be totally awesome and totally cool.

I'm working on whistleblowing some stuff as it pertains to torture and the deaths of more than one hundred. So, kid gloves please. Let's be nice.",5,1524646326.0,8epk4f,False,"HiHi

I posted recently in r/statistics. I posted in the hope that statiticians would confirm something about statistics. Sadly, my last post was downvoted into oblivion and filled with high-level misdirection before anyone had a chance to explain things to me in plain English. [(old post for those who are interested)](https://www.reddit.com/r/statistics/comments/8cya0o/statistics_in_psychiatry/)

Round 2
Almost all of what pschiatrists claim to know about mental illness stems from psychiatric studies. However, befor a psychiatric study can be done, one would assume that the test-subjects with the illness relevant must be found. And if psychiatrists have not discovered the illness yet. One would assume the study cannot begin, because such a study would be statistically fraudulent. Which brings me to my question. In statistical terms, how inalienably correct would a lawyer be, were a lawyer to state, that one cannot make causal connections between data gleaned from test-subjects and a mental illness unless one has a medically discovered whether the test-subjects have the mental illness?

That is my question. It has to do with causality. Luckily for me, I'm final year law. The law has given me a good grasp of causality and behaviour. No need to hit me up with the high level stuff. I have a Mech Eng degree too, so, if we could keep things in English (I work for a living) that would be totally awesome and totally cool.

I'm working on whistleblowing some stuff as it pertains to torture and the deaths of more than one hundred. So, kid gloves please. Let's be nice.",0,"HiHi

I posted recently in r/statistics. I posted in the hope that statiticians would confirm something about statistics. Sadly, my last post was downvoted into oblivion and filled with high-level misdirection before anyone had a chance to explain things to me in plain English. [(old post for those who are interested)](https://www.reddit.com/r/statistics/comments/8cya0o/statistics_in_psychiatry/)

Round 2
Almost all of what pschiatrists claim to know about mental illness stems from psychiatric studies. However, befor a psychiatric study can be done, one would assume that the test-subjects with the illness relevant must be found. And if psychiatrists have not discovered the illness yet. One would assume the study cannot begin, because such a study would be statistically fraudulent. Which brings me to my question. In statistical terms, how inalienably correct would a lawyer be, were a lawyer to state, that one cannot make causal connections between data gleaned from test-subjects and a mental illness unless one has a medically discovered whether the test-subjects have the mental illness?

That is my question. It has to do with causality. Luckily for me, I'm final year law. The law has given me a good grasp of causality and behaviour. No need to hit me up with the high level stuff. I have a Mech Eng degree too, so, if we could keep things in English (I work for a living) that would be totally awesome and totally cool.

I'm working on whistleblowing some stuff as it pertains to torture and the deaths of more than one hundred. So, kid gloves please. Let's be nice.",0,statistics,54935,,Statistics in Psychiatry (Round 2),https://www.reddit.com/r/statistics/comments/8epk4f/statistics_in_psychiatry_round_2/,all_ads,2018-04-25 04:52:06,38 days 20:43:28.036584000,
"Hi,

I am supposed to calculate the impact of an action over a long term metric. However, there is the action is available to all the users. One analogy would be a meditaiton class available in the university that every student can part-take. I want to know if attending this class affects student productivity(their grade, lecture attendance etc.) in longer term. Ideally the comparison would also be done with-in groups, i.e.the differences between students attending different levels/types of meditation classes.

Initially, I was under the impression that difference in difference (DID) estimation could be used but after looking at the data I am not too sure because the time when a student becomes the part of group that takes the class can be different.

Any pointers or ideas as to which area of statistical inference I should look into would be really helpful. Books/references are also welcome. 
",3,1524607123.0,8eklhe,False,"Hi,

I am supposed to calculate the impact of an action over a long term metric. However, there is the action is available to all the users. One analogy would be a meditaiton class available in the university that every student can part-take. I want to know if attending this class affects student productivity(their grade, lecture attendance etc.) in longer term. Ideally the comparison would also be done with-in groups, i.e.the differences between students attending different levels/types of meditation classes.

Initially, I was under the impression that difference in difference (DID) estimation could be used but after looking at the data I am not too sure because the time when a student becomes the part of group that takes the class can be different.

Any pointers or ideas as to which area of statistical inference I should look into would be really helpful. Books/references are also welcome. 
",0,"Hi,

I am supposed to calculate the impact of an action over a long term metric. However, there is the action is available to all the users. One analogy would be a meditaiton class available in the university that every student can part-take. I want to know if attending this class affects student productivity(their grade, lecture attendance etc.) in longer term. Ideally the comparison would also be done with-in groups, i.e.the differences between students attending different levels/types of meditation classes.

Initially, I was under the impression that difference in difference (DID) estimation could be used but after looking at the data I am not too sure because the time when a student becomes the part of group that takes the class can be different.

Any pointers or ideas as to which area of statistical inference I should look into would be really helpful. Books/references are also welcome. 
",2,statistics,54935,,Causal inference in a participant action oriented study without control group,https://www.reddit.com/r/statistics/comments/8eklhe/causal_inference_in_a_participant_action_oriented/,all_ads,2018-04-24 17:58:43,39 days 07:36:51.036584000,
Hello. I am a student in geo sciences and I just got an assignment to work on some remote sensing data. The data include soil reflectance values in the visible and near infrared spectrum as well as in sure soil measurements of different chemical parameters of this points. We use PLSR model to distinguish each parameter of the mixed reflectance signal. What would an alternative regression model would be chosen for a fair comparison of results? ,0,1524617672.0,8elybe,False,Hello. I am a student in geo sciences and I just got an assignment to work on some remote sensing data. The data include soil reflectance values in the visible and near infrared spectrum as well as in sure soil measurements of different chemical parameters of this points. We use PLSR model to distinguish each parameter of the mixed reflectance signal. What would an alternative regression model would be chosen for a fair comparison of results? ,0,Hello. I am a student in geo sciences and I just got an assignment to work on some remote sensing data. The data include soil reflectance values in the visible and near infrared spectrum as well as in sure soil measurements of different chemical parameters of this points. We use PLSR model to distinguish each parameter of the mixed reflectance signal. What would an alternative regression model would be chosen for a fair comparison of results? ,1,statistics,54935,,Regression model for spectrum data,https://www.reddit.com/r/statistics/comments/8elybe/regression_model_for_spectrum_data/,all_ads,2018-04-24 20:54:32,39 days 04:41:02.036584000,
"So I am trying to fully understand the Central Limit Theorem and solidify my understanding of confidence intervals before I start a MOOC course on inferential statistics. I specifically need help with understanding when the *standard deviation of a sampling distribution* is appropriate for constructing a confidence interval, or whether to use the *standard error of the mean*. I am only concerned about cases where the population variance is not known.

I am running simulations in [this notebook](https://github.com/dm3ll3n/Stat_Notebooks/blob/master/Central%20Limit%20Theorem-Halp.ipynb) that I have made available on GitHub. I am hoping someone much smarter than me can help me pinpoint where my understanding is flawed.

I've scoured the internet for clarity, but nothing has really hit the nail on the head for me. I read that [the standard error (SE) of a statistic...is the standard deviation of its sampling distribution.](https://en.wikipedia.org/wiki/Standard_error). I also read that the standard error of a sample is defined as `SE = SD / sqrt(N)`.

So far, I may have a proper understanding of the computations when drawing a single sample from the simulated population. I obtain the sample, compute the standard deviation (using Bessel's correction), then find the S.E.M. using `SE = SD / sqrt(N)`. When I bootstrap this one sample, I find that the standard deviation of the bootstrap sampling distribution is roughly equal to the calculated standard error of the single sample. Awesome.

Where things get hazy for me is when I repeatedly draw samples from the population (e.g., 5 samples, each of size 50). Since I am drawing several samples and computing the mean of each sample, I am left with a sampling distribution of means. I can conclude that the mean of these sample means is in the ball-park of the true population mean, but I'm unsure how to properly report confidence in this estimate.

For one, when equations refer to `n`, I'm unsure this refers to the sample size (50), or the number of samples (5). Also, since this sampling method produces a sampling distribution of means, I am thinking the standard deviation of these sample means is the S.E.M., but that appears to be incorrect. The values obtained by calculating `SE = SD / sqrt(N)` are in line with what I obtain when I bootstrap the distribution of sample means, so I'm more sure this is the correct method. However, in my simulations, it is not uncommon for the true population parameter to be outside of the 95% confidence interval, which makes me less confident in my methods (pun intended). Given that I am drawing many samples, I feel like this approach would lead to more precise confidence intervals.

Any insight is appreciated!

https://github.com/dm3ll3n/Stat_Notebooks/blob/master/Central%20Limit%20Theorem-Halp.ipynb

TLDR: Where I get confused is when I perform several samples of size n directly from the population (in this case, 5 samples of size 50). The distribution of the means of these 5 samples is a sampling distribution, correct? If so, then the standard deviation of the sampling distribution of means is equal to the standard error. But then, why not bootstrap the means of the 5 samples, or calculate the standard error from the formula SE = SD / sqrt(N). And if I use the formula, what is N (5 samples, or size of 50, or 5*50)?
",19,1524554293.0,8efw9d,False,"So I am trying to fully understand the Central Limit Theorem and solidify my understanding of confidence intervals before I start a MOOC course on inferential statistics. I specifically need help with understanding when the *standard deviation of a sampling distribution* is appropriate for constructing a confidence interval, or whether to use the *standard error of the mean*. I am only concerned about cases where the population variance is not known.

I am running simulations in [this notebook](https://github.com/dm3ll3n/Stat_Notebooks/blob/master/Central%20Limit%20Theorem-Halp.ipynb) that I have made available on GitHub. I am hoping someone much smarter than me can help me pinpoint where my understanding is flawed.

I've scoured the internet for clarity, but nothing has really hit the nail on the head for me. I read that [the standard error (SE) of a statistic...is the standard deviation of its sampling distribution.](https://en.wikipedia.org/wiki/Standard_error). I also read that the standard error of a sample is defined as `SE = SD / sqrt(N)`.

So far, I may have a proper understanding of the computations when drawing a single sample from the simulated population. I obtain the sample, compute the standard deviation (using Bessel's correction), then find the S.E.M. using `SE = SD / sqrt(N)`. When I bootstrap this one sample, I find that the standard deviation of the bootstrap sampling distribution is roughly equal to the calculated standard error of the single sample. Awesome.

Where things get hazy for me is when I repeatedly draw samples from the population (e.g., 5 samples, each of size 50). Since I am drawing several samples and computing the mean of each sample, I am left with a sampling distribution of means. I can conclude that the mean of these sample means is in the ball-park of the true population mean, but I'm unsure how to properly report confidence in this estimate.

For one, when equations refer to `n`, I'm unsure this refers to the sample size (50), or the number of samples (5). Also, since this sampling method produces a sampling distribution of means, I am thinking the standard deviation of these sample means is the S.E.M., but that appears to be incorrect. The values obtained by calculating `SE = SD / sqrt(N)` are in line with what I obtain when I bootstrap the distribution of sample means, so I'm more sure this is the correct method. However, in my simulations, it is not uncommon for the true population parameter to be outside of the 95% confidence interval, which makes me less confident in my methods (pun intended). Given that I am drawing many samples, I feel like this approach would lead to more precise confidence intervals.

Any insight is appreciated!

https://github.com/dm3ll3n/Stat_Notebooks/blob/master/Central%20Limit%20Theorem-Halp.ipynb

TLDR: Where I get confused is when I perform several samples of size n directly from the population (in this case, 5 samples of size 50). The distribution of the means of these 5 samples is a sampling distribution, correct? If so, then the standard deviation of the sampling distribution of means is equal to the standard error. But then, why not bootstrap the means of the 5 samples, or calculate the standard error from the formula SE = SD / sqrt(N). And if I use the formula, what is N (5 samples, or size of 50, or 5*50)?
",0,"So I am trying to fully understand the Central Limit Theorem and solidify my understanding of confidence intervals before I start a MOOC course on inferential statistics. I specifically need help with understanding when the *standard deviation of a sampling distribution* is appropriate for constructing a confidence interval, or whether to use the *standard error of the mean*. I am only concerned about cases where the population variance is not known.

I am running simulations in [this notebook](https://github.com/dm3ll3n/Stat_Notebooks/blob/master/Central%20Limit%20Theorem-Halp.ipynb) that I have made available on GitHub. I am hoping someone much smarter than me can help me pinpoint where my understanding is flawed.

I've scoured the internet for clarity, but nothing has really hit the nail on the head for me. I read that [the standard error (SE) of a statistic...is the standard deviation of its sampling distribution.](https://en.wikipedia.org/wiki/Standard_error). I also read that the standard error of a sample is defined as `SE = SD / sqrt(N)`.

So far, I may have a proper understanding of the computations when drawing a single sample from the simulated population. I obtain the sample, compute the standard deviation (using Bessel's correction), then find the S.E.M. using `SE = SD / sqrt(N)`. When I bootstrap this one sample, I find that the standard deviation of the bootstrap sampling distribution is roughly equal to the calculated standard error of the single sample. Awesome.

Where things get hazy for me is when I repeatedly draw samples from the population (e.g., 5 samples, each of size 50). Since I am drawing several samples and computing the mean of each sample, I am left with a sampling distribution of means. I can conclude that the mean of these sample means is in the ball-park of the true population mean, but I'm unsure how to properly report confidence in this estimate.

For one, when equations refer to `n`, I'm unsure this refers to the sample size (50), or the number of samples (5). Also, since this sampling method produces a sampling distribution of means, I am thinking the standard deviation of these sample means is the S.E.M., but that appears to be incorrect. The values obtained by calculating `SE = SD / sqrt(N)` are in line with what I obtain when I bootstrap the distribution of sample means, so I'm more sure this is the correct method. However, in my simulations, it is not uncommon for the true population parameter to be outside of the 95% confidence interval, which makes me less confident in my methods (pun intended). Given that I am drawing many samples, I feel like this approach would lead to more precise confidence intervals.

Any insight is appreciated!

https://github.com/dm3ll3n/Stat_Notebooks/blob/master/Central%20Limit%20Theorem-Halp.ipynb

TLDR: Where I get confused is when I perform several samples of size n directly from the population (in this case, 5 samples of size 50). The distribution of the means of these 5 samples is a sampling distribution, correct? If so, then the standard deviation of the sampling distribution of means is equal to the standard error. But then, why not bootstrap the means of the 5 samples, or calculate the standard error from the formula SE = SD / sqrt(N). And if I use the formula, what is N (5 samples, or size of 50, or 5*50)?
",19,statistics,54935,,Another S.D. vs S.E. question,https://www.reddit.com/r/statistics/comments/8efw9d/another_sd_vs_se_question/,all_ads,2018-04-24 03:18:13,39 days 22:17:21.036584000,
"My mother was born January 9th, 1977
I was born July 22nd, 1996
My brother was born April 23rd, 2004

Kate Middleton was born January 9th, 1982
Her oldest son George was born July 22nd, 2013
Her youngest was born April 23rd, 2018 

While I don't believe that this isn't anything but a mere coincidence, I wonder what are the chances of coincidences like this happening. ",24,1524565536.0,8eh35p,False,"My mother was born January 9th, 1977
I was born July 22nd, 1996
My brother was born April 23rd, 2004

Kate Middleton was born January 9th, 1982
Her oldest son George was born July 22nd, 2013
Her youngest was born April 23rd, 2018 

While I don't believe that this isn't anything but a mere coincidence, I wonder what are the chances of coincidences like this happening. ",0,"My mother was born January 9th, 1977
I was born July 22nd, 1996
My brother was born April 23rd, 2004

Kate Middleton was born January 9th, 1982
Her oldest son George was born July 22nd, 2013
Her youngest was born April 23rd, 2018 

While I don't believe that this isn't anything but a mere coincidence, I wonder what are the chances of coincidences like this happening. ",6,statistics,54935,,What are the chances of two families having the same birthdays?,https://www.reddit.com/r/statistics/comments/8eh35p/what_are_the_chances_of_two_families_having_the/,all_ads,2018-04-24 06:25:36,39 days 19:09:58.036584000,
"So I'm working with a dataset that has multiple rows of data per participant \(long format\). One row is the baseline measurement and the remaining measurements are the post\-baseline measurements. My dilemma is how to structure the data and/or the model to control for the baseline measurement in a linear mixed model. I'm not sure which of the following methods is the best choice:

1. Create an indicator variable for baseline measurements and include this in the model
2. Create a new variable which is equal to the baseline measurement for a participant, and remove the row which contains the baseline measurement. So it would looks something like

From this table

|ID|Measure|Baseline|Week|
|:-|:-|:-|:-|
|1|80|1|0|
|1|120|0|1|
|1|130|0|2|

To this table

|ID|Post\-Baseline Measure|Baseline Measure|Week|
|:-|:-|:-|:-|
|1|120|80|1|
|1|130|80|2|

Is one method better than the other? Do they both do the same thing at the end of the day?",7,1524570120.0,8ehjfm,False,"So I'm working with a dataset that has multiple rows of data per participant \(long format\). One row is the baseline measurement and the remaining measurements are the post\-baseline measurements. My dilemma is how to structure the data and/or the model to control for the baseline measurement in a linear mixed model. I'm not sure which of the following methods is the best choice:

1. Create an indicator variable for baseline measurements and include this in the model
2. Create a new variable which is equal to the baseline measurement for a participant, and remove the row which contains the baseline measurement. So it would looks something like

From this table

|ID|Measure|Baseline|Week|
|:-|:-|:-|:-|
|1|80|1|0|
|1|120|0|1|
|1|130|0|2|

To this table

|ID|Post\-Baseline Measure|Baseline Measure|Week|
|:-|:-|:-|:-|
|1|120|80|1|
|1|130|80|2|

Is one method better than the other? Do they both do the same thing at the end of the day?",0,"So I'm working with a dataset that has multiple rows of data per participant \(long format\). One row is the baseline measurement and the remaining measurements are the post\-baseline measurements. My dilemma is how to structure the data and/or the model to control for the baseline measurement in a linear mixed model. I'm not sure which of the following methods is the best choice:

1. Create an indicator variable for baseline measurements and include this in the model
2. Create a new variable which is equal to the baseline measurement for a participant, and remove the row which contains the baseline measurement. So it would looks something like

From this table

|ID|Measure|Baseline|Week|
|:-|:-|:-|:-|
|1|80|1|0|
|1|120|0|1|
|1|130|0|2|

To this table

|ID|Post\-Baseline Measure|Baseline Measure|Week|
|:-|:-|:-|:-|
|1|120|80|1|
|1|130|80|2|

Is one method better than the other? Do they both do the same thing at the end of the day?",4,statistics,54935,,Controlling for baseline measurements,https://www.reddit.com/r/statistics/comments/8ehjfm/controlling_for_baseline_measurements/,all_ads,2018-04-24 07:42:00,39 days 17:53:34.036584000,
"I have system where there are 36 unique problem issues.

Test group A using T method finds 26 of the problem issues.
Test group B using E method finds 28 of the problem issues.

How do I get the P value using Fishers Exact Test?

It should give P as being 0.786, but I am unable to find this.",1,1524578907.0,8eiad5,False,"I have system where there are 36 unique problem issues.

Test group A using T method finds 26 of the problem issues.
Test group B using E method finds 28 of the problem issues.

How do I get the P value using Fishers Exact Test?

It should give P as being 0.786, but I am unable to find this.",0,"I have system where there are 36 unique problem issues.

Test group A using T method finds 26 of the problem issues.
Test group B using E method finds 28 of the problem issues.

How do I get the P value using Fishers Exact Test?

It should give P as being 0.786, but I am unable to find this.",0,statistics,54935,,Problems with Fishers Exact Test,https://www.reddit.com/r/statistics/comments/8eiad5/problems_with_fishers_exact_test/,all_ads,2018-04-24 10:08:27,39 days 15:27:07.036584000,
"Hey everyone - I'd really appreciate it, if someone could help me out!  

Background: I am currently planning a clinical study, where I will administer active medication and a placebo drug. I will follow the patients for 8 weeks, obtaining data once weekly. My variables are measuring the degree of adverse reactions to radiation therapy.  

I've calculated my sample-size based on the outcome on the 7th week data collection, for both of my three variables.  

My three variables are:  

1) an ordered categorical grading-scale (0, 1, 2, 3 and 4)  
2) a continuous scale  
3) also a continuous scale  

Now, my primary outcome (data at 7 weeks) will be compared between the active and placebo groups through either unpaired t-tests or Mann-Whitney U depending on if data are normally distributed.  

My question is, how do I compare the remaining data points for the two groups? I would like to investigate if the active treatment diminishes the degree of adverse reactions the patients receive from radiation throughout the 8 weeks.  

I'm a bit lost here - Could this comparison be performed with an ANOVA? Would Friedman be better? A bio-statistiscian I spoke with briefly suggested a ""mixed linear model"", which I have no idea what is?  

Also, seeing as I am including patients, I will most likely have to deal with missing data at some point. From what little I understand, the mixed linear model would take missing data into account, but I have no idea how?  

Also: English is not my primary language, so some of the statistical terms might not be completely on point. Feel very free to ask for any clarification! - I don't know if educational background is relevant, nonetheless: I'm an MD, Ph.D.-student. Even though I've had a couple of large statistical courses, it just hasn't ""clicked"" in my head yet - statistical theory just won't stick..  

Again, any and all help is very much appreciated!  

EDIT: Thanks a lot for all of your answers! I'll try and go through them all! Just got up (7 AM here), so I'll look it over asap :)  

EDIT 2:  
I've found that I might not have been very clear originally. For a bit of clarification:  
I will include patients in a single center, the hospital they are receiving radiation therapy in. Patients will be randomized to either active or placebo treatment, and there will be no cross over of patients. They will receive radiation therapy for 5 weeks, where I (or another investigator) will see them once weekly.  

The outcomes measured once weekly are:  
- A RTOG skin toxicity rating score, which is an objective assessment of skin erythema, evaluated by the investigator.  
- A clinical photograph will be taken, and then run through software analyzing the degree of erythema, resulting in a continuous variable  
- A questionnaire, where patients score symptoms on three different scales - these scores are then calculated into a common Breast Symptom Score, which ranges from 0 (no symptoms) to 100 (a lot of symptoms).  

We are trying to alleviate the side-effects of radiation treatment with the active drug we are applying. Normally, the side-effects of the radiation will gradually evolve, peaking two weeks after completed radiation therapy (so, at week 7).  

So, what we'd like to compare, is if our active drug, diminishes the skin-symptoms of the radiation therapy, measured through our three outcome variables (evaluation by MD, evaluation through analytic software, and self-reported by the patients). What we'd like to investigate, is if there is a difference over time between the two treatments (active and placebo) (i.e. does the active drug improve the radiation treatment course, in a way that diminishes symptoms for the patients). We will test specifically at 7 weeks, seeing our sample size calculations are based on the outcomes, where the symptoms will peak (at 7 weeks).",16,1524578820.0,8eia1y,False,"Hey everyone - I'd really appreciate it, if someone could help me out!  

Background: I am currently planning a clinical study, where I will administer active medication and a placebo drug. I will follow the patients for 8 weeks, obtaining data once weekly. My variables are measuring the degree of adverse reactions to radiation therapy.  

I've calculated my sample-size based on the outcome on the 7th week data collection, for both of my three variables.  

My three variables are:  

1) an ordered categorical grading-scale (0, 1, 2, 3 and 4)  
2) a continuous scale  
3) also a continuous scale  

Now, my primary outcome (data at 7 weeks) will be compared between the active and placebo groups through either unpaired t-tests or Mann-Whitney U depending on if data are normally distributed.  

My question is, how do I compare the remaining data points for the two groups? I would like to investigate if the active treatment diminishes the degree of adverse reactions the patients receive from radiation throughout the 8 weeks.  

I'm a bit lost here - Could this comparison be performed with an ANOVA? Would Friedman be better? A bio-statistiscian I spoke with briefly suggested a ""mixed linear model"", which I have no idea what is?  

Also, seeing as I am including patients, I will most likely have to deal with missing data at some point. From what little I understand, the mixed linear model would take missing data into account, but I have no idea how?  

Also: English is not my primary language, so some of the statistical terms might not be completely on point. Feel very free to ask for any clarification! - I don't know if educational background is relevant, nonetheless: I'm an MD, Ph.D.-student. Even though I've had a couple of large statistical courses, it just hasn't ""clicked"" in my head yet - statistical theory just won't stick..  

Again, any and all help is very much appreciated!  

EDIT: Thanks a lot for all of your answers! I'll try and go through them all! Just got up (7 AM here), so I'll look it over asap :)  

EDIT 2:  
I've found that I might not have been very clear originally. For a bit of clarification:  
I will include patients in a single center, the hospital they are receiving radiation therapy in. Patients will be randomized to either active or placebo treatment, and there will be no cross over of patients. They will receive radiation therapy for 5 weeks, where I (or another investigator) will see them once weekly.  

The outcomes measured once weekly are:  
- A RTOG skin toxicity rating score, which is an objective assessment of skin erythema, evaluated by the investigator.  
- A clinical photograph will be taken, and then run through software analyzing the degree of erythema, resulting in a continuous variable  
- A questionnaire, where patients score symptoms on three different scales - these scores are then calculated into a common Breast Symptom Score, which ranges from 0 (no symptoms) to 100 (a lot of symptoms).  

We are trying to alleviate the side-effects of radiation treatment with the active drug we are applying. Normally, the side-effects of the radiation will gradually evolve, peaking two weeks after completed radiation therapy (so, at week 7).  

So, what we'd like to compare, is if our active drug, diminishes the skin-symptoms of the radiation therapy, measured through our three outcome variables (evaluation by MD, evaluation through analytic software, and self-reported by the patients). What we'd like to investigate, is if there is a difference over time between the two treatments (active and placebo) (i.e. does the active drug improve the radiation treatment course, in a way that diminishes symptoms for the patients). We will test specifically at 7 weeks, seeing our sample size calculations are based on the outcomes, where the symptoms will peak (at 7 weeks).",0,"Hey everyone - I'd really appreciate it, if someone could help me out!  

Background: I am currently planning a clinical study, where I will administer active medication and a placebo drug. I will follow the patients for 8 weeks, obtaining data once weekly. My variables are measuring the degree of adverse reactions to radiation therapy.  

I've calculated my sample-size based on the outcome on the 7th week data collection, for both of my three variables.  

My three variables are:  

1) an ordered categorical grading-scale (0, 1, 2, 3 and 4)  
2) a continuous scale  
3) also a continuous scale  

Now, my primary outcome (data at 7 weeks) will be compared between the active and placebo groups through either unpaired t-tests or Mann-Whitney U depending on if data are normally distributed.  

My question is, how do I compare the remaining data points for the two groups? I would like to investigate if the active treatment diminishes the degree of adverse reactions the patients receive from radiation throughout the 8 weeks.  

I'm a bit lost here - Could this comparison be performed with an ANOVA? Would Friedman be better? A bio-statistiscian I spoke with briefly suggested a ""mixed linear model"", which I have no idea what is?  

Also, seeing as I am including patients, I will most likely have to deal with missing data at some point. From what little I understand, the mixed linear model would take missing data into account, but I have no idea how?  

Also: English is not my primary language, so some of the statistical terms might not be completely on point. Feel very free to ask for any clarification! - I don't know if educational background is relevant, nonetheless: I'm an MD, Ph.D.-student. Even though I've had a couple of large statistical courses, it just hasn't ""clicked"" in my head yet - statistical theory just won't stick..  

Again, any and all help is very much appreciated!  

EDIT: Thanks a lot for all of your answers! I'll try and go through them all! Just got up (7 AM here), so I'll look it over asap :)  

EDIT 2:  
I've found that I might not have been very clear originally. For a bit of clarification:  
I will include patients in a single center, the hospital they are receiving radiation therapy in. Patients will be randomized to either active or placebo treatment, and there will be no cross over of patients. They will receive radiation therapy for 5 weeks, where I (or another investigator) will see them once weekly.  

The outcomes measured once weekly are:  
- A RTOG skin toxicity rating score, which is an objective assessment of skin erythema, evaluated by the investigator.  
- A clinical photograph will be taken, and then run through software analyzing the degree of erythema, resulting in a continuous variable  
- A questionnaire, where patients score symptoms on three different scales - these scores are then calculated into a common Breast Symptom Score, which ranges from 0 (no symptoms) to 100 (a lot of symptoms).  

We are trying to alleviate the side-effects of radiation treatment with the active drug we are applying. Normally, the side-effects of the radiation will gradually evolve, peaking two weeks after completed radiation therapy (so, at week 7).  

So, what we'd like to compare, is if our active drug, diminishes the skin-symptoms of the radiation therapy, measured through our three outcome variables (evaluation by MD, evaluation through analytic software, and self-reported by the patients). What we'd like to investigate, is if there is a difference over time between the two treatments (active and placebo) (i.e. does the active drug improve the radiation treatment course, in a way that diminishes symptoms for the patients). We will test specifically at 7 weeks, seeing our sample size calculations are based on the outcomes, where the symptoms will peak (at 7 weeks).",1,statistics,54935,,Unsure what statistical test to use in my clinical trial?,https://www.reddit.com/r/statistics/comments/8eia1y/unsure_what_statistical_test_to_use_in_my/,all_ads,2018-04-24 10:07:00,39 days 15:28:34.036584000,
"I'm looking for a literature review on meta analysis to learn the basics. Because the words ""literature review"" usually come up in articles about meta analysis, I haven't had any luck with my own searches.

Can anyone suggest a good introduction to meta analysis? I'm looking for an overview similar to this: [pdf link] (https://www4.stat.ncsu.edu/~davidian/jointoverview.pdf). I need a paper which goes over the basics in detail.",6,1524519058.0,8ebggf,False,"I'm looking for a literature review on meta analysis to learn the basics. Because the words ""literature review"" usually come up in articles about meta analysis, I haven't had any luck with my own searches.

Can anyone suggest a good introduction to meta analysis? I'm looking for an overview similar to this: [pdf link] (https://www4.stat.ncsu.edu/~davidian/jointoverview.pdf). I need a paper which goes over the basics in detail.",0,"I'm looking for a literature review on meta analysis to learn the basics. Because the words ""literature review"" usually come up in articles about meta analysis, I haven't had any luck with my own searches.

Can anyone suggest a good introduction to meta analysis? I'm looking for an overview similar to this: [pdf link] (https://www4.stat.ncsu.edu/~davidian/jointoverview.pdf). I need a paper which goes over the basics in detail.",16,statistics,54935,,Request: A overview/introduction to meta analysis,https://www.reddit.com/r/statistics/comments/8ebggf/request_a_overviewintroduction_to_meta_analysis/,all_ads,2018-04-23 17:30:58,40 days 08:04:36.036584000,
"Sounds like a way to get out of the 9-5 grind and be your own boss. For example, sports betting. It is possible with bankroll management and a model that produces 60-70% probabilities, right? ",34,1524527852.0,8ecj44,False,"Sounds like a way to get out of the 9-5 grind and be your own boss. For example, sports betting. It is possible with bankroll management and a model that produces 60-70% probabilities, right? ",0,"Sounds like a way to get out of the 9-5 grind and be your own boss. For example, sports betting. It is possible with bankroll management and a model that produces 60-70% probabilities, right? ",9,statistics,54935,,Any statisticians that gamble professionally?,https://www.reddit.com/r/statistics/comments/8ecj44/any_statisticians_that_gamble_professionally/,all_ads,2018-04-23 19:57:32,40 days 05:38:02.036584000,
"I am in a Technical Writing class at UC Davis, and I need to ask certain questions to someone in the field I plan on going into which is Data Science / data analysis.

If you would like to help me, please Reddit message me and I'll send you the questions in Google doc form and you could just fill out your answer under each as little or as much as you want. I need this done pretty soon (I kind of put doing this off too long). Also it would be nice for you to tell me your name and company you work or worked for.

Thanks for your time!",0,1524568288.0,8ehdb1,False,"I am in a Technical Writing class at UC Davis, and I need to ask certain questions to someone in the field I plan on going into which is Data Science / data analysis.

If you would like to help me, please Reddit message me and I'll send you the questions in Google doc form and you could just fill out your answer under each as little or as much as you want. I need this done pretty soon (I kind of put doing this off too long). Also it would be nice for you to tell me your name and company you work or worked for.

Thanks for your time!",0,"I am in a Technical Writing class at UC Davis, and I need to ask certain questions to someone in the field I plan on going into which is Data Science / data analysis.

If you would like to help me, please Reddit message me and I'll send you the questions in Google doc form and you could just fill out your answer under each as little or as much as you want. I need this done pretty soon (I kind of put doing this off too long). Also it would be nice for you to tell me your name and company you work or worked for.

Thanks for your time!",0,statistics,54935,,Need a Data Sciencist/Analyst professional or intern to answer 20 questions about their work,https://www.reddit.com/r/statistics/comments/8ehdb1/need_a_data_sciencistanalyst_professional_or/,all_ads,2018-04-24 07:11:28,39 days 18:24:06.036584000,
"Hi all,

I'm just beginning to learn some biostatistics \- specifically in the fields of physiotherapy/occupational therapy/allied health. I've got some questions on the top two topics

Does anyone happen to chance across books on statistics that make it slightly easier to understand and does away with the technicalities? \- most books I have are pretty much equations all around in mathematical terms which make it quite hard to comprehend \(I don't have much resources to go on a paid course to receive a beginner's course\). 

\_\_\_\_

I've also had been thrown to take over a project due for presentation soon and it involves using a mediation model, SPSS indirect and bootstrapping. I've read Hayes paper but I was wondering if anyone is able to explain more simply what mediation and bootstrapping means. 

Specifically for mediation, if the effect of X on Y can be mediated by A \- with the result being statistically significant, am I right to say that removing A will result in the effect of X on Y becoming statistically insignificant now?

For bootstrapping \- am I right to say we just resample from the sample until desired *n* is achieved say 5000 as recommended, so now my sample size is essentially* *n = 5000?

\_\_\_\_

Essentially, I've had some basic stats training in uni but am really rusty and currently I'm trying to make do with drips and drabs of information on stats which are getting quite confusing. 

Thanks! Sorry I couldnt be more specific. 

Hayes paper \- [http://quantpsy.org/pubs/preacher\_hayes\_2008b.pdf](http://quantpsy.org/pubs/preacher_hayes_2008b.pdf)

Cheers

legitthinker",12,1524521140.0,8ebp1q,False,"Hi all,

I'm just beginning to learn some biostatistics \- specifically in the fields of physiotherapy/occupational therapy/allied health. I've got some questions on the top two topics

Does anyone happen to chance across books on statistics that make it slightly easier to understand and does away with the technicalities? \- most books I have are pretty much equations all around in mathematical terms which make it quite hard to comprehend \(I don't have much resources to go on a paid course to receive a beginner's course\). 

\_\_\_\_

I've also had been thrown to take over a project due for presentation soon and it involves using a mediation model, SPSS indirect and bootstrapping. I've read Hayes paper but I was wondering if anyone is able to explain more simply what mediation and bootstrapping means. 

Specifically for mediation, if the effect of X on Y can be mediated by A \- with the result being statistically significant, am I right to say that removing A will result in the effect of X on Y becoming statistically insignificant now?

For bootstrapping \- am I right to say we just resample from the sample until desired *n* is achieved say 5000 as recommended, so now my sample size is essentially* *n = 5000?

\_\_\_\_

Essentially, I've had some basic stats training in uni but am really rusty and currently I'm trying to make do with drips and drabs of information on stats which are getting quite confusing. 

Thanks! Sorry I couldnt be more specific. 

Hayes paper \- [http://quantpsy.org/pubs/preacher\_hayes\_2008b.pdf](http://quantpsy.org/pubs/preacher_hayes_2008b.pdf)

Cheers

legitthinker",0,"Hi all,

I'm just beginning to learn some biostatistics \- specifically in the fields of physiotherapy/occupational therapy/allied health. I've got some questions on the top two topics

Does anyone happen to chance across books on statistics that make it slightly easier to understand and does away with the technicalities? \- most books I have are pretty much equations all around in mathematical terms which make it quite hard to comprehend \(I don't have much resources to go on a paid course to receive a beginner's course\). 

\_\_\_\_

I've also had been thrown to take over a project due for presentation soon and it involves using a mediation model, SPSS indirect and bootstrapping. I've read Hayes paper but I was wondering if anyone is able to explain more simply what mediation and bootstrapping means. 

Specifically for mediation, if the effect of X on Y can be mediated by A \- with the result being statistically significant, am I right to say that removing A will result in the effect of X on Y becoming statistically insignificant now?

For bootstrapping \- am I right to say we just resample from the sample until desired *n* is achieved say 5000 as recommended, so now my sample size is essentially* *n = 5000?

\_\_\_\_

Essentially, I've had some basic stats training in uni but am really rusty and currently I'm trying to make do with drips and drabs of information on stats which are getting quite confusing. 

Thanks! Sorry I couldnt be more specific. 

Hayes paper \- [http://quantpsy.org/pubs/preacher\_hayes\_2008b.pdf](http://quantpsy.org/pubs/preacher_hayes_2008b.pdf)

Cheers

legitthinker",9,statistics,54935,,Beginner's guide to statistics / Mediation SPSS INDIRECT,https://www.reddit.com/r/statistics/comments/8ebp1q/beginners_guide_to_statistics_mediation_spss/,all_ads,2018-04-23 18:05:40,40 days 07:29:54.036584000,
I was wondering mostly in regards to how deadly the 2 world wars really were in comparitive terms.,1,1524537388.0,8eds79,False,I was wondering mostly in regards to how deadly the 2 world wars really were in comparitive terms.,0,I was wondering mostly in regards to how deadly the 2 world wars really were in comparitive terms.,0,statistics,54935,,Is there data on the total global number of deaths per year?,https://www.reddit.com/r/statistics/comments/8eds79/is_there_data_on_the_total_global_number_of/,all_ads,2018-04-23 22:36:28,40 days 02:59:06.036584000,
"Suppose I am running a regression with two dummy coded dichotomous variables. Unless I am mistaken, the coefficient for IV1 reflects the effect of IV1 only at IV2 = 0.

How does this change when the model also includes the IV1 x IV2 interaction?

Edit: To provide the full story, I am running a regression predicting a continuous DV. I have a half dozen dichotomous control variables, one continuous predictor of interest (viz. IV1), three-level predictor of interest (that is dummy coded; viz. IV2), and their interaction. Am I correct that the parameters for IVs 1 and 2 will only reflect their effects at level 0 of each control variable? That is, am I really only testing their effects at level 0 of each control variable. Or is it just that their coefficients reflect their effects at level 0 of each control variable. The former seems to imply an interaction?",7,1524536590.0,8edod4,False,"Suppose I am running a regression with two dummy coded dichotomous variables. Unless I am mistaken, the coefficient for IV1 reflects the effect of IV1 only at IV2 = 0.

How does this change when the model also includes the IV1 x IV2 interaction?

Edit: To provide the full story, I am running a regression predicting a continuous DV. I have a half dozen dichotomous control variables, one continuous predictor of interest (viz. IV1), three-level predictor of interest (that is dummy coded; viz. IV2), and their interaction. Am I correct that the parameters for IVs 1 and 2 will only reflect their effects at level 0 of each control variable? That is, am I really only testing their effects at level 0 of each control variable. Or is it just that their coefficients reflect their effects at level 0 of each control variable. The former seems to imply an interaction?",0,"Suppose I am running a regression with two dummy coded dichotomous variables. Unless I am mistaken, the coefficient for IV1 reflects the effect of IV1 only at IV2 = 0.

How does this change when the model also includes the IV1 x IV2 interaction?

Edit: To provide the full story, I am running a regression predicting a continuous DV. I have a half dozen dichotomous control variables, one continuous predictor of interest (viz. IV1), three-level predictor of interest (that is dummy coded; viz. IV2), and their interaction. Am I correct that the parameters for IVs 1 and 2 will only reflect their effects at level 0 of each control variable? That is, am I really only testing their effects at level 0 of each control variable. Or is it just that their coefficients reflect their effects at level 0 of each control variable. The former seems to imply an interaction?",0,statistics,54935,,Interpreting coefficients of dummy coded variables.,https://www.reddit.com/r/statistics/comments/8edod4/interpreting_coefficients_of_dummy_coded_variables/,all_ads,2018-04-23 22:23:10,40 days 03:12:24.036584000,
"Hello, for my college level statistics class we were asked to do a chi squared independent test and difference of two proportions test to compare data on men and women, and how they responded to a particular survey. I did both of these tests, came to the same conclusion, but received different z/p scores for the two tests. Is this normal? Why is it normal to receive two different scores for these tests if they're evaluating the same data?",4,1524492890.0,8e9d8m,False,"Hello, for my college level statistics class we were asked to do a chi squared independent test and difference of two proportions test to compare data on men and women, and how they responded to a particular survey. I did both of these tests, came to the same conclusion, but received different z/p scores for the two tests. Is this normal? Why is it normal to receive two different scores for these tests if they're evaluating the same data?",0,"Hello, for my college level statistics class we were asked to do a chi squared independent test and difference of two proportions test to compare data on men and women, and how they responded to a particular survey. I did both of these tests, came to the same conclusion, but received different z/p scores for the two tests. Is this normal? Why is it normal to receive two different scores for these tests if they're evaluating the same data?",8,statistics,54935,,"Two difference tests, different z scores?",https://www.reddit.com/r/statistics/comments/8e9d8m/two_difference_tests_different_z_scores/,all_ads,2018-04-23 10:14:50,40 days 15:20:44.036584000,
"Blog Mirror (includes download links for all 94 such questions): http://www.stats-et-al.com/2018/04/scenario-based-exam-questions-for-intro.html


A scenario based question is one that includes a brief description of a dataset or model, along with some information like a plot, set of summary statistics, or computer output. From this information, the student needs to answer several questions about the data, such as 'what to these parameters represent?' or 'would the correlation be stronger or weaker without the outlier?'. It's my preferred way of asking questions on exams and assignments, as it more closely mimics the sort of problems that someone would actually encounter in a non-classroom setting.

Below are four examples with post-mortem commentary.


The difficulty is in compartmentalizing the questions in such a way that there is a clear way to mark them that isn't too open to interpretation. These problems need to be consistently gradable by senior undergraduate teaching assistants.


**Example 1:**
Consider the following regression output of the amount of flowers in an area in response to the number of beehives in the area. The explanatory variable is between 0 and 6 beehives.

A) What does the intercept describe in the situation in real-world terms? (2 pts)

B) What does the slope describe in the situation in real-world terms? (2 pts)

C) Predict the number of flowers in an area with 2 beehives. (2 pts)

D) Is the prediction in Part C reasonable in real-world terms? Why or why not? (2 pts)

E) Which prediction would have more uncertainty, one for a single area with 2 beehives, or the average of many areas with 2 beehives each? (1 pts)


**Commentary:**

I've made the explanatory variable 'number of beehives' instead of 'thousands of bees', or something similar because I've found that in the past that students get confused about inserting '2' or '2000' into a regression equation. 'Amount of flowers' is ambiguous, as 'thousands of flowers' would cause similar trouble and 'number of flowers' would cause confusion surrounding negative numbers and rounding.

The first two parts are to prime the student into seeing what each variable and parameter in the model is doing. In class, I would have told them that 'in real-world terms' should include at least some mention of the question context, in this case beehives and flowers.

Part C is the only calculation. This particular question is light on calculation, but it's not unusual for my exam questions. The emphasis is on understanding the numbers the computer gives out, not on reproducing them. 

Part D refers to interpolation vs extrapolation. Part E is about the difference between a prediction band and a confidence band, or equally about the sources of uncertainty (model uncertainty vs. natural variation between observations).


**Example 2:** We have a sample of vet records of 50 purebred dogs and  mutts' lifespans and are interested in comparing the mean life expectancies. Assume all factors other than purebred vs. Mutt are irrelevant.  (7 pts total)


   Purebred    Mutt    
 Life Expectancy (yrs)     10.6    14.3           
s    6.4    1.8           
n    16    34        

a) Describe the test to be done to test the given hypothesis.  (2 pts, 0.5 pts per part):


b) Get the t-score, assume you cannot pool standard deviation. (3 pts) (show work)


c) Get the t-critical at alpha = 0.05, assume you cannot pool standard deviation.  (1 pt)


d) State your conclusion in terms of dogs and lifespans. (1 pt)


**Commentary:**
Part A is just asking what should be done, which is again meant to prime students and reinforce a process of considering such things before jumping into mathematics. The '0.5 pts per part' is meant to indicate that at least four facts should be stated (The answer was any four of the following: this is a t-test, between two groups, of their means, as a two-tailed test, independent groups, and we cannot assume equal variance). Each part is written to avoid ambiguities if possible.


Another thing to be avoided when possible is the potential for 'cascading failures'. These are questions were an incorrect answer in one part dooms the student to give an incorrect answer in another part. Here, the answer for part C does not depend on the answer for part B, even though they pertain to the same hypothesis test. If student gets their answer in part A wrong, they likely would have employed the wrong statistical test and the wrong formula regardless. In a worst case scenario, having part A there has not made the subsequent parts harder; in most cases it should make them easier.



**Example 3:**
Last year, Canada helped develop a vaccine to Ebola, to be tested in Sierra Leone. They know it will work, but they don't know which dosage will be the most effective. They set up a randomized trial and split the treatments into 3 groups of 1000 people each. The treatments were low, medium, and high dose. For each group, they measured the proportion of people that ended up infected after a year.


a) What is the explanatory/independent variable? The response/dependent variable? (1 pt)


b) Draw an experiment diagram outlining this experiment (2 pts)


c) What are some lurking variables that could affect the outcome? List at least 2. (2 pts)


d) What is the name of this design? (1 pt)


**Commentary:** Part A again is there to reinforce the process or identifying the problem and solution before executing it. Note that due to differences in terminology between fields, the x variable is referred to as both explanatory AND independent. 


**Example 4:**
A baby can be born either pre-term, at the normal time, or late term. Every baby fits exactly one of these categories (no overlap, no missing values). Without knowing anything else about a given baby, the following probabilities apply:

Pr(pre-term) = 0.20        Pr(normal) = 0.70        Pr(late) = 0.10

a) Find Pr(pre-term OR normal)


b) Find Pr(pre-term AND normal)


c) Find Pr(late | not pre-term)


d) Assume each baby is independent. Find the probability that three particular, unrelated babies are all born at the normal time?


**Commentary:**
The ordering of the question parts here was a tough decision, and in retrospect I messed up. Ideally the question parts progress from easy to hard, but part C requires multiple steps that few people were able to parse. A recurring misconception is that 'conditional on' means the same thing as 'and'; I haven't been able to fix this issue before exam time yet. There was a Part E that asked about independence and common genetic factors, but it has been removed because during the exam, it was found to be badly ambiguous.",0,1524535395.0,8edinb,False,"Blog Mirror (includes download links for all 94 such questions): http://www.stats-et-al.com/2018/04/scenario-based-exam-questions-for-intro.html


A scenario based question is one that includes a brief description of a dataset or model, along with some information like a plot, set of summary statistics, or computer output. From this information, the student needs to answer several questions about the data, such as 'what to these parameters represent?' or 'would the correlation be stronger or weaker without the outlier?'. It's my preferred way of asking questions on exams and assignments, as it more closely mimics the sort of problems that someone would actually encounter in a non-classroom setting.

Below are four examples with post-mortem commentary.


The difficulty is in compartmentalizing the questions in such a way that there is a clear way to mark them that isn't too open to interpretation. These problems need to be consistently gradable by senior undergraduate teaching assistants.


**Example 1:**
Consider the following regression output of the amount of flowers in an area in response to the number of beehives in the area. The explanatory variable is between 0 and 6 beehives.

A) What does the intercept describe in the situation in real-world terms? (2 pts)

B) What does the slope describe in the situation in real-world terms? (2 pts)

C) Predict the number of flowers in an area with 2 beehives. (2 pts)

D) Is the prediction in Part C reasonable in real-world terms? Why or why not? (2 pts)

E) Which prediction would have more uncertainty, one for a single area with 2 beehives, or the average of many areas with 2 beehives each? (1 pts)


**Commentary:**

I've made the explanatory variable 'number of beehives' instead of 'thousands of bees', or something similar because I've found that in the past that students get confused about inserting '2' or '2000' into a regression equation. 'Amount of flowers' is ambiguous, as 'thousands of flowers' would cause similar trouble and 'number of flowers' would cause confusion surrounding negative numbers and rounding.

The first two parts are to prime the student into seeing what each variable and parameter in the model is doing. In class, I would have told them that 'in real-world terms' should include at least some mention of the question context, in this case beehives and flowers.

Part C is the only calculation. This particular question is light on calculation, but it's not unusual for my exam questions. The emphasis is on understanding the numbers the computer gives out, not on reproducing them. 

Part D refers to interpolation vs extrapolation. Part E is about the difference between a prediction band and a confidence band, or equally about the sources of uncertainty (model uncertainty vs. natural variation between observations).


**Example 2:** We have a sample of vet records of 50 purebred dogs and  mutts' lifespans and are interested in comparing the mean life expectancies. Assume all factors other than purebred vs. Mutt are irrelevant.  (7 pts total)


   Purebred    Mutt    
 Life Expectancy (yrs)     10.6    14.3           
s    6.4    1.8           
n    16    34        

a) Describe the test to be done to test the given hypothesis.  (2 pts, 0.5 pts per part):


b) Get the t-score, assume you cannot pool standard deviation. (3 pts) (show work)


c) Get the t-critical at alpha = 0.05, assume you cannot pool standard deviation.  (1 pt)


d) State your conclusion in terms of dogs and lifespans. (1 pt)


**Commentary:**
Part A is just asking what should be done, which is again meant to prime students and reinforce a process of considering such things before jumping into mathematics. The '0.5 pts per part' is meant to indicate that at least four facts should be stated (The answer was any four of the following: this is a t-test, between two groups, of their means, as a two-tailed test, independent groups, and we cannot assume equal variance). Each part is written to avoid ambiguities if possible.


Another thing to be avoided when possible is the potential for 'cascading failures'. These are questions were an incorrect answer in one part dooms the student to give an incorrect answer in another part. Here, the answer for part C does not depend on the answer for part B, even though they pertain to the same hypothesis test. If student gets their answer in part A wrong, they likely would have employed the wrong statistical test and the wrong formula regardless. In a worst case scenario, having part A there has not made the subsequent parts harder; in most cases it should make them easier.



**Example 3:**
Last year, Canada helped develop a vaccine to Ebola, to be tested in Sierra Leone. They know it will work, but they don't know which dosage will be the most effective. They set up a randomized trial and split the treatments into 3 groups of 1000 people each. The treatments were low, medium, and high dose. For each group, they measured the proportion of people that ended up infected after a year.


a) What is the explanatory/independent variable? The response/dependent variable? (1 pt)


b) Draw an experiment diagram outlining this experiment (2 pts)


c) What are some lurking variables that could affect the outcome? List at least 2. (2 pts)


d) What is the name of this design? (1 pt)


**Commentary:** Part A again is there to reinforce the process or identifying the problem and solution before executing it. Note that due to differences in terminology between fields, the x variable is referred to as both explanatory AND independent. 


**Example 4:**
A baby can be born either pre-term, at the normal time, or late term. Every baby fits exactly one of these categories (no overlap, no missing values). Without knowing anything else about a given baby, the following probabilities apply:

Pr(pre-term) = 0.20        Pr(normal) = 0.70        Pr(late) = 0.10

a) Find Pr(pre-term OR normal)


b) Find Pr(pre-term AND normal)


c) Find Pr(late | not pre-term)


d) Assume each baby is independent. Find the probability that three particular, unrelated babies are all born at the normal time?


**Commentary:**
The ordering of the question parts here was a tough decision, and in retrospect I messed up. Ideally the question parts progress from easy to hard, but part C requires multiple steps that few people were able to parse. A recurring misconception is that 'conditional on' means the same thing as 'and'; I haven't been able to fix this issue before exam time yet. There was a Part E that asked about independence and common genetic factors, but it has been removed because during the exam, it was found to be badly ambiguous.",0,"Blog Mirror (includes download links for all 94 such questions): http://www.stats-et-al.com/2018/04/scenario-based-exam-questions-for-intro.html


A scenario based question is one that includes a brief description of a dataset or model, along with some information like a plot, set of summary statistics, or computer output. From this information, the student needs to answer several questions about the data, such as 'what to these parameters represent?' or 'would the correlation be stronger or weaker without the outlier?'. It's my preferred way of asking questions on exams and assignments, as it more closely mimics the sort of problems that someone would actually encounter in a non-classroom setting.

Below are four examples with post-mortem commentary.


The difficulty is in compartmentalizing the questions in such a way that there is a clear way to mark them that isn't too open to interpretation. These problems need to be consistently gradable by senior undergraduate teaching assistants.


**Example 1:**
Consider the following regression output of the amount of flowers in an area in response to the number of beehives in the area. The explanatory variable is between 0 and 6 beehives.

A) What does the intercept describe in the situation in real-world terms? (2 pts)

B) What does the slope describe in the situation in real-world terms? (2 pts)

C) Predict the number of flowers in an area with 2 beehives. (2 pts)

D) Is the prediction in Part C reasonable in real-world terms? Why or why not? (2 pts)

E) Which prediction would have more uncertainty, one for a single area with 2 beehives, or the average of many areas with 2 beehives each? (1 pts)


**Commentary:**

I've made the explanatory variable 'number of beehives' instead of 'thousands of bees', or something similar because I've found that in the past that students get confused about inserting '2' or '2000' into a regression equation. 'Amount of flowers' is ambiguous, as 'thousands of flowers' would cause similar trouble and 'number of flowers' would cause confusion surrounding negative numbers and rounding.

The first two parts are to prime the student into seeing what each variable and parameter in the model is doing. In class, I would have told them that 'in real-world terms' should include at least some mention of the question context, in this case beehives and flowers.

Part C is the only calculation. This particular question is light on calculation, but it's not unusual for my exam questions. The emphasis is on understanding the numbers the computer gives out, not on reproducing them. 

Part D refers to interpolation vs extrapolation. Part E is about the difference between a prediction band and a confidence band, or equally about the sources of uncertainty (model uncertainty vs. natural variation between observations).


**Example 2:** We have a sample of vet records of 50 purebred dogs and  mutts' lifespans and are interested in comparing the mean life expectancies. Assume all factors other than purebred vs. Mutt are irrelevant.  (7 pts total)


   Purebred    Mutt    
 Life Expectancy (yrs)     10.6    14.3           
s    6.4    1.8           
n    16    34        

a) Describe the test to be done to test the given hypothesis.  (2 pts, 0.5 pts per part):


b) Get the t-score, assume you cannot pool standard deviation. (3 pts) (show work)


c) Get the t-critical at alpha = 0.05, assume you cannot pool standard deviation.  (1 pt)


d) State your conclusion in terms of dogs and lifespans. (1 pt)


**Commentary:**
Part A is just asking what should be done, which is again meant to prime students and reinforce a process of considering such things before jumping into mathematics. The '0.5 pts per part' is meant to indicate that at least four facts should be stated (The answer was any four of the following: this is a t-test, between two groups, of their means, as a two-tailed test, independent groups, and we cannot assume equal variance). Each part is written to avoid ambiguities if possible.


Another thing to be avoided when possible is the potential for 'cascading failures'. These are questions were an incorrect answer in one part dooms the student to give an incorrect answer in another part. Here, the answer for part C does not depend on the answer for part B, even though they pertain to the same hypothesis test. If student gets their answer in part A wrong, they likely would have employed the wrong statistical test and the wrong formula regardless. In a worst case scenario, having part A there has not made the subsequent parts harder; in most cases it should make them easier.



**Example 3:**
Last year, Canada helped develop a vaccine to Ebola, to be tested in Sierra Leone. They know it will work, but they don't know which dosage will be the most effective. They set up a randomized trial and split the treatments into 3 groups of 1000 people each. The treatments were low, medium, and high dose. For each group, they measured the proportion of people that ended up infected after a year.


a) What is the explanatory/independent variable? The response/dependent variable? (1 pt)


b) Draw an experiment diagram outlining this experiment (2 pts)


c) What are some lurking variables that could affect the outcome? List at least 2. (2 pts)


d) What is the name of this design? (1 pt)


**Commentary:** Part A again is there to reinforce the process or identifying the problem and solution before executing it. Note that due to differences in terminology between fields, the x variable is referred to as both explanatory AND independent. 


**Example 4:**
A baby can be born either pre-term, at the normal time, or late term. Every baby fits exactly one of these categories (no overlap, no missing values). Without knowing anything else about a given baby, the following probabilities apply:

Pr(pre-term) = 0.20        Pr(normal) = 0.70        Pr(late) = 0.10

a) Find Pr(pre-term OR normal)


b) Find Pr(pre-term AND normal)


c) Find Pr(late | not pre-term)


d) Assume each baby is independent. Find the probability that three particular, unrelated babies are all born at the normal time?


**Commentary:**
The ordering of the question parts here was a tough decision, and in retrospect I messed up. Ideally the question parts progress from easy to hard, but part C requires multiple steps that few people were able to parse. A recurring misconception is that 'conditional on' means the same thing as 'and'; I haven't been able to fix this issue before exam time yet. There was a Part E that asked about independence and common genetic factors, but it has been removed because during the exam, it was found to be badly ambiguous.",1,statistics,54935,,Scenario Based Exam Questions for Intro Statistics,https://www.reddit.com/r/statistics/comments/8edinb/scenario_based_exam_questions_for_intro_statistics/,all_ads,2018-04-23 22:03:15,40 days 03:32:19.036584000,
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5000555/#!po=21.4286,6,1524426479.0,8e2rlp,False,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5000555/#!po=21.4286,0,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5000555/#!po=21.4286,72,statistics,54935,,LongCatEDA: An R package to visualize longitudinal change in categorical variables,https://www.reddit.com/r/statistics/comments/8e2rlp/longcateda_an_r_package_to_visualize_longitudinal/,all_ads,2018-04-22 15:47:59,41 days 09:47:35.036584000,
"I'm wondering what people mean by this term. In my stats classes we make models that ""predict"" a variable Y \*given\* the input of X. Or for a particular variable's time series, we can attempt to do a forecast. 

So what is ""predictive analytics""? Does the company ask you to make a model, then gives you the X that they expect to occur in the future? Perhaps you generate these Xs yourself first doing many individual time series forecasts? ",7,1524482185.0,8e8gwt,False,"I'm wondering what people mean by this term. In my stats classes we make models that ""predict"" a variable Y \*given\* the input of X. Or for a particular variable's time series, we can attempt to do a forecast. 

So what is ""predictive analytics""? Does the company ask you to make a model, then gives you the X that they expect to occur in the future? Perhaps you generate these Xs yourself first doing many individual time series forecasts? ",0,"I'm wondering what people mean by this term. In my stats classes we make models that ""predict"" a variable Y \*given\* the input of X. Or for a particular variable's time series, we can attempt to do a forecast. 

So what is ""predictive analytics""? Does the company ask you to make a model, then gives you the X that they expect to occur in the future? Perhaps you generate these Xs yourself first doing many individual time series forecasts? ",5,statistics,54935,,"What specifically do people mean by ""predictive analytics""?",https://www.reddit.com/r/statistics/comments/8e8gwt/what_specifically_do_people_mean_by_predictive/,all_ads,2018-04-23 07:16:25,40 days 18:19:09.036584000,
"I want to start learning statistics, and was wondering if starting with statistical inference Casella and Berger is a good choice. In terms of my mathematical experience, I know some Linear Algebra (worked through Axler, Friedberg, and Hoffman) and some Calculus (Spivak and Wade).

Or is there perhaps a better alternative? I want a textbook that is rigorous and contains challenging exercises.",19,1524490410.0,8e96ip,False,"I want to start learning statistics, and was wondering if starting with statistical inference Casella and Berger is a good choice. In terms of my mathematical experience, I know some Linear Algebra (worked through Axler, Friedberg, and Hoffman) and some Calculus (Spivak and Wade).

Or is there perhaps a better alternative? I want a textbook that is rigorous and contains challenging exercises.",0,"I want to start learning statistics, and was wondering if starting with statistical inference Casella and Berger is a good choice. In terms of my mathematical experience, I know some Linear Algebra (worked through Axler, Friedberg, and Hoffman) and some Calculus (Spivak and Wade).

Or is there perhaps a better alternative? I want a textbook that is rigorous and contains challenging exercises.",2,statistics,54935,,Opinion on Statistical Inference by Casella and Berger,https://www.reddit.com/r/statistics/comments/8e96ip/opinion_on_statistical_inference_by_casella_and/,all_ads,2018-04-23 09:33:30,40 days 16:02:04.036584000,
"Hey friends, I'm trying to convert an SPSS file to Stata or .xls

Do you have any links for free resources on doing this? I'm not averse to torrent, and I'm running Windows. 

Thanks!",5,1524454641.0,8e5lvi,False,"Hey friends, I'm trying to convert an SPSS file to Stata or .xls

Do you have any links for free resources on doing this? I'm not averse to torrent, and I'm running Windows. 

Thanks!",0,"Hey friends, I'm trying to convert an SPSS file to Stata or .xls

Do you have any links for free resources on doing this? I'm not averse to torrent, and I'm running Windows. 

Thanks!",4,statistics,54935,,SPSS file conversion? Need free software.,https://www.reddit.com/r/statistics/comments/8e5lvi/spss_file_conversion_need_free_software/,all_ads,2018-04-22 23:37:21,41 days 01:58:13.036584000,
"Medical student doing his first research paper here, so please forgive me if the answer to this question is too obvious for you.

I conducted a survey (n=173) in which two questions were:
-Out of 100 men, how many do you think will develop cancer in their lifetime?
-Out of 100 women, how many do you think will develop cancer in their lifetime?

I'm using SPSS for the statistical analysis.

The mean for men was 48,9% and for women 52,2%.

As you can see, they are both pretty close to 50% and to each other. I'd like to know if the answer to one question depends on the other one. I.e. if I said incidence in men is 70%, I also said women's is close to 70%. Basically, what I'm trying to prove, is that people think that cancer incidence is not gender-dependent.

What's the proper statistical test for this?

I'm not even sure I'm wording this correctly, but I'll be glad to try and explain it again if needed.

Edit: typo.",16,1524455458.0,8e5p8b,False,"Medical student doing his first research paper here, so please forgive me if the answer to this question is too obvious for you.

I conducted a survey (n=173) in which two questions were:
-Out of 100 men, how many do you think will develop cancer in their lifetime?
-Out of 100 women, how many do you think will develop cancer in their lifetime?

I'm using SPSS for the statistical analysis.

The mean for men was 48,9% and for women 52,2%.

As you can see, they are both pretty close to 50% and to each other. I'd like to know if the answer to one question depends on the other one. I.e. if I said incidence in men is 70%, I also said women's is close to 70%. Basically, what I'm trying to prove, is that people think that cancer incidence is not gender-dependent.

What's the proper statistical test for this?

I'm not even sure I'm wording this correctly, but I'll be glad to try and explain it again if needed.

Edit: typo.",0,"Medical student doing his first research paper here, so please forgive me if the answer to this question is too obvious for you.

I conducted a survey (n=173) in which two questions were:
-Out of 100 men, how many do you think will develop cancer in their lifetime?
-Out of 100 women, how many do you think will develop cancer in their lifetime?

I'm using SPSS for the statistical analysis.

The mean for men was 48,9% and for women 52,2%.

As you can see, they are both pretty close to 50% and to each other. I'd like to know if the answer to one question depends on the other one. I.e. if I said incidence in men is 70%, I also said women's is close to 70%. Basically, what I'm trying to prove, is that people think that cancer incidence is not gender-dependent.

What's the proper statistical test for this?

I'm not even sure I'm wording this correctly, but I'll be glad to try and explain it again if needed.

Edit: typo.",1,statistics,54935,,Which statistical test to use,https://www.reddit.com/r/statistics/comments/8e5p8b/which_statistical_test_to_use/,all_ads,2018-04-22 23:50:58,41 days 01:44:36.036584000,
"Hi All, 

I'm trying to think through when it makes sense to square or cube a variable. For example, if we have a gender variable then it obviously doesn't make sense to since neither gender\^2 or gender\^3 would have a sensible interpretation. 

What about proportion variables? For example, if we have unemployment rate as a variable. I'm guessing that we wouldn't especially because it's possible for a proportion to end greater than one if we square or cube, but was curious to find out for sure. 

Thanks!",5,1524451213.0,8e57rp,False,"Hi All, 

I'm trying to think through when it makes sense to square or cube a variable. For example, if we have a gender variable then it obviously doesn't make sense to since neither gender\^2 or gender\^3 would have a sensible interpretation. 

What about proportion variables? For example, if we have unemployment rate as a variable. I'm guessing that we wouldn't especially because it's possible for a proportion to end greater than one if we square or cube, but was curious to find out for sure. 

Thanks!",0,"Hi All, 

I'm trying to think through when it makes sense to square or cube a variable. For example, if we have a gender variable then it obviously doesn't make sense to since neither gender\^2 or gender\^3 would have a sensible interpretation. 

What about proportion variables? For example, if we have unemployment rate as a variable. I'm guessing that we wouldn't especially because it's possible for a proportion to end greater than one if we square or cube, but was curious to find out for sure. 

Thanks!",2,statistics,54935,,Polynomial Regression - Proportion Variables?,https://www.reddit.com/r/statistics/comments/8e57rp/polynomial_regression_proportion_variables/,all_ads,2018-04-22 22:40:13,41 days 02:55:21.036584000,
"Hey guys! So I'm currently working on a little project to find the ""center of mass"" of per capita GDP in Continental Europe. Fun stuff! Because this is only small, informal project, I'm using the NUTS1 statistical units that essentially divide EU States+ into 98 different regions based on economic factors, etc.

What I've already done is find the per capita GDP of each of these regions. What I have yet to do is devise some coordinate system so that I can find a double Riemann Sum of the ""density"" (aka per capita GDP) of these regions. 

**What I'm having trouble with is finding the area of each of these regions. Does anyone know where I can find this?**",2,1524435405.0,8e3hp8,False,"Hey guys! So I'm currently working on a little project to find the ""center of mass"" of per capita GDP in Continental Europe. Fun stuff! Because this is only small, informal project, I'm using the NUTS1 statistical units that essentially divide EU States+ into 98 different regions based on economic factors, etc.

What I've already done is find the per capita GDP of each of these regions. What I have yet to do is devise some coordinate system so that I can find a double Riemann Sum of the ""density"" (aka per capita GDP) of these regions. 

**What I'm having trouble with is finding the area of each of these regions. Does anyone know where I can find this?**",0,"Hey guys! So I'm currently working on a little project to find the ""center of mass"" of per capita GDP in Continental Europe. Fun stuff! Because this is only small, informal project, I'm using the NUTS1 statistical units that essentially divide EU States+ into 98 different regions based on economic factors, etc.

What I've already done is find the per capita GDP of each of these regions. What I have yet to do is devise some coordinate system so that I can find a double Riemann Sum of the ""density"" (aka per capita GDP) of these regions. 

**What I'm having trouble with is finding the area of each of these regions. Does anyone know where I can find this?**",3,statistics,54935,,Areas of NUTS1 Regions?,https://www.reddit.com/r/statistics/comments/8e3hp8/areas_of_nuts1_regions/,all_ads,2018-04-22 18:16:45,41 days 07:18:49.036584000,
"Hi all,

I was offered a statistical consultant intern position at a hospital. To the people who have worked similar positions before, was the experience helpful? Other than you get to put it on your CV. 

The position I'm offered is unpaid. While I don't mind being an unpaid intern, my schedule is looking pretty tight right now for the summer. I'm afraid it's going to be a waste of my time and adding on extra stress. 

Any opinion is welcome. Thanks 

",7,1524408651.0,8e1opm,False,"Hi all,

I was offered a statistical consultant intern position at a hospital. To the people who have worked similar positions before, was the experience helpful? Other than you get to put it on your CV. 

The position I'm offered is unpaid. While I don't mind being an unpaid intern, my schedule is looking pretty tight right now for the summer. I'm afraid it's going to be a waste of my time and adding on extra stress. 

Any opinion is welcome. Thanks 

",0,"Hi all,

I was offered a statistical consultant intern position at a hospital. To the people who have worked similar positions before, was the experience helpful? Other than you get to put it on your CV. 

The position I'm offered is unpaid. While I don't mind being an unpaid intern, my schedule is looking pretty tight right now for the summer. I'm afraid it's going to be a waste of my time and adding on extra stress. 

Any opinion is welcome. Thanks 

",10,statistics,54935,,Intern as a statistical consultant. People who have done it. Was it helpful for your career building?,https://www.reddit.com/r/statistics/comments/8e1opm/intern_as_a_statistical_consultant_people_who/,all_ads,2018-04-22 10:50:51,41 days 14:44:43.036584000,
,16,1524428549.0,8e2wxu,False,,0,,1,statistics,54935,,"How do you react to laymen butchering stats for some agenda, political or otherwise? Any examples?",https://www.reddit.com/r/statistics/comments/8e2wxu/how_do_you_react_to_laymen_butchering_stats_for/,all_ads,2018-04-22 16:22:29,41 days 09:13:05.036584000,
"Which of the three is the best to learn and why? 

I'm think this may be context dependent, so maybe it's better to ask which is the best to learn and why for different sectors (e.g. academia, govt, or private sector?) or fields (e.g. poli sci, psych, or econ?).

EDIT: I'll definitely start learning R.",126,1524368690.0,8dy3aj,False,"Which of the three is the best to learn and why? 

I'm think this may be context dependent, so maybe it's better to ask which is the best to learn and why for different sectors (e.g. academia, govt, or private sector?) or fields (e.g. poli sci, psych, or econ?).

EDIT: I'll definitely start learning R.",0,"Which of the three is the best to learn and why? 

I'm think this may be context dependent, so maybe it's better to ask which is the best to learn and why for different sectors (e.g. academia, govt, or private sector?) or fields (e.g. poli sci, psych, or econ?).

EDIT: I'll definitely start learning R.",29,statistics,54935,,SPSS v. SAS v. STATA,https://www.reddit.com/r/statistics/comments/8dy3aj/spss_v_sas_v_stata/,all_ads,2018-04-21 23:44:50,42 days 01:50:44.036584000,
"Hello r/statistics, 

I don't know where else I should direct this question but I'm hoping someone here might be able to help since I feel like this is somewhat related to statistical analytics. I've read the rules and I think this post is okay.  

I'm looking for some kind of google doc/online spreadsheet program that will allow me to compute arithmetic mean on a fluctuating set of numbers entered by people whom I personally give permission to. Basically, a group of my friends and I play an online game where items sell for different prices based on economic forces and we're trying to make a list that tracks selling prices. I'd like it to work such that I can be the creator/owner of the spreadsheet and allow everyone in the world to look at it but for only approved people to edit it so I can make sure no one trolls it to throw off the prices. I'd like to do something where the users entering data have a dropdown/MSAccess type of form to select the item they're selling and then a text box to enter the price they sold it for. I then want them to hit 'submit' and have that sale go into the back-end of the database such that the front end does the math for us and displays the average price for the world to see.  

I could probably code something in Excel but that wouldn't be easily editable by many people. Google Docs is a good start but I haven't seen the automated averaging after the price has been entered. I don't know where to ask so I'm taking a shot in the dark here - anyone have any ideas? Thanks in advance!",4,1524433764.0,8e3c56,False,"Hello r/statistics, 

I don't know where else I should direct this question but I'm hoping someone here might be able to help since I feel like this is somewhat related to statistical analytics. I've read the rules and I think this post is okay.  

I'm looking for some kind of google doc/online spreadsheet program that will allow me to compute arithmetic mean on a fluctuating set of numbers entered by people whom I personally give permission to. Basically, a group of my friends and I play an online game where items sell for different prices based on economic forces and we're trying to make a list that tracks selling prices. I'd like it to work such that I can be the creator/owner of the spreadsheet and allow everyone in the world to look at it but for only approved people to edit it so I can make sure no one trolls it to throw off the prices. I'd like to do something where the users entering data have a dropdown/MSAccess type of form to select the item they're selling and then a text box to enter the price they sold it for. I then want them to hit 'submit' and have that sale go into the back-end of the database such that the front end does the math for us and displays the average price for the world to see.  

I could probably code something in Excel but that wouldn't be easily editable by many people. Google Docs is a good start but I haven't seen the automated averaging after the price has been entered. I don't know where to ask so I'm taking a shot in the dark here - anyone have any ideas? Thanks in advance!",0,"Hello r/statistics, 

I don't know where else I should direct this question but I'm hoping someone here might be able to help since I feel like this is somewhat related to statistical analytics. I've read the rules and I think this post is okay.  

I'm looking for some kind of google doc/online spreadsheet program that will allow me to compute arithmetic mean on a fluctuating set of numbers entered by people whom I personally give permission to. Basically, a group of my friends and I play an online game where items sell for different prices based on economic forces and we're trying to make a list that tracks selling prices. I'd like it to work such that I can be the creator/owner of the spreadsheet and allow everyone in the world to look at it but for only approved people to edit it so I can make sure no one trolls it to throw off the prices. I'd like to do something where the users entering data have a dropdown/MSAccess type of form to select the item they're selling and then a text box to enter the price they sold it for. I then want them to hit 'submit' and have that sale go into the back-end of the database such that the front end does the math for us and displays the average price for the world to see.  

I could probably code something in Excel but that wouldn't be easily editable by many people. Google Docs is a good start but I haven't seen the automated averaging after the price has been entered. I don't know where to ask so I'm taking a shot in the dark here - anyone have any ideas? Thanks in advance!",1,statistics,54935,,Software/program recommendation,https://www.reddit.com/r/statistics/comments/8e3c56/softwareprogram_recommendation/,all_ads,2018-04-22 17:49:24,41 days 07:46:10.036584000,
"Hi,
I'm looking at correlation between monthly historical gold price and the CBOE Volatility index (VIX). The VIX number is pending between 1-100 and the gold price is all the way from 300 to 1700 USD or so. 
When looking at correlation between those two, is it generally more interesting to look at the % change from each month instead of looking at the actual price itself?
Hope I managed to explain decently, no real background in statistics. ",5,1524422449.0,8e2i3i,False,"Hi,
I'm looking at correlation between monthly historical gold price and the CBOE Volatility index (VIX). The VIX number is pending between 1-100 and the gold price is all the way from 300 to 1700 USD or so. 
When looking at correlation between those two, is it generally more interesting to look at the % change from each month instead of looking at the actual price itself?
Hope I managed to explain decently, no real background in statistics. ",0,"Hi,
I'm looking at correlation between monthly historical gold price and the CBOE Volatility index (VIX). The VIX number is pending between 1-100 and the gold price is all the way from 300 to 1700 USD or so. 
When looking at correlation between those two, is it generally more interesting to look at the % change from each month instead of looking at the actual price itself?
Hope I managed to explain decently, no real background in statistics. ",1,statistics,54935,,"Correlation, look at % change or the price itself?",https://www.reddit.com/r/statistics/comments/8e2i3i/correlation_look_at_change_or_the_price_itself/,all_ads,2018-04-22 14:40:49,41 days 10:54:45.036584000,
"Using the normal distribution with mean u and varaince v:
The codomain of the probability density is (0, 1/sqrt(2*pi*v)]
1. What is the probability density function of the codomain? Does one just replace ""X"" with the normal function applied to X in the integral?
2. What is the expected value of the probability density (although this can be found once the pdf of the codomain is)
3. How does one go about calculating it for any PDF?
Thank you",6,1524407301.0,8e1loq,False,"Using the normal distribution with mean u and varaince v:
The codomain of the probability density is (0, 1/sqrt(2*pi*v)]
1. What is the probability density function of the codomain? Does one just replace ""X"" with the normal function applied to X in the integral?
2. What is the expected value of the probability density (although this can be found once the pdf of the codomain is)
3. How does one go about calculating it for any PDF?
Thank you",0,"Using the normal distribution with mean u and varaince v:
The codomain of the probability density is (0, 1/sqrt(2*pi*v)]
1. What is the probability density function of the codomain? Does one just replace ""X"" with the normal function applied to X in the integral?
2. What is the expected value of the probability density (although this can be found once the pdf of the codomain is)
3. How does one go about calculating it for any PDF?
Thank you",0,statistics,54935,,Distribution of the support of a PDF,https://www.reddit.com/r/statistics/comments/8e1loq/distribution_of_the_support_of_a_pdf/,all_ads,2018-04-22 10:28:21,41 days 15:07:13.036584000,
"I'm working on a big data, project, and the data is can be split into 14 different groups. There's too much data for my computer to handle pooling the data and running fixed/random effects, so I decided to run 14 separate regressions to analyze each group individually. 

In addition to reporting the results of the individual regressions, I wanted to report the average coefficient, so that we can get a point estimate for the ""true"" effect. But, averaging across models isn't something we've covered in any of my statistics courses, and most of the papers I'm seeing online are (arguing against) averaging coefficients from different models with different covariates, which isn't what I'm doing here. 

So, can someone point me in the right direction for this? Is it as simple as just calculating the arithmetic mean of the coefficients? Also, the standard error is probably important (ish), and that always seems more complicated that the coefficient. ",5,1524406087.0,8e1io1,False,"I'm working on a big data, project, and the data is can be split into 14 different groups. There's too much data for my computer to handle pooling the data and running fixed/random effects, so I decided to run 14 separate regressions to analyze each group individually. 

In addition to reporting the results of the individual regressions, I wanted to report the average coefficient, so that we can get a point estimate for the ""true"" effect. But, averaging across models isn't something we've covered in any of my statistics courses, and most of the papers I'm seeing online are (arguing against) averaging coefficients from different models with different covariates, which isn't what I'm doing here. 

So, can someone point me in the right direction for this? Is it as simple as just calculating the arithmetic mean of the coefficients? Also, the standard error is probably important (ish), and that always seems more complicated that the coefficient. ",0,"I'm working on a big data, project, and the data is can be split into 14 different groups. There's too much data for my computer to handle pooling the data and running fixed/random effects, so I decided to run 14 separate regressions to analyze each group individually. 

In addition to reporting the results of the individual regressions, I wanted to report the average coefficient, so that we can get a point estimate for the ""true"" effect. But, averaging across models isn't something we've covered in any of my statistics courses, and most of the papers I'm seeing online are (arguing against) averaging coefficients from different models with different covariates, which isn't what I'm doing here. 

So, can someone point me in the right direction for this? Is it as simple as just calculating the arithmetic mean of the coefficients? Also, the standard error is probably important (ish), and that always seems more complicated that the coefficient. ",1,statistics,54935,,Averaging Model Coefficients,https://www.reddit.com/r/statistics/comments/8e1io1/averaging_model_coefficients/,all_ads,2018-04-22 10:08:07,41 days 15:27:27.036584000,
"Hi all, 

So I know that:

    power = 1 - B where B is the probability of a type II error

I was taught that the power can be thought of as the probability that we correctly reject Ho. However, isn't a certain portion of the power going to include type I errors where we incorrectly reject Ho?

I have a feeling this might have a simple answer or I'm missing something simple, but I would greatly appreciate any help. Thanks!",4,1524359959.0,8dx4zb,False,"Hi all, 

So I know that:

    power = 1 - B where B is the probability of a type II error

I was taught that the power can be thought of as the probability that we correctly reject Ho. However, isn't a certain portion of the power going to include type I errors where we incorrectly reject Ho?

I have a feeling this might have a simple answer or I'm missing something simple, but I would greatly appreciate any help. Thanks!",0,"Hi all, 

So I know that:

    power = 1 - B where B is the probability of a type II error

I was taught that the power can be thought of as the probability that we correctly reject Ho. However, isn't a certain portion of the power going to include type I errors where we incorrectly reject Ho?

I have a feeling this might have a simple answer or I'm missing something simple, but I would greatly appreciate any help. Thanks!",8,statistics,54935,,Understanding power,https://www.reddit.com/r/statistics/comments/8dx4zb/understanding_power/,all_ads,2018-04-21 21:19:19,42 days 04:16:15.036584000,
"I see people taking stands that mathematics is more useful than statistics to get a PhD in statistics. I am not entirely set on getting a PhD to become a professor yet, but it is my plan to do so. What courses are the people talking about?",6,1524366178.0,8dxto2,False,"I see people taking stands that mathematics is more useful than statistics to get a PhD in statistics. I am not entirely set on getting a PhD to become a professor yet, but it is my plan to do so. What courses are the people talking about?",0,"I see people taking stands that mathematics is more useful than statistics to get a PhD in statistics. I am not entirely set on getting a PhD to become a professor yet, but it is my plan to do so. What courses are the people talking about?",2,statistics,54935,,What math courses can a statistics undergraduate student take to complement its curriculum?,https://www.reddit.com/r/statistics/comments/8dxto2/what_math_courses_can_a_statistics_undergraduate/,all_ads,2018-04-21 23:02:58,42 days 02:32:36.036584000,
"Hello all!

I am an undergraduate attempting to understand a dataset-- I am trying to see how three predictor variables (I'll call them A, B, C) impact a criterion (D).  This is a project that won't get published anywhere or turned in, but I am supposed to report back to somebody about the data.  

I was told that I have to run a stepwise regression with backward elimination, which is fine, I understand that, but I was told to run it without a constant.  I have not worked with too much regression before, so I understand the basic premise, but I cannot for the life of me understand the reason why I would run it without a constant. I need to be able to say why when I make my report and my supervisor feels that I can figure out why on my own.  After tons of hunting around the internet, I have decided to ask the internet. 

Variable D does not include 0 in its range, the minimum possible value is 16 (which is why I'm really confused). However, the basic assumption I am making is that when Variable D does not exist, it starts at 0. 

Can anybody attempt to help? I can provide more context if necessary!",6,1524387103.0,8dzytm,False,"Hello all!

I am an undergraduate attempting to understand a dataset-- I am trying to see how three predictor variables (I'll call them A, B, C) impact a criterion (D).  This is a project that won't get published anywhere or turned in, but I am supposed to report back to somebody about the data.  

I was told that I have to run a stepwise regression with backward elimination, which is fine, I understand that, but I was told to run it without a constant.  I have not worked with too much regression before, so I understand the basic premise, but I cannot for the life of me understand the reason why I would run it without a constant. I need to be able to say why when I make my report and my supervisor feels that I can figure out why on my own.  After tons of hunting around the internet, I have decided to ask the internet. 

Variable D does not include 0 in its range, the minimum possible value is 16 (which is why I'm really confused). However, the basic assumption I am making is that when Variable D does not exist, it starts at 0. 

Can anybody attempt to help? I can provide more context if necessary!",0,"Hello all!

I am an undergraduate attempting to understand a dataset-- I am trying to see how three predictor variables (I'll call them A, B, C) impact a criterion (D).  This is a project that won't get published anywhere or turned in, but I am supposed to report back to somebody about the data.  

I was told that I have to run a stepwise regression with backward elimination, which is fine, I understand that, but I was told to run it without a constant.  I have not worked with too much regression before, so I understand the basic premise, but I cannot for the life of me understand the reason why I would run it without a constant. I need to be able to say why when I make my report and my supervisor feels that I can figure out why on my own.  After tons of hunting around the internet, I have decided to ask the internet. 

Variable D does not include 0 in its range, the minimum possible value is 16 (which is why I'm really confused). However, the basic assumption I am making is that when Variable D does not exist, it starts at 0. 

Can anybody attempt to help? I can provide more context if necessary!",0,statistics,54935,,Help re: constants (or lack of) in regression equations,https://www.reddit.com/r/statistics/comments/8dzytm/help_re_constants_or_lack_of_in_regression/,all_ads,2018-04-22 04:51:43,41 days 20:43:51.036584000,
"So I work at an automotive paintshop. We have a few dozen types of defects and we have a specially trained quality analyst that calls on what exactly the defect is. I already conducted the R&R on an Excel spreadsheet and I know for a fact that it passes, but I dont know how to run this type of MSA on Minitab (need the variability percentage and graphs screenshots). 

To explain myself better look [here](https://imgur.com/O0Ot99t) and [here](https://imgur.com/sef4i9J).

I believe this type of MSA is called a Round Robin Test. Thank you very much.",0,1524387006.0,8dzyi2,False,"So I work at an automotive paintshop. We have a few dozen types of defects and we have a specially trained quality analyst that calls on what exactly the defect is. I already conducted the R&R on an Excel spreadsheet and I know for a fact that it passes, but I dont know how to run this type of MSA on Minitab (need the variability percentage and graphs screenshots). 

To explain myself better look [here](https://imgur.com/O0Ot99t) and [here](https://imgur.com/sef4i9J).

I believe this type of MSA is called a Round Robin Test. Thank you very much.",0,"So I work at an automotive paintshop. We have a few dozen types of defects and we have a specially trained quality analyst that calls on what exactly the defect is. I already conducted the R&R on an Excel spreadsheet and I know for a fact that it passes, but I dont know how to run this type of MSA on Minitab (need the variability percentage and graphs screenshots). 

To explain myself better look [here](https://imgur.com/O0Ot99t) and [here](https://imgur.com/sef4i9J).

I believe this type of MSA is called a Round Robin Test. Thank you very much.",0,statistics,54935,,[HELP] Running THIS specific MSA on Minitab,https://www.reddit.com/r/statistics/comments/8dzyi2/help_running_this_specific_msa_on_minitab/,all_ads,2018-04-22 04:50:06,41 days 20:45:28.036584000,
"Hey everyone.  I am a student trying to figure out how to calculate statistical significance of data sets.  I am trying to figure out if one method of evaluating something is better done by humans or computers.  I have calculated sensitivity, specificity, positive predictive value and negative predictive value with a difference in the 2 methods compared to a gold standard that they are both compared to.  How would I go about figuring out if the results of the study are significant?

in the end out of the 100 or so items evaluated only about 30 of them were able to be analyzed for various reasons.

I tried looking this up, reviewing some khan academy videos to see if I can learn the basics of statistics but I need some ELI5 level of explanations about whether or not statical significance can be calculated and if so how.

I would be appreciate any help or direction!",8,1524370998.0,8dycbr,False,"Hey everyone.  I am a student trying to figure out how to calculate statistical significance of data sets.  I am trying to figure out if one method of evaluating something is better done by humans or computers.  I have calculated sensitivity, specificity, positive predictive value and negative predictive value with a difference in the 2 methods compared to a gold standard that they are both compared to.  How would I go about figuring out if the results of the study are significant?

in the end out of the 100 or so items evaluated only about 30 of them were able to be analyzed for various reasons.

I tried looking this up, reviewing some khan academy videos to see if I can learn the basics of statistics but I need some ELI5 level of explanations about whether or not statical significance can be calculated and if so how.

I would be appreciate any help or direction!",0,"Hey everyone.  I am a student trying to figure out how to calculate statistical significance of data sets.  I am trying to figure out if one method of evaluating something is better done by humans or computers.  I have calculated sensitivity, specificity, positive predictive value and negative predictive value with a difference in the 2 methods compared to a gold standard that they are both compared to.  How would I go about figuring out if the results of the study are significant?

in the end out of the 100 or so items evaluated only about 30 of them were able to be analyzed for various reasons.

I tried looking this up, reviewing some khan academy videos to see if I can learn the basics of statistics but I need some ELI5 level of explanations about whether or not statical significance can be calculated and if so how.

I would be appreciate any help or direction!",2,statistics,54935,,statistical significance help,https://www.reddit.com/r/statistics/comments/8dycbr/statistical_significance_help/,all_ads,2018-04-22 00:23:18,42 days 01:12:16.036584000,
"Hello! I am hoping someone would be able to help me. I have collected some data: https://imgur.com/a/VCC0MBd

I would like to find out the correlation between each of the factors and market cap. This is simple when comparing market cap to team size as they are both numerical. With regards to the 'yes', 'no' answers is that a case of just converting to 1/0 and then I can perform a test through excel? I'm assuming its even more complicated for the 'type', 'stage' and 'location'. How would I go about testing the correlation for these against market cap? I will probably be using spearman rank test. Thanks for your help!

",9,1524379350.0,8dz7mr,False,"Hello! I am hoping someone would be able to help me. I have collected some data: https://imgur.com/a/VCC0MBd

I would like to find out the correlation between each of the factors and market cap. This is simple when comparing market cap to team size as they are both numerical. With regards to the 'yes', 'no' answers is that a case of just converting to 1/0 and then I can perform a test through excel? I'm assuming its even more complicated for the 'type', 'stage' and 'location'. How would I go about testing the correlation for these against market cap? I will probably be using spearman rank test. Thanks for your help!

",0,"Hello! I am hoping someone would be able to help me. I have collected some data: https://imgur.com/a/VCC0MBd

I would like to find out the correlation between each of the factors and market cap. This is simple when comparing market cap to team size as they are both numerical. With regards to the 'yes', 'no' answers is that a case of just converting to 1/0 and then I can perform a test through excel? I'm assuming its even more complicated for the 'type', 'stage' and 'location'. How would I go about testing the correlation for these against market cap? I will probably be using spearman rank test. Thanks for your help!

",1,statistics,54935,,Correlation test help! Non-numerical data,https://www.reddit.com/r/statistics/comments/8dz7mr/correlation_test_help_nonnumerical_data/,all_ads,2018-04-22 02:42:30,41 days 22:53:04.036584000,
Hey /r/statistics I need your help. I am writing a make-up paper for statistics and part of the paper is to relate t-test to another topic in statistics. I am stumped on this part of the paper and any help would mean a lot !!!!,4,1524378603.0,8dz4uu,False,Hey /r/statistics I need your help. I am writing a make-up paper for statistics and part of the paper is to relate t-test to another topic in statistics. I am stumped on this part of the paper and any help would mean a lot !!!!,0,Hey /r/statistics I need your help. I am writing a make-up paper for statistics and part of the paper is to relate t-test to another topic in statistics. I am stumped on this part of the paper and any help would mean a lot !!!!,0,statistics,54935,,T-test help,https://www.reddit.com/r/statistics/comments/8dz4uu/ttest_help/,all_ads,2018-04-22 02:30:03,41 days 23:05:31.036584000,
"Hello there! I'm currently working in behavioral health and looking to change my career to biostatistics. While the subject matter and job itself appear to be *fascinating*, there's one thing I'm wary of: the hell that is the open office layout.

I've been fortunate enough to have avoided this environment for the most part (I even had my own office at one point), but when I was in a situation that involved me sharing a small room with 12 other people, I almost lost my damn mind. I didn't even have my own desk. The only way I was able to keep my sanity was the ability to do field work/home visits and limit my time in that office as much as possible. I don't understand how anyone can get any work done in these conditions.

Before I take the plunge to continue my education, is there any way to avoid the open office? It may seem a bit extreme, but this would literally be a deal breaker for me in a job. Having a cubicle or sharing a closed office with 1-2 other people probably wouldn't be a huge deal, if that's of any consolation.

**TL;DR: What's your work environment like? Do you work in an open office layout? Your own office/cubicle? From home?**",56,1524281556.0,8dqbp2,False,"Hello there! I'm currently working in behavioral health and looking to change my career to biostatistics. While the subject matter and job itself appear to be *fascinating*, there's one thing I'm wary of: the hell that is the open office layout.

I've been fortunate enough to have avoided this environment for the most part (I even had my own office at one point), but when I was in a situation that involved me sharing a small room with 12 other people, I almost lost my damn mind. I didn't even have my own desk. The only way I was able to keep my sanity was the ability to do field work/home visits and limit my time in that office as much as possible. I don't understand how anyone can get any work done in these conditions.

Before I take the plunge to continue my education, is there any way to avoid the open office? It may seem a bit extreme, but this would literally be a deal breaker for me in a job. Having a cubicle or sharing a closed office with 1-2 other people probably wouldn't be a huge deal, if that's of any consolation.

**TL;DR: What's your work environment like? Do you work in an open office layout? Your own office/cubicle? From home?**",0,"Hello there! I'm currently working in behavioral health and looking to change my career to biostatistics. While the subject matter and job itself appear to be *fascinating*, there's one thing I'm wary of: the hell that is the open office layout.

I've been fortunate enough to have avoided this environment for the most part (I even had my own office at one point), but when I was in a situation that involved me sharing a small room with 12 other people, I almost lost my damn mind. I didn't even have my own desk. The only way I was able to keep my sanity was the ability to do field work/home visits and limit my time in that office as much as possible. I don't understand how anyone can get any work done in these conditions.

Before I take the plunge to continue my education, is there any way to avoid the open office? It may seem a bit extreme, but this would literally be a deal breaker for me in a job. Having a cubicle or sharing a closed office with 1-2 other people probably wouldn't be a huge deal, if that's of any consolation.

**TL;DR: What's your work environment like? Do you work in an open office layout? Your own office/cubicle? From home?**",37,statistics,54935,,Statisticians: What are your working conditions like?,https://www.reddit.com/r/statistics/comments/8dqbp2/statisticians_what_are_your_working_conditions/,all_ads,2018-04-20 23:32:36,43 days 02:02:58.036584000,
"Hello, 

I tried finding a simple answer to this, and found quite a few different results (and a few more than go over my head-- I'm not a statistics expert, but I do try to learn more whenever I can)

Basically, I have a set of data relating to various factors that I think influence demand of a certain product.  I have many ""complete"" rows for each quarter of the past 10 years (each row has what I consider 4 independent variables).  One of the variables is not yet available for the last 2 quarters of 2017.  I'm debating either just ""lopping them off"" or carrying over the most recent available number (both of which I see are legitimate options).  However, I'm curious if there is any way that Excel would automatically assess these without considering the missing variable when it does the analysis for the entire dataset (i.e. by conducting regression with only the 3 available variables for the last two datapoints, but considers all variables for everything else)

Apologies if my terminology is incorrect, but I think it should be somewhat clear what I am trying to ask :)",9,1524303989.0,8dsqwh,False,"Hello, 

I tried finding a simple answer to this, and found quite a few different results (and a few more than go over my head-- I'm not a statistics expert, but I do try to learn more whenever I can)

Basically, I have a set of data relating to various factors that I think influence demand of a certain product.  I have many ""complete"" rows for each quarter of the past 10 years (each row has what I consider 4 independent variables).  One of the variables is not yet available for the last 2 quarters of 2017.  I'm debating either just ""lopping them off"" or carrying over the most recent available number (both of which I see are legitimate options).  However, I'm curious if there is any way that Excel would automatically assess these without considering the missing variable when it does the analysis for the entire dataset (i.e. by conducting regression with only the 3 available variables for the last two datapoints, but considers all variables for everything else)

Apologies if my terminology is incorrect, but I think it should be somewhat clear what I am trying to ask :)",0,"Hello, 

I tried finding a simple answer to this, and found quite a few different results (and a few more than go over my head-- I'm not a statistics expert, but I do try to learn more whenever I can)

Basically, I have a set of data relating to various factors that I think influence demand of a certain product.  I have many ""complete"" rows for each quarter of the past 10 years (each row has what I consider 4 independent variables).  One of the variables is not yet available for the last 2 quarters of 2017.  I'm debating either just ""lopping them off"" or carrying over the most recent available number (both of which I see are legitimate options).  However, I'm curious if there is any way that Excel would automatically assess these without considering the missing variable when it does the analysis for the entire dataset (i.e. by conducting regression with only the 3 available variables for the last two datapoints, but considers all variables for everything else)

Apologies if my terminology is incorrect, but I think it should be somewhat clear what I am trying to ask :)",3,statistics,54935,,Trying to do regression analysis with missing data,https://www.reddit.com/r/statistics/comments/8dsqwh/trying_to_do_regression_analysis_with_missing_data/,all_ads,2018-04-21 05:46:29,42 days 19:49:05.036584000,
"Hello again r/statistics

I made a post the other day (which can be found [here](https://www.reddit.com/r/statistics/comments/8d7b4o/total_statistics_novice_looking_for_some_guidance/) should anyone think it’s at all useful for clarification), and was very kindly advised on how to approach my problem. 

However, I return with some new issues that hopefully someone can assist me with.

Originally, I was attempting to ascertain to what extent each of these 30 skills….


https://imgur.com/a/5oe02

... impacted a soccer player’s ability to score goals. I was advised to establish the regression coefficient for each as a means of deducing this. 

Prior to my original post, I took a sample size of 150 strikers (whose aim it is to score goals), and established the mean value of each of the 30 skills to give myself a rough overview of the order of importance... 



https://imgur.com/a/8fZ7R



Having gone away and calculated what I believe are the regression coefficients I’ve hit upon a stumbling block which is hopefully, in part, illustrated by the following scatter graph:



https://imgur.com/a/tV7lFDu





As we see in the graph, according to my data sample “decisions” has a higher r^2 value than Finishing, which is down to the limits of what I can sample.  

Sadly,  I cannot sample strikers with a broader range of “finishing” skill, as those with a low finishing attribute are simply not played, and thus I cannot create for them the same test conditions which would for sure result in a much higher value for r^2. 

The graph above shows that the starting point of the trend line begins quite high up on the Y axis, which to me indicated that it is very important (as the range of values is consistently high) but precludes me from ever learning numerically how important it is relative to any of the other 29 skills. 

Is there a way for me to navigate around this problem? Can I somehow combine my understanding of the mean values for each skill into understanding their significance with respect to the others? 

I’m basically attempting to attribute each skill a score out of 100 in terms of its influence on a player’s ability to score goals. 

I hope i’ve explained myself clearly enough. Sorry if i’ve used any terms incorrectly, or if my understanding of this subject matter is entirely wrong. 

Thanks for your time :D
",1,1524297511.0,8ds57s,False,"Hello again r/statistics

I made a post the other day (which can be found [here](https://www.reddit.com/r/statistics/comments/8d7b4o/total_statistics_novice_looking_for_some_guidance/) should anyone think it’s at all useful for clarification), and was very kindly advised on how to approach my problem. 

However, I return with some new issues that hopefully someone can assist me with.

Originally, I was attempting to ascertain to what extent each of these 30 skills….


https://imgur.com/a/5oe02

... impacted a soccer player’s ability to score goals. I was advised to establish the regression coefficient for each as a means of deducing this. 

Prior to my original post, I took a sample size of 150 strikers (whose aim it is to score goals), and established the mean value of each of the 30 skills to give myself a rough overview of the order of importance... 



https://imgur.com/a/8fZ7R



Having gone away and calculated what I believe are the regression coefficients I’ve hit upon a stumbling block which is hopefully, in part, illustrated by the following scatter graph:



https://imgur.com/a/tV7lFDu





As we see in the graph, according to my data sample “decisions” has a higher r^2 value than Finishing, which is down to the limits of what I can sample.  

Sadly,  I cannot sample strikers with a broader range of “finishing” skill, as those with a low finishing attribute are simply not played, and thus I cannot create for them the same test conditions which would for sure result in a much higher value for r^2. 

The graph above shows that the starting point of the trend line begins quite high up on the Y axis, which to me indicated that it is very important (as the range of values is consistently high) but precludes me from ever learning numerically how important it is relative to any of the other 29 skills. 

Is there a way for me to navigate around this problem? Can I somehow combine my understanding of the mean values for each skill into understanding their significance with respect to the others? 

I’m basically attempting to attribute each skill a score out of 100 in terms of its influence on a player’s ability to score goals. 

I hope i’ve explained myself clearly enough. Sorry if i’ve used any terms incorrectly, or if my understanding of this subject matter is entirely wrong. 

Thanks for your time :D
",0,"Hello again r/statistics

I made a post the other day (which can be found [here](https://www.reddit.com/r/statistics/comments/8d7b4o/total_statistics_novice_looking_for_some_guidance/) should anyone think it’s at all useful for clarification), and was very kindly advised on how to approach my problem. 

However, I return with some new issues that hopefully someone can assist me with.

Originally, I was attempting to ascertain to what extent each of these 30 skills….


https://imgur.com/a/5oe02

... impacted a soccer player’s ability to score goals. I was advised to establish the regression coefficient for each as a means of deducing this. 

Prior to my original post, I took a sample size of 150 strikers (whose aim it is to score goals), and established the mean value of each of the 30 skills to give myself a rough overview of the order of importance... 



https://imgur.com/a/8fZ7R



Having gone away and calculated what I believe are the regression coefficients I’ve hit upon a stumbling block which is hopefully, in part, illustrated by the following scatter graph:



https://imgur.com/a/tV7lFDu





As we see in the graph, according to my data sample “decisions” has a higher r^2 value than Finishing, which is down to the limits of what I can sample.  

Sadly,  I cannot sample strikers with a broader range of “finishing” skill, as those with a low finishing attribute are simply not played, and thus I cannot create for them the same test conditions which would for sure result in a much higher value for r^2. 

The graph above shows that the starting point of the trend line begins quite high up on the Y axis, which to me indicated that it is very important (as the range of values is consistently high) but precludes me from ever learning numerically how important it is relative to any of the other 29 skills. 

Is there a way for me to navigate around this problem? Can I somehow combine my understanding of the mean values for each skill into understanding their significance with respect to the others? 

I’m basically attempting to attribute each skill a score out of 100 in terms of its influence on a player’s ability to score goals. 

I hope i’ve explained myself clearly enough. Sorry if i’ve used any terms incorrectly, or if my understanding of this subject matter is entirely wrong. 

Thanks for your time :D
",3,statistics,54935,,Total statistics novice looking for some guidance (Part 2),https://www.reddit.com/r/statistics/comments/8ds57s/total_statistics_novice_looking_for_some_guidance/,all_ads,2018-04-21 03:58:31,42 days 21:37:03.036584000,
"So, I am struggling to understand the distinction between the two. This is what I (think I) understand:

Internal consistency is a form of reliability, and it tests whether items on my questionnaire measure different parts of the same construct by virtue of responses to these items correlating with one another.

Construct validity is (of course) a form a validity, and it measures the extent to which ""a test measures what it claims to theoretically measure"". EFA explores the data structure and returns the number of factors emergent in the data as well as the loadings of each question on these factor/s. 

But...I still don't understand how this is different from each other? ",6,1524265877.0,8dobiw,False,"So, I am struggling to understand the distinction between the two. This is what I (think I) understand:

Internal consistency is a form of reliability, and it tests whether items on my questionnaire measure different parts of the same construct by virtue of responses to these items correlating with one another.

Construct validity is (of course) a form a validity, and it measures the extent to which ""a test measures what it claims to theoretically measure"". EFA explores the data structure and returns the number of factors emergent in the data as well as the loadings of each question on these factor/s. 

But...I still don't understand how this is different from each other? ",0,"So, I am struggling to understand the distinction between the two. This is what I (think I) understand:

Internal consistency is a form of reliability, and it tests whether items on my questionnaire measure different parts of the same construct by virtue of responses to these items correlating with one another.

Construct validity is (of course) a form a validity, and it measures the extent to which ""a test measures what it claims to theoretically measure"". EFA explores the data structure and returns the number of factors emergent in the data as well as the loadings of each question on these factor/s. 

But...I still don't understand how this is different from each other? ",3,statistics,54935,,Internal consistency (Cronbach) vs. construct validity (EFA),https://www.reddit.com/r/statistics/comments/8dobiw/internal_consistency_cronbach_vs_construct/,all_ads,2018-04-20 19:11:17,43 days 06:24:17.036584000,
I am not very good at statistics and I have run into this problem. Is it a problem if the moderator is correlated with the IV?,13,1524239059.0,8dlvys,False,I am not very good at statistics and I have run into this problem. Is it a problem if the moderator is correlated with the IV?,0,I am not very good at statistics and I have run into this problem. Is it a problem if the moderator is correlated with the IV?,11,statistics,54935,,"Correlation between IV and moderator variable, is this a problem?",https://www.reddit.com/r/statistics/comments/8dlvys/correlation_between_iv_and_moderator_variable_is/,all_ads,2018-04-20 11:44:19,43 days 13:51:15.036584000,
"I had an R class and enjoyed the tool quite a bit which is why I dug my teeth a bit deeper into it, furthering my knowledge past the class's requirements. I've done some research on data science and apparently Python seems to be growing faster in the industry and in academia alike. I wonder if I should stop sinking any more time into R and just learn Python instead? Is there a proper GGplot alternative in Python? The entire Tidyverse package is quite useful really. Does Python match that? Will my R knowledge help me pick up Python faster?

Does it make sense to keep up with both?

Thanks in advance!

*EDIT*: Thanks everyone! I will stick with R because I really enjoy it and y'all made a great case as to why it's worthwhile. I'll dig into Python down the line.",152,1524168467.0,8de54s,False,"I had an R class and enjoyed the tool quite a bit which is why I dug my teeth a bit deeper into it, furthering my knowledge past the class's requirements. I've done some research on data science and apparently Python seems to be growing faster in the industry and in academia alike. I wonder if I should stop sinking any more time into R and just learn Python instead? Is there a proper GGplot alternative in Python? The entire Tidyverse package is quite useful really. Does Python match that? Will my R knowledge help me pick up Python faster?

Does it make sense to keep up with both?

Thanks in advance!

*EDIT*: Thanks everyone! I will stick with R because I really enjoy it and y'all made a great case as to why it's worthwhile. I'll dig into Python down the line.",0,"I had an R class and enjoyed the tool quite a bit which is why I dug my teeth a bit deeper into it, furthering my knowledge past the class's requirements. I've done some research on data science and apparently Python seems to be growing faster in the industry and in academia alike. I wonder if I should stop sinking any more time into R and just learn Python instead? Is there a proper GGplot alternative in Python? The entire Tidyverse package is quite useful really. Does Python match that? Will my R knowledge help me pick up Python faster?

Does it make sense to keep up with both?

Thanks in advance!

*EDIT*: Thanks everyone! I will stick with R because I really enjoy it and y'all made a great case as to why it's worthwhile. I'll dig into Python down the line.",110,statistics,54935,,Is R better than Python at anything? I started learning R half a year ago and I wonder if I should switch.,https://www.reddit.com/r/statistics/comments/8de54s/is_r_better_than_python_at_anything_i_started/,all_ads,2018-04-19 16:07:47,44 days 09:27:47.036584000,
"Hi r/statistics,

I'm currently a 3rd year forensic psychology student completing a research project. I'm really struggling to find any guides on how to write up my findings as I think that I have found a a significant finding. If anyone has any resources on doing a write\-up or interpreting results, preferably in APA style, it would be much appreciated as I am struggling to find anything that isnt for a one\-way MANOVA! 

Also, does anyone know if you have to calculate Bonferroni Adjustment when doing a 2 by 2 MANOVA as if I recall correctly, this only needs to be calculate if you have more than 3 IV/DV's. ",2,1524257485.0,8dnbct,False,"Hi r/statistics,

I'm currently a 3rd year forensic psychology student completing a research project. I'm really struggling to find any guides on how to write up my findings as I think that I have found a a significant finding. If anyone has any resources on doing a write\-up or interpreting results, preferably in APA style, it would be much appreciated as I am struggling to find anything that isnt for a one\-way MANOVA! 

Also, does anyone know if you have to calculate Bonferroni Adjustment when doing a 2 by 2 MANOVA as if I recall correctly, this only needs to be calculate if you have more than 3 IV/DV's. ",0,"Hi r/statistics,

I'm currently a 3rd year forensic psychology student completing a research project. I'm really struggling to find any guides on how to write up my findings as I think that I have found a a significant finding. If anyone has any resources on doing a write\-up or interpreting results, preferably in APA style, it would be much appreciated as I am struggling to find anything that isnt for a one\-way MANOVA! 

Also, does anyone know if you have to calculate Bonferroni Adjustment when doing a 2 by 2 MANOVA as if I recall correctly, this only needs to be calculate if you have more than 3 IV/DV's. ",0,statistics,54935,,2 by 2 MANOVA help,https://www.reddit.com/r/statistics/comments/8dnbct/2_by_2_manova_help/,all_ads,2018-04-20 16:51:25,43 days 08:44:09.036584000,
"Hey, could someone give me an adequate description of the Glivenko-Cantelli Theorem for someone who only has math knowledge up to Calc 3. I'm a first year double major in Business and Stats (love stats), and I really want to know how the Glivenko-Cantelli Theorem works. I want to know since I've heard its ""The Fundamental Theory of Statistics"". I've tried looking for other online resources, but the notation is extremely hard to read. If the answer is I just have to wait until I take more math classes then I understand!",6,1524210336.0,8dj9oc,False,"Hey, could someone give me an adequate description of the Glivenko-Cantelli Theorem for someone who only has math knowledge up to Calc 3. I'm a first year double major in Business and Stats (love stats), and I really want to know how the Glivenko-Cantelli Theorem works. I want to know since I've heard its ""The Fundamental Theory of Statistics"". I've tried looking for other online resources, but the notation is extremely hard to read. If the answer is I just have to wait until I take more math classes then I understand!",0,"Hey, could someone give me an adequate description of the Glivenko-Cantelli Theorem for someone who only has math knowledge up to Calc 3. I'm a first year double major in Business and Stats (love stats), and I really want to know how the Glivenko-Cantelli Theorem works. I want to know since I've heard its ""The Fundamental Theory of Statistics"". I've tried looking for other online resources, but the notation is extremely hard to read. If the answer is I just have to wait until I take more math classes then I understand!",7,statistics,54935,,Simpler (if possible) explanation of the Glivenko-Cantelli Theorem?,https://www.reddit.com/r/statistics/comments/8dj9oc/simpler_if_possible_explanation_of_the/,all_ads,2018-04-20 03:45:36,43 days 21:49:58.036584000,
"So if I have the value of r^2 how would I explain it as the Proportionate reduction of Error? Lets say r^2 is .75/75%

Having a hard time understanding this concept! Thank you reddit",1,1524228207.0,8dl0bk,False,"So if I have the value of r^2 how would I explain it as the Proportionate reduction of Error? Lets say r^2 is .75/75%

Having a hard time understanding this concept! Thank you reddit",0,"So if I have the value of r^2 how would I explain it as the Proportionate reduction of Error? Lets say r^2 is .75/75%

Having a hard time understanding this concept! Thank you reddit",2,statistics,54935,,Can someone ELI5 the Proportionate reduction in Error?,https://www.reddit.com/r/statistics/comments/8dl0bk/can_someone_eli5_the_proportionate_reduction_in/,all_ads,2018-04-20 08:43:27,43 days 16:52:07.036584000,
"Greetings,

I'm trying to find some literature on the topic (especially as it pertains to social science research) to increase my theoretical understanding. Any recommendations will be much appreciated. Thanks!

",2,1524216215.0,8djw9j,False,"Greetings,

I'm trying to find some literature on the topic (especially as it pertains to social science research) to increase my theoretical understanding. Any recommendations will be much appreciated. Thanks!

",0,"Greetings,

I'm trying to find some literature on the topic (especially as it pertains to social science research) to increase my theoretical understanding. Any recommendations will be much appreciated. Thanks!

",2,statistics,54935,,Can anyone recommend literature that discusses the advantages/pitfalls of using proxy measures and additive indices?,https://www.reddit.com/r/statistics/comments/8djw9j/can_anyone_recommend_literature_that_discusses/,all_ads,2018-04-20 05:23:35,43 days 20:11:59.036584000,
"Hi r/statistics, 

I have a little problem that starts with the distribution of funds to a set of subsidiaries on a percentage basis. The percentage is derived from a reward system. So Subsidiary A gets 32%, Subsidiary B gets 13%, etc., up to 100% of the funding. All money must go!  Call this percentage the Starting Percentage Share (SPS) for each subsidiary. 

From there we’d like to adjust for size. Some subsidiaries are bigger than others.  For this we can use employee count. Subsidiary A has 16% of all employees, Subsidiary B has 28% of employees, etc., again up to 100%.  For this I created an index by dividing the employee percentage of each subsidiary by the average employee percentage. Call this the Size Index (SI) for each subsidiary 

By multiplying the SPS by the SI I get a result that basically works. The result doesn’t necessarily add up to 100%, but I can rescale it to add up to 100%. Take each result and divide it by the sum of all results. That gives me the Final Percentage Share (FPS). 

I was curious in looking at the relative weight of the the starting percentage and the size percentage in the FPS. It turns out size is more variable than the SPS, and so the differences in size appear to dominate in the FPS. Running an R squared calculation on each seemed to confirm.  The R squared value for FPS on Size is much bigger than the R squared value for FPS on SPS.  

My question is about these two R squared results.  Together they almost add up to 1, but not quite. Should they add up to one?  I am using the rudimentary understanding that R squared shows the amount of variance in the dependent variable that is “explained” by the independent variable.  Simplistically, why wouldn’t these explain the total variation when the R squared values are summed?

I hope this is clear enough. . For some time I have been interested to understand this meaning of R squared better.  ",6,1524182600.0,8dfrkr,False,"Hi r/statistics, 

I have a little problem that starts with the distribution of funds to a set of subsidiaries on a percentage basis. The percentage is derived from a reward system. So Subsidiary A gets 32%, Subsidiary B gets 13%, etc., up to 100% of the funding. All money must go!  Call this percentage the Starting Percentage Share (SPS) for each subsidiary. 

From there we’d like to adjust for size. Some subsidiaries are bigger than others.  For this we can use employee count. Subsidiary A has 16% of all employees, Subsidiary B has 28% of employees, etc., again up to 100%.  For this I created an index by dividing the employee percentage of each subsidiary by the average employee percentage. Call this the Size Index (SI) for each subsidiary 

By multiplying the SPS by the SI I get a result that basically works. The result doesn’t necessarily add up to 100%, but I can rescale it to add up to 100%. Take each result and divide it by the sum of all results. That gives me the Final Percentage Share (FPS). 

I was curious in looking at the relative weight of the the starting percentage and the size percentage in the FPS. It turns out size is more variable than the SPS, and so the differences in size appear to dominate in the FPS. Running an R squared calculation on each seemed to confirm.  The R squared value for FPS on Size is much bigger than the R squared value for FPS on SPS.  

My question is about these two R squared results.  Together they almost add up to 1, but not quite. Should they add up to one?  I am using the rudimentary understanding that R squared shows the amount of variance in the dependent variable that is “explained” by the independent variable.  Simplistically, why wouldn’t these explain the total variation when the R squared values are summed?

I hope this is clear enough. . For some time I have been interested to understand this meaning of R squared better.  ",0,"Hi r/statistics, 

I have a little problem that starts with the distribution of funds to a set of subsidiaries on a percentage basis. The percentage is derived from a reward system. So Subsidiary A gets 32%, Subsidiary B gets 13%, etc., up to 100% of the funding. All money must go!  Call this percentage the Starting Percentage Share (SPS) for each subsidiary. 

From there we’d like to adjust for size. Some subsidiaries are bigger than others.  For this we can use employee count. Subsidiary A has 16% of all employees, Subsidiary B has 28% of employees, etc., again up to 100%.  For this I created an index by dividing the employee percentage of each subsidiary by the average employee percentage. Call this the Size Index (SI) for each subsidiary 

By multiplying the SPS by the SI I get a result that basically works. The result doesn’t necessarily add up to 100%, but I can rescale it to add up to 100%. Take each result and divide it by the sum of all results. That gives me the Final Percentage Share (FPS). 

I was curious in looking at the relative weight of the the starting percentage and the size percentage in the FPS. It turns out size is more variable than the SPS, and so the differences in size appear to dominate in the FPS. Running an R squared calculation on each seemed to confirm.  The R squared value for FPS on Size is much bigger than the R squared value for FPS on SPS.  

My question is about these two R squared results.  Together they almost add up to 1, but not quite. Should they add up to one?  I am using the rudimentary understanding that R squared shows the amount of variance in the dependent variable that is “explained” by the independent variable.  Simplistically, why wouldn’t these explain the total variation when the R squared values are summed?

I hope this is clear enough. . For some time I have been interested to understand this meaning of R squared better.  ",6,statistics,54935,,R squared,https://www.reddit.com/r/statistics/comments/8dfrkr/r_squared/,all_ads,2018-04-19 20:03:20,44 days 05:32:14.036584000,
"I have 20 items rated on 25 different dimensions. These items can be classified in two ways. They belong to Group A or B; also, orthogonally, they belong to Groups W, X, Y or Z. Items were rated by ~ 30 different people.

What I want to know is which dimensions Groups A and B differ on; also, on which dimensions Groups W, X, Y and Z differ on.

I am hoping to conduct the analysis at the trial level (i.e., a trial would entail a single participant's rating of a single item, on all 25 dimensions). So whatever analysis method I choose would have to be able to include random subject and item effects.

What comes to mind is multivariate linear regression: having each of the 25 dimensions be a separate DV, and use group membership to predict them. Does that make sense? Is there a type of mixture model that would be superior to this?",5,1524196771.0,8dhmu9,False,"I have 20 items rated on 25 different dimensions. These items can be classified in two ways. They belong to Group A or B; also, orthogonally, they belong to Groups W, X, Y or Z. Items were rated by ~ 30 different people.

What I want to know is which dimensions Groups A and B differ on; also, on which dimensions Groups W, X, Y and Z differ on.

I am hoping to conduct the analysis at the trial level (i.e., a trial would entail a single participant's rating of a single item, on all 25 dimensions). So whatever analysis method I choose would have to be able to include random subject and item effects.

What comes to mind is multivariate linear regression: having each of the 25 dimensions be a separate DV, and use group membership to predict them. Does that make sense? Is there a type of mixture model that would be superior to this?",0,"I have 20 items rated on 25 different dimensions. These items can be classified in two ways. They belong to Group A or B; also, orthogonally, they belong to Groups W, X, Y or Z. Items were rated by ~ 30 different people.

What I want to know is which dimensions Groups A and B differ on; also, on which dimensions Groups W, X, Y and Z differ on.

I am hoping to conduct the analysis at the trial level (i.e., a trial would entail a single participant's rating of a single item, on all 25 dimensions). So whatever analysis method I choose would have to be able to include random subject and item effects.

What comes to mind is multivariate linear regression: having each of the 25 dimensions be a separate DV, and use group membership to predict them. Does that make sense? Is there a type of mixture model that would be superior to this?",3,statistics,54935,,Is multivariate linear regression appropriate for this analysis?,https://www.reddit.com/r/statistics/comments/8dhmu9/is_multivariate_linear_regression_appropriate_for/,all_ads,2018-04-19 23:59:31,44 days 01:36:03.036584000,
"Up front, my statistics background is virtually nil (will likely be apparent from this post).
So in no apparent order, here are a series of questions that will hopefully identify my problem.
Is using the correlation coefficient even a thing used for determining the correlation for an exponential regression?  It was presented to me that comparing the correlation coefficient from a given data set, using the Ti84, the way to compare what fits the data best (linear or exponential regression) is to compare the linear regression correlation coefficient and the exponential correlation coefficient to determine which model fits the data better (the number closer to 1 or -1 would indicate the better fit).  However, from all my ""research"" it appears that the correlation coefficient is a measure of linear fit only.  I have not really found any reference to a correlation coefficient for exponential regression.

If there is no exponential regression correlation coefficient than why does the calculator give an r value for exponential regression but not quadratic?

I found some resources that said you need to check the scatter plot of the residuals based on a linear regression to see if exponential or quadratic regression is best.

I am confused on why the calculator has an r value for exponential regression, but I cannot find any references to it online.  Everything says correlation coefficient is a measure of the best line fit for the data.

Thank you",7,1524212575.0,8djicn,False,"Up front, my statistics background is virtually nil (will likely be apparent from this post).
So in no apparent order, here are a series of questions that will hopefully identify my problem.
Is using the correlation coefficient even a thing used for determining the correlation for an exponential regression?  It was presented to me that comparing the correlation coefficient from a given data set, using the Ti84, the way to compare what fits the data best (linear or exponential regression) is to compare the linear regression correlation coefficient and the exponential correlation coefficient to determine which model fits the data better (the number closer to 1 or -1 would indicate the better fit).  However, from all my ""research"" it appears that the correlation coefficient is a measure of linear fit only.  I have not really found any reference to a correlation coefficient for exponential regression.

If there is no exponential regression correlation coefficient than why does the calculator give an r value for exponential regression but not quadratic?

I found some resources that said you need to check the scatter plot of the residuals based on a linear regression to see if exponential or quadratic regression is best.

I am confused on why the calculator has an r value for exponential regression, but I cannot find any references to it online.  Everything says correlation coefficient is a measure of the best line fit for the data.

Thank you",0,"Up front, my statistics background is virtually nil (will likely be apparent from this post).
So in no apparent order, here are a series of questions that will hopefully identify my problem.
Is using the correlation coefficient even a thing used for determining the correlation for an exponential regression?  It was presented to me that comparing the correlation coefficient from a given data set, using the Ti84, the way to compare what fits the data best (linear or exponential regression) is to compare the linear regression correlation coefficient and the exponential correlation coefficient to determine which model fits the data better (the number closer to 1 or -1 would indicate the better fit).  However, from all my ""research"" it appears that the correlation coefficient is a measure of linear fit only.  I have not really found any reference to a correlation coefficient for exponential regression.

If there is no exponential regression correlation coefficient than why does the calculator give an r value for exponential regression but not quadratic?

I found some resources that said you need to check the scatter plot of the residuals based on a linear regression to see if exponential or quadratic regression is best.

I am confused on why the calculator has an r value for exponential regression, but I cannot find any references to it online.  Everything says correlation coefficient is a measure of the best line fit for the data.

Thank you",1,statistics,54935,,Correlation Coefficient for exponential regression,https://www.reddit.com/r/statistics/comments/8djicn/correlation_coefficient_for_exponential_regression/,all_ads,2018-04-20 04:22:55,43 days 21:12:39.036584000,
"Is it more, less, or equally likely that a coin flipped 100 times hits heads every time, tails every time, or heads then tails 50 times? (by this I mean no two heads or tails in a row)
Just wondering this and thought maybe it would make for good conversation, more fun than just asking google and I may end up learning something extra along the way.",3,1524210057.0,8dj8le,False,"Is it more, less, or equally likely that a coin flipped 100 times hits heads every time, tails every time, or heads then tails 50 times? (by this I mean no two heads or tails in a row)
Just wondering this and thought maybe it would make for good conversation, more fun than just asking google and I may end up learning something extra along the way.",0,"Is it more, less, or equally likely that a coin flipped 100 times hits heads every time, tails every time, or heads then tails 50 times? (by this I mean no two heads or tails in a row)
Just wondering this and thought maybe it would make for good conversation, more fun than just asking google and I may end up learning something extra along the way.",0,statistics,54935,,Odds of a coin flip,https://www.reddit.com/r/statistics/comments/8dj8le/odds_of_a_coin_flip/,all_ads,2018-04-20 03:40:57,43 days 21:54:37.036584000,
"I've been really interested in [what](https://search-proquest-com.ezproxy.rit.edu/docview/1865256797?pq-origsite=summon) [I've](https://ac-els-cdn-com.ezproxy.rit.edu/S1470160X16300735/1-s2.0-S1470160X16300735-main.pdf?_tid=5de8d5b8-d1d5-4109-a9fa-b34a50c546cd&acdnat=1523974469_bf97f254bbc6f95b0b77370688a7ea36) [read](https://ac.els-cdn.com/S0079661114000020/1-s2.0-S0079661114000020-main.pdf?_tid=7d84bf5f-55eb-4d28-ad71-d4e4566359e9&acdnat=1523988408_3d6d61d9dcdba4654091c28801d6b994) [so far](https://www.gislounge.com/monitoring-algal-blooms-remote-sensing/) about Geostatistics and statistics for Environmental Science. I had a couple of questions...

1. Does anyone have any recommendations for books to read or places to get info? Currently working my way through my college's library. There's 3 books total on related topics.

2. If you have a job working with spatial / spatiotemporal data, as an environmental scientist using statistics, or have any experience with these topics, would you have any advice for someone who is interested? Some questions are what is your job description, how do you use statistics, do you find your work valuable, and/or what collegiate level courses did you find most valuable in your job?

For background, I'm currently a Software Engineering major with a Stats minor working towards either a GIS or Math minor as well. I'm much more interested in the analysis side though, not a huge fan of Software Engineering as a career path.",5,1524185647.0,8dg5s6,False,"I've been really interested in [what](https://search-proquest-com.ezproxy.rit.edu/docview/1865256797?pq-origsite=summon) [I've](https://ac-els-cdn-com.ezproxy.rit.edu/S1470160X16300735/1-s2.0-S1470160X16300735-main.pdf?_tid=5de8d5b8-d1d5-4109-a9fa-b34a50c546cd&acdnat=1523974469_bf97f254bbc6f95b0b77370688a7ea36) [read](https://ac.els-cdn.com/S0079661114000020/1-s2.0-S0079661114000020-main.pdf?_tid=7d84bf5f-55eb-4d28-ad71-d4e4566359e9&acdnat=1523988408_3d6d61d9dcdba4654091c28801d6b994) [so far](https://www.gislounge.com/monitoring-algal-blooms-remote-sensing/) about Geostatistics and statistics for Environmental Science. I had a couple of questions...

1. Does anyone have any recommendations for books to read or places to get info? Currently working my way through my college's library. There's 3 books total on related topics.

2. If you have a job working with spatial / spatiotemporal data, as an environmental scientist using statistics, or have any experience with these topics, would you have any advice for someone who is interested? Some questions are what is your job description, how do you use statistics, do you find your work valuable, and/or what collegiate level courses did you find most valuable in your job?

For background, I'm currently a Software Engineering major with a Stats minor working towards either a GIS or Math minor as well. I'm much more interested in the analysis side though, not a huge fan of Software Engineering as a career path.",0,"I've been really interested in [what](https://search-proquest-com.ezproxy.rit.edu/docview/1865256797?pq-origsite=summon) [I've](https://ac-els-cdn-com.ezproxy.rit.edu/S1470160X16300735/1-s2.0-S1470160X16300735-main.pdf?_tid=5de8d5b8-d1d5-4109-a9fa-b34a50c546cd&acdnat=1523974469_bf97f254bbc6f95b0b77370688a7ea36) [read](https://ac.els-cdn.com/S0079661114000020/1-s2.0-S0079661114000020-main.pdf?_tid=7d84bf5f-55eb-4d28-ad71-d4e4566359e9&acdnat=1523988408_3d6d61d9dcdba4654091c28801d6b994) [so far](https://www.gislounge.com/monitoring-algal-blooms-remote-sensing/) about Geostatistics and statistics for Environmental Science. I had a couple of questions...

1. Does anyone have any recommendations for books to read or places to get info? Currently working my way through my college's library. There's 3 books total on related topics.

2. If you have a job working with spatial / spatiotemporal data, as an environmental scientist using statistics, or have any experience with these topics, would you have any advice for someone who is interested? Some questions are what is your job description, how do you use statistics, do you find your work valuable, and/or what collegiate level courses did you find most valuable in your job?

For background, I'm currently a Software Engineering major with a Stats minor working towards either a GIS or Math minor as well. I'm much more interested in the analysis side though, not a huge fan of Software Engineering as a career path.",2,statistics,54935,,Geo and Environmental Statistics Questions,https://www.reddit.com/r/statistics/comments/8dg5s6/geo_and_environmental_statistics_questions/,all_ads,2018-04-19 20:54:07,44 days 04:41:27.036584000,
"Hi all! I'm in a human rights class where we design a research project without doing the actual research. My question I'm designing is, how has mental health care in South Africa been affected by the UN CRPD.  I have a few questions I was hoping people could answer.

1- Id like to compare the increase or decrease in budgets that psychiatric hospitals and psychiatric wards receive relative to the general hospitals and wards. What statistical methods can I use to accomplish this? 

2- I'd also like to compare the rates of invoulantary incapacitation before and after the ratification of UN CRPD.  Also the rates of mental illness diagnoses ( though that is more difficult).  How can I prove a correlation, if one exists, between the rates before and after? 

3- I'll be ""sending"" out a survey to see how many people who identify as having mental illness or have dealt with mental illness, sought or have access to treatment. Is a simple random sample the best way to send out the survey? How large is a sample size necessary to gather a good rate of response and get quality data?


4- If anyone has any tips or suggestions on types of statistical questions I should ask or methods I should use please add them. I need all the help I can get.",1,1524197977.0,8dhspy,False,"Hi all! I'm in a human rights class where we design a research project without doing the actual research. My question I'm designing is, how has mental health care in South Africa been affected by the UN CRPD.  I have a few questions I was hoping people could answer.

1- Id like to compare the increase or decrease in budgets that psychiatric hospitals and psychiatric wards receive relative to the general hospitals and wards. What statistical methods can I use to accomplish this? 

2- I'd also like to compare the rates of invoulantary incapacitation before and after the ratification of UN CRPD.  Also the rates of mental illness diagnoses ( though that is more difficult).  How can I prove a correlation, if one exists, between the rates before and after? 

3- I'll be ""sending"" out a survey to see how many people who identify as having mental illness or have dealt with mental illness, sought or have access to treatment. Is a simple random sample the best way to send out the survey? How large is a sample size necessary to gather a good rate of response and get quality data?


4- If anyone has any tips or suggestions on types of statistical questions I should ask or methods I should use please add them. I need all the help I can get.",0,"Hi all! I'm in a human rights class where we design a research project without doing the actual research. My question I'm designing is, how has mental health care in South Africa been affected by the UN CRPD.  I have a few questions I was hoping people could answer.

1- Id like to compare the increase or decrease in budgets that psychiatric hospitals and psychiatric wards receive relative to the general hospitals and wards. What statistical methods can I use to accomplish this? 

2- I'd also like to compare the rates of invoulantary incapacitation before and after the ratification of UN CRPD.  Also the rates of mental illness diagnoses ( though that is more difficult).  How can I prove a correlation, if one exists, between the rates before and after? 

3- I'll be ""sending"" out a survey to see how many people who identify as having mental illness or have dealt with mental illness, sought or have access to treatment. Is a simple random sample the best way to send out the survey? How large is a sample size necessary to gather a good rate of response and get quality data?


4- If anyone has any tips or suggestions on types of statistical questions I should ask or methods I should use please add them. I need all the help I can get.",0,statistics,54935,,Help with college project,https://www.reddit.com/r/statistics/comments/8dhspy/help_with_college_project/,all_ads,2018-04-20 00:19:37,44 days 01:15:57.036584000,
"Hi I'm analysing some data. It's a split plot of two N levels (high + low). Within each N level there are seven treatments these are: five rates of Fertiliser A, a single rate of Fertiliser B, and a control. 


Any ideas for how to go about this? 
A split-plot ANOVA doesn't take into account the increase in rate of Fertiliser A and I'm not sure how to incorporate fertiliser B. Perhaps grouped as ""A1,A2,A3,A4,A5,B1,control""


A regression with groups (N) allows me to look at the change with rate, but I'm not sure it is correct given the split plot design and again I'm not sure how to incorporate Fertiliser B.

Thanks",0,1524187355.0,8dgdvw,False,"Hi I'm analysing some data. It's a split plot of two N levels (high + low). Within each N level there are seven treatments these are: five rates of Fertiliser A, a single rate of Fertiliser B, and a control. 


Any ideas for how to go about this? 
A split-plot ANOVA doesn't take into account the increase in rate of Fertiliser A and I'm not sure how to incorporate fertiliser B. Perhaps grouped as ""A1,A2,A3,A4,A5,B1,control""


A regression with groups (N) allows me to look at the change with rate, but I'm not sure it is correct given the split plot design and again I'm not sure how to incorporate Fertiliser B.

Thanks",0,"Hi I'm analysing some data. It's a split plot of two N levels (high + low). Within each N level there are seven treatments these are: five rates of Fertiliser A, a single rate of Fertiliser B, and a control. 


Any ideas for how to go about this? 
A split-plot ANOVA doesn't take into account the increase in rate of Fertiliser A and I'm not sure how to incorporate fertiliser B. Perhaps grouped as ""A1,A2,A3,A4,A5,B1,control""


A regression with groups (N) allows me to look at the change with rate, but I'm not sure it is correct given the split plot design and again I'm not sure how to incorporate Fertiliser B.

Thanks",1,statistics,54935,,Split plot with unbalanced? design,https://www.reddit.com/r/statistics/comments/8dgdvw/split_plot_with_unbalanced_design/,all_ads,2018-04-19 21:22:35,44 days 04:12:59.036584000,
"Hello,
I'm trying to figure out the expected value of selecting n coins from a bag; let's say 3.
Given:
12 coins: 3 quarters, 5 dimes, and 4 nickels.

How do I calculate the expected value here? I understand the expected value of selecting a single coin, but how do I calculate the second and third coin when there is not replacement?
It would be helpful if this has already been asked/solved somewhere else online and someone has a link.

Thank you!",4,1524185814.0,8dg6ks,False,"Hello,
I'm trying to figure out the expected value of selecting n coins from a bag; let's say 3.
Given:
12 coins: 3 quarters, 5 dimes, and 4 nickels.

How do I calculate the expected value here? I understand the expected value of selecting a single coin, but how do I calculate the second and third coin when there is not replacement?
It would be helpful if this has already been asked/solved somewhere else online and someone has a link.

Thank you!",0,"Hello,
I'm trying to figure out the expected value of selecting n coins from a bag; let's say 3.
Given:
12 coins: 3 quarters, 5 dimes, and 4 nickels.

How do I calculate the expected value here? I understand the expected value of selecting a single coin, but how do I calculate the second and third coin when there is not replacement?
It would be helpful if this has already been asked/solved somewhere else online and someone has a link.

Thank you!",0,statistics,54935,,Expected Value of selecting X coins from bag,https://www.reddit.com/r/statistics/comments/8dg6ks/expected_value_of_selecting_x_coins_from_bag/,all_ads,2018-04-19 20:56:54,44 days 04:38:40.036584000,
"Hi,

I am trying to learn regression by myself. I understand it is used to predict  correlation between variables but  I don't get the part that mentions linear regression is used to estimate values, hence a model can be formed. Why can't B0 and B1 be worked out, why can they only be estimated?

I also don't really get the regression line in the regression scatter plot. That's a line of best fit, but is it constructed using the estimates for B0 and B1?    

Thanks",9,1524180376.0,8dfh89,False,"Hi,

I am trying to learn regression by myself. I understand it is used to predict  correlation between variables but  I don't get the part that mentions linear regression is used to estimate values, hence a model can be formed. Why can't B0 and B1 be worked out, why can they only be estimated?

I also don't really get the regression line in the regression scatter plot. That's a line of best fit, but is it constructed using the estimates for B0 and B1?    

Thanks",0,"Hi,

I am trying to learn regression by myself. I understand it is used to predict  correlation between variables but  I don't get the part that mentions linear regression is used to estimate values, hence a model can be formed. Why can't B0 and B1 be worked out, why can they only be estimated?

I also don't really get the regression line in the regression scatter plot. That's a line of best fit, but is it constructed using the estimates for B0 and B1?    

Thanks",0,statistics,54935,,Question on linear regression,https://www.reddit.com/r/statistics/comments/8dfh89/question_on_linear_regression/,all_ads,2018-04-19 19:26:16,44 days 06:09:18.036584000,
"Last year as part of my undergraduate requirements I took an engineering statistics course. I really loved the course and found a lot of what I learned could be applied in my future career as an industrial engineer. The issue I have is that with my course load I don't have time to take another statistics course, and by not really using everything in the course of treatment the past year my memory is starting to fog on everything I learned. Can anyone recommend a book, textbook, or author to help me brush up and continue my education on my own? Also before anyone asks my University rents out text books and the one we had was trash (really great professor though) and I do not want to buy it if there is a better option. Thank you all in advance.",24,1524107428.0,8d89z9,False,"Last year as part of my undergraduate requirements I took an engineering statistics course. I really loved the course and found a lot of what I learned could be applied in my future career as an industrial engineer. The issue I have is that with my course load I don't have time to take another statistics course, and by not really using everything in the course of treatment the past year my memory is starting to fog on everything I learned. Can anyone recommend a book, textbook, or author to help me brush up and continue my education on my own? Also before anyone asks my University rents out text books and the one we had was trash (really great professor though) and I do not want to buy it if there is a better option. Thank you all in advance.",0,"Last year as part of my undergraduate requirements I took an engineering statistics course. I really loved the course and found a lot of what I learned could be applied in my future career as an industrial engineer. The issue I have is that with my course load I don't have time to take another statistics course, and by not really using everything in the course of treatment the past year my memory is starting to fog on everything I learned. Can anyone recommend a book, textbook, or author to help me brush up and continue my education on my own? Also before anyone asks my University rents out text books and the one we had was trash (really great professor though) and I do not want to buy it if there is a better option. Thank you all in advance.",40,statistics,54935,,Statistics Reading Recommendations,https://www.reddit.com/r/statistics/comments/8d89z9/statistics_reading_recommendations/,all_ads,2018-04-18 23:10:28,45 days 02:25:06.036584000,
"I am wondering how you can justify the use of lower tail test in hypothesis testing. I know that this test is used when you have the H1 of mu<0 but what exactly does mu<0 mean other than showing there is a negative relationship? 

Thanks",4,1524179499.0,8dfd5h,False,"I am wondering how you can justify the use of lower tail test in hypothesis testing. I know that this test is used when you have the H1 of mu<0 but what exactly does mu<0 mean other than showing there is a negative relationship? 

Thanks",0,"I am wondering how you can justify the use of lower tail test in hypothesis testing. I know that this test is used when you have the H1 of mu<0 but what exactly does mu<0 mean other than showing there is a negative relationship? 

Thanks",1,statistics,54935,,Hypothesis in hypothesis testing,https://www.reddit.com/r/statistics/comments/8dfd5h/hypothesis_in_hypothesis_testing/,all_ads,2018-04-19 19:11:39,44 days 06:23:55.036584000,
"Okay, I'm now embarking my journey on Statistics. Now I'm trying to understand the concept of **Mean** and **Variance** and etc... Anyway, I was looking at the formula of sample variance, and there was another formula which invoked my curiosity. Here's the example.

This is the sample of time to get prepared in every morning.

**39, 29, 43, 52, 39, 44, 40, 31, 44, 35(Mins)**

**(sample size)number of values = 10**

**(sample mean)mean = 39.6**

my question **why is the sum of the differences between each value and the mean to be zero?** I kinda get the sense of it that because mean is the average of all values, and that's when you make summation of these values, you get 0. But, I don't this supports the proper explanation. Is there a good way to explain **why the sum of the differences between each value and the mean to be zero?** Thanks.
",3,1524172158.0,8dei4i,False,"Okay, I'm now embarking my journey on Statistics. Now I'm trying to understand the concept of **Mean** and **Variance** and etc... Anyway, I was looking at the formula of sample variance, and there was another formula which invoked my curiosity. Here's the example.

This is the sample of time to get prepared in every morning.

**39, 29, 43, 52, 39, 44, 40, 31, 44, 35(Mins)**

**(sample size)number of values = 10**

**(sample mean)mean = 39.6**

my question **why is the sum of the differences between each value and the mean to be zero?** I kinda get the sense of it that because mean is the average of all values, and that's when you make summation of these values, you get 0. But, I don't this supports the proper explanation. Is there a good way to explain **why the sum of the differences between each value and the mean to be zero?** Thanks.
",0,"Okay, I'm now embarking my journey on Statistics. Now I'm trying to understand the concept of **Mean** and **Variance** and etc... Anyway, I was looking at the formula of sample variance, and there was another formula which invoked my curiosity. Here's the example.

This is the sample of time to get prepared in every morning.

**39, 29, 43, 52, 39, 44, 40, 31, 44, 35(Mins)**

**(sample size)number of values = 10**

**(sample mean)mean = 39.6**

my question **why is the sum of the differences between each value and the mean to be zero?** I kinda get the sense of it that because mean is the average of all values, and that's when you make summation of these values, you get 0. But, I don't this supports the proper explanation. Is there a good way to explain **why the sum of the differences between each value and the mean to be zero?** Thanks.
",1,statistics,54935,,"Beginner: question from my wonder(Mean, Variance)",https://www.reddit.com/r/statistics/comments/8dei4i/beginner_question_from_my_wondermean_variance/,all_ads,2018-04-19 17:09:18,44 days 08:26:16.036584000,
"Hi,

Long time lurker.. just a general question which I get confused about and have heard multiple views on..

For data transformations, I have had people saying X shouldn't be transformed, and others saying it should.. so when should I, and shouldn't I transform? Is it possible for a dataset to have been historically transformed as a log, only to later find out it shouldn't be? 


What should I be thinking/doing to ensure that the transformation is actually useful, and won't add in bias to the results? ",4,1524160237.0,8ddh7b,False,"Hi,

Long time lurker.. just a general question which I get confused about and have heard multiple views on..

For data transformations, I have had people saying X shouldn't be transformed, and others saying it should.. so when should I, and shouldn't I transform? Is it possible for a dataset to have been historically transformed as a log, only to later find out it shouldn't be? 


What should I be thinking/doing to ensure that the transformation is actually useful, and won't add in bias to the results? ",0,"Hi,

Long time lurker.. just a general question which I get confused about and have heard multiple views on..

For data transformations, I have had people saying X shouldn't be transformed, and others saying it should.. so when should I, and shouldn't I transform? Is it possible for a dataset to have been historically transformed as a log, only to later find out it shouldn't be? 


What should I be thinking/doing to ensure that the transformation is actually useful, and won't add in bias to the results? ",1,statistics,54935,,Data Transformations,https://www.reddit.com/r/statistics/comments/8ddh7b/data_transformations/,all_ads,2018-04-19 13:50:37,44 days 11:44:57.036584000,
"To be honest, I'm not very knowledgable about regression modelling in general. I just know that my software defaults to not estimating dispersion on binomial data, but I don't understand why and what effect it has.

I'd be glad to have this explained to me.",3,1524152577.0,8dcy8m,False,"To be honest, I'm not very knowledgable about regression modelling in general. I just know that my software defaults to not estimating dispersion on binomial data, but I don't understand why and what effect it has.

I'd be glad to have this explained to me.",0,"To be honest, I'm not very knowledgable about regression modelling in general. I just know that my software defaults to not estimating dispersion on binomial data, but I don't understand why and what effect it has.

I'd be glad to have this explained to me.",1,statistics,54935,,"When fitting GLMs on binomial response data, when should I choose to estimate its dispersion parameter (over fixing it to 1)?",https://www.reddit.com/r/statistics/comments/8dcy8m/when_fitting_glms_on_binomial_response_data_when/,all_ads,2018-04-19 11:42:57,44 days 13:52:37.036584000,
"Hi, I have an exam in a few days on linear regression (single variable ). However, I lack a conceptual understanding of it as my teacher just gave a bunch of equations to memorize. 

I would really appreciate if someone can provide an online resource that explains it conceptually and logically. Thanks. ",9,1524145867.0,8dcgrj,False,"Hi, I have an exam in a few days on linear regression (single variable ). However, I lack a conceptual understanding of it as my teacher just gave a bunch of equations to memorize. 

I would really appreciate if someone can provide an online resource that explains it conceptually and logically. Thanks. ",0,"Hi, I have an exam in a few days on linear regression (single variable ). However, I lack a conceptual understanding of it as my teacher just gave a bunch of equations to memorize. 

I would really appreciate if someone can provide an online resource that explains it conceptually and logically. Thanks. ",0,statistics,54935,,Linear regression help on AP stats level,https://www.reddit.com/r/statistics/comments/8dcgrj/linear_regression_help_on_ap_stats_level/,all_ads,2018-04-19 09:51:07,44 days 15:44:27.036584000,
"In a project I'm working on, we'd like to show that certain stereotyped brain areas (represented by shapes at certain locations) are reproducible enough in terms of shape and location to be ""classified"". 

Currently, we apply the Mann-Whitney U test separately on the distances between areas and a landmark-based area metric to each pairwise comparison (with Bonferroni correction) between all identified areas. Independently, I constructed and cross-validated an ensemble classifier that applies k-Nearest Neighbors and a convolutional network trained on the shapes; I tested the trained classifier against one on shuffled labels (the null hypothesis) as a permutation test. Of course, the construction of the Ensemble classifier was informed by priors that would elicit good performance (that shape and location were important) but so was the statistical test.

Is there a compelling reason to use one or the other irrespective of p-value/classifier performance. Is there some resource that describes a theoretical framework of how and when a classifier can/should be used in equivalent or superior capacity to a more traditional statistical test? I was able to find almost no resources on this although [this](https://www.nature.com/articles/nmeth.4642) comes close.

Thanks in advance.

",0,1524132178.0,8db74r,False,"In a project I'm working on, we'd like to show that certain stereotyped brain areas (represented by shapes at certain locations) are reproducible enough in terms of shape and location to be ""classified"". 

Currently, we apply the Mann-Whitney U test separately on the distances between areas and a landmark-based area metric to each pairwise comparison (with Bonferroni correction) between all identified areas. Independently, I constructed and cross-validated an ensemble classifier that applies k-Nearest Neighbors and a convolutional network trained on the shapes; I tested the trained classifier against one on shuffled labels (the null hypothesis) as a permutation test. Of course, the construction of the Ensemble classifier was informed by priors that would elicit good performance (that shape and location were important) but so was the statistical test.

Is there a compelling reason to use one or the other irrespective of p-value/classifier performance. Is there some resource that describes a theoretical framework of how and when a classifier can/should be used in equivalent or superior capacity to a more traditional statistical test? I was able to find almost no resources on this although [this](https://www.nature.com/articles/nmeth.4642) comes close.

Thanks in advance.

",0,"In a project I'm working on, we'd like to show that certain stereotyped brain areas (represented by shapes at certain locations) are reproducible enough in terms of shape and location to be ""classified"". 

Currently, we apply the Mann-Whitney U test separately on the distances between areas and a landmark-based area metric to each pairwise comparison (with Bonferroni correction) between all identified areas. Independently, I constructed and cross-validated an ensemble classifier that applies k-Nearest Neighbors and a convolutional network trained on the shapes; I tested the trained classifier against one on shuffled labels (the null hypothesis) as a permutation test. Of course, the construction of the Ensemble classifier was informed by priors that would elicit good performance (that shape and location were important) but so was the statistical test.

Is there a compelling reason to use one or the other irrespective of p-value/classifier performance. Is there some resource that describes a theoretical framework of how and when a classifier can/should be used in equivalent or superior capacity to a more traditional statistical test? I was able to find almost no resources on this although [this](https://www.nature.com/articles/nmeth.4642) comes close.

Thanks in advance.

",2,statistics,54935,,Ensemble classifier as an alternative to a statistical test?,https://www.reddit.com/r/statistics/comments/8db74r/ensemble_classifier_as_an_alternative_to_a/,all_ads,2018-04-19 06:02:58,44 days 19:32:36.036584000,
"I've been studying the local FDR and q-values by Efron and Storey, and while I kind of understand the theories, I feel pretty vague about how they're used in actual practice.  Can someone please explain? ",3,1524109155.0,8d8hx3,False,"I've been studying the local FDR and q-values by Efron and Storey, and while I kind of understand the theories, I feel pretty vague about how they're used in actual practice.  Can someone please explain? ",0,"I've been studying the local FDR and q-values by Efron and Storey, and while I kind of understand the theories, I feel pretty vague about how they're used in actual practice.  Can someone please explain? ",4,statistics,54935,,Using local FDR and q-values in practice,https://www.reddit.com/r/statistics/comments/8d8hx3/using_local_fdr_and_qvalues_in_practice/,all_ads,2018-04-18 23:39:15,45 days 01:56:19.036584000,
"Good day all.

I’d like to preface my post by stating that my understanding of statistics is minimal, having only ever studied it some 14 years ago at a very introductory level.

Also, I must apologise that the reason for this post is rather juvenile, as it’s motivation is to improve my understanding of a football (soccer) management computer game. I hope this does not detract from a willingness to assist with the problem. 

I’ll try and be as succinct as possible…

In the game each football player assumes a position, and each position has specific responsibilities. The example I will use is that of a striker, whose aim is predominantly to score goals. 

In order to assist in making the choice of who should play in what role, each player is assigned values to skills, like so:

https://imgur.com/a/5oe02

With the maximum of each of these skills being 20, a “manager” of a team has to make judgement calls as to which skills to prioritise for each role. Common sense dictates that for a striker a manager must look to things like “Finishing”, which corresponds to a player’s ability to shoot the ball well. 

Despite this being the most likely priority skill for a striker, other skills are also significant, and my dilemma is trying to establish what they are, and most importantly, which ones to prioritise and in what order.

In an attempt to establish this, I decided to gather information on 150 strikers. The strikers that I selected were rated >7.2 out of 10 on their performance,  averaged over a minimum of 10 games played.

The first problem I believe I face is that players who play it the top leagues are proficient in many more skills than those in the lower leagues, meaning that they will inevitably bring up the average score of less relevant skills. Lower league players on the other hand are likely to perform well against their weaker opponents, despite having a very limited strong skill set, which I fear does not average proportionally to their relevance. 

In any case, these are my results:

https://imgur.com/a/cC0uL (EDIT: Changed to include skills on range from 0-100 rather than only 0-20 for improved accuracy)

Although this gives me some idea of the skills I should prioritise, my concern is that it does not reflect the weight of importance of each skill. 

That is to say, “finishing” is above all else the most important skill needed for a striker to score, yet the difference between it and something less relevant e.g. “Passing”, is only 2. Furthermore, skills that I know to be entirely useless to the striking role such as marking and tackling have yielded an average of 9 (which I consider quite high).

What i’m hoping for someone to point me in the right direction of (as I certainly don’t want for anyone to waste their time on this) is a method by which I can most reasonably establish what ‘weighting’, or alternatively what ‘handicap’, to place on each skill.

I feel that what I have done so far shows me roughly which skills are most important (although this is debatable as my sampling methodology may also be totally ill advised), it doesn’t show me the disparity that I had anticipated between each skill.

Is there away to more accurately learn which skills are most important, and how much more important they are, based on the information I have provided? Is my sampling method to restrictive or perhaps too broad? Is there perhaps other information that should inform my process?

I googled “statistical weighting” or similar, but to no avail. I suspect the terminology i’m using is hindering my ability to research an improved method for achieving my aims. 

In any case, If you have read this far then I want to say thank you. I really appreciate it.

I hope this isn’t too goofy a reason to engage in understanding more about stats. 

Cheers! 
",12,1524100213.0,8d7b4o,False,"Good day all.

I’d like to preface my post by stating that my understanding of statistics is minimal, having only ever studied it some 14 years ago at a very introductory level.

Also, I must apologise that the reason for this post is rather juvenile, as it’s motivation is to improve my understanding of a football (soccer) management computer game. I hope this does not detract from a willingness to assist with the problem. 

I’ll try and be as succinct as possible…

In the game each football player assumes a position, and each position has specific responsibilities. The example I will use is that of a striker, whose aim is predominantly to score goals. 

In order to assist in making the choice of who should play in what role, each player is assigned values to skills, like so:

https://imgur.com/a/5oe02

With the maximum of each of these skills being 20, a “manager” of a team has to make judgement calls as to which skills to prioritise for each role. Common sense dictates that for a striker a manager must look to things like “Finishing”, which corresponds to a player’s ability to shoot the ball well. 

Despite this being the most likely priority skill for a striker, other skills are also significant, and my dilemma is trying to establish what they are, and most importantly, which ones to prioritise and in what order.

In an attempt to establish this, I decided to gather information on 150 strikers. The strikers that I selected were rated >7.2 out of 10 on their performance,  averaged over a minimum of 10 games played.

The first problem I believe I face is that players who play it the top leagues are proficient in many more skills than those in the lower leagues, meaning that they will inevitably bring up the average score of less relevant skills. Lower league players on the other hand are likely to perform well against their weaker opponents, despite having a very limited strong skill set, which I fear does not average proportionally to their relevance. 

In any case, these are my results:

https://imgur.com/a/cC0uL (EDIT: Changed to include skills on range from 0-100 rather than only 0-20 for improved accuracy)

Although this gives me some idea of the skills I should prioritise, my concern is that it does not reflect the weight of importance of each skill. 

That is to say, “finishing” is above all else the most important skill needed for a striker to score, yet the difference between it and something less relevant e.g. “Passing”, is only 2. Furthermore, skills that I know to be entirely useless to the striking role such as marking and tackling have yielded an average of 9 (which I consider quite high).

What i’m hoping for someone to point me in the right direction of (as I certainly don’t want for anyone to waste their time on this) is a method by which I can most reasonably establish what ‘weighting’, or alternatively what ‘handicap’, to place on each skill.

I feel that what I have done so far shows me roughly which skills are most important (although this is debatable as my sampling methodology may also be totally ill advised), it doesn’t show me the disparity that I had anticipated between each skill.

Is there away to more accurately learn which skills are most important, and how much more important they are, based on the information I have provided? Is my sampling method to restrictive or perhaps too broad? Is there perhaps other information that should inform my process?

I googled “statistical weighting” or similar, but to no avail. I suspect the terminology i’m using is hindering my ability to research an improved method for achieving my aims. 

In any case, If you have read this far then I want to say thank you. I really appreciate it.

I hope this isn’t too goofy a reason to engage in understanding more about stats. 

Cheers! 
",0,"Good day all.

I’d like to preface my post by stating that my understanding of statistics is minimal, having only ever studied it some 14 years ago at a very introductory level.

Also, I must apologise that the reason for this post is rather juvenile, as it’s motivation is to improve my understanding of a football (soccer) management computer game. I hope this does not detract from a willingness to assist with the problem. 

I’ll try and be as succinct as possible…

In the game each football player assumes a position, and each position has specific responsibilities. The example I will use is that of a striker, whose aim is predominantly to score goals. 

In order to assist in making the choice of who should play in what role, each player is assigned values to skills, like so:

https://imgur.com/a/5oe02

With the maximum of each of these skills being 20, a “manager” of a team has to make judgement calls as to which skills to prioritise for each role. Common sense dictates that for a striker a manager must look to things like “Finishing”, which corresponds to a player’s ability to shoot the ball well. 

Despite this being the most likely priority skill for a striker, other skills are also significant, and my dilemma is trying to establish what they are, and most importantly, which ones to prioritise and in what order.

In an attempt to establish this, I decided to gather information on 150 strikers. The strikers that I selected were rated >7.2 out of 10 on their performance,  averaged over a minimum of 10 games played.

The first problem I believe I face is that players who play it the top leagues are proficient in many more skills than those in the lower leagues, meaning that they will inevitably bring up the average score of less relevant skills. Lower league players on the other hand are likely to perform well against their weaker opponents, despite having a very limited strong skill set, which I fear does not average proportionally to their relevance. 

In any case, these are my results:

https://imgur.com/a/cC0uL (EDIT: Changed to include skills on range from 0-100 rather than only 0-20 for improved accuracy)

Although this gives me some idea of the skills I should prioritise, my concern is that it does not reflect the weight of importance of each skill. 

That is to say, “finishing” is above all else the most important skill needed for a striker to score, yet the difference between it and something less relevant e.g. “Passing”, is only 2. Furthermore, skills that I know to be entirely useless to the striking role such as marking and tackling have yielded an average of 9 (which I consider quite high).

What i’m hoping for someone to point me in the right direction of (as I certainly don’t want for anyone to waste their time on this) is a method by which I can most reasonably establish what ‘weighting’, or alternatively what ‘handicap’, to place on each skill.

I feel that what I have done so far shows me roughly which skills are most important (although this is debatable as my sampling methodology may also be totally ill advised), it doesn’t show me the disparity that I had anticipated between each skill.

Is there away to more accurately learn which skills are most important, and how much more important they are, based on the information I have provided? Is my sampling method to restrictive or perhaps too broad? Is there perhaps other information that should inform my process?

I googled “statistical weighting” or similar, but to no avail. I suspect the terminology i’m using is hindering my ability to research an improved method for achieving my aims. 

In any case, If you have read this far then I want to say thank you. I really appreciate it.

I hope this isn’t too goofy a reason to engage in understanding more about stats. 

Cheers! 
",6,statistics,54935,,"Total statistics novice looking for some guidance or just to be pointed in the right direction with regards to ""weighting"".",https://www.reddit.com/r/statistics/comments/8d7b4o/total_statistics_novice_looking_for_some_guidance/,all_ads,2018-04-18 21:10:13,45 days 04:25:21.036584000,
"So, I'm thinking about this one every once in a while:      
Let's assume there is group of people which consists of 10 people. All of them get a colored card every round. Every round 3 random players get an red card, the 7 other people get a blue one. Totally random.      
Now, I know, every new round the probability is the same to get the red card, even if you had it in the last 2,3,4....or 1000 rounds.      
     
Now let's think of an outside player who's betting on who is going to get the red card. Wouldn't it be 'smart' for the outsider to bet after several rounds: 'person x isn't going to get the red card because he/she had it more often now'? Like, at some point, every percentage share of every person will be the same, and at the moment player x got the red card too 'often'. Does this make sense?       

In it's system=ever round: the probability is the same.     
In the higher system=a lot of rounds: people who already got the red card a lot will get is less often in the future? I mean that's what's happening?      
Maybe it's a more philosphical question an mathematics don't give the answer to that. Because nobody would interfer in probabilities, it wouldn't change just because player x had it more often than other players. His probability next round would be the same. ",4,1524117100.0,8d9ijl,False,"So, I'm thinking about this one every once in a while:      
Let's assume there is group of people which consists of 10 people. All of them get a colored card every round. Every round 3 random players get an red card, the 7 other people get a blue one. Totally random.      
Now, I know, every new round the probability is the same to get the red card, even if you had it in the last 2,3,4....or 1000 rounds.      
     
Now let's think of an outside player who's betting on who is going to get the red card. Wouldn't it be 'smart' for the outsider to bet after several rounds: 'person x isn't going to get the red card because he/she had it more often now'? Like, at some point, every percentage share of every person will be the same, and at the moment player x got the red card too 'often'. Does this make sense?       

In it's system=ever round: the probability is the same.     
In the higher system=a lot of rounds: people who already got the red card a lot will get is less often in the future? I mean that's what's happening?      
Maybe it's a more philosphical question an mathematics don't give the answer to that. Because nobody would interfer in probabilities, it wouldn't change just because player x had it more often than other players. His probability next round would be the same. ",0,"So, I'm thinking about this one every once in a while:      
Let's assume there is group of people which consists of 10 people. All of them get a colored card every round. Every round 3 random players get an red card, the 7 other people get a blue one. Totally random.      
Now, I know, every new round the probability is the same to get the red card, even if you had it in the last 2,3,4....or 1000 rounds.      
     
Now let's think of an outside player who's betting on who is going to get the red card. Wouldn't it be 'smart' for the outsider to bet after several rounds: 'person x isn't going to get the red card because he/she had it more often now'? Like, at some point, every percentage share of every person will be the same, and at the moment player x got the red card too 'often'. Does this make sense?       

In it's system=ever round: the probability is the same.     
In the higher system=a lot of rounds: people who already got the red card a lot will get is less often in the future? I mean that's what's happening?      
Maybe it's a more philosphical question an mathematics don't give the answer to that. Because nobody would interfer in probabilities, it wouldn't change just because player x had it more often than other players. His probability next round would be the same. ",3,statistics,54935,,Noob question. Probabilities?,https://www.reddit.com/r/statistics/comments/8d9ijl/noob_question_probabilities/,all_ads,2018-04-19 01:51:40,44 days 23:43:54.036584000,
"https://www.theatlantic.com/science/archive/2018/04/a-groundbreaking-new-mathematical-tool/557903/

>Lately, Richards has taken up a powerful new tool for detecting correlations, the “distance correlation” method. In 2014, he and his wife, the Jamaican American astrophysicist Mercedes Richards, and a third coauthor applied the distance correlation method to detect previously unknown associations among 63,500 galaxies—identifying, for instance, that some were “starburst galaxies” that quickly burn themselves up.",19,1524057635.0,8d389r,False,"https://www.theatlantic.com/science/archive/2018/04/a-groundbreaking-new-mathematical-tool/557903/

>Lately, Richards has taken up a powerful new tool for detecting correlations, the “distance correlation” method. In 2014, he and his wife, the Jamaican American astrophysicist Mercedes Richards, and a third coauthor applied the distance correlation method to detect previously unknown associations among 63,500 galaxies—identifying, for instance, that some were “starburst galaxies” that quickly burn themselves up.",0,"https://www.theatlantic.com/science/archive/2018/04/a-groundbreaking-new-mathematical-tool/557903/

>Lately, Richards has taken up a powerful new tool for detecting correlations, the “distance correlation” method. In 2014, he and his wife, the Jamaican American astrophysicist Mercedes Richards, and a third coauthor applied the distance correlation method to detect previously unknown associations among 63,500 galaxies—identifying, for instance, that some were “starburst galaxies” that quickly burn themselves up.",25,statistics,54935,,A Powerful New Weapon in the Fight Against Shoddy Statistics [distance correlation],https://www.reddit.com/r/statistics/comments/8d389r/a_powerful_new_weapon_in_the_fight_against_shoddy/,all_ads,2018-04-18 09:20:35,45 days 16:14:59.036584000,
"I took a statistics class when I was in high school (2008), but I shrugged it off and didn't learn anything due to lack of effort.

This week, I took a HIGHLY condensed biostatistics class and was extremely interested!

I'm sorry if this is answered in an FAQ somewhere, but I was hoping to get some ideas for a starting point to learn about statistics? Thanks in advance!",0,1524120304.0,8d9wfm,False,"I took a statistics class when I was in high school (2008), but I shrugged it off and didn't learn anything due to lack of effort.

This week, I took a HIGHLY condensed biostatistics class and was extremely interested!

I'm sorry if this is answered in an FAQ somewhere, but I was hoping to get some ideas for a starting point to learn about statistics? Thanks in advance!",0,"I took a statistics class when I was in high school (2008), but I shrugged it off and didn't learn anything due to lack of effort.

This week, I took a HIGHLY condensed biostatistics class and was extremely interested!

I'm sorry if this is answered in an FAQ somewhere, but I was hoping to get some ideas for a starting point to learn about statistics? Thanks in advance!",1,statistics,54935,,Where to begin?,https://www.reddit.com/r/statistics/comments/8d9wfm/where_to_begin/,all_ads,2018-04-19 02:45:04,44 days 22:50:30.036584000,
"I am doing a study where patients with a congenital anomaly either underwent a surgical procedure or did not and the amount of change in Parameter A is compared between the two groups. Using simple t-tests we can say that Parameter A improved more in the group that did undergo the surgery vs. the group that did not. 

Now my supervisor would like me to answer the question ""Is the surgical intervention more beneficial for patients with Mild, Moderate or Severe deformity?"" So again I am looking at the change in Parameter A for patients who did vs. did not undergo the surgical intervention, and trying to see if there is any difference in that change for patients with mild, moderate or severe deformity. 

Is there a statistical test that fits this situation?

I am trying to be vague to not get too bogged down in the details, but I can explain more if anything I wrote is confusing. Thanks for your help! ",0,1524109207.0,8d8i50,False,"I am doing a study where patients with a congenital anomaly either underwent a surgical procedure or did not and the amount of change in Parameter A is compared between the two groups. Using simple t-tests we can say that Parameter A improved more in the group that did undergo the surgery vs. the group that did not. 

Now my supervisor would like me to answer the question ""Is the surgical intervention more beneficial for patients with Mild, Moderate or Severe deformity?"" So again I am looking at the change in Parameter A for patients who did vs. did not undergo the surgical intervention, and trying to see if there is any difference in that change for patients with mild, moderate or severe deformity. 

Is there a statistical test that fits this situation?

I am trying to be vague to not get too bogged down in the details, but I can explain more if anything I wrote is confusing. Thanks for your help! ",0,"I am doing a study where patients with a congenital anomaly either underwent a surgical procedure or did not and the amount of change in Parameter A is compared between the two groups. Using simple t-tests we can say that Parameter A improved more in the group that did undergo the surgery vs. the group that did not. 

Now my supervisor would like me to answer the question ""Is the surgical intervention more beneficial for patients with Mild, Moderate or Severe deformity?"" So again I am looking at the change in Parameter A for patients who did vs. did not undergo the surgical intervention, and trying to see if there is any difference in that change for patients with mild, moderate or severe deformity. 

Is there a statistical test that fits this situation?

I am trying to be vague to not get too bogged down in the details, but I can explain more if anything I wrote is confusing. Thanks for your help! ",1,statistics,54935,,Test for intervention effect across different groups?,https://www.reddit.com/r/statistics/comments/8d8i50/test_for_intervention_effect_across_different/,all_ads,2018-04-18 23:40:07,45 days 01:55:27.036584000,
"Hi, I’m wondering what would be the best way to report the effect size of a Mann-Whitney test? Currently using eta squared, but I’ve read online some people only reporting the group’s mean ranks. What’s the best way to report an effect found with a Mann Whitney? ",3,1524107674.0,8d8b23,False,"Hi, I’m wondering what would be the best way to report the effect size of a Mann-Whitney test? Currently using eta squared, but I’ve read online some people only reporting the group’s mean ranks. What’s the best way to report an effect found with a Mann Whitney? ",0,"Hi, I’m wondering what would be the best way to report the effect size of a Mann-Whitney test? Currently using eta squared, but I’ve read online some people only reporting the group’s mean ranks. What’s the best way to report an effect found with a Mann Whitney? ",1,statistics,54935,,Reporting effect size of a Mann Whitney,https://www.reddit.com/r/statistics/comments/8d8b23/reporting_effect_size_of_a_mann_whitney/,all_ads,2018-04-18 23:14:34,45 days 02:21:00.036584000,
"Q: https://imgur.com/a/0e5Zz

Hi, I have to interpret the data from regression to prove a negative relationship between 2 variables. I am wondering which result gives a more reliable analysis? I read somewhere that if you have a large sample, which I do (40), the analysis of the R square value is more useful. I am not sure about the statistical explanation for this however. What can I say about multiple R value also? I know this tells you how strong the relationship is but isn't this similar to R^2? Final question is that my R square value shows a very weak relationship, but I can't conclude yet ? Do I also have to analyse other data to see if there is any contradiction? 

Thanks",5,1524110122.0,8d8ma5,False,"Q: https://imgur.com/a/0e5Zz

Hi, I have to interpret the data from regression to prove a negative relationship between 2 variables. I am wondering which result gives a more reliable analysis? I read somewhere that if you have a large sample, which I do (40), the analysis of the R square value is more useful. I am not sure about the statistical explanation for this however. What can I say about multiple R value also? I know this tells you how strong the relationship is but isn't this similar to R^2? Final question is that my R square value shows a very weak relationship, but I can't conclude yet ? Do I also have to analyse other data to see if there is any contradiction? 

Thanks",0,"Q: https://imgur.com/a/0e5Zz

Hi, I have to interpret the data from regression to prove a negative relationship between 2 variables. I am wondering which result gives a more reliable analysis? I read somewhere that if you have a large sample, which I do (40), the analysis of the R square value is more useful. I am not sure about the statistical explanation for this however. What can I say about multiple R value also? I know this tells you how strong the relationship is but isn't this similar to R^2? Final question is that my R square value shows a very weak relationship, but I can't conclude yet ? Do I also have to analyse other data to see if there is any contradiction? 

Thanks",0,statistics,54935,,Regression statistics question,https://www.reddit.com/r/statistics/comments/8d8ma5/regression_statistics_question/,all_ads,2018-04-18 23:55:22,45 days 01:40:12.036584000,
"Hypothetical situation using real data I need to test. John, Jim and Sue are in a room. For the first trial, John is given a hat with a nearly infinite number of slips in it numbered 1-5 in set proportions, with 1 being the highest and 5 being the lowest. However, John does not know the proportion of each number. After pulling 100 slips from the hat, John has 39 1s, 31 2s, 21 3s, 7 4s, and 2 5s. John then leaves the room and Jim takes the hat. Jim promised not adjust the proportion of numbers in the hat while John has left, but when John comes back and pulls 100 more numbers from the hat, getting 32 1s, 26 2s, 23 3s, 13 4s, and 6 5s. Based on this, is there evidence to suggest Jim lied and adjusted the hat? After this test, Jim gives the hat to Sue with the same circumstances as John had on his first trial. Sue pulls 100 numbers at the proportion of 38, 33, 19, 6, and 4. She leaves returns, and pulls 30, 28, 23, 11 and 8. Do we have evidence Jim lied to Sue? Based on both tests, can we suggest that Jim is lying to both people and changing the proportions while they are gone? How do you test this, and what is the conclusion?",3,1524093514.0,8d6esh,False,"Hypothetical situation using real data I need to test. John, Jim and Sue are in a room. For the first trial, John is given a hat with a nearly infinite number of slips in it numbered 1-5 in set proportions, with 1 being the highest and 5 being the lowest. However, John does not know the proportion of each number. After pulling 100 slips from the hat, John has 39 1s, 31 2s, 21 3s, 7 4s, and 2 5s. John then leaves the room and Jim takes the hat. Jim promised not adjust the proportion of numbers in the hat while John has left, but when John comes back and pulls 100 more numbers from the hat, getting 32 1s, 26 2s, 23 3s, 13 4s, and 6 5s. Based on this, is there evidence to suggest Jim lied and adjusted the hat? After this test, Jim gives the hat to Sue with the same circumstances as John had on his first trial. Sue pulls 100 numbers at the proportion of 38, 33, 19, 6, and 4. She leaves returns, and pulls 30, 28, 23, 11 and 8. Do we have evidence Jim lied to Sue? Based on both tests, can we suggest that Jim is lying to both people and changing the proportions while they are gone? How do you test this, and what is the conclusion?",0,"Hypothetical situation using real data I need to test. John, Jim and Sue are in a room. For the first trial, John is given a hat with a nearly infinite number of slips in it numbered 1-5 in set proportions, with 1 being the highest and 5 being the lowest. However, John does not know the proportion of each number. After pulling 100 slips from the hat, John has 39 1s, 31 2s, 21 3s, 7 4s, and 2 5s. John then leaves the room and Jim takes the hat. Jim promised not adjust the proportion of numbers in the hat while John has left, but when John comes back and pulls 100 more numbers from the hat, getting 32 1s, 26 2s, 23 3s, 13 4s, and 6 5s. Based on this, is there evidence to suggest Jim lied and adjusted the hat? After this test, Jim gives the hat to Sue with the same circumstances as John had on his first trial. Sue pulls 100 numbers at the proportion of 38, 33, 19, 6, and 4. She leaves returns, and pulls 30, 28, 23, 11 and 8. Do we have evidence Jim lied to Sue? Based on both tests, can we suggest that Jim is lying to both people and changing the proportions while they are gone? How do you test this, and what is the conclusion?",1,statistics,54935,,What test to use?,https://www.reddit.com/r/statistics/comments/8d6esh/what_test_to_use/,all_ads,2018-04-18 19:18:34,45 days 06:17:00.036584000,
"Okay, so I’m pregnant with fraternal twins and I understand that the chances of having a boy and girl are 50%, 2 boys-25%, and 2 girls-25%.

So I just did a blood test to see if a Y chromosome is present, meaning at least one baby will be a boy. The Y chromosome is present, so I’m not sure if I’ll have one boy and one girl or 2 boys. I think I figured out that it’s a 66% chance of boy/girl and a 33% chance of boy/boy. Is that accurate or am I missing something?

I came up with it by these 3 scenarios:
1. Baby A-boy/ Baby B-boy
2. Baby A-boy/ Baby B-girl
3. Baby A-girl/ Baby B-boy

Is there anything else I need to factor in?",10,1524043103.0,8d1ts6,False,"Okay, so I’m pregnant with fraternal twins and I understand that the chances of having a boy and girl are 50%, 2 boys-25%, and 2 girls-25%.

So I just did a blood test to see if a Y chromosome is present, meaning at least one baby will be a boy. The Y chromosome is present, so I’m not sure if I’ll have one boy and one girl or 2 boys. I think I figured out that it’s a 66% chance of boy/girl and a 33% chance of boy/boy. Is that accurate or am I missing something?

I came up with it by these 3 scenarios:
1. Baby A-boy/ Baby B-boy
2. Baby A-boy/ Baby B-girl
3. Baby A-girl/ Baby B-boy

Is there anything else I need to factor in?",0,"Okay, so I’m pregnant with fraternal twins and I understand that the chances of having a boy and girl are 50%, 2 boys-25%, and 2 girls-25%.

So I just did a blood test to see if a Y chromosome is present, meaning at least one baby will be a boy. The Y chromosome is present, so I’m not sure if I’ll have one boy and one girl or 2 boys. I think I figured out that it’s a 66% chance of boy/girl and a 33% chance of boy/boy. Is that accurate or am I missing something?

I came up with it by these 3 scenarios:
1. Baby A-boy/ Baby B-boy
2. Baby A-boy/ Baby B-girl
3. Baby A-girl/ Baby B-boy

Is there anything else I need to factor in?",12,statistics,54935,,Stats on sex of babies,https://www.reddit.com/r/statistics/comments/8d1ts6/stats_on_sex_of_babies/,all_ads,2018-04-18 05:18:23,45 days 20:17:11.036584000,
"I am currently a Statistics major with a Philosophy minor. I'll also have sufficient coursework in Computer Science, Spanish, Mathematics, and Applied Mathematics. My job with these fields of study is to be enlightened and obtain skills that may positively affect X community. My goal as of now is to become a professor at a teaching university and be a Statistician at a reputable company/organization that helps people (school district, EPA, etc.). The latter only is much more likely, and I have second thoughts of becoming a professor occasionally.

Anyway, I'm looking for an answer like [this](https://www.reddit.com/r/cscareerquestions/comments/7gf9k9/what_are_some_of_the_best_minors_or_double_majors/dqinyxy/) for CS. Thank you!",19,1524022595.0,8czczs,False,"I am currently a Statistics major with a Philosophy minor. I'll also have sufficient coursework in Computer Science, Spanish, Mathematics, and Applied Mathematics. My job with these fields of study is to be enlightened and obtain skills that may positively affect X community. My goal as of now is to become a professor at a teaching university and be a Statistician at a reputable company/organization that helps people (school district, EPA, etc.). The latter only is much more likely, and I have second thoughts of becoming a professor occasionally.

Anyway, I'm looking for an answer like [this](https://www.reddit.com/r/cscareerquestions/comments/7gf9k9/what_are_some_of_the_best_minors_or_double_majors/dqinyxy/) for CS. Thank you!",0,"I am currently a Statistics major with a Philosophy minor. I'll also have sufficient coursework in Computer Science, Spanish, Mathematics, and Applied Mathematics. My job with these fields of study is to be enlightened and obtain skills that may positively affect X community. My goal as of now is to become a professor at a teaching university and be a Statistician at a reputable company/organization that helps people (school district, EPA, etc.). The latter only is much more likely, and I have second thoughts of becoming a professor occasionally.

Anyway, I'm looking for an answer like [this](https://www.reddit.com/r/cscareerquestions/comments/7gf9k9/what_are_some_of_the_best_minors_or_double_majors/dqinyxy/) for CS. Thank you!",19,statistics,54935,,"Disregarding personal interests, what are the best minors to complement Statistics?",https://www.reddit.com/r/statistics/comments/8czczs/disregarding_personal_interests_what_are_the_best/,all_ads,2018-04-17 23:36:35,46 days 01:58:59.036584000,
"In regression, when you construct a hypothesis H0 or H1, why is B1 used? I know B1 is the population coefficient slope but why is Yi not used in the hypothesis?  For example H0: B1 = 0  shows that there is a linear relationship between the independent and the dependent variables. Why is it not Y1=0?

Thanks",4,1524079259.0,8d4us3,False,"In regression, when you construct a hypothesis H0 or H1, why is B1 used? I know B1 is the population coefficient slope but why is Yi not used in the hypothesis?  For example H0: B1 = 0  shows that there is a linear relationship between the independent and the dependent variables. Why is it not Y1=0?

Thanks",0,"In regression, when you construct a hypothesis H0 or H1, why is B1 used? I know B1 is the population coefficient slope but why is Yi not used in the hypothesis?  For example H0: B1 = 0  shows that there is a linear relationship between the independent and the dependent variables. Why is it not Y1=0?

Thanks",0,statistics,54935,,Hypothesis question,https://www.reddit.com/r/statistics/comments/8d4us3/hypothesis_question/,all_ads,2018-04-18 15:20:59,45 days 10:14:35.036584000,
"Hi everyone, I’m sorry if this isn’t the right place to ask this, but I am in need of help! 

I’m in the middle of a lab report for one of my classes, and I’ve confused myself with some of the statistics. 

How is it possible that I have a weak linear relationship for pearson’s correlation (.285) but it is still statistically significant (.032) 

The sample size was 57 if that helps at all. 

Any help would be greatly appreciated, thank you!! ",7,1524056682.0,8d3577,False,"Hi everyone, I’m sorry if this isn’t the right place to ask this, but I am in need of help! 

I’m in the middle of a lab report for one of my classes, and I’ve confused myself with some of the statistics. 

How is it possible that I have a weak linear relationship for pearson’s correlation (.285) but it is still statistically significant (.032) 

The sample size was 57 if that helps at all. 

Any help would be greatly appreciated, thank you!! ",0,"Hi everyone, I’m sorry if this isn’t the right place to ask this, but I am in need of help! 

I’m in the middle of a lab report for one of my classes, and I’ve confused myself with some of the statistics. 

How is it possible that I have a weak linear relationship for pearson’s correlation (.285) but it is still statistically significant (.032) 

The sample size was 57 if that helps at all. 

Any help would be greatly appreciated, thank you!! ",1,statistics,54935,,What does a weak linear relationship that is statistically significant mean?,https://www.reddit.com/r/statistics/comments/8d3577/what_does_a_weak_linear_relationship_that_is/,all_ads,2018-04-18 09:04:42,45 days 16:30:52.036584000,
"I am interested in examining the impact crime rate by county (panel data) has on the level of funding a particular county program gets from the state each year.  
i.e does a steady decrease in crime rate result in a steady decreases allocated funds in a relevant program.  
I have the data, but am hoping to discuss some routes I could take to execute this.
It’s been a while since my last stats class but my knee-jerk feeling to do a multivariate regression where the independent variables would be :

county population
existing size of the program
crime rate

and the dependent variable being:
amount of funds allocated to said program.

Does this sound like a correct way to begin going about tackling this question?  What are some obvious or not so obvious mistakes I might be making?  What might be viable alternatives to answering this question?

Thanks, looking forward to any feedback.
",7,1524018720.0,8cyufb,False,"I am interested in examining the impact crime rate by county (panel data) has on the level of funding a particular county program gets from the state each year.  
i.e does a steady decrease in crime rate result in a steady decreases allocated funds in a relevant program.  
I have the data, but am hoping to discuss some routes I could take to execute this.
It’s been a while since my last stats class but my knee-jerk feeling to do a multivariate regression where the independent variables would be :

county population
existing size of the program
crime rate

and the dependent variable being:
amount of funds allocated to said program.

Does this sound like a correct way to begin going about tackling this question?  What are some obvious or not so obvious mistakes I might be making?  What might be viable alternatives to answering this question?

Thanks, looking forward to any feedback.
",0,"I am interested in examining the impact crime rate by county (panel data) has on the level of funding a particular county program gets from the state each year.  
i.e does a steady decrease in crime rate result in a steady decreases allocated funds in a relevant program.  
I have the data, but am hoping to discuss some routes I could take to execute this.
It’s been a while since my last stats class but my knee-jerk feeling to do a multivariate regression where the independent variables would be :

county population
existing size of the program
crime rate

and the dependent variable being:
amount of funds allocated to said program.

Does this sound like a correct way to begin going about tackling this question?  What are some obvious or not so obvious mistakes I might be making?  What might be viable alternatives to answering this question?

Thanks, looking forward to any feedback.
",6,statistics,54935,,What are some potential appropriate statistical tools for...,https://www.reddit.com/r/statistics/comments/8cyufb/what_are_some_potential_appropriate_statistical/,all_ads,2018-04-17 22:32:00,46 days 03:03:34.036584000,
"Hi guys!  
I've been invited to a pre-congress course (at an endocrinology meeting) and the organising committee has asked me to prepare a 45-minutes talk on ""errors in statistics for medical research"".  

Here's a list of what I thought so far:

* performing dozens of t-test 
* reducing numerical variables in categorical variables (unnecessary age bends)
* performing regression in non normally distributed data
* paying attention to p-values instead of assessing clinical meaning of any finding
* correlation does not imply causation

Any other ideas on this topic?

Thanks a lot!
",38,1523996843.0,8cw4i4,False,"Hi guys!  
I've been invited to a pre-congress course (at an endocrinology meeting) and the organising committee has asked me to prepare a 45-minutes talk on ""errors in statistics for medical research"".  

Here's a list of what I thought so far:

* performing dozens of t-test 
* reducing numerical variables in categorical variables (unnecessary age bends)
* performing regression in non normally distributed data
* paying attention to p-values instead of assessing clinical meaning of any finding
* correlation does not imply causation

Any other ideas on this topic?

Thanks a lot!
",0,"Hi guys!  
I've been invited to a pre-congress course (at an endocrinology meeting) and the organising committee has asked me to prepare a 45-minutes talk on ""errors in statistics for medical research"".  

Here's a list of what I thought so far:

* performing dozens of t-test 
* reducing numerical variables in categorical variables (unnecessary age bends)
* performing regression in non normally distributed data
* paying attention to p-values instead of assessing clinical meaning of any finding
* correlation does not imply causation

Any other ideas on this topic?

Thanks a lot!
",12,statistics,54935,,"""Errors in statistics for medical research"". Any suggestions?",https://www.reddit.com/r/statistics/comments/8cw4i4/errors_in_statistics_for_medical_research_any/,all_ads,2018-04-17 16:27:23,46 days 09:08:11.036584000,
"When you run a linear regression model for 2 variables, what are the important interpretations you must make from the data generated? What makes a good regression analysis? For example, if my R square value is 0.10, I understand this is not really showing any relationship at all but what exactly do I also need to mention? I read that even if you have a good model, it could still give you a low R square value. I am unsure about this.

Thanks",6,1524046920.0,8d288z,False,"When you run a linear regression model for 2 variables, what are the important interpretations you must make from the data generated? What makes a good regression analysis? For example, if my R square value is 0.10, I understand this is not really showing any relationship at all but what exactly do I also need to mention? I read that even if you have a good model, it could still give you a low R square value. I am unsure about this.

Thanks",0,"When you run a linear regression model for 2 variables, what are the important interpretations you must make from the data generated? What makes a good regression analysis? For example, if my R square value is 0.10, I understand this is not really showing any relationship at all but what exactly do I also need to mention? I read that even if you have a good model, it could still give you a low R square value. I am unsure about this.

Thanks",0,statistics,54935,,Regression question,https://www.reddit.com/r/statistics/comments/8d288z/regression_question/,all_ads,2018-04-18 06:22:00,45 days 19:13:34.036584000,
"Hi,

I am currently investigating whether there is a positive relationship between 2 variables. I have worked out the sample mean but I am not quite sure what to comment about it because the main focus is to run the regression for the data. The 2 mean values for the 2 sets of data is similar. What kind of comment could I make to show the importance of the mean values or is working out the mean values a bit pointless when you are trying to prove there is a relationship between 2 variables?

Thanks ",4,1524046015.0,8d24yp,False,"Hi,

I am currently investigating whether there is a positive relationship between 2 variables. I have worked out the sample mean but I am not quite sure what to comment about it because the main focus is to run the regression for the data. The 2 mean values for the 2 sets of data is similar. What kind of comment could I make to show the importance of the mean values or is working out the mean values a bit pointless when you are trying to prove there is a relationship between 2 variables?

Thanks ",0,"Hi,

I am currently investigating whether there is a positive relationship between 2 variables. I have worked out the sample mean but I am not quite sure what to comment about it because the main focus is to run the regression for the data. The 2 mean values for the 2 sets of data is similar. What kind of comment could I make to show the importance of the mean values or is working out the mean values a bit pointless when you are trying to prove there is a relationship between 2 variables?

Thanks ",0,statistics,54935,,Descriptive statistics question,https://www.reddit.com/r/statistics/comments/8d24yp/descriptive_statistics_question/,all_ads,2018-04-18 06:06:55,45 days 19:28:39.036584000,
"So I am stuck in a simulation. Theoretically, my model specification should be correct. Computationally, the bias seems abit off. So have you come across such situations? Usually when such cases happen, we say ""either your model is wrong or your code is wrong"" but are there cases when neither of it is wrong? ",3,1524045701.0,8d23q0,False,"So I am stuck in a simulation. Theoretically, my model specification should be correct. Computationally, the bias seems abit off. So have you come across such situations? Usually when such cases happen, we say ""either your model is wrong or your code is wrong"" but are there cases when neither of it is wrong? ",0,"So I am stuck in a simulation. Theoretically, my model specification should be correct. Computationally, the bias seems abit off. So have you come across such situations? Usually when such cases happen, we say ""either your model is wrong or your code is wrong"" but are there cases when neither of it is wrong? ",1,statistics,54935,,Research: Were there times when your simulation doesnt match up to the model you have come up with?,https://www.reddit.com/r/statistics/comments/8d23q0/research_were_there_times_when_your_simulation/,all_ads,2018-04-18 06:01:41,45 days 19:33:53.036584000,
"Perhaps the incorrect place to ask, if so apologies...


But does anyone know if it is possible to run a Minitab macro on every open worksheet?


I've got several hundred individual sheets that all need the exact same analysis done on them and will need them done quite frequently so being able to loop across all the sheets would be fantastic.


Thanks in advance!

Joe",0,1524031064.0,8d0fx8,False,"Perhaps the incorrect place to ask, if so apologies...


But does anyone know if it is possible to run a Minitab macro on every open worksheet?


I've got several hundred individual sheets that all need the exact same analysis done on them and will need them done quite frequently so being able to loop across all the sheets would be fantastic.


Thanks in advance!

Joe",0,"Perhaps the incorrect place to ask, if so apologies...


But does anyone know if it is possible to run a Minitab macro on every open worksheet?


I've got several hundred individual sheets that all need the exact same analysis done on them and will need them done quite frequently so being able to loop across all the sheets would be fantastic.


Thanks in advance!

Joe",2,statistics,54935,,Minitab Macro,https://www.reddit.com/r/statistics/comments/8d0fx8/minitab_macro/,all_ads,2018-04-18 01:57:44,45 days 23:37:50.036584000,
"Hi,

My question is by working out these values you know how spread out the data is but what else does it tell you? If the data is more disperse then does it mean the relationship between the variables being investigated is more reliable?

I have also read that standard deviation is a better measurement of the spreading out of data as it is not squared, what is the difference between SD and variance precisely?

Thanks",5,1524043838.0,8d1whz,False,"Hi,

My question is by working out these values you know how spread out the data is but what else does it tell you? If the data is more disperse then does it mean the relationship between the variables being investigated is more reliable?

I have also read that standard deviation is a better measurement of the spreading out of data as it is not squared, what is the difference between SD and variance precisely?

Thanks",0,"Hi,

My question is by working out these values you know how spread out the data is but what else does it tell you? If the data is more disperse then does it mean the relationship between the variables being investigated is more reliable?

I have also read that standard deviation is a better measurement of the spreading out of data as it is not squared, what is the difference between SD and variance precisely?

Thanks",0,statistics,54935,,Standard deviation question,https://www.reddit.com/r/statistics/comments/8d1whz/standard_deviation_question/,all_ads,2018-04-18 05:30:38,45 days 20:04:56.036584000,
"What is the covariance of a mixture distribution of normal distributions assuming that they have equal weightings?
My intuition for the 1D case is that the variance is the mean of the variances of the distributions + variance of the means of the distributions? Is this correct? If so how does that extend to the co-variance case?

Thank you",1,1524034611.0,8d0uyi,False,"What is the covariance of a mixture distribution of normal distributions assuming that they have equal weightings?
My intuition for the 1D case is that the variance is the mean of the variances of the distributions + variance of the means of the distributions? Is this correct? If so how does that extend to the co-variance case?

Thank you",0,"What is the covariance of a mixture distribution of normal distributions assuming that they have equal weightings?
My intuition for the 1D case is that the variance is the mean of the variances of the distributions + variance of the means of the distributions? Is this correct? If so how does that extend to the co-variance case?

Thank you",1,statistics,54935,,Covariance of Mixture Distributions,https://www.reddit.com/r/statistics/comments/8d0uyi/covariance_of_mixture_distributions/,all_ads,2018-04-18 02:56:51,45 days 22:38:43.036584000,
"Hi everyone
I am having trouble finding the addecuate approach to use in order to describe a set of data.My goal is to describe the sales of a product, so that i could use this information as input of a simulation model.I have the historical information of sales per day, these values vary between 0 and 11 being the 0 and the 1 the most inflated values in the sample (0 represents about 49% of the sample  and 1 is about 20%).I already tried to fit the data to all the discrete distributions i know about, but the highest p value I have obtained is 0.001.I would highly appreciate any advice on how to model this behavior.Thanks :)",7,1524028296.0,8d03n8,False,"Hi everyone
I am having trouble finding the addecuate approach to use in order to describe a set of data.My goal is to describe the sales of a product, so that i could use this information as input of a simulation model.I have the historical information of sales per day, these values vary between 0 and 11 being the 0 and the 1 the most inflated values in the sample (0 represents about 49% of the sample  and 1 is about 20%).I already tried to fit the data to all the discrete distributions i know about, but the highest p value I have obtained is 0.001.I would highly appreciate any advice on how to model this behavior.Thanks :)",0,"Hi everyone
I am having trouble finding the addecuate approach to use in order to describe a set of data.My goal is to describe the sales of a product, so that i could use this information as input of a simulation model.I have the historical information of sales per day, these values vary between 0 and 11 being the 0 and the 1 the most inflated values in the sample (0 represents about 49% of the sample  and 1 is about 20%).I already tried to fit the data to all the discrete distributions i know about, but the highest p value I have obtained is 0.001.I would highly appreciate any advice on how to model this behavior.Thanks :)",1,statistics,54935,,Correct approach to describe a set of discrete data,https://www.reddit.com/r/statistics/comments/8d03n8/correct_approach_to_describe_a_set_of_discrete/,all_ads,2018-04-18 01:11:36,46 days 00:23:58.036584000,
"Hello,

My thesis project involves creatig dose-response curves for 2 drugs being used to treat a cell line. I have 4 concentrations for each drug and as well as a vehicle control. We calculate the curve by plating a known number of cells per well, treating separate wells with each concentration of drug and then counting the number of cells after 72h and plotting concentration vs cell number.

When I run these experiment, I often often plate twice as many wells as necessary and treat with concentrations of both drugs separately, using the same wells as control for both (i.e. 3 wells of each of control, drug A dose 1, drug B dose 1, drug A dose 2, drug B dose 2, and so on for a total of 9 groups). No cells are ever treated with both drugs simultaneously and I am not at all interested in comparing the two drugs to each other at the moment. However, given that al of the data was collected together, it makes sense to me for it all to be analyzed together.

I want to know if, in this situation, the data for the two drugs should be analyzed together as 9 treatment groups or separately as 5 treatment groups (Control and all 4 doses from a single drug) each.

Thanks!",0,1523994292.0,8cvvv0,False,"Hello,

My thesis project involves creatig dose-response curves for 2 drugs being used to treat a cell line. I have 4 concentrations for each drug and as well as a vehicle control. We calculate the curve by plating a known number of cells per well, treating separate wells with each concentration of drug and then counting the number of cells after 72h and plotting concentration vs cell number.

When I run these experiment, I often often plate twice as many wells as necessary and treat with concentrations of both drugs separately, using the same wells as control for both (i.e. 3 wells of each of control, drug A dose 1, drug B dose 1, drug A dose 2, drug B dose 2, and so on for a total of 9 groups). No cells are ever treated with both drugs simultaneously and I am not at all interested in comparing the two drugs to each other at the moment. However, given that al of the data was collected together, it makes sense to me for it all to be analyzed together.

I want to know if, in this situation, the data for the two drugs should be analyzed together as 9 treatment groups or separately as 5 treatment groups (Control and all 4 doses from a single drug) each.

Thanks!",0,"Hello,

My thesis project involves creatig dose-response curves for 2 drugs being used to treat a cell line. I have 4 concentrations for each drug and as well as a vehicle control. We calculate the curve by plating a known number of cells per well, treating separate wells with each concentration of drug and then counting the number of cells after 72h and plotting concentration vs cell number.

When I run these experiment, I often often plate twice as many wells as necessary and treat with concentrations of both drugs separately, using the same wells as control for both (i.e. 3 wells of each of control, drug A dose 1, drug B dose 1, drug A dose 2, drug B dose 2, and so on for a total of 9 groups). No cells are ever treated with both drugs simultaneously and I am not at all interested in comparing the two drugs to each other at the moment. However, given that al of the data was collected together, it makes sense to me for it all to be analyzed together.

I want to know if, in this situation, the data for the two drugs should be analyzed together as 9 treatment groups or separately as 5 treatment groups (Control and all 4 doses from a single drug) each.

Thanks!",3,statistics,54935,,What data should be analyzed together in my situation,https://www.reddit.com/r/statistics/comments/8cvvv0/what_data_should_be_analyzed_together_in_my/,all_ads,2018-04-17 15:44:52,46 days 09:50:42.036584000,
"Hi r/statistics

In 2008/9 the Office of the High Commissioner on Human Rights reported on the torture and cruel, inhuman and degrading treatment of the mentally ill. Kid gloves, please.

I would like to receive comment and criticism on my grasp of the statistics in this part of an appendix I am writing. Here goes.  

The Three Questions Psychiatry must be held to Account for. 

(This version of the Three Questions uses Schizophrenia as an example, but they apply to most mental illness too)

Question 1. Is Schizophrenia a Medical Illness or a Medical Theory?

When a person has a cold (the common cold), they are suffering from an illness. We know the common cold is an illness because the cold has been discovered to be an illness. Before the cold was discovered, medical thoughts on the subject amounted to theory, hypothesis, conjecture, speculation and consensus. However, when the cold was discovered, false theories and misguided consensus fell by the way-side and the truth was laid bare.  

The first question flows from the fact that a real illness differs from medical theory because a real illness is given a name when it is discovered. When the cold was discovered, Doctors named the common cold a Rhinovirus.

Question 1. Doctor, please name the underlying illness for Schizophrenia. (or any other mental illness for that matter.)

A Profound Consequence flowing from Question 1.

If your psychiatrist cannot name the underlying illness, it is probably because the illness is theoretical. The question becomes, if psychiatry has not discovered your illness in a human being, how are they going to ""discover"" the illness in you? The question leads to questions 2 and 3.

Questions 2 and 3 Can psychiatry diagnose an undiscovered illness, or is every diagnosis theoretical?

Psychiatry is a large and rich industry. Further, their income is a large source of tax revenue. Both psychiatrists and The Man have embroiled themselves in a convoluted dialogue designed to defend against questions like these three. Luckily, there are things one can do to reveal the truth to a judge, jury and the public at large, should you be interested in the ""cure"" that a good lawyer can provide you. 

Let's split the word diagnosis into two parts, namely: direct and indirect diagnosis. Further, let's define direct and indirect diagnosis in a manner which is going to be useful in a court of law. Let's start with direct diagnosis. Let's define direct diagnosis to mean, any legitimate, peer-reviewed, medical procedure which, if properly performed, would result in the discovery or re-discovery of schizophrenia.

Question 2. Doctor, can you directly diagnose someone with schizophrenia?

A Profound Consequence of Question 2.

Psychiatry hasn't discovered schizophrenia. Therefore, it stands to reason that psychiatry cannot point out an individual, or group of people, they discovered to be schizophrenic. I say again, psychiatry cannot isolate a person, or a group of people, they discovered to be schizophrenic because they lack the medical ability to discover schizophrenia in a human being. Apologies for being a little redundant, but the fact has a profound bearing on the crux of the matter, namely; question 3.

Question 3 Can psychiatry diagnose schizophrenia in an indirect fashion, like they claim to be doing?

Question 1 reveals that psychiatry hasn't discovered schizophrenia yet. Schizophrenia remains a puddle of medical belief, theory, conjecture, speculation, and consensus. Schizophrenia will remain theoretical until schizophrenia is discovered to factually exist.
Question 2 reveals that psychiatry cannot point out a person, or a group of people, they discovered to be schizophrenic because psychiatrists lack the ability to discover schizophrenia in a human being. 

Question 3 deals with indirect diagnosis. Question 3 is the crux of the fraud and tort that is mental illness. Question 3 is the crux because psychiatry is built, almost entirely, on statistics and statistical method. Believe it or not, psychiatry is structured as follows; Psychiatrists host psychiatric studies for their many theoretical illnesses. They publish their findings from their studies. Those findings make their way to the American Psychiatric Association, amongst other places. Every now and again, a select few psychiatrists will vote behind closed doors at the American Psychiatric Association. They vote on which psychiatric theories about undiscovered illnesses have come to publish sufficient findings to be approved by the American Psychiatric Association. Psychiatric theories which are approved are publicly rolled out, exported overseas and publicly portrayed as real illnesses from that day forward.  

The trouble is, the underlying psychiatric studies and their findings are fraudulent, intentionally and culpably so.

Paranoid Schizophrenia is a form of schizophrenia. As such, it is an undiscovered illness. Psychiatrists describe paranoid schizophrenia as; typically we believe the F.B.I. are out to get us. Paranoid Schizophrenia is a perfect example for this document because this document seeks the assistance of the F.B.I. in this matter. 

In pursuit of an answer to Question 3, let's host a paper-version of a psychiatric study of the illness of paranoid schizophrenia. Suppose we fill a paper-room with 5 test subjects. Each test subject is willing to complain about the F.B.I. or similar. Let us paper-study our 5 test subjects in order to add to the pool of statistics that Psychiatry claims to possess and the American Psychiatric Association claims to base their votes on. 

Statistics is a mathematical discipline. Statistics concerns the Mathematical Analysis of Data, no more and no less. And so, before our paper-version of a psychiatric study can add to the statistical pool for paranoid schizophrenia, we must collect data on our 5 test-subjects. Let us gather data so that we have something to apply our statistical tools to.

Blood pressure will serve as the data that our paper-study collects. You see, most laypeople, including judges, jurors and the public accept that blood pressure is a matter of medical fact. Blood pressure, ordinary members of the public feel, is not a medical theory Doctors use to confuse people.

Let us work through our 5 test subjects, reading their blood pressure as we go. 

John		aa
Zhi Ruo		bb
Sipho		cc
Mohammed	dd
Jimena		ee

Properly done, those 5 blood pressure readings count as real data. Armed with real data, we can begin using statistical tools. For example, the Average Blood Pressure for our 5 test subjects can be determined by adding all the blood pressure readings up and dividing by the number of readings in the data pool. (aa+bb+cc+dd+ee)/5 = ff

Done properly, the Average Blood Pressure (ff) is a good example of statistics. It is an honest example of statistical method. Please listen! This next part is an important bit. The Three Questions agree that many, and possibly most, psychiatric data and statistics analysis is honest too. Many, and possibly most, of the psychiatric data collected, was collected in a fashion which is similar to our paper example. 

The problem will fraudulent psychiatric studies has nothing to do with the collection and statistical analysis of data. The fraud has to do with the causal connections psychiatrists make.

Questions 1 and 2 hold the key to the fraud and the tort. You see, psychiatrists cannot discover which test subjects are schizophrenic because psychiatrists lack the ability to discover schizophrenia in a human being. No matter how diligent psychiatrists are in the collection of data and their subsequent statistical analysis, it remains an act of medical malpractice for psychiatrists to causally connect their findings to schizophrenia because they do not know which test subjects are schizophrenic. 

Question 3 Doctor, is it criminally inappropriate for psychiatrists to causally connect statistics to an undiscovered illness because it is impossible for psychiatrists to determine whether their test-subjects have the requisite illness?

A Profound Consequence of Question 3

There are probably many levels of causality. However, Question 3 deals with the most basic level of causality. And by most basic I mean, you cannot attribute wins to a racehorse if the horse has never raced. You cannot attribute K.O.'s to a boxer if the boxer has never entered the ring. You cannot record how many coin tosses were heads if nobody has tossed the coin. While you can speculate and farm agreement among your peers, arguing how well your horse, boxer or coin might perform when it arrives on the scene at a future point, statistics cannot analyse theory because there are so many possible theories, statisticians saw the wisdom in drawing a line and saying no. The only statistics Doctors can claim to be legitimate are based on discovered illnesses because test subjects must be shown to have the requisite illness before data collection can begin.

Every undiscovered mental illness the American Psychiatric Association voted into existence is an intentional, culpable misrepresentation which is prejudicial, or potentially prejudicial. In short, the American Psychiatric Association is engaged in fraud. This document is an open request to the F.B.I. for their assistance in this regard.


Bonus Question (A nail in the coffin, if you would)

Schizophrenia Twin Studies are arguably the most famous set of psychiatric studies. Further, Schizophrenia Twin Studies are arguably the set of studies that psychiatrists refer to when seeking to appease the non-believers. Through use, Schizophrenia Twin Studies have become a cornerstone for the justification of psychiatry. 

Schizophrenia Twin Studies are a pool of many studies, each of which has added to the whole. Let's hold a paper-version of a Schizophrenia Twin Study over a few paragraphs. Let's fill a room with four sets of twins, eight individuals total. Two sets of twins will be identical twins and two will be fraternal twins. Our study is ready to begin.

A Schizophrenia Twin Study begins by diagnosing each individual in the study group. The study begins by determining whether one or both individuals in a set of twins is schizophrenic. The study does this for the identical and fraternal twins.

Please try to comprehend the stupidity of such a thing. Schizophrenia Twin Studies are used to justify psychiatries failure to discover schizophrenia in anyone. Yet, the worth of Schizophrenia Twin Studies hinges on being able to discover who is schizophrenic.

The truth about Schizophrenia Twin Studies is; Schizophrenia Twin Studies are used to complicate the issue, to obfuscate the truth, to defer justice. Schizophrenia Twin Studies are fraudulent, they are delictual. Schizophrenia Twin Studies are a good example of the dangers of consensus and nod-farming.",21,1524014460.0,8cya0o,False,"Hi r/statistics

In 2008/9 the Office of the High Commissioner on Human Rights reported on the torture and cruel, inhuman and degrading treatment of the mentally ill. Kid gloves, please.

I would like to receive comment and criticism on my grasp of the statistics in this part of an appendix I am writing. Here goes.  

The Three Questions Psychiatry must be held to Account for. 

(This version of the Three Questions uses Schizophrenia as an example, but they apply to most mental illness too)

Question 1. Is Schizophrenia a Medical Illness or a Medical Theory?

When a person has a cold (the common cold), they are suffering from an illness. We know the common cold is an illness because the cold has been discovered to be an illness. Before the cold was discovered, medical thoughts on the subject amounted to theory, hypothesis, conjecture, speculation and consensus. However, when the cold was discovered, false theories and misguided consensus fell by the way-side and the truth was laid bare.  

The first question flows from the fact that a real illness differs from medical theory because a real illness is given a name when it is discovered. When the cold was discovered, Doctors named the common cold a Rhinovirus.

Question 1. Doctor, please name the underlying illness for Schizophrenia. (or any other mental illness for that matter.)

A Profound Consequence flowing from Question 1.

If your psychiatrist cannot name the underlying illness, it is probably because the illness is theoretical. The question becomes, if psychiatry has not discovered your illness in a human being, how are they going to ""discover"" the illness in you? The question leads to questions 2 and 3.

Questions 2 and 3 Can psychiatry diagnose an undiscovered illness, or is every diagnosis theoretical?

Psychiatry is a large and rich industry. Further, their income is a large source of tax revenue. Both psychiatrists and The Man have embroiled themselves in a convoluted dialogue designed to defend against questions like these three. Luckily, there are things one can do to reveal the truth to a judge, jury and the public at large, should you be interested in the ""cure"" that a good lawyer can provide you. 

Let's split the word diagnosis into two parts, namely: direct and indirect diagnosis. Further, let's define direct and indirect diagnosis in a manner which is going to be useful in a court of law. Let's start with direct diagnosis. Let's define direct diagnosis to mean, any legitimate, peer-reviewed, medical procedure which, if properly performed, would result in the discovery or re-discovery of schizophrenia.

Question 2. Doctor, can you directly diagnose someone with schizophrenia?

A Profound Consequence of Question 2.

Psychiatry hasn't discovered schizophrenia. Therefore, it stands to reason that psychiatry cannot point out an individual, or group of people, they discovered to be schizophrenic. I say again, psychiatry cannot isolate a person, or a group of people, they discovered to be schizophrenic because they lack the medical ability to discover schizophrenia in a human being. Apologies for being a little redundant, but the fact has a profound bearing on the crux of the matter, namely; question 3.

Question 3 Can psychiatry diagnose schizophrenia in an indirect fashion, like they claim to be doing?

Question 1 reveals that psychiatry hasn't discovered schizophrenia yet. Schizophrenia remains a puddle of medical belief, theory, conjecture, speculation, and consensus. Schizophrenia will remain theoretical until schizophrenia is discovered to factually exist.
Question 2 reveals that psychiatry cannot point out a person, or a group of people, they discovered to be schizophrenic because psychiatrists lack the ability to discover schizophrenia in a human being. 

Question 3 deals with indirect diagnosis. Question 3 is the crux of the fraud and tort that is mental illness. Question 3 is the crux because psychiatry is built, almost entirely, on statistics and statistical method. Believe it or not, psychiatry is structured as follows; Psychiatrists host psychiatric studies for their many theoretical illnesses. They publish their findings from their studies. Those findings make their way to the American Psychiatric Association, amongst other places. Every now and again, a select few psychiatrists will vote behind closed doors at the American Psychiatric Association. They vote on which psychiatric theories about undiscovered illnesses have come to publish sufficient findings to be approved by the American Psychiatric Association. Psychiatric theories which are approved are publicly rolled out, exported overseas and publicly portrayed as real illnesses from that day forward.  

The trouble is, the underlying psychiatric studies and their findings are fraudulent, intentionally and culpably so.

Paranoid Schizophrenia is a form of schizophrenia. As such, it is an undiscovered illness. Psychiatrists describe paranoid schizophrenia as; typically we believe the F.B.I. are out to get us. Paranoid Schizophrenia is a perfect example for this document because this document seeks the assistance of the F.B.I. in this matter. 

In pursuit of an answer to Question 3, let's host a paper-version of a psychiatric study of the illness of paranoid schizophrenia. Suppose we fill a paper-room with 5 test subjects. Each test subject is willing to complain about the F.B.I. or similar. Let us paper-study our 5 test subjects in order to add to the pool of statistics that Psychiatry claims to possess and the American Psychiatric Association claims to base their votes on. 

Statistics is a mathematical discipline. Statistics concerns the Mathematical Analysis of Data, no more and no less. And so, before our paper-version of a psychiatric study can add to the statistical pool for paranoid schizophrenia, we must collect data on our 5 test-subjects. Let us gather data so that we have something to apply our statistical tools to.

Blood pressure will serve as the data that our paper-study collects. You see, most laypeople, including judges, jurors and the public accept that blood pressure is a matter of medical fact. Blood pressure, ordinary members of the public feel, is not a medical theory Doctors use to confuse people.

Let us work through our 5 test subjects, reading their blood pressure as we go. 

John		aa
Zhi Ruo		bb
Sipho		cc
Mohammed	dd
Jimena		ee

Properly done, those 5 blood pressure readings count as real data. Armed with real data, we can begin using statistical tools. For example, the Average Blood Pressure for our 5 test subjects can be determined by adding all the blood pressure readings up and dividing by the number of readings in the data pool. (aa+bb+cc+dd+ee)/5 = ff

Done properly, the Average Blood Pressure (ff) is a good example of statistics. It is an honest example of statistical method. Please listen! This next part is an important bit. The Three Questions agree that many, and possibly most, psychiatric data and statistics analysis is honest too. Many, and possibly most, of the psychiatric data collected, was collected in a fashion which is similar to our paper example. 

The problem will fraudulent psychiatric studies has nothing to do with the collection and statistical analysis of data. The fraud has to do with the causal connections psychiatrists make.

Questions 1 and 2 hold the key to the fraud and the tort. You see, psychiatrists cannot discover which test subjects are schizophrenic because psychiatrists lack the ability to discover schizophrenia in a human being. No matter how diligent psychiatrists are in the collection of data and their subsequent statistical analysis, it remains an act of medical malpractice for psychiatrists to causally connect their findings to schizophrenia because they do not know which test subjects are schizophrenic. 

Question 3 Doctor, is it criminally inappropriate for psychiatrists to causally connect statistics to an undiscovered illness because it is impossible for psychiatrists to determine whether their test-subjects have the requisite illness?

A Profound Consequence of Question 3

There are probably many levels of causality. However, Question 3 deals with the most basic level of causality. And by most basic I mean, you cannot attribute wins to a racehorse if the horse has never raced. You cannot attribute K.O.'s to a boxer if the boxer has never entered the ring. You cannot record how many coin tosses were heads if nobody has tossed the coin. While you can speculate and farm agreement among your peers, arguing how well your horse, boxer or coin might perform when it arrives on the scene at a future point, statistics cannot analyse theory because there are so many possible theories, statisticians saw the wisdom in drawing a line and saying no. The only statistics Doctors can claim to be legitimate are based on discovered illnesses because test subjects must be shown to have the requisite illness before data collection can begin.

Every undiscovered mental illness the American Psychiatric Association voted into existence is an intentional, culpable misrepresentation which is prejudicial, or potentially prejudicial. In short, the American Psychiatric Association is engaged in fraud. This document is an open request to the F.B.I. for their assistance in this regard.


Bonus Question (A nail in the coffin, if you would)

Schizophrenia Twin Studies are arguably the most famous set of psychiatric studies. Further, Schizophrenia Twin Studies are arguably the set of studies that psychiatrists refer to when seeking to appease the non-believers. Through use, Schizophrenia Twin Studies have become a cornerstone for the justification of psychiatry. 

Schizophrenia Twin Studies are a pool of many studies, each of which has added to the whole. Let's hold a paper-version of a Schizophrenia Twin Study over a few paragraphs. Let's fill a room with four sets of twins, eight individuals total. Two sets of twins will be identical twins and two will be fraternal twins. Our study is ready to begin.

A Schizophrenia Twin Study begins by diagnosing each individual in the study group. The study begins by determining whether one or both individuals in a set of twins is schizophrenic. The study does this for the identical and fraternal twins.

Please try to comprehend the stupidity of such a thing. Schizophrenia Twin Studies are used to justify psychiatries failure to discover schizophrenia in anyone. Yet, the worth of Schizophrenia Twin Studies hinges on being able to discover who is schizophrenic.

The truth about Schizophrenia Twin Studies is; Schizophrenia Twin Studies are used to complicate the issue, to obfuscate the truth, to defer justice. Schizophrenia Twin Studies are fraudulent, they are delictual. Schizophrenia Twin Studies are a good example of the dangers of consensus and nod-farming.",0,"Hi r/statistics

In 2008/9 the Office of the High Commissioner on Human Rights reported on the torture and cruel, inhuman and degrading treatment of the mentally ill. Kid gloves, please.

I would like to receive comment and criticism on my grasp of the statistics in this part of an appendix I am writing. Here goes.  

The Three Questions Psychiatry must be held to Account for. 

(This version of the Three Questions uses Schizophrenia as an example, but they apply to most mental illness too)

Question 1. Is Schizophrenia a Medical Illness or a Medical Theory?

When a person has a cold (the common cold), they are suffering from an illness. We know the common cold is an illness because the cold has been discovered to be an illness. Before the cold was discovered, medical thoughts on the subject amounted to theory, hypothesis, conjecture, speculation and consensus. However, when the cold was discovered, false theories and misguided consensus fell by the way-side and the truth was laid bare.  

The first question flows from the fact that a real illness differs from medical theory because a real illness is given a name when it is discovered. When the cold was discovered, Doctors named the common cold a Rhinovirus.

Question 1. Doctor, please name the underlying illness for Schizophrenia. (or any other mental illness for that matter.)

A Profound Consequence flowing from Question 1.

If your psychiatrist cannot name the underlying illness, it is probably because the illness is theoretical. The question becomes, if psychiatry has not discovered your illness in a human being, how are they going to ""discover"" the illness in you? The question leads to questions 2 and 3.

Questions 2 and 3 Can psychiatry diagnose an undiscovered illness, or is every diagnosis theoretical?

Psychiatry is a large and rich industry. Further, their income is a large source of tax revenue. Both psychiatrists and The Man have embroiled themselves in a convoluted dialogue designed to defend against questions like these three. Luckily, there are things one can do to reveal the truth to a judge, jury and the public at large, should you be interested in the ""cure"" that a good lawyer can provide you. 

Let's split the word diagnosis into two parts, namely: direct and indirect diagnosis. Further, let's define direct and indirect diagnosis in a manner which is going to be useful in a court of law. Let's start with direct diagnosis. Let's define direct diagnosis to mean, any legitimate, peer-reviewed, medical procedure which, if properly performed, would result in the discovery or re-discovery of schizophrenia.

Question 2. Doctor, can you directly diagnose someone with schizophrenia?

A Profound Consequence of Question 2.

Psychiatry hasn't discovered schizophrenia. Therefore, it stands to reason that psychiatry cannot point out an individual, or group of people, they discovered to be schizophrenic. I say again, psychiatry cannot isolate a person, or a group of people, they discovered to be schizophrenic because they lack the medical ability to discover schizophrenia in a human being. Apologies for being a little redundant, but the fact has a profound bearing on the crux of the matter, namely; question 3.

Question 3 Can psychiatry diagnose schizophrenia in an indirect fashion, like they claim to be doing?

Question 1 reveals that psychiatry hasn't discovered schizophrenia yet. Schizophrenia remains a puddle of medical belief, theory, conjecture, speculation, and consensus. Schizophrenia will remain theoretical until schizophrenia is discovered to factually exist.
Question 2 reveals that psychiatry cannot point out a person, or a group of people, they discovered to be schizophrenic because psychiatrists lack the ability to discover schizophrenia in a human being. 

Question 3 deals with indirect diagnosis. Question 3 is the crux of the fraud and tort that is mental illness. Question 3 is the crux because psychiatry is built, almost entirely, on statistics and statistical method. Believe it or not, psychiatry is structured as follows; Psychiatrists host psychiatric studies for their many theoretical illnesses. They publish their findings from their studies. Those findings make their way to the American Psychiatric Association, amongst other places. Every now and again, a select few psychiatrists will vote behind closed doors at the American Psychiatric Association. They vote on which psychiatric theories about undiscovered illnesses have come to publish sufficient findings to be approved by the American Psychiatric Association. Psychiatric theories which are approved are publicly rolled out, exported overseas and publicly portrayed as real illnesses from that day forward.  

The trouble is, the underlying psychiatric studies and their findings are fraudulent, intentionally and culpably so.

Paranoid Schizophrenia is a form of schizophrenia. As such, it is an undiscovered illness. Psychiatrists describe paranoid schizophrenia as; typically we believe the F.B.I. are out to get us. Paranoid Schizophrenia is a perfect example for this document because this document seeks the assistance of the F.B.I. in this matter. 

In pursuit of an answer to Question 3, let's host a paper-version of a psychiatric study of the illness of paranoid schizophrenia. Suppose we fill a paper-room with 5 test subjects. Each test subject is willing to complain about the F.B.I. or similar. Let us paper-study our 5 test subjects in order to add to the pool of statistics that Psychiatry claims to possess and the American Psychiatric Association claims to base their votes on. 

Statistics is a mathematical discipline. Statistics concerns the Mathematical Analysis of Data, no more and no less. And so, before our paper-version of a psychiatric study can add to the statistical pool for paranoid schizophrenia, we must collect data on our 5 test-subjects. Let us gather data so that we have something to apply our statistical tools to.

Blood pressure will serve as the data that our paper-study collects. You see, most laypeople, including judges, jurors and the public accept that blood pressure is a matter of medical fact. Blood pressure, ordinary members of the public feel, is not a medical theory Doctors use to confuse people.

Let us work through our 5 test subjects, reading their blood pressure as we go. 

John		aa
Zhi Ruo		bb
Sipho		cc
Mohammed	dd
Jimena		ee

Properly done, those 5 blood pressure readings count as real data. Armed with real data, we can begin using statistical tools. For example, the Average Blood Pressure for our 5 test subjects can be determined by adding all the blood pressure readings up and dividing by the number of readings in the data pool. (aa+bb+cc+dd+ee)/5 = ff

Done properly, the Average Blood Pressure (ff) is a good example of statistics. It is an honest example of statistical method. Please listen! This next part is an important bit. The Three Questions agree that many, and possibly most, psychiatric data and statistics analysis is honest too. Many, and possibly most, of the psychiatric data collected, was collected in a fashion which is similar to our paper example. 

The problem will fraudulent psychiatric studies has nothing to do with the collection and statistical analysis of data. The fraud has to do with the causal connections psychiatrists make.

Questions 1 and 2 hold the key to the fraud and the tort. You see, psychiatrists cannot discover which test subjects are schizophrenic because psychiatrists lack the ability to discover schizophrenia in a human being. No matter how diligent psychiatrists are in the collection of data and their subsequent statistical analysis, it remains an act of medical malpractice for psychiatrists to causally connect their findings to schizophrenia because they do not know which test subjects are schizophrenic. 

Question 3 Doctor, is it criminally inappropriate for psychiatrists to causally connect statistics to an undiscovered illness because it is impossible for psychiatrists to determine whether their test-subjects have the requisite illness?

A Profound Consequence of Question 3

There are probably many levels of causality. However, Question 3 deals with the most basic level of causality. And by most basic I mean, you cannot attribute wins to a racehorse if the horse has never raced. You cannot attribute K.O.'s to a boxer if the boxer has never entered the ring. You cannot record how many coin tosses were heads if nobody has tossed the coin. While you can speculate and farm agreement among your peers, arguing how well your horse, boxer or coin might perform when it arrives on the scene at a future point, statistics cannot analyse theory because there are so many possible theories, statisticians saw the wisdom in drawing a line and saying no. The only statistics Doctors can claim to be legitimate are based on discovered illnesses because test subjects must be shown to have the requisite illness before data collection can begin.

Every undiscovered mental illness the American Psychiatric Association voted into existence is an intentional, culpable misrepresentation which is prejudicial, or potentially prejudicial. In short, the American Psychiatric Association is engaged in fraud. This document is an open request to the F.B.I. for their assistance in this regard.


Bonus Question (A nail in the coffin, if you would)

Schizophrenia Twin Studies are arguably the most famous set of psychiatric studies. Further, Schizophrenia Twin Studies are arguably the set of studies that psychiatrists refer to when seeking to appease the non-believers. Through use, Schizophrenia Twin Studies have become a cornerstone for the justification of psychiatry. 

Schizophrenia Twin Studies are a pool of many studies, each of which has added to the whole. Let's hold a paper-version of a Schizophrenia Twin Study over a few paragraphs. Let's fill a room with four sets of twins, eight individuals total. Two sets of twins will be identical twins and two will be fraternal twins. Our study is ready to begin.

A Schizophrenia Twin Study begins by diagnosing each individual in the study group. The study begins by determining whether one or both individuals in a set of twins is schizophrenic. The study does this for the identical and fraternal twins.

Please try to comprehend the stupidity of such a thing. Schizophrenia Twin Studies are used to justify psychiatries failure to discover schizophrenia in anyone. Yet, the worth of Schizophrenia Twin Studies hinges on being able to discover who is schizophrenic.

The truth about Schizophrenia Twin Studies is; Schizophrenia Twin Studies are used to complicate the issue, to obfuscate the truth, to defer justice. Schizophrenia Twin Studies are fraudulent, they are delictual. Schizophrenia Twin Studies are a good example of the dangers of consensus and nod-farming.",0,statistics,54935,,Statistics in Psychiatry,https://www.reddit.com/r/statistics/comments/8cya0o/statistics_in_psychiatry/,all_ads,2018-04-17 21:21:00,46 days 04:14:34.036584000,
"I've been out of college and working for a couple years now, and have really been wanting to return to grad school to eventually get a PhD in Epidemiology and (ideally) work in academia. Before I get a PhD, though, I figure I should probably get an MS. I was hoping some of you could offer insight into how competitive I'd be for biostats MS programs, given my overall credentials below:

Got a BSc in mathematics and statistics, with a minor in data science/data mining, and a 3.2 overall GPA.

While an undergrad I had 3 different research assistantships.

Since college I've worked at a pharmaceutical sales company as a data analyst and, more recently, at a clinical trials research company that studies bacteria antibiotic susceptibility in the IT department as a data manager. In both cases, statistical analysis was more of a secondary or tertiary job function.

I haven't studied for or taken the GRE yet, but I took a practice test last night and got 162 quant (83rd percentile) and 157 verbal (74th percentile).

Any help figuring out about where I stand relative to the competition would be greatly appreciated. Any other advice is also welcome. Thank you!",19,1524008780.0,8cxj3g,False,"I've been out of college and working for a couple years now, and have really been wanting to return to grad school to eventually get a PhD in Epidemiology and (ideally) work in academia. Before I get a PhD, though, I figure I should probably get an MS. I was hoping some of you could offer insight into how competitive I'd be for biostats MS programs, given my overall credentials below:

Got a BSc in mathematics and statistics, with a minor in data science/data mining, and a 3.2 overall GPA.

While an undergrad I had 3 different research assistantships.

Since college I've worked at a pharmaceutical sales company as a data analyst and, more recently, at a clinical trials research company that studies bacteria antibiotic susceptibility in the IT department as a data manager. In both cases, statistical analysis was more of a secondary or tertiary job function.

I haven't studied for or taken the GRE yet, but I took a practice test last night and got 162 quant (83rd percentile) and 157 verbal (74th percentile).

Any help figuring out about where I stand relative to the competition would be greatly appreciated. Any other advice is also welcome. Thank you!",0,"I've been out of college and working for a couple years now, and have really been wanting to return to grad school to eventually get a PhD in Epidemiology and (ideally) work in academia. Before I get a PhD, though, I figure I should probably get an MS. I was hoping some of you could offer insight into how competitive I'd be for biostats MS programs, given my overall credentials below:

Got a BSc in mathematics and statistics, with a minor in data science/data mining, and a 3.2 overall GPA.

While an undergrad I had 3 different research assistantships.

Since college I've worked at a pharmaceutical sales company as a data analyst and, more recently, at a clinical trials research company that studies bacteria antibiotic susceptibility in the IT department as a data manager. In both cases, statistical analysis was more of a secondary or tertiary job function.

I haven't studied for or taken the GRE yet, but I took a practice test last night and got 162 quant (83rd percentile) and 157 verbal (74th percentile).

Any help figuring out about where I stand relative to the competition would be greatly appreciated. Any other advice is also welcome. Thank you!",1,statistics,54935,,How competitive would I be for a biostats MS?,https://www.reddit.com/r/statistics/comments/8cxj3g/how_competitive_would_i_be_for_a_biostats_ms/,all_ads,2018-04-17 19:46:20,46 days 05:49:14.036584000,
"I have two regressions from the same data set, one for income on weight of aliens and one for income on weight of humans. I now have two regressions. To test the significance of the difference between the Beta1's, I would do an unpaired sample ttest, correct?
   Assuming that is correct, I am now adding restrictions to the alien regression based on their marital status. Now i have a new beta1 coefficient. To test this difference from the beta1 coefficient of my original alien regression, would I now use a paired sample ttest, since it is the same sample?
   Any help would be much appreciated 
   ",2,1523960727.0,8ct729,False,"I have two regressions from the same data set, one for income on weight of aliens and one for income on weight of humans. I now have two regressions. To test the significance of the difference between the Beta1's, I would do an unpaired sample ttest, correct?
   Assuming that is correct, I am now adding restrictions to the alien regression based on their marital status. Now i have a new beta1 coefficient. To test this difference from the beta1 coefficient of my original alien regression, would I now use a paired sample ttest, since it is the same sample?
   Any help would be much appreciated 
   ",0,"I have two regressions from the same data set, one for income on weight of aliens and one for income on weight of humans. I now have two regressions. To test the significance of the difference between the Beta1's, I would do an unpaired sample ttest, correct?
   Assuming that is correct, I am now adding restrictions to the alien regression based on their marital status. Now i have a new beta1 coefficient. To test this difference from the beta1 coefficient of my original alien regression, would I now use a paired sample ttest, since it is the same sample?
   Any help would be much appreciated 
   ",10,statistics,54935,,Is this assumption right?,https://www.reddit.com/r/statistics/comments/8ct729/is_this_assumption_right/,all_ads,2018-04-17 06:25:27,46 days 19:10:07.036584000,
Someone told me that getting a PhD in Biostats isn't great if you want to do your own original research. You are more likely to get hired by another group to help design a study than to design your own study that you thought of. ,12,1523932068.0,8cpsb1,False,Someone told me that getting a PhD in Biostats isn't great if you want to do your own original research. You are more likely to get hired by another group to help design a study than to design your own study that you thought of. ,0,Someone told me that getting a PhD in Biostats isn't great if you want to do your own original research. You are more likely to get hired by another group to help design a study than to design your own study that you thought of. ,30,statistics,54935,,Do PhD statistics typically lead their own research or are their hired by other groups to design studies?,https://www.reddit.com/r/statistics/comments/8cpsb1/do_phd_statistics_typically_lead_their_own/,all_ads,2018-04-16 22:27:48,47 days 03:07:46.036584000,
"Hi all - I would like to validate a regression model by first splitting the data in two, a 70% dataset and a 30% holdout dataset respectively. I would run a stepwise multiple regression and generate the regression equation with my 70% dataset.

Would I then use this equation to calculate predicted values in my holdout dataset DV, and compare these values to the observed values of my DV in the holdout dataset using the RMSE statistic, or is there a more nuanced way of validating my model by looking at other parameters? What guidelines are there for interpreting the RMSE statistic (i.e is there a threshold after which the model may be judged as invalid?) ",8,1523980226.0,8cuuoe,False,"Hi all - I would like to validate a regression model by first splitting the data in two, a 70% dataset and a 30% holdout dataset respectively. I would run a stepwise multiple regression and generate the regression equation with my 70% dataset.

Would I then use this equation to calculate predicted values in my holdout dataset DV, and compare these values to the observed values of my DV in the holdout dataset using the RMSE statistic, or is there a more nuanced way of validating my model by looking at other parameters? What guidelines are there for interpreting the RMSE statistic (i.e is there a threshold after which the model may be judged as invalid?) ",0,"Hi all - I would like to validate a regression model by first splitting the data in two, a 70% dataset and a 30% holdout dataset respectively. I would run a stepwise multiple regression and generate the regression equation with my 70% dataset.

Would I then use this equation to calculate predicted values in my holdout dataset DV, and compare these values to the observed values of my DV in the holdout dataset using the RMSE statistic, or is there a more nuanced way of validating my model by looking at other parameters? What guidelines are there for interpreting the RMSE statistic (i.e is there a threshold after which the model may be judged as invalid?) ",2,statistics,54935,,Validating a regression model,https://www.reddit.com/r/statistics/comments/8cuuoe/validating_a_regression_model/,all_ads,2018-04-17 11:50:26,46 days 13:45:08.036584000,
"Hello everyone,

I posted here a while back: 
https://www.reddit.com/r/statistics/comments/7h5f7q/am_i_getting_in_over_my_head/

To recap, I am planning on entering masters program in applied statistics and analytics and have a minor background in stats. My primary skill set is in the biological sciences. I have the prerequisites (calc I and II, intermediate stats) completed for the program, and would like to give it a whirl. 

Anyway, I was wondering if there was any kind of core competencies anyone would suggest to someone in my shoes. I was thinking of going through all the khan academy videos on stats and linear algebra before the program starts. Does this sound like a good idea? Is there anything else I should look into? Thanks in advance!

",23,1523947253.0,8crqcf,False,"Hello everyone,

I posted here a while back: 
https://www.reddit.com/r/statistics/comments/7h5f7q/am_i_getting_in_over_my_head/

To recap, I am planning on entering masters program in applied statistics and analytics and have a minor background in stats. My primary skill set is in the biological sciences. I have the prerequisites (calc I and II, intermediate stats) completed for the program, and would like to give it a whirl. 

Anyway, I was wondering if there was any kind of core competencies anyone would suggest to someone in my shoes. I was thinking of going through all the khan academy videos on stats and linear algebra before the program starts. Does this sound like a good idea? Is there anything else I should look into? Thanks in advance!

",0,"Hello everyone,

I posted here a while back: 
https://www.reddit.com/r/statistics/comments/7h5f7q/am_i_getting_in_over_my_head/

To recap, I am planning on entering masters program in applied statistics and analytics and have a minor background in stats. My primary skill set is in the biological sciences. I have the prerequisites (calc I and II, intermediate stats) completed for the program, and would like to give it a whirl. 

Anyway, I was wondering if there was any kind of core competencies anyone would suggest to someone in my shoes. I was thinking of going through all the khan academy videos on stats and linear algebra before the program starts. Does this sound like a good idea? Is there anything else I should look into? Thanks in advance!

",10,statistics,54935,,Prep for grad school,https://www.reddit.com/r/statistics/comments/8crqcf/prep_for_grad_school/,all_ads,2018-04-17 02:40:53,46 days 22:54:41.036584000,
"I am analysing some data to see if there is any correlation between the difficulty of a game and the duration that a user plays. To measure the difficulty, I am using a win-ratio, which is the number of times a player wins in relation to the total number of games played.

I used a [scatter plot](https://imgur.com/a/5l2Lq) to plot the win-ratio (x-axis) vs the number of games played (y-axis), which resulted in a bell-shaped distribution peaking at a win-ratio of 0.3. I also plotted a [histogram](https://imgur.com/a/3qr3q) for the win-ratios and it showed that the majority of players have a win-ratio in the range of 0.3. 

Is there a hypothesis test I can run to verify that the number of spins is increasing because of the win ratio and not because there are more samples. (if there are more samples, it is more likely to find people that play longer)

Thanks in advance",4,1523984148.0,8cv42z,False,"I am analysing some data to see if there is any correlation between the difficulty of a game and the duration that a user plays. To measure the difficulty, I am using a win-ratio, which is the number of times a player wins in relation to the total number of games played.

I used a [scatter plot](https://imgur.com/a/5l2Lq) to plot the win-ratio (x-axis) vs the number of games played (y-axis), which resulted in a bell-shaped distribution peaking at a win-ratio of 0.3. I also plotted a [histogram](https://imgur.com/a/3qr3q) for the win-ratios and it showed that the majority of players have a win-ratio in the range of 0.3. 

Is there a hypothesis test I can run to verify that the number of spins is increasing because of the win ratio and not because there are more samples. (if there are more samples, it is more likely to find people that play longer)

Thanks in advance",0,"I am analysing some data to see if there is any correlation between the difficulty of a game and the duration that a user plays. To measure the difficulty, I am using a win-ratio, which is the number of times a player wins in relation to the total number of games played.

I used a [scatter plot](https://imgur.com/a/5l2Lq) to plot the win-ratio (x-axis) vs the number of games played (y-axis), which resulted in a bell-shaped distribution peaking at a win-ratio of 0.3. I also plotted a [histogram](https://imgur.com/a/3qr3q) for the win-ratios and it showed that the majority of players have a win-ratio in the range of 0.3. 

Is there a hypothesis test I can run to verify that the number of spins is increasing because of the win ratio and not because there are more samples. (if there are more samples, it is more likely to find people that play longer)

Thanks in advance",1,statistics,54935,,Hypothesis testing on a gaming problem,https://www.reddit.com/r/statistics/comments/8cv42z/hypothesis_testing_on_a_gaming_problem/,all_ads,2018-04-17 12:55:48,46 days 12:39:46.036584000,
"Does a neural network with no hidden layers guarantee a convex loss function?

If so, could someone link me to a proof of some sort that would help me prove this?",2,1523982202.0,8cuzfx,False,"Does a neural network with no hidden layers guarantee a convex loss function?

If so, could someone link me to a proof of some sort that would help me prove this?",0,"Does a neural network with no hidden layers guarantee a convex loss function?

If so, could someone link me to a proof of some sort that would help me prove this?",1,statistics,54935,,Quick question regarding optimization,https://www.reddit.com/r/statistics/comments/8cuzfx/quick_question_regarding_optimization/,all_ads,2018-04-17 12:23:22,46 days 13:12:12.036584000,
"[Academic] Personality and Relationships at Work. Survey for university dissertation. (18+, currently working)

https://www.isurvey.soton.ac.uk/26418 This study is aimed at understanding the effect that a number of personality dimensions may have on your behaviors and relationships at work. We are seeking people to complete a 20 minute online survey to help answer this question. You will be asked to fill in some questionnaires regarding yourself, your personality, your relationships with others, and your behaviors at work. You are advised to complete the survey in private so you can be honest and open. As a thank you for taking part in this study, you can choose to be entered into a prize-draw that will give you an opportunity to win an Amazon Gift Voucher worth £50 (or equivalent). To take part in this study you must be 18 years of age or older and currently be working in an organization. https://www.isurvey.soton.ac.uk/26418",2,1524008951.0,8cxjyq,False,"[Academic] Personality and Relationships at Work. Survey for university dissertation. (18+, currently working)

https://www.isurvey.soton.ac.uk/26418 This study is aimed at understanding the effect that a number of personality dimensions may have on your behaviors and relationships at work. We are seeking people to complete a 20 minute online survey to help answer this question. You will be asked to fill in some questionnaires regarding yourself, your personality, your relationships with others, and your behaviors at work. You are advised to complete the survey in private so you can be honest and open. As a thank you for taking part in this study, you can choose to be entered into a prize-draw that will give you an opportunity to win an Amazon Gift Voucher worth £50 (or equivalent). To take part in this study you must be 18 years of age or older and currently be working in an organization. https://www.isurvey.soton.ac.uk/26418",0,"[Academic] Personality and Relationships at Work. Survey for university dissertation. (18+, currently working)

https://www.isurvey.soton.ac.uk/26418 This study is aimed at understanding the effect that a number of personality dimensions may have on your behaviors and relationships at work. We are seeking people to complete a 20 minute online survey to help answer this question. You will be asked to fill in some questionnaires regarding yourself, your personality, your relationships with others, and your behaviors at work. You are advised to complete the survey in private so you can be honest and open. As a thank you for taking part in this study, you can choose to be entered into a prize-draw that will give you an opportunity to win an Amazon Gift Voucher worth £50 (or equivalent). To take part in this study you must be 18 years of age or older and currently be working in an organization. https://www.isurvey.soton.ac.uk/26418",0,statistics,54935,,**Participants needed for an academic survey**,https://www.reddit.com/r/statistics/comments/8cxjyq/participants_needed_for_an_academic_survey/,all_ads,2018-04-17 19:49:11,46 days 05:46:23.036584000,
"Hey guys.

So, we're doing presentations over poems in my English class, and they way my teacher has it is that we're gonna walk in the room, and randomly pick one of 12 folders, each folder with a different poem in it, and then analyze it. Now, there are only 5 copies of a poem within a folder -- ie. after 5 people pick the same folder, no one else can get that poem.

There are 5 days of presentations (12 per day), and my teacher said she'll notify us day after day of which poems are remaining and which poems are gone (gone meaning 5 people have already done them). I'm going on the fifth day, so what're the odds of there being 12 poems left for me to study? 6 poems? 5 poems? what's the probability distribution?

This will really help me ease my anxiety, since if there is a 30% chance there are only 4 poems left on my day, it would really make my life easier knowing that.

Please help -- thanks<3",3,1523962708.0,8cte8e,False,"Hey guys.

So, we're doing presentations over poems in my English class, and they way my teacher has it is that we're gonna walk in the room, and randomly pick one of 12 folders, each folder with a different poem in it, and then analyze it. Now, there are only 5 copies of a poem within a folder -- ie. after 5 people pick the same folder, no one else can get that poem.

There are 5 days of presentations (12 per day), and my teacher said she'll notify us day after day of which poems are remaining and which poems are gone (gone meaning 5 people have already done them). I'm going on the fifth day, so what're the odds of there being 12 poems left for me to study? 6 poems? 5 poems? what's the probability distribution?

This will really help me ease my anxiety, since if there is a 30% chance there are only 4 poems left on my day, it would really make my life easier knowing that.

Please help -- thanks<3",0,"Hey guys.

So, we're doing presentations over poems in my English class, and they way my teacher has it is that we're gonna walk in the room, and randomly pick one of 12 folders, each folder with a different poem in it, and then analyze it. Now, there are only 5 copies of a poem within a folder -- ie. after 5 people pick the same folder, no one else can get that poem.

There are 5 days of presentations (12 per day), and my teacher said she'll notify us day after day of which poems are remaining and which poems are gone (gone meaning 5 people have already done them). I'm going on the fifth day, so what're the odds of there being 12 poems left for me to study? 6 poems? 5 poems? what's the probability distribution?

This will really help me ease my anxiety, since if there is a 30% chance there are only 4 poems left on my day, it would really make my life easier knowing that.

Please help -- thanks<3",0,statistics,54935,,"Wondering if someone can calculate this in real life situation's probability for me, since it's way beyond any stats ive ever learned",https://www.reddit.com/r/statistics/comments/8cte8e/wondering_if_someone_can_calculate_this_in_real/,all_ads,2018-04-17 06:58:28,46 days 18:37:06.036584000,
"Hey guys, first time here so I apologize in advance if I'm breaking any sub rules.  I'm currently trying to distill the value of a fantasy baseball player into one statistic and I have a quick question about weighted averages.

Essentially, I'm trying to figure out the mean and standard deviation for league ERA (earned runs/innings pitched) but I realize the results will be skewed if I just use standard averages.  Would a simple weighted average using the total innings pitched work or would I have to dig a little deeper?",0,1523940879.0,8cqye1,False,"Hey guys, first time here so I apologize in advance if I'm breaking any sub rules.  I'm currently trying to distill the value of a fantasy baseball player into one statistic and I have a quick question about weighted averages.

Essentially, I'm trying to figure out the mean and standard deviation for league ERA (earned runs/innings pitched) but I realize the results will be skewed if I just use standard averages.  Would a simple weighted average using the total innings pitched work or would I have to dig a little deeper?",0,"Hey guys, first time here so I apologize in advance if I'm breaking any sub rules.  I'm currently trying to distill the value of a fantasy baseball player into one statistic and I have a quick question about weighted averages.

Essentially, I'm trying to figure out the mean and standard deviation for league ERA (earned runs/innings pitched) but I realize the results will be skewed if I just use standard averages.  Would a simple weighted average using the total innings pitched work or would I have to dig a little deeper?",3,statistics,54935,,Quantifying the Value of a Fantasy Baseball Player,https://www.reddit.com/r/statistics/comments/8cqye1/quantifying_the_value_of_a_fantasy_baseball_player/,all_ads,2018-04-17 00:54:39,47 days 00:40:55.036584000,
"I'm new to stats, so if someone could decipher the following I'd appreciate it.
RR: 0.40 95% CI 0.21-0.70

I'm aware of relative risk, but the range after the confidence interval is something I'm having a hard time understanding ",5,1523945700.0,8crjpv,False,"I'm new to stats, so if someone could decipher the following I'd appreciate it.
RR: 0.40 95% CI 0.21-0.70

I'm aware of relative risk, but the range after the confidence interval is something I'm having a hard time understanding ",0,"I'm new to stats, so if someone could decipher the following I'd appreciate it.
RR: 0.40 95% CI 0.21-0.70

I'm aware of relative risk, but the range after the confidence interval is something I'm having a hard time understanding ",2,statistics,54935,,Question regarding research article I'm reviewing,https://www.reddit.com/r/statistics/comments/8crjpv/question_regarding_research_article_im_reviewing/,all_ads,2018-04-17 02:15:00,46 days 23:20:34.036584000,
"As the title suggests, I'm currently having a little bit of trouble properly interpreting the coefficients from a linear regression model that I estimated.

y= C + alphaX

Where Y is log-transformed then first-differenced GDP, and X is log-transformed then first-differenced export data. Eg: X=(logx(t)-logx(t-1)). I'm unsure as to how should I interpret the coefficient of alpha. Would it be 'a change of magnitude alpha in the percentage change in exports causes a unit change in the percentage change of GDP?' ",3,1523907739.0,8cmvxq,False,"As the title suggests, I'm currently having a little bit of trouble properly interpreting the coefficients from a linear regression model that I estimated.

y= C + alphaX

Where Y is log-transformed then first-differenced GDP, and X is log-transformed then first-differenced export data. Eg: X=(logx(t)-logx(t-1)). I'm unsure as to how should I interpret the coefficient of alpha. Would it be 'a change of magnitude alpha in the percentage change in exports causes a unit change in the percentage change of GDP?' ",0,"As the title suggests, I'm currently having a little bit of trouble properly interpreting the coefficients from a linear regression model that I estimated.

y= C + alphaX

Where Y is log-transformed then first-differenced GDP, and X is log-transformed then first-differenced export data. Eg: X=(logx(t)-logx(t-1)). I'm unsure as to how should I interpret the coefficient of alpha. Would it be 'a change of magnitude alpha in the percentage change in exports causes a unit change in the percentage change of GDP?' ",6,statistics,54935,,Help with interpreting regression coefficients,https://www.reddit.com/r/statistics/comments/8cmvxq/help_with_interpreting_regression_coefficients/,all_ads,2018-04-16 15:42:19,47 days 09:53:15.036584000,
"assume we have a sample of size s, that is a fraction Px of the population.
An event E is targeting the population uniformly, the probability of the event E to occur within the observed sample (a Success) follows a Bernoulli(Px). 

when we repeat this experience N times, the probability of the event E to occur k times within the observed sample (k Successes out of N) follows a Binomial(N,Px).

My question is :
when the number of experiences N is unknown, and we got k successes within the observed sample, What can we say about N (how can we infer N, CI of N), given the sample size, its fraction Px, the population size, the number of successes...

Thanks in advance.",10,1523922596.0,8coixn,False,"assume we have a sample of size s, that is a fraction Px of the population.
An event E is targeting the population uniformly, the probability of the event E to occur within the observed sample (a Success) follows a Bernoulli(Px). 

when we repeat this experience N times, the probability of the event E to occur k times within the observed sample (k Successes out of N) follows a Binomial(N,Px).

My question is :
when the number of experiences N is unknown, and we got k successes within the observed sample, What can we say about N (how can we infer N, CI of N), given the sample size, its fraction Px, the population size, the number of successes...

Thanks in advance.",0,"assume we have a sample of size s, that is a fraction Px of the population.
An event E is targeting the population uniformly, the probability of the event E to occur within the observed sample (a Success) follows a Bernoulli(Px). 

when we repeat this experience N times, the probability of the event E to occur k times within the observed sample (k Successes out of N) follows a Binomial(N,Px).

My question is :
when the number of experiences N is unknown, and we got k successes within the observed sample, What can we say about N (how can we infer N, CI of N), given the sample size, its fraction Px, the population size, the number of successes...

Thanks in advance.",1,statistics,54935,,"Estimate parameter N of a binomial(N,p)",https://www.reddit.com/r/statistics/comments/8coixn/estimate_parameter_n_of_a_binomialnp/,all_ads,2018-04-16 19:49:56,47 days 05:45:38.036584000,
"IBM sells SPSS as something that’s easy to use. The price is ridiculous. With practice, R is super easy. Do you think IBM is afraid of R growing in popularity and use? If one wants an interface, JASP can do plenty too and it’s free.",43,1523853661.0,8ci2bu,False,"IBM sells SPSS as something that’s easy to use. The price is ridiculous. With practice, R is super easy. Do you think IBM is afraid of R growing in popularity and use? If one wants an interface, JASP can do plenty too and it’s free.",0,"IBM sells SPSS as something that’s easy to use. The price is ridiculous. With practice, R is super easy. Do you think IBM is afraid of R growing in popularity and use? If one wants an interface, JASP can do plenty too and it’s free.",24,statistics,54935,,Do you think IBM is afraid of R growing in popularity and use?,https://www.reddit.com/r/statistics/comments/8ci2bu/do_you_think_ibm_is_afraid_of_r_growing_in/,all_ads,2018-04-16 00:41:01,48 days 00:54:33.036584000,
"For those of  you that have a PhD in stats, what was your motivator?",43,1523841810.0,8cgml2,False,"For those of  you that have a PhD in stats, what was your motivator?",0,"For those of  you that have a PhD in stats, what was your motivator?",42,statistics,54935,,Why did you choose to study statistics for your PhD?,https://www.reddit.com/r/statistics/comments/8cgml2/why_did_you_choose_to_study_statistics_for_your/,all_ads,2018-04-15 21:23:30,48 days 04:12:04.036584000,
"If you choose an answer to this question at random, what is the chance you will be correct?

A) 25%
B) 50%
C) 50%
D) 75%",8,1523935574.0,8cq98u,False,"If you choose an answer to this question at random, what is the chance you will be correct?

A) 25%
B) 50%
C) 50%
D) 75%",0,"If you choose an answer to this question at random, what is the chance you will be correct?

A) 25%
B) 50%
C) 50%
D) 75%",0,statistics,54935,,"You will ""probably"" answer this wrong",https://www.reddit.com/r/statistics/comments/8cq98u/you_will_probably_answer_this_wrong/,all_ads,2018-04-16 23:26:14,47 days 02:09:20.036584000,
"I'm sorry, I still have problems understanding how spike and slab is used for variable selection. I've read all that I could online, but I can't seem to grasp it. Could anyone please  me on where I'm getting these steps wrong?

1. First a sparse distribution and a spike at zero are mixed according to a specified prior ""p"" for each beta. How is this ""p"" determined? What determines the variance for this sparse distribution part?

2. I think I understand how the Metropolis Hastings Algorithm comes into play here. You add or remove variables randomly, eventually coming up with a stationary distribution of variables that are kept in. But how do you determine which variables to keep in after running this simulation? Do you take the mode(s) of the variables kept in the models?

3. How does Gibbs Sampler come into play here? Is it used when we assume variables aren't independent?",10,1523874208.0,8ck9wz,False,"I'm sorry, I still have problems understanding how spike and slab is used for variable selection. I've read all that I could online, but I can't seem to grasp it. Could anyone please  me on where I'm getting these steps wrong?

1. First a sparse distribution and a spike at zero are mixed according to a specified prior ""p"" for each beta. How is this ""p"" determined? What determines the variance for this sparse distribution part?

2. I think I understand how the Metropolis Hastings Algorithm comes into play here. You add or remove variables randomly, eventually coming up with a stationary distribution of variables that are kept in. But how do you determine which variables to keep in after running this simulation? Do you take the mode(s) of the variables kept in the models?

3. How does Gibbs Sampler come into play here? Is it used when we assume variables aren't independent?",0,"I'm sorry, I still have problems understanding how spike and slab is used for variable selection. I've read all that I could online, but I can't seem to grasp it. Could anyone please  me on where I'm getting these steps wrong?

1. First a sparse distribution and a spike at zero are mixed according to a specified prior ""p"" for each beta. How is this ""p"" determined? What determines the variance for this sparse distribution part?

2. I think I understand how the Metropolis Hastings Algorithm comes into play here. You add or remove variables randomly, eventually coming up with a stationary distribution of variables that are kept in. But how do you determine which variables to keep in after running this simulation? Do you take the mode(s) of the variables kept in the models?

3. How does Gibbs Sampler come into play here? Is it used when we assume variables aren't independent?",3,statistics,54935,,Spike and Slab intuition?,https://www.reddit.com/r/statistics/comments/8ck9wz/spike_and_slab_intuition/,all_ads,2018-04-16 06:23:28,47 days 19:12:06.036584000,
"Howdy folks. any splendor fans out there? For those not in the know, Splendor is a chip and card collecting game. Chips can be used to buy cards, cards can stack with chips to buy more cards and some of those cards have points. First player to 15 (base game) wins.  Game is pretty neat because it's easy to pack up and plays 2 - 4. My husband and I play each other (2 player game) almost every night. Which lead to the following hypothesis:

I think the first player of splendor (in a 2 player game only) is more likely to win than the second player

I've started a spreadsheet here to help record the games we play, final scores, who went first, and who won:
https://docs.google.com/spreadsheets/d/1Ix5pR1lKH_S4XxiKty-Tbqy9nohH7vWWsywLWxl3wZE/edit#gid=0

I'd like some help thinking through a few things.  First, should I randomize who starts? As in, we each roll a die and the higher die roll goes first (Catan rules) or should we go every other/assign to be sure we each have an equal number of 1st plays?

How many trials are necessary? I usually use a sample of n=40 in some of my work. think that could work out here too?

Finally, I have a few friends (and maybe some of you?) that I can coerce into playing a game a night for the next month or two months to get a total of 40 games.  Is 5 pairs of friends, aggregate, enough to have some data at the end that might be worth a damn?

Oh, some constraints. I'm in grad school, so I'm a hermit. I can really only play my husband.  Even tho it would be great, a randomized trial really isn't an option here, and maybe not ideal either because playing each other is the best representation of most common use case for this game. 

I'd like to say I only play using the chips, but we also play online, which shows us all available options. I'm going to try to turn that feature off to keep it apples to apples ish. 

thanks for your thoughts!",6,1523868322.0,8cjoin,False,"Howdy folks. any splendor fans out there? For those not in the know, Splendor is a chip and card collecting game. Chips can be used to buy cards, cards can stack with chips to buy more cards and some of those cards have points. First player to 15 (base game) wins.  Game is pretty neat because it's easy to pack up and plays 2 - 4. My husband and I play each other (2 player game) almost every night. Which lead to the following hypothesis:

I think the first player of splendor (in a 2 player game only) is more likely to win than the second player

I've started a spreadsheet here to help record the games we play, final scores, who went first, and who won:
https://docs.google.com/spreadsheets/d/1Ix5pR1lKH_S4XxiKty-Tbqy9nohH7vWWsywLWxl3wZE/edit#gid=0

I'd like some help thinking through a few things.  First, should I randomize who starts? As in, we each roll a die and the higher die roll goes first (Catan rules) or should we go every other/assign to be sure we each have an equal number of 1st plays?

How many trials are necessary? I usually use a sample of n=40 in some of my work. think that could work out here too?

Finally, I have a few friends (and maybe some of you?) that I can coerce into playing a game a night for the next month or two months to get a total of 40 games.  Is 5 pairs of friends, aggregate, enough to have some data at the end that might be worth a damn?

Oh, some constraints. I'm in grad school, so I'm a hermit. I can really only play my husband.  Even tho it would be great, a randomized trial really isn't an option here, and maybe not ideal either because playing each other is the best representation of most common use case for this game. 

I'd like to say I only play using the chips, but we also play online, which shows us all available options. I'm going to try to turn that feature off to keep it apples to apples ish. 

thanks for your thoughts!",0,"Howdy folks. any splendor fans out there? For those not in the know, Splendor is a chip and card collecting game. Chips can be used to buy cards, cards can stack with chips to buy more cards and some of those cards have points. First player to 15 (base game) wins.  Game is pretty neat because it's easy to pack up and plays 2 - 4. My husband and I play each other (2 player game) almost every night. Which lead to the following hypothesis:

I think the first player of splendor (in a 2 player game only) is more likely to win than the second player

I've started a spreadsheet here to help record the games we play, final scores, who went first, and who won:
https://docs.google.com/spreadsheets/d/1Ix5pR1lKH_S4XxiKty-Tbqy9nohH7vWWsywLWxl3wZE/edit#gid=0

I'd like some help thinking through a few things.  First, should I randomize who starts? As in, we each roll a die and the higher die roll goes first (Catan rules) or should we go every other/assign to be sure we each have an equal number of 1st plays?

How many trials are necessary? I usually use a sample of n=40 in some of my work. think that could work out here too?

Finally, I have a few friends (and maybe some of you?) that I can coerce into playing a game a night for the next month or two months to get a total of 40 games.  Is 5 pairs of friends, aggregate, enough to have some data at the end that might be worth a damn?

Oh, some constraints. I'm in grad school, so I'm a hermit. I can really only play my husband.  Even tho it would be great, a randomized trial really isn't an option here, and maybe not ideal either because playing each other is the best representation of most common use case for this game. 

I'd like to say I only play using the chips, but we also play online, which shows us all available options. I'm going to try to turn that feature off to keep it apples to apples ish. 

thanks for your thoughts!",0,statistics,54935,,Need help quantifying board game hypothesis,https://www.reddit.com/r/statistics/comments/8cjoin/need_help_quantifying_board_game_hypothesis/,all_ads,2018-04-16 04:45:22,47 days 20:50:12.036584000,
Does Statistics B.A. fall under the STEM category? ,11,1523859112.0,8cip1t,False,Does Statistics B.A. fall under the STEM category? ,0,Does Statistics B.A. fall under the STEM category? ,0,statistics,54935,,Silly question but...,https://www.reddit.com/r/statistics/comments/8cip1t/silly_question_but/,all_ads,2018-04-16 02:11:52,47 days 23:23:42.036584000,
"Hi all, I was watching this video on dependent probability by Khan Academy - https://www.youtube.com/watch?v=VjLEoo3hIoM

And got a curious question. So probability of two green marbles in a row is 3/5 * 2/4 = 30%, makes sense.

But does it not change if you pick two marbles at once? Wouldnt the calculation then be 3/5*3/5 = 36% ?

Which one is true probability?",7,1523806009.0,8cdpq8,False,"Hi all, I was watching this video on dependent probability by Khan Academy - https://www.youtube.com/watch?v=VjLEoo3hIoM

And got a curious question. So probability of two green marbles in a row is 3/5 * 2/4 = 30%, makes sense.

But does it not change if you pick two marbles at once? Wouldnt the calculation then be 3/5*3/5 = 36% ?

Which one is true probability?",0,"Hi all, I was watching this video on dependent probability by Khan Academy - https://www.youtube.com/watch?v=VjLEoo3hIoM

And got a curious question. So probability of two green marbles in a row is 3/5 * 2/4 = 30%, makes sense.

But does it not change if you pick two marbles at once? Wouldnt the calculation then be 3/5*3/5 = 36% ?

Which one is true probability?",6,statistics,54935,,"Probability of picking 2 marbles from a bag of 5 marbles, picking one at a time vs two at once?",https://www.reddit.com/r/statistics/comments/8cdpq8/probability_of_picking_2_marbles_from_a_bag_of_5/,all_ads,2018-04-15 11:26:49,48 days 14:08:45.036584000,
See title,7,1523799838.0,8cdbeq,False,See title,0,See title,5,statistics,54935,,Can Pearson correlation coefficient be used when one or both variables are linear on a log scale?,https://www.reddit.com/r/statistics/comments/8cdbeq/can_pearson_correlation_coefficient_be_used_when/,all_ads,2018-04-15 09:43:58,48 days 15:51:36.036584000,
"Hello,

I am actually a PhD candidate in Statistics graduating this year.  My department has asked me, among others, to participate in presenting to a group of high school students taking AP statistics.  The goal is to try to get them excited about statistics as a field, and I have 10 minutes to communicate this to them.

Does anyone have any suggestions?  I am a pretty passionate person but I am a little worried that I am too far removed from them to be able to effectively communicate the use of statistics in the modern day and how cool/fun it can be.

My ideas now are to talk to them about some events we have done for our undergrads that they have all enjoyed tremendously, and also to discuss how there is a quantitative ""bubble"" in the real world right now(referring to people doing ""data science"" without proper statistical training), and once it pops, I believe the people with the real quantitative skills will rise to the top.

I would like help, though, so if anyone has any ideas please let me know!",46,1523747363.0,8c848c,False,"Hello,

I am actually a PhD candidate in Statistics graduating this year.  My department has asked me, among others, to participate in presenting to a group of high school students taking AP statistics.  The goal is to try to get them excited about statistics as a field, and I have 10 minutes to communicate this to them.

Does anyone have any suggestions?  I am a pretty passionate person but I am a little worried that I am too far removed from them to be able to effectively communicate the use of statistics in the modern day and how cool/fun it can be.

My ideas now are to talk to them about some events we have done for our undergrads that they have all enjoyed tremendously, and also to discuss how there is a quantitative ""bubble"" in the real world right now(referring to people doing ""data science"" without proper statistical training), and once it pops, I believe the people with the real quantitative skills will rise to the top.

I would like help, though, so if anyone has any ideas please let me know!",0,"Hello,

I am actually a PhD candidate in Statistics graduating this year.  My department has asked me, among others, to participate in presenting to a group of high school students taking AP statistics.  The goal is to try to get them excited about statistics as a field, and I have 10 minutes to communicate this to them.

Does anyone have any suggestions?  I am a pretty passionate person but I am a little worried that I am too far removed from them to be able to effectively communicate the use of statistics in the modern day and how cool/fun it can be.

My ideas now are to talk to them about some events we have done for our undergrads that they have all enjoyed tremendously, and also to discuss how there is a quantitative ""bubble"" in the real world right now(referring to people doing ""data science"" without proper statistical training), and once it pops, I believe the people with the real quantitative skills will rise to the top.

I would like help, though, so if anyone has any ideas please let me know!",38,statistics,54935,,Trying to get high schoolers excited about statistics,https://www.reddit.com/r/statistics/comments/8c848c/trying_to_get_high_schoolers_excited_about/,all_ads,2018-04-14 19:09:23,49 days 06:26:11.036584000,
What are some examples that you think truly show the power and utility of statistics? ,5,1523768983.0,8cahv7,False,What are some examples that you think truly show the power and utility of statistics? ,0,What are some examples that you think truly show the power and utility of statistics? ,6,statistics,54935,,What is your favorite applied statistics success story?,https://www.reddit.com/r/statistics/comments/8cahv7/what_is_your_favorite_applied_statistics_success/,all_ads,2018-04-15 01:09:43,49 days 00:25:51.036584000,
"Program in question:
https://zicklin.baruch.cuny.edu/academic-programs/undergraduate/majors/statistics-quantitative-modeling/
Also hoping to do something research-related.",4,1523772315.0,8cauoq,False,"Program in question:
https://zicklin.baruch.cuny.edu/academic-programs/undergraduate/majors/statistics-quantitative-modeling/
Also hoping to do something research-related.",0,"Program in question:
https://zicklin.baruch.cuny.edu/academic-programs/undergraduate/majors/statistics-quantitative-modeling/
Also hoping to do something research-related.",3,statistics,54935,,Would a BBA in Statistics and Quantitative Modeling qualify me for a Statistics MA or MS in most grad schools?,https://www.reddit.com/r/statistics/comments/8cauoq/would_a_bba_in_statistics_and_quantitative/,all_ads,2018-04-15 02:05:15,48 days 23:30:19.036584000,
"I've been learning about generalised estimating equations for the last couple of weeks, and one part of the readings briefly explained why repeated measures ANOVA typically isn't appropriate for longitudinal data.

Given that there is often a trend within individuals/clusters in longitudinal data, it naturally seems important to want to account for how the outcome measures will be correlated with each other. In addition to this, the order of the measurements is of importance too - the fact that measurements that are closer together in time tend to be more highly correlated was a satisfying realisation to have, and one that seemed quite obvious in hindsight. 

But that aside, I learned something interesting about repeated measures ANOVA which has really stuck in my mind. The fact that the method does not take into consideration that there is a natural ordering of time, that the measurement which was taken at time *t*+1 occurs after the measurement at time *t*, but before *t*+2. In fact, time is treated as a random effect, which makes no sense, especially when considering the quote ""we cannot randomise Tuesday to make it equally likely to fall before or after Monday"". 

Anyway, I thought that was a neat explanation of the random effects assumption of the repeated measures ANOVA method, which tries to allow for correlation within individuals, is not appropriate for longitudinal data.",13,1523738565.0,8c7b1y,False,"I've been learning about generalised estimating equations for the last couple of weeks, and one part of the readings briefly explained why repeated measures ANOVA typically isn't appropriate for longitudinal data.

Given that there is often a trend within individuals/clusters in longitudinal data, it naturally seems important to want to account for how the outcome measures will be correlated with each other. In addition to this, the order of the measurements is of importance too - the fact that measurements that are closer together in time tend to be more highly correlated was a satisfying realisation to have, and one that seemed quite obvious in hindsight. 

But that aside, I learned something interesting about repeated measures ANOVA which has really stuck in my mind. The fact that the method does not take into consideration that there is a natural ordering of time, that the measurement which was taken at time *t*+1 occurs after the measurement at time *t*, but before *t*+2. In fact, time is treated as a random effect, which makes no sense, especially when considering the quote ""we cannot randomise Tuesday to make it equally likely to fall before or after Monday"". 

Anyway, I thought that was a neat explanation of the random effects assumption of the repeated measures ANOVA method, which tries to allow for correlation within individuals, is not appropriate for longitudinal data.",0,"I've been learning about generalised estimating equations for the last couple of weeks, and one part of the readings briefly explained why repeated measures ANOVA typically isn't appropriate for longitudinal data.

Given that there is often a trend within individuals/clusters in longitudinal data, it naturally seems important to want to account for how the outcome measures will be correlated with each other. In addition to this, the order of the measurements is of importance too - the fact that measurements that are closer together in time tend to be more highly correlated was a satisfying realisation to have, and one that seemed quite obvious in hindsight. 

But that aside, I learned something interesting about repeated measures ANOVA which has really stuck in my mind. The fact that the method does not take into consideration that there is a natural ordering of time, that the measurement which was taken at time *t*+1 occurs after the measurement at time *t*, but before *t*+2. In fact, time is treated as a random effect, which makes no sense, especially when considering the quote ""we cannot randomise Tuesday to make it equally likely to fall before or after Monday"". 

Anyway, I thought that was a neat explanation of the random effects assumption of the repeated measures ANOVA method, which tries to allow for correlation within individuals, is not appropriate for longitudinal data.",12,statistics,54935,,The pitfalls of repeated measures ANOVA for longitudinal data,https://www.reddit.com/r/statistics/comments/8c7b1y/the_pitfalls_of_repeated_measures_anova_for/,all_ads,2018-04-14 16:42:45,49 days 08:52:49.036584000,
,19,1523711109.0,8c5hfx,False,,0,,31,statistics,54935,,"Hello Everyone, what statistics book would you recommend for a beginner with no math background? I was thinking about getting ""Statistics"" by Freedman and Pisani. Is it a good choice? I'd really love it to have lots of examples because I learn best by doing exercises. Thank you!",https://www.reddit.com/r/statistics/comments/8c5hfx/hello_everyone_what_statistics_book_would_you/,all_ads,2018-04-14 09:05:09,49 days 16:30:25.036584000,
"Side note: I am an industrial Engineer with minimal/basic stats knowledge.

I work in a manufacturing plant. We make devices on production lines.

One of our lines is performing SO POORLY it’s insane. Instead of going to the line and gathering time studies, I pulled cycle data from our equipment to show average time from beginning of one unit to beginning of next, all the way down the line. Then I gathered the total count of units that took a range of intervals of time to complete to see on average, how long is it taking these units. (Like 600 units took 5-10 sec, 700 units took between 10-15, etc).

Looking at this data, I took the median 70% time intervals of where the cycles were highest to give me average cycle time.. that’s all well and good, and matches up with our previous expectations of timings on the line,,,. HOWEVER.

It totally doesn’t account for why we’re performing so shittily! So then, I separately just pulled all data in and said ok, anything greater than 5 mins I’ll throw out but I’ll look at everything else including outliers. BOOM. It showed a cycle time that jives with actual output performance.

Just came here to ask, is that method (including outliers) considered legitimate in order to explain our poor performance? Or am I looking at it wrong? I’ve been scratching my head on this one and could use some input. Thanks :) ",6,1523779277.0,8cbk0j,False,"Side note: I am an industrial Engineer with minimal/basic stats knowledge.

I work in a manufacturing plant. We make devices on production lines.

One of our lines is performing SO POORLY it’s insane. Instead of going to the line and gathering time studies, I pulled cycle data from our equipment to show average time from beginning of one unit to beginning of next, all the way down the line. Then I gathered the total count of units that took a range of intervals of time to complete to see on average, how long is it taking these units. (Like 600 units took 5-10 sec, 700 units took between 10-15, etc).

Looking at this data, I took the median 70% time intervals of where the cycles were highest to give me average cycle time.. that’s all well and good, and matches up with our previous expectations of timings on the line,,,. HOWEVER.

It totally doesn’t account for why we’re performing so shittily! So then, I separately just pulled all data in and said ok, anything greater than 5 mins I’ll throw out but I’ll look at everything else including outliers. BOOM. It showed a cycle time that jives with actual output performance.

Just came here to ask, is that method (including outliers) considered legitimate in order to explain our poor performance? Or am I looking at it wrong? I’ve been scratching my head on this one and could use some input. Thanks :) ",0,"Side note: I am an industrial Engineer with minimal/basic stats knowledge.

I work in a manufacturing plant. We make devices on production lines.

One of our lines is performing SO POORLY it’s insane. Instead of going to the line and gathering time studies, I pulled cycle data from our equipment to show average time from beginning of one unit to beginning of next, all the way down the line. Then I gathered the total count of units that took a range of intervals of time to complete to see on average, how long is it taking these units. (Like 600 units took 5-10 sec, 700 units took between 10-15, etc).

Looking at this data, I took the median 70% time intervals of where the cycles were highest to give me average cycle time.. that’s all well and good, and matches up with our previous expectations of timings on the line,,,. HOWEVER.

It totally doesn’t account for why we’re performing so shittily! So then, I separately just pulled all data in and said ok, anything greater than 5 mins I’ll throw out but I’ll look at everything else including outliers. BOOM. It showed a cycle time that jives with actual output performance.

Just came here to ask, is that method (including outliers) considered legitimate in order to explain our poor performance? Or am I looking at it wrong? I’ve been scratching my head on this one and could use some input. Thanks :) ",1,statistics,54935,,Question about data I’m (attempting) to analyze.,https://www.reddit.com/r/statistics/comments/8cbk0j/question_about_data_im_attempting_to_analyze/,all_ads,2018-04-15 04:01:17,48 days 21:34:17.036584000,
"Let's say I own a company responsible for maintaining all of the bridges in Europe. Let's say there are 100,000 bridges.

One of my staff, Sarah, maintains bridges in Ulm. She has a geographic area of 50km^2 and on average she drives 20 minutes from bridge to bridge. Germany requires tip-top bridges so these need to be inspected every 6 months. Sarah has a variety of bridges to look at, but they are largely simple to inspect. She is responsible for 500 bridges. Sarah likes her job but 10% of Ulm's bridges are very old and she spends lots of time requesting repairs on specific ones.

Gerald is in a different situation. He is only responsible for 100 bridges. He maintains them in Scotland, and the government don't care whether their bridges fall down. So he only has to inspect them once a year. But the bridges are more complicated, mainly large suspension bridges, and it takes him much longer to check them. The travel distance is larger because Scotland is more sparsely populated than Ulm, so it takes him 2 hours on average to travel from bridge to bridge. Scotland's bridges are all new so he doesn't have to request repairs very often.

Everything about their job, which bridges, what issues, etc, gets recorded in a multi-gigabyte database of high quality data.

Currently, as the company boss, I have 2,000 staff, and each of their geographic areas of responsibility have been chosen by a finger-in-the-air method, in a meeting between senior people the day before the contract started. Now, I want to employ a statistical or algorithmic process to decide how many people I need and which bridges to give to each person.

- Number of bridges
- Size of geographic area
- Different inspection frequencies
- Different issues of complexity
- Other issues like weather and repairs

I have shockingly just resigned at the company, and now you're the boss. 

You have two tasks: 1. decide how many staff you need. 2. Subdivide Europe into a series of polygons and assign the bridges within each polygon to each member of staff in the most optimal way possible, balancing the interests of each of the variables.

How would you approach solving this?",6,1523774071.0,8cb1ec,False,"Let's say I own a company responsible for maintaining all of the bridges in Europe. Let's say there are 100,000 bridges.

One of my staff, Sarah, maintains bridges in Ulm. She has a geographic area of 50km^2 and on average she drives 20 minutes from bridge to bridge. Germany requires tip-top bridges so these need to be inspected every 6 months. Sarah has a variety of bridges to look at, but they are largely simple to inspect. She is responsible for 500 bridges. Sarah likes her job but 10% of Ulm's bridges are very old and she spends lots of time requesting repairs on specific ones.

Gerald is in a different situation. He is only responsible for 100 bridges. He maintains them in Scotland, and the government don't care whether their bridges fall down. So he only has to inspect them once a year. But the bridges are more complicated, mainly large suspension bridges, and it takes him much longer to check them. The travel distance is larger because Scotland is more sparsely populated than Ulm, so it takes him 2 hours on average to travel from bridge to bridge. Scotland's bridges are all new so he doesn't have to request repairs very often.

Everything about their job, which bridges, what issues, etc, gets recorded in a multi-gigabyte database of high quality data.

Currently, as the company boss, I have 2,000 staff, and each of their geographic areas of responsibility have been chosen by a finger-in-the-air method, in a meeting between senior people the day before the contract started. Now, I want to employ a statistical or algorithmic process to decide how many people I need and which bridges to give to each person.

- Number of bridges
- Size of geographic area
- Different inspection frequencies
- Different issues of complexity
- Other issues like weather and repairs

I have shockingly just resigned at the company, and now you're the boss. 

You have two tasks: 1. decide how many staff you need. 2. Subdivide Europe into a series of polygons and assign the bridges within each polygon to each member of staff in the most optimal way possible, balancing the interests of each of the variables.

How would you approach solving this?",0,"Let's say I own a company responsible for maintaining all of the bridges in Europe. Let's say there are 100,000 bridges.

One of my staff, Sarah, maintains bridges in Ulm. She has a geographic area of 50km^2 and on average she drives 20 minutes from bridge to bridge. Germany requires tip-top bridges so these need to be inspected every 6 months. Sarah has a variety of bridges to look at, but they are largely simple to inspect. She is responsible for 500 bridges. Sarah likes her job but 10% of Ulm's bridges are very old and she spends lots of time requesting repairs on specific ones.

Gerald is in a different situation. He is only responsible for 100 bridges. He maintains them in Scotland, and the government don't care whether their bridges fall down. So he only has to inspect them once a year. But the bridges are more complicated, mainly large suspension bridges, and it takes him much longer to check them. The travel distance is larger because Scotland is more sparsely populated than Ulm, so it takes him 2 hours on average to travel from bridge to bridge. Scotland's bridges are all new so he doesn't have to request repairs very often.

Everything about their job, which bridges, what issues, etc, gets recorded in a multi-gigabyte database of high quality data.

Currently, as the company boss, I have 2,000 staff, and each of their geographic areas of responsibility have been chosen by a finger-in-the-air method, in a meeting between senior people the day before the contract started. Now, I want to employ a statistical or algorithmic process to decide how many people I need and which bridges to give to each person.

- Number of bridges
- Size of geographic area
- Different inspection frequencies
- Different issues of complexity
- Other issues like weather and repairs

I have shockingly just resigned at the company, and now you're the boss. 

You have two tasks: 1. decide how many staff you need. 2. Subdivide Europe into a series of polygons and assign the bridges within each polygon to each member of staff in the most optimal way possible, balancing the interests of each of the variables.

How would you approach solving this?",0,statistics,54935,,"Advice on how to solve my optimisation problem, or: how many staff do I need, and how are they best arranged?",https://www.reddit.com/r/statistics/comments/8cb1ec/advice_on_how_to_solve_my_optimisation_problem_or/,all_ads,2018-04-15 02:34:31,48 days 23:01:03.036584000,
"Hi all! I   just got done with the first iteration of my new financial asset   comparison tool, which allows for portfolio, return, and risk\-adjusted   return comparisons of a myriad of assets \(all equities \+ select   non\-tradition investments\).

I would   really appreciate your feedback! Please check out the walk\-through and   methodology at the GitHub page below, where you will also find a link to   the live hosted app. And, if you like it, please consider leaving it a   star on GitHub so that more folks will see it, critique it, and   improve   it! Thanks :\)

[https://github.com/pmaji/financial\-asset\-comparison\-tool](https://github.com/pmaji/financial-asset-comparison-tool)",0,1523753666.0,8c8s55,False,"Hi all! I   just got done with the first iteration of my new financial asset   comparison tool, which allows for portfolio, return, and risk\-adjusted   return comparisons of a myriad of assets \(all equities \+ select   non\-tradition investments\).

I would   really appreciate your feedback! Please check out the walk\-through and   methodology at the GitHub page below, where you will also find a link to   the live hosted app. And, if you like it, please consider leaving it a   star on GitHub so that more folks will see it, critique it, and   improve   it! Thanks :\)

[https://github.com/pmaji/financial\-asset\-comparison\-tool](https://github.com/pmaji/financial-asset-comparison-tool)",0,"Hi all! I   just got done with the first iteration of my new financial asset   comparison tool, which allows for portfolio, return, and risk\-adjusted   return comparisons of a myriad of assets \(all equities \+ select   non\-tradition investments\).

I would   really appreciate your feedback! Please check out the walk\-through and   methodology at the GitHub page below, where you will also find a link to   the live hosted app. And, if you like it, please consider leaving it a   star on GitHub so that more folks will see it, critique it, and   improve   it! Thanks :\)

[https://github.com/pmaji/financial\-asset\-comparison\-tool](https://github.com/pmaji/financial-asset-comparison-tool)",2,statistics,54935,,New Tool for Statistical Comparison of Financial Assets (Feedback Appreciated!),https://www.reddit.com/r/statistics/comments/8c8s55/new_tool_for_statistical_comparison_of_financial/,all_ads,2018-04-14 20:54:26,49 days 04:41:08.036584000,
"hi,

I'm currently in the middle of a research study in which i have 2 IVs:
gender on 2 levels (m/f)
a specific personality type on a continuous scale.

DV:
score on pathology scale (continuous).

I am looking at the interaction between gender and personality type and its effects on pathology score.

I initially ran correlations within each gender - for example females - personality type score vs pathology score, and then the same again for males. they were significant for females, and insignificant for males, which is what i expected to see.

however, after running a hierarchical multiple regression, with the first model with just gender, second model with the addition of personality type, and the third including the first two plus an additional interaction variable, only gender showed significance. 

i am not sure how to interpret this, and why personality type is not a significant predictor of pathology score in the regression when it is controlled for gender. surely, when running the correlations i have controlled for gender?
I was wondering if this was a type 1 error issue? 

i am sorry if this a really simple question! i'm really rusty with stats currently.  


",3,1523739871.0,8c7esq,False,"hi,

I'm currently in the middle of a research study in which i have 2 IVs:
gender on 2 levels (m/f)
a specific personality type on a continuous scale.

DV:
score on pathology scale (continuous).

I am looking at the interaction between gender and personality type and its effects on pathology score.

I initially ran correlations within each gender - for example females - personality type score vs pathology score, and then the same again for males. they were significant for females, and insignificant for males, which is what i expected to see.

however, after running a hierarchical multiple regression, with the first model with just gender, second model with the addition of personality type, and the third including the first two plus an additional interaction variable, only gender showed significance. 

i am not sure how to interpret this, and why personality type is not a significant predictor of pathology score in the regression when it is controlled for gender. surely, when running the correlations i have controlled for gender?
I was wondering if this was a type 1 error issue? 

i am sorry if this a really simple question! i'm really rusty with stats currently.  


",0,"hi,

I'm currently in the middle of a research study in which i have 2 IVs:
gender on 2 levels (m/f)
a specific personality type on a continuous scale.

DV:
score on pathology scale (continuous).

I am looking at the interaction between gender and personality type and its effects on pathology score.

I initially ran correlations within each gender - for example females - personality type score vs pathology score, and then the same again for males. they were significant for females, and insignificant for males, which is what i expected to see.

however, after running a hierarchical multiple regression, with the first model with just gender, second model with the addition of personality type, and the third including the first two plus an additional interaction variable, only gender showed significance. 

i am not sure how to interpret this, and why personality type is not a significant predictor of pathology score in the regression when it is controlled for gender. surely, when running the correlations i have controlled for gender?
I was wondering if this was a type 1 error issue? 

i am sorry if this a really simple question! i'm really rusty with stats currently.  


",4,statistics,54935,,contradictory results,https://www.reddit.com/r/statistics/comments/8c7esq/contradictory_results/,all_ads,2018-04-14 17:04:31,49 days 08:31:03.036584000,
What’s the difference between identifying a study as retrospective cohort and case control/case series? I am confused because I thought case control and case series are two types under retrospective cohort? ,6,1523752117.0,8c8max,False,What’s the difference between identifying a study as retrospective cohort and case control/case series? I am confused because I thought case control and case series are two types under retrospective cohort? ,0,What’s the difference between identifying a study as retrospective cohort and case control/case series? I am confused because I thought case control and case series are two types under retrospective cohort? ,2,statistics,54935,,Retrospective cohort vs case control and case series,https://www.reddit.com/r/statistics/comments/8c8max/retrospective_cohort_vs_case_control_and_case/,all_ads,2018-04-14 20:28:37,49 days 05:06:57.036584000,
"Hello all, I am a student currently on placement at a manufacturing site and I'm using excel to create a report/presentation on the current stock levels among other things. I have over 500 unique items that are each made up of a range of 1 to 25 batches. I will be including the number of batches in the table.

A quick example of what I need:

Item A has a total quantity of 1000KG, each batch has 200KG of stock.

Item B has a total quantity of 1000KG also, but batch 1 and 2 makes up 800KG.

I was think of a number between 0-1 so like a percentage would be the best for a visual indicator, it doesn't need to be exactly technical so even a traffic light ranking system could work.

Just thought I would post it here to get any suggestions for you guys. Thanks :)

",0,1523763439.0,8c9vzl,False,"Hello all, I am a student currently on placement at a manufacturing site and I'm using excel to create a report/presentation on the current stock levels among other things. I have over 500 unique items that are each made up of a range of 1 to 25 batches. I will be including the number of batches in the table.

A quick example of what I need:

Item A has a total quantity of 1000KG, each batch has 200KG of stock.

Item B has a total quantity of 1000KG also, but batch 1 and 2 makes up 800KG.

I was think of a number between 0-1 so like a percentage would be the best for a visual indicator, it doesn't need to be exactly technical so even a traffic light ranking system could work.

Just thought I would post it here to get any suggestions for you guys. Thanks :)

",0,"Hello all, I am a student currently on placement at a manufacturing site and I'm using excel to create a report/presentation on the current stock levels among other things. I have over 500 unique items that are each made up of a range of 1 to 25 batches. I will be including the number of batches in the table.

A quick example of what I need:

Item A has a total quantity of 1000KG, each batch has 200KG of stock.

Item B has a total quantity of 1000KG also, but batch 1 and 2 makes up 800KG.

I was think of a number between 0-1 so like a percentage would be the best for a visual indicator, it doesn't need to be exactly technical so even a traffic light ranking system could work.

Just thought I would post it here to get any suggestions for you guys. Thanks :)

",1,statistics,54935,,"Have a table representing the total quantity for each item, but want a simple calculations to indicate the spread of amounts that make up the total quantity. Any suggestions?",https://www.reddit.com/r/statistics/comments/8c9vzl/have_a_table_representing_the_total_quantity_for/,all_ads,2018-04-14 23:37:19,49 days 01:58:15.036584000,
"As the title says, is there any popular ones? Ones that is widely used by professionals such as professors?

Thanks.",0,1523774394.0,8cb2lp,False,"As the title says, is there any popular ones? Ones that is widely used by professionals such as professors?

Thanks.",0,"As the title says, is there any popular ones? Ones that is widely used by professionals such as professors?

Thanks.",0,statistics,54935,,Is there MOSS-type plagiarism detector for R?,https://www.reddit.com/r/statistics/comments/8cb2lp/is_there_mosstype_plagiarism_detector_for_r/,all_ads,2018-04-15 02:39:54,48 days 22:55:40.036584000,
"I used to be a statistical analyst in the past and am now working as a data scientist. A good chunk of my job is cleaning data, presenting insight, and only a small percentage of my job is around modeling (~30%).

I have been looking for a new job as a statistician and was wondering the day to day work of statisticians is like?",4,1523689050.0,8c3e15,False,"I used to be a statistical analyst in the past and am now working as a data scientist. A good chunk of my job is cleaning data, presenting insight, and only a small percentage of my job is around modeling (~30%).

I have been looking for a new job as a statistician and was wondering the day to day work of statisticians is like?",0,"I used to be a statistical analyst in the past and am now working as a data scientist. A good chunk of my job is cleaning data, presenting insight, and only a small percentage of my job is around modeling (~30%).

I have been looking for a new job as a statistician and was wondering the day to day work of statisticians is like?",14,statistics,54935,,"For those with the job title of Statistician, what does your job involve?",https://www.reddit.com/r/statistics/comments/8c3e15/for_those_with_the_job_title_of_statistician_what/,all_ads,2018-04-14 02:57:30,49 days 22:38:04.036584000,
I have done the coursera course on ML and learnt the basics of sklearn for ML. I am currently reading Mitchell's ML book. I want to know more about the deep inner workings of the algorithms as I want to dive into research. Should I refer the ISLR or the ESL? I am skeptical about the ISLR as I don't wanna learn R + Dont want to re-learn concepts.,5,1523726846.0,8c6iy2,False,I have done the coursera course on ML and learnt the basics of sklearn for ML. I am currently reading Mitchell's ML book. I want to know more about the deep inner workings of the algorithms as I want to dive into research. Should I refer the ISLR or the ESL? I am skeptical about the ISLR as I don't wanna learn R + Dont want to re-learn concepts.,0,I have done the coursera course on ML and learnt the basics of sklearn for ML. I am currently reading Mitchell's ML book. I want to know more about the deep inner workings of the algorithms as I want to dive into research. Should I refer the ISLR or the ESL? I am skeptical about the ISLR as I don't wanna learn R + Dont want to re-learn concepts.,2,statistics,54935,,ISLR or ESL?,https://www.reddit.com/r/statistics/comments/8c6iy2/islr_or_esl/,all_ads,2018-04-14 13:27:26,49 days 12:08:08.036584000,
"I understand that A/B testing is essentially a re-branding of experiment design and statistical inference. I am looking for a good resource for learning this material, be it a book or a web course. 

Thanks for any advice. ",0,1523717667.0,8c5ybo,False,"I understand that A/B testing is essentially a re-branding of experiment design and statistical inference. I am looking for a good resource for learning this material, be it a book or a web course. 

Thanks for any advice. ",0,"I understand that A/B testing is essentially a re-branding of experiment design and statistical inference. I am looking for a good resource for learning this material, be it a book or a web course. 

Thanks for any advice. ",3,statistics,54935,,What is the best resource to learn about A/B testing?,https://www.reddit.com/r/statistics/comments/8c5ybo/what_is_the_best_resource_to_learn_about_ab/,all_ads,2018-04-14 10:54:27,49 days 14:41:07.036584000,
"It is very often that I read statements like ""Obese men are 19% more likely to *die*"" (Which may or may not be true, but I believe the example gives you a rough idea).

What does it mean, exactly? Does it mean that it's that much more likely to happen in a given time period? My confusion roots from the fact that we're all just as likely to die as each other - 100%, if a time period is not concerned.

If someone could clear this very-trivial confusion of mine, I'd be very grateful. Thank you in advance!",5,1523711650.0,8c5iyk,False,"It is very often that I read statements like ""Obese men are 19% more likely to *die*"" (Which may or may not be true, but I believe the example gives you a rough idea).

What does it mean, exactly? Does it mean that it's that much more likely to happen in a given time period? My confusion roots from the fact that we're all just as likely to die as each other - 100%, if a time period is not concerned.

If someone could clear this very-trivial confusion of mine, I'd be very grateful. Thank you in advance!",0,"It is very often that I read statements like ""Obese men are 19% more likely to *die*"" (Which may or may not be true, but I believe the example gives you a rough idea).

What does it mean, exactly? Does it mean that it's that much more likely to happen in a given time period? My confusion roots from the fact that we're all just as likely to die as each other - 100%, if a time period is not concerned.

If someone could clear this very-trivial confusion of mine, I'd be very grateful. Thank you in advance!",2,statistics,54935,,"What does 'probability of death' indicate? (If this is not the right sub. to ask this question on, I'll gladly move the post)",https://www.reddit.com/r/statistics/comments/8c5iyk/what_does_probability_of_death_indicate_if_this/,all_ads,2018-04-14 09:14:10,49 days 16:21:24.036584000,
"so lets say I'm rolling for for a certain percent on 100 sided dice say 65 percent to succeed, but i have to choose whether or not the roll is the low side 1-35 fails or the high side of the dice so 65 to 100 fails. does flipping a coin with heads being high and tails being low before hand decrease the probability that this succeeds?

edit* so I'm more talking like double pass so take a marble rolling strait towards a tunnel it has a 50% chance to have a tree fall on it and a 35% chance for the tunnel to collapse in its way. so it what is the percent chance that the marble makes it through
",5,1523732706.0,8c6vv8,False,"so lets say I'm rolling for for a certain percent on 100 sided dice say 65 percent to succeed, but i have to choose whether or not the roll is the low side 1-35 fails or the high side of the dice so 65 to 100 fails. does flipping a coin with heads being high and tails being low before hand decrease the probability that this succeeds?

edit* so I'm more talking like double pass so take a marble rolling strait towards a tunnel it has a 50% chance to have a tree fall on it and a 35% chance for the tunnel to collapse in its way. so it what is the percent chance that the marble makes it through
",0,"so lets say I'm rolling for for a certain percent on 100 sided dice say 65 percent to succeed, but i have to choose whether or not the roll is the low side 1-35 fails or the high side of the dice so 65 to 100 fails. does flipping a coin with heads being high and tails being low before hand decrease the probability that this succeeds?

edit* so I'm more talking like double pass so take a marble rolling strait towards a tunnel it has a 50% chance to have a tree fall on it and a 35% chance for the tunnel to collapse in its way. so it what is the percent chance that the marble makes it through
",1,statistics,54935,,Flipping a coin and rolling a die,https://www.reddit.com/r/statistics/comments/8c6vv8/flipping_a_coin_and_rolling_a_die/,all_ads,2018-04-14 15:05:06,49 days 10:30:28.036584000,
https://twitter.com/twitter/statuses/960558261190606848,5,1523719384.0,8c62gp,False,https://twitter.com/twitter/statuses/960558261190606848,0,https://twitter.com/twitter/statuses/960558261190606848,1,statistics,54935,,What do you call this sort of chart?,https://www.reddit.com/r/statistics/comments/8c62gp/what_do_you_call_this_sort_of_chart/,all_ads,2018-04-14 11:23:04,49 days 14:12:30.036584000,
Is there a significant difference? It seems to me like data science is mostly just a sexy name for machine learning focused statistics.,6,1523703646.0,8c4ux1,False,Is there a significant difference? It seems to me like data science is mostly just a sexy name for machine learning focused statistics.,0,Is there a significant difference? It seems to me like data science is mostly just a sexy name for machine learning focused statistics.,2,statistics,54935,,Statistics vs Data science,https://www.reddit.com/r/statistics/comments/8c4ux1/statistics_vs_data_science/,all_ads,2018-04-14 07:00:46,49 days 18:34:48.036584000,
"I have a large 20+ variable dataset and I don’t even know where to begin, I feel overwhelmed just because I’m not experienced enough to know exactly where to start . In the end I want to graphically visualize the data but how to start....",20,1523694582.0,8c3ykh,False,"I have a large 20+ variable dataset and I don’t even know where to begin, I feel overwhelmed just because I’m not experienced enough to know exactly where to start . In the end I want to graphically visualize the data but how to start....",0,"I have a large 20+ variable dataset and I don’t even know where to begin, I feel overwhelmed just because I’m not experienced enough to know exactly where to start . In the end I want to graphically visualize the data but how to start....",3,statistics,54935,,"With a large dataset (I’m talking millions) , what’s the best approach to being analyzing the data?",https://www.reddit.com/r/statistics/comments/8c3ykh/with_a_large_dataset_im_talking_millions_whats/,all_ads,2018-04-14 04:29:42,49 days 21:05:52.036584000,
"Not sure if this is the right place to ask, but is statistics a degree with low roi on its own in the first place or am i job hunting the wrong way? 

I have a diploma in biomedical science, bachelors in statistics and a soon to be fresh grad masters in biostatistics in Canada in two weeks. Both my degrees are from canada and my diploma is from Singapore. I have been looking for jobs for about a month or so beforehand and I have been getting rejection letters. I have been applying to positions such as data scientist, statistical programmer analyst, statistician, junior statistician, biostatistician, etc on job sites.  

But all I've received were rejection letters without even a chance at an interview. I have 8 month work experience with a research institute but all other places seem to want so much more. 

I understand networking is important as well and I have been doing so when I was on my work term. It makes me start to wonder the value of statistics as an STEM degree. It definitely is not an easy major. There will be hours where I spend writing code or troubleshooting code and questioning the results of my simulation/analyses. 
The job prospects for a statistics degree doesnt seem to be fantastic either.  R is my main language but I know SAS and python to a certain extent and have gone on to do a few certifications myself in SAS so i dont think im that shitty. 

Do I really need a phd in this field or am i jobhunting the wrong way? i know applying online doesnt help much, u either go through HR or a computer to sieve out applicants and meeting people helps more. but the one time i did meet someone (a hiring manager) and then subsequently applied a job from there (job requires bachelors with 2 to 3 years work experience) , he told me he preferred applicants with work experience but might give me an interview depending on the pool of the applicants (ie if they are same level or shittier than you, we might give u an interview)... 

What was your job hunt experience in this field like, esp if u are in canada? 

 
",20,1523654578.0,8bz43n,False,"Not sure if this is the right place to ask, but is statistics a degree with low roi on its own in the first place or am i job hunting the wrong way? 

I have a diploma in biomedical science, bachelors in statistics and a soon to be fresh grad masters in biostatistics in Canada in two weeks. Both my degrees are from canada and my diploma is from Singapore. I have been looking for jobs for about a month or so beforehand and I have been getting rejection letters. I have been applying to positions such as data scientist, statistical programmer analyst, statistician, junior statistician, biostatistician, etc on job sites.  

But all I've received were rejection letters without even a chance at an interview. I have 8 month work experience with a research institute but all other places seem to want so much more. 

I understand networking is important as well and I have been doing so when I was on my work term. It makes me start to wonder the value of statistics as an STEM degree. It definitely is not an easy major. There will be hours where I spend writing code or troubleshooting code and questioning the results of my simulation/analyses. 
The job prospects for a statistics degree doesnt seem to be fantastic either.  R is my main language but I know SAS and python to a certain extent and have gone on to do a few certifications myself in SAS so i dont think im that shitty. 

Do I really need a phd in this field or am i jobhunting the wrong way? i know applying online doesnt help much, u either go through HR or a computer to sieve out applicants and meeting people helps more. but the one time i did meet someone (a hiring manager) and then subsequently applied a job from there (job requires bachelors with 2 to 3 years work experience) , he told me he preferred applicants with work experience but might give me an interview depending on the pool of the applicants (ie if they are same level or shittier than you, we might give u an interview)... 

What was your job hunt experience in this field like, esp if u are in canada? 

 
",0,"Not sure if this is the right place to ask, but is statistics a degree with low roi on its own in the first place or am i job hunting the wrong way? 

I have a diploma in biomedical science, bachelors in statistics and a soon to be fresh grad masters in biostatistics in Canada in two weeks. Both my degrees are from canada and my diploma is from Singapore. I have been looking for jobs for about a month or so beforehand and I have been getting rejection letters. I have been applying to positions such as data scientist, statistical programmer analyst, statistician, junior statistician, biostatistician, etc on job sites.  

But all I've received were rejection letters without even a chance at an interview. I have 8 month work experience with a research institute but all other places seem to want so much more. 

I understand networking is important as well and I have been doing so when I was on my work term. It makes me start to wonder the value of statistics as an STEM degree. It definitely is not an easy major. There will be hours where I spend writing code or troubleshooting code and questioning the results of my simulation/analyses. 
The job prospects for a statistics degree doesnt seem to be fantastic either.  R is my main language but I know SAS and python to a certain extent and have gone on to do a few certifications myself in SAS so i dont think im that shitty. 

Do I really need a phd in this field or am i jobhunting the wrong way? i know applying online doesnt help much, u either go through HR or a computer to sieve out applicants and meeting people helps more. but the one time i did meet someone (a hiring manager) and then subsequently applied a job from there (job requires bachelors with 2 to 3 years work experience) , he told me he preferred applicants with work experience but might give me an interview depending on the pool of the applicants (ie if they are same level or shittier than you, we might give u an interview)... 

What was your job hunt experience in this field like, esp if u are in canada? 

 
",13,statistics,54935,,How to job hunt correctly in this field?,https://www.reddit.com/r/statistics/comments/8bz43n/how_to_job_hunt_correctly_in_this_field/,all_ads,2018-04-13 17:22:58,50 days 08:12:36.036584000,
"I have a complex problem but the title sums it up pretty easily.

Long story short: 

I have four types of cages that manipulate water flow, but I also have an actual measure of water flow from inside the cages. I'm wondering if I can just use one or the other, if I should include both, or if I should nest them.

The best fit seems to be with just the actual measure of flow, but if I use them both, the cage type has a significant effect.

Any tips?",19,1523668148.0,8c0tbb,False,"I have a complex problem but the title sums it up pretty easily.

Long story short: 

I have four types of cages that manipulate water flow, but I also have an actual measure of water flow from inside the cages. I'm wondering if I can just use one or the other, if I should include both, or if I should nest them.

The best fit seems to be with just the actual measure of flow, but if I use them both, the cage type has a significant effect.

Any tips?",0,"I have a complex problem but the title sums it up pretty easily.

Long story short: 

I have four types of cages that manipulate water flow, but I also have an actual measure of water flow from inside the cages. I'm wondering if I can just use one or the other, if I should include both, or if I should nest them.

The best fit seems to be with just the actual measure of flow, but if I use them both, the cage type has a significant effect.

Any tips?",6,statistics,54935,,"Can you remove factors from a model if they have a significant effect, but their removal improves AIC and R square?",https://www.reddit.com/r/statistics/comments/8c0tbb/can_you_remove_factors_from_a_model_if_they_have/,all_ads,2018-04-13 21:09:08,50 days 04:26:26.036584000,
"BI != Statistics. An ""interesting"" perspective from SD Times: https://sdtimes.com/data/analyst-view-demise-statistician/

Shared not for agreement, but interested in the discussion.",25,1523669001.0,8c0xbr,False,"BI != Statistics. An ""interesting"" perspective from SD Times: https://sdtimes.com/data/analyst-view-demise-statistician/

Shared not for agreement, but interested in the discussion.",0,"BI != Statistics. An ""interesting"" perspective from SD Times: https://sdtimes.com/data/analyst-view-demise-statistician/

Shared not for agreement, but interested in the discussion.",4,statistics,54935,,The demise of the statistician,https://www.reddit.com/r/statistics/comments/8c0xbr/the_demise_of_the_statistician/,all_ads,2018-04-13 21:23:21,50 days 04:12:13.036584000,
"Five human subjects (A-E) listened to the same ten songs (1-10) and adjusted three parameters (a-c) in response for a total of 150 data points. My thesis advisor told me to separate this data into three one-way ANOVAs of 50 data points each, one per adjusted parameter.

I have the XLStat plugin for MS Excel but I'm not sure what to plug into the prompt. The default parameters for all three values would just be 0.",16,1523682289.0,8c2m5i,False,"Five human subjects (A-E) listened to the same ten songs (1-10) and adjusted three parameters (a-c) in response for a total of 150 data points. My thesis advisor told me to separate this data into three one-way ANOVAs of 50 data points each, one per adjusted parameter.

I have the XLStat plugin for MS Excel but I'm not sure what to plug into the prompt. The default parameters for all three values would just be 0.",0,"Five human subjects (A-E) listened to the same ten songs (1-10) and adjusted three parameters (a-c) in response for a total of 150 data points. My thesis advisor told me to separate this data into three one-way ANOVAs of 50 data points each, one per adjusted parameter.

I have the XLStat plugin for MS Excel but I'm not sure what to plug into the prompt. The default parameters for all three values would just be 0.",2,statistics,54935,,Is an ANOVA necessary?,https://www.reddit.com/r/statistics/comments/8c2m5i/is_an_anova_necessary/,all_ads,2018-04-14 01:04:49,50 days 00:30:45.036584000,
"Are there any textbooks or manuals on this type of research? Particularly with Medicare? It seems like all the data is claim-based (duh) and truly calculating the TRUE costs seems to be a bit complicated due to things like (i.e. staff utilization, pharmacy, etc.). In the past, I would look at CCR and so forth...

Thanks. Sorry if this is a dumb question.",10,1523662791.0,8c040f,False,"Are there any textbooks or manuals on this type of research? Particularly with Medicare? It seems like all the data is claim-based (duh) and truly calculating the TRUE costs seems to be a bit complicated due to things like (i.e. staff utilization, pharmacy, etc.). In the past, I would look at CCR and so forth...

Thanks. Sorry if this is a dumb question.",0,"Are there any textbooks or manuals on this type of research? Particularly with Medicare? It seems like all the data is claim-based (duh) and truly calculating the TRUE costs seems to be a bit complicated due to things like (i.e. staff utilization, pharmacy, etc.). In the past, I would look at CCR and so forth...

Thanks. Sorry if this is a dumb question.",5,statistics,54935,,Those within the health care industry... Issues with calculating provider expenses (or costs for giving care) based on dx or cm code?,https://www.reddit.com/r/statistics/comments/8c040f/those_within_the_health_care_industry_issues_with/,all_ads,2018-04-13 19:39:51,50 days 05:55:43.036584000,
"I'm doing a project in which I compare two sets of papers, one of 25 pages, the other of 140 pages, for mentions of a certain phrase. for example, any instance of a 'blue' phrase would be marked down, and the total number of blue phrases would be divided by page number to get the total number of blue mentions per page on both the 25 page document and the 140 page document. 

I am trying to find a test (and ideally, an online calculator for said test) to determine if there is a statistically significant (95%) difference in the number of mentions in each category between the two documents.

Data I have: total pages in each document

Total mentions of each phrase in each document (9 total phrases)

Average mentions of each phrase per page in each document (the things I want to test significance for)",2,1523689300.0,8c3f0i,False,"I'm doing a project in which I compare two sets of papers, one of 25 pages, the other of 140 pages, for mentions of a certain phrase. for example, any instance of a 'blue' phrase would be marked down, and the total number of blue phrases would be divided by page number to get the total number of blue mentions per page on both the 25 page document and the 140 page document. 

I am trying to find a test (and ideally, an online calculator for said test) to determine if there is a statistically significant (95%) difference in the number of mentions in each category between the two documents.

Data I have: total pages in each document

Total mentions of each phrase in each document (9 total phrases)

Average mentions of each phrase per page in each document (the things I want to test significance for)",0,"I'm doing a project in which I compare two sets of papers, one of 25 pages, the other of 140 pages, for mentions of a certain phrase. for example, any instance of a 'blue' phrase would be marked down, and the total number of blue phrases would be divided by page number to get the total number of blue mentions per page on both the 25 page document and the 140 page document. 

I am trying to find a test (and ideally, an online calculator for said test) to determine if there is a statistically significant (95%) difference in the number of mentions in each category between the two documents.

Data I have: total pages in each document

Total mentions of each phrase in each document (9 total phrases)

Average mentions of each phrase per page in each document (the things I want to test significance for)",0,statistics,54935,,Looking for appropriate significance test,https://www.reddit.com/r/statistics/comments/8c3f0i/looking_for_appropriate_significance_test/,all_ads,2018-04-14 03:01:40,49 days 22:33:54.036584000,
"A project I've been working on recently has got me thinking about how to better fit distributions to data - specifically when the data doesn't fit any known distribution very well.

For anyone familiar with R, there's a function in the fitdistplus package called decsdist() which produces a graph showing where your data set lies in relation to the gamma, lognormal, normal, uniform, beta, weibull, and exponential functions.

If a dataset were to be within the field of the beta distribution but no where near close to any other distribution, would it be feasible to fit the data to the beta distribution by dividing the dataset by the maximum value in the set (to get them in values from 0-1, as is necessary for the beda distribution) and then multiply the numbers generated by the fit beta distribution by that maximum number?

Something is telling me that it shouldn't work this way, as the dividing/multiplication wouldn't work that way due to there being the step of distribution fitting between the two. If it is possible, are there any necessary assumptions or precautions that need to be made/taken to assure that this transformation will produce numbers in a distribution similar to the dataset?

Purely educational, I won't really be able to use the information unless a future job requires it - thanks in advance!",4,1523652321.0,8byvcu,False,"A project I've been working on recently has got me thinking about how to better fit distributions to data - specifically when the data doesn't fit any known distribution very well.

For anyone familiar with R, there's a function in the fitdistplus package called decsdist() which produces a graph showing where your data set lies in relation to the gamma, lognormal, normal, uniform, beta, weibull, and exponential functions.

If a dataset were to be within the field of the beta distribution but no where near close to any other distribution, would it be feasible to fit the data to the beta distribution by dividing the dataset by the maximum value in the set (to get them in values from 0-1, as is necessary for the beda distribution) and then multiply the numbers generated by the fit beta distribution by that maximum number?

Something is telling me that it shouldn't work this way, as the dividing/multiplication wouldn't work that way due to there being the step of distribution fitting between the two. If it is possible, are there any necessary assumptions or precautions that need to be made/taken to assure that this transformation will produce numbers in a distribution similar to the dataset?

Purely educational, I won't really be able to use the information unless a future job requires it - thanks in advance!",0,"A project I've been working on recently has got me thinking about how to better fit distributions to data - specifically when the data doesn't fit any known distribution very well.

For anyone familiar with R, there's a function in the fitdistplus package called decsdist() which produces a graph showing where your data set lies in relation to the gamma, lognormal, normal, uniform, beta, weibull, and exponential functions.

If a dataset were to be within the field of the beta distribution but no where near close to any other distribution, would it be feasible to fit the data to the beta distribution by dividing the dataset by the maximum value in the set (to get them in values from 0-1, as is necessary for the beda distribution) and then multiply the numbers generated by the fit beta distribution by that maximum number?

Something is telling me that it shouldn't work this way, as the dividing/multiplication wouldn't work that way due to there being the step of distribution fitting between the two. If it is possible, are there any necessary assumptions or precautions that need to be made/taken to assure that this transformation will produce numbers in a distribution similar to the dataset?

Purely educational, I won't really be able to use the information unless a future job requires it - thanks in advance!",7,statistics,54935,,"Question on manipulating data for distribution fitting, and distributed random number generator not",https://www.reddit.com/r/statistics/comments/8byvcu/question_on_manipulating_data_for_distribution/,all_ads,2018-04-13 16:45:21,50 days 08:50:13.036584000,
"I will be collecting data on 3 IVs: participant gender (M/F), task dependence (high/low), and task variability (high/low). I'll also ask participants about leadership preferences, which is a higher-order construct comprised of five sub-dimensions (autocratic behavior, democratic behavior, positive feedback, training advice, and social support), all of which are continuous. If I'm specifically interested in:    
    
1. the relationship between gender and autocratic behavior    
    
2. task dependence and positive feedback    
    
3. task variability and social support    
    
Which type(s) of statistical tests/analysis is(are) appropriate? Thanks for any info and suggestions you might be able to offer. ",3,1523684214.0,8c2u7i,False,"I will be collecting data on 3 IVs: participant gender (M/F), task dependence (high/low), and task variability (high/low). I'll also ask participants about leadership preferences, which is a higher-order construct comprised of five sub-dimensions (autocratic behavior, democratic behavior, positive feedback, training advice, and social support), all of which are continuous. If I'm specifically interested in:    
    
1. the relationship between gender and autocratic behavior    
    
2. task dependence and positive feedback    
    
3. task variability and social support    
    
Which type(s) of statistical tests/analysis is(are) appropriate? Thanks for any info and suggestions you might be able to offer. ",0,"I will be collecting data on 3 IVs: participant gender (M/F), task dependence (high/low), and task variability (high/low). I'll also ask participants about leadership preferences, which is a higher-order construct comprised of five sub-dimensions (autocratic behavior, democratic behavior, positive feedback, training advice, and social support), all of which are continuous. If I'm specifically interested in:    
    
1. the relationship between gender and autocratic behavior    
    
2. task dependence and positive feedback    
    
3. task variability and social support    
    
Which type(s) of statistical tests/analysis is(are) appropriate? Thanks for any info and suggestions you might be able to offer. ",0,statistics,54935,,What type of statistical analysis is appropriate?,https://www.reddit.com/r/statistics/comments/8c2u7i/what_type_of_statistical_analysis_is_appropriate/,all_ads,2018-04-14 01:36:54,49 days 23:58:40.036584000,
"I want to know if wearing a hat will decrease conversation duration. I have collected two sets of data in minutes. The first set of data is from not wearing a hat and the other set of data is wearing a hat. Can I do a hypothesis test with this data?

Not wearing a hat|Wearing a hat|
:--|:--|
7,8,9,6,5,6,3,7,8,11,3,4,6,7,9|4,6,3,2,3,5,7,5,4,3,2,8,2,4,5|
||

I'm not very good at this, but I was hoping to get some help.",3,1523677708.0,8c21lc,False,"I want to know if wearing a hat will decrease conversation duration. I have collected two sets of data in minutes. The first set of data is from not wearing a hat and the other set of data is wearing a hat. Can I do a hypothesis test with this data?

Not wearing a hat|Wearing a hat|
:--|:--|
7,8,9,6,5,6,3,7,8,11,3,4,6,7,9|4,6,3,2,3,5,7,5,4,3,2,8,2,4,5|
||

I'm not very good at this, but I was hoping to get some help.",0,"I want to know if wearing a hat will decrease conversation duration. I have collected two sets of data in minutes. The first set of data is from not wearing a hat and the other set of data is wearing a hat. Can I do a hypothesis test with this data?

Not wearing a hat|Wearing a hat|
:--|:--|
7,8,9,6,5,6,3,7,8,11,3,4,6,7,9|4,6,3,2,3,5,7,5,4,3,2,8,2,4,5|
||

I'm not very good at this, but I was hoping to get some help.",0,statistics,54935,,Help on project,https://www.reddit.com/r/statistics/comments/8c21lc/help_on_project/,all_ads,2018-04-13 23:48:28,50 days 01:47:06.036584000,
"Hello everyone, I am a recent college graduate and have been applying to a lot of entry level data analyst roles with not much luck. I wanted to ask what other kinds of jobs should I be applying to that look for a statistics degree?",24,1523605481.0,8bust8,False,"Hello everyone, I am a recent college graduate and have been applying to a lot of entry level data analyst roles with not much luck. I wanted to ask what other kinds of jobs should I be applying to that look for a statistics degree?",0,"Hello everyone, I am a recent college graduate and have been applying to a lot of entry level data analyst roles with not much luck. I wanted to ask what other kinds of jobs should I be applying to that look for a statistics degree?",24,statistics,54935,,Jobs for someone with a bachelors degree in statistics,https://www.reddit.com/r/statistics/comments/8bust8/jobs_for_someone_with_a_bachelors_degree_in/,all_ads,2018-04-13 03:44:41,50 days 21:50:53.036584000,
"I’m soon done with my undergrad in statistics and will proceed with grad school shortly thereafter. My university has focused on SAS as the main programming software. After looking at different job descriptions I rarely encounter ones who require proficiency in SAS. This is making me slightly worried.

Is SAS a relevant tool to have in your toolbox as a statistician/data scientist and which kind of employers mainly use SAS?

I have tried Python for a bit but I’m far from being able to do anything relevant. 

At which level och proficiency in Python/R would make it viable to work with as a data scientist? Does one need advanced programming skills or intermediate or something in between?
",48,1523628166.0,8bwzkn,False,"I’m soon done with my undergrad in statistics and will proceed with grad school shortly thereafter. My university has focused on SAS as the main programming software. After looking at different job descriptions I rarely encounter ones who require proficiency in SAS. This is making me slightly worried.

Is SAS a relevant tool to have in your toolbox as a statistician/data scientist and which kind of employers mainly use SAS?

I have tried Python for a bit but I’m far from being able to do anything relevant. 

At which level och proficiency in Python/R would make it viable to work with as a data scientist? Does one need advanced programming skills or intermediate or something in between?
",0,"I’m soon done with my undergrad in statistics and will proceed with grad school shortly thereafter. My university has focused on SAS as the main programming software. After looking at different job descriptions I rarely encounter ones who require proficiency in SAS. This is making me slightly worried.

Is SAS a relevant tool to have in your toolbox as a statistician/data scientist and which kind of employers mainly use SAS?

I have tried Python for a bit but I’m far from being able to do anything relevant. 

At which level och proficiency in Python/R would make it viable to work with as a data scientist? Does one need advanced programming skills or intermediate or something in between?
",4,statistics,54935,,Jobs availible with SAS?,https://www.reddit.com/r/statistics/comments/8bwzkn/jobs_availible_with_sas/,all_ads,2018-04-13 10:02:46,50 days 15:32:48.036584000,
"Hi, so I am a Electrical Engineering major interested in Digital Signal Processing and plan to go into neuroimaging. I have heard that Statistics is really important for this. I just finished my first intro stats course and we worked mainly in excel.

Basically I just want to know which courses are worth taking and which are not. 


My department has a a decent amount of Stats courses. Most they say are very applied and meant to help get jobs, but I have no idea. The next course people tend to take after this one is the intermediate stats class. The only prereq is the intro class, no calc or anything. It introduces R programming. 

The next course in this sequence is advanced linear regression which also doesn't require any Calc. Just the intermediate course. 

We also offer a course in Experimental Design, Statistical Consulting, and a Data Wrangling and Visualization course.

We just have 2 theory courses though. A upper division Probability course and a Baysian Stats course. These both require Calc 3 and a intro Stats course. 


Intermediate stats: 
""Z tests, t tests and chi-squared tests as well as their nonparametric equivalents, ANOVA for one or more factors, multiple linear regression, and logistic regression--all using the R software.""

Advanced Linear Regression:
""This course is designed to explore fundamentals of regression, theory of regression models, residuals and residual analysis, multiple regression, remediation, transformations, and ANOVA. Students will use R for statistical analyses.""

",9,1523626212.0,8bwu15,False,"Hi, so I am a Electrical Engineering major interested in Digital Signal Processing and plan to go into neuroimaging. I have heard that Statistics is really important for this. I just finished my first intro stats course and we worked mainly in excel.

Basically I just want to know which courses are worth taking and which are not. 


My department has a a decent amount of Stats courses. Most they say are very applied and meant to help get jobs, but I have no idea. The next course people tend to take after this one is the intermediate stats class. The only prereq is the intro class, no calc or anything. It introduces R programming. 

The next course in this sequence is advanced linear regression which also doesn't require any Calc. Just the intermediate course. 

We also offer a course in Experimental Design, Statistical Consulting, and a Data Wrangling and Visualization course.

We just have 2 theory courses though. A upper division Probability course and a Baysian Stats course. These both require Calc 3 and a intro Stats course. 


Intermediate stats: 
""Z tests, t tests and chi-squared tests as well as their nonparametric equivalents, ANOVA for one or more factors, multiple linear regression, and logistic regression--all using the R software.""

Advanced Linear Regression:
""This course is designed to explore fundamentals of regression, theory of regression models, residuals and residual analysis, multiple regression, remediation, transformations, and ANOVA. Students will use R for statistical analyses.""

",0,"Hi, so I am a Electrical Engineering major interested in Digital Signal Processing and plan to go into neuroimaging. I have heard that Statistics is really important for this. I just finished my first intro stats course and we worked mainly in excel.

Basically I just want to know which courses are worth taking and which are not. 


My department has a a decent amount of Stats courses. Most they say are very applied and meant to help get jobs, but I have no idea. The next course people tend to take after this one is the intermediate stats class. The only prereq is the intro class, no calc or anything. It introduces R programming. 

The next course in this sequence is advanced linear regression which also doesn't require any Calc. Just the intermediate course. 

We also offer a course in Experimental Design, Statistical Consulting, and a Data Wrangling and Visualization course.

We just have 2 theory courses though. A upper division Probability course and a Baysian Stats course. These both require Calc 3 and a intro Stats course. 


Intermediate stats: 
""Z tests, t tests and chi-squared tests as well as their nonparametric equivalents, ANOVA for one or more factors, multiple linear regression, and logistic regression--all using the R software.""

Advanced Linear Regression:
""This course is designed to explore fundamentals of regression, theory of regression models, residuals and residual analysis, multiple regression, remediation, transformations, and ANOVA. Students will use R for statistical analyses.""

",3,statistics,54935,,EE major looking for advice on more stats courses,https://www.reddit.com/r/statistics/comments/8bwu15/ee_major_looking_for_advice_on_more_stats_courses/,all_ads,2018-04-13 09:30:12,50 days 16:05:22.036584000,
"Hello r/statistics 😃
I´m taking the not so recent post https://www.reddit.com/r/statistics/comments/85xp2c/this_sub_is_a_microcosm_of_the_field_of/?utm_content=title&utm_medium=hot&utm_source=reddit&utm_name=statistics as an opportunity for a discussion thread that I am thinking about for a while now. 

First of all: I am a total statistics newbie. I may know more than many other people, but still not nearly enough and want to changen this. But I want to do it well, thorough and quick, so that my time does not go to waste. One thing I realized recently is that often when reading more complicated textbooks, which give either a broade overview of the whole topic or goes into a specific topic, the introductory chapters and thus explaining basic concepts feel incerdible difficult to understand at first. Don´t get me wrong. It´s not like I am understanding nothing. It´s just that It felt like key concepts didn´t quite get into my mind, that while I understand them, they are not intuitively there. I hate this feeling. The feeling of not understanding deeply enough what you are learning.

But I discovered along the way that often, reading and studying certain different topics helps a lot of getting a intuitive understanding of other topics, that I actually want to study, right away. Like understanding more about the basics of the normality curve to understand parametric and then non parametric test, or when reading mathematical symbols directly knowing them and not having to look them up helps in understanding what they try to say. At this point many will likely say of course that´s a thing. You have to learn the basics first. And well..of course you have. This is why things are thaught in a certain order in school.
My point is rather that especially when self teaching a lot of the different ressources (online or textbook) never tell what you should actually know in order to get a complete understanding of the topic and the only way I know of is to spend hours and hours behind dozens of different textbooks while occasionaly googling. 

The closest thing to what I mean I ever saw is the tree view of the working topics in Rosalind.info. The only problem I have with it is that reality is not really like a strict tree. Your more likely learn A to get a better head start to B, B in turn get relatively easy. Then you can learn C with relative ease and understand it, but then C helps a tremendous amount in understanding A, and so creating an even deeper understanding of A and the whole topic altogether. Along the way A,B and C help in understanding a complete different concept Z. It is similiar to a network.

So again to my problem is: I think a lot of ressources you can use to study such topics actualy begin with B and you have to look A up in another book. Sometimes it is even worse and the whole topic begins with C or even Z. So I would like to ask the people here:

	1. Can you relate to this? Do you have experienced similiar things.
	2. Do you mabye have any ressources, books or similiar which can show you were you are and thus guide you somehow?
	3. In what way did you learn statistics?
	4. And lastly do you think it´s possible to create such a ""network"" with maybe a common starting point like basic mathematical knowledge and then guide you through the whole process of mastering a topic (In the way of ""my current knowledge seems to mostly be at those topics, so I should actually start with this and that topic, instead of X, which is my actual goal"").

",11,1523588447.0,8bsp13,False,"Hello r/statistics 😃
I´m taking the not so recent post https://www.reddit.com/r/statistics/comments/85xp2c/this_sub_is_a_microcosm_of_the_field_of/?utm_content=title&utm_medium=hot&utm_source=reddit&utm_name=statistics as an opportunity for a discussion thread that I am thinking about for a while now. 

First of all: I am a total statistics newbie. I may know more than many other people, but still not nearly enough and want to changen this. But I want to do it well, thorough and quick, so that my time does not go to waste. One thing I realized recently is that often when reading more complicated textbooks, which give either a broade overview of the whole topic or goes into a specific topic, the introductory chapters and thus explaining basic concepts feel incerdible difficult to understand at first. Don´t get me wrong. It´s not like I am understanding nothing. It´s just that It felt like key concepts didn´t quite get into my mind, that while I understand them, they are not intuitively there. I hate this feeling. The feeling of not understanding deeply enough what you are learning.

But I discovered along the way that often, reading and studying certain different topics helps a lot of getting a intuitive understanding of other topics, that I actually want to study, right away. Like understanding more about the basics of the normality curve to understand parametric and then non parametric test, or when reading mathematical symbols directly knowing them and not having to look them up helps in understanding what they try to say. At this point many will likely say of course that´s a thing. You have to learn the basics first. And well..of course you have. This is why things are thaught in a certain order in school.
My point is rather that especially when self teaching a lot of the different ressources (online or textbook) never tell what you should actually know in order to get a complete understanding of the topic and the only way I know of is to spend hours and hours behind dozens of different textbooks while occasionaly googling. 

The closest thing to what I mean I ever saw is the tree view of the working topics in Rosalind.info. The only problem I have with it is that reality is not really like a strict tree. Your more likely learn A to get a better head start to B, B in turn get relatively easy. Then you can learn C with relative ease and understand it, but then C helps a tremendous amount in understanding A, and so creating an even deeper understanding of A and the whole topic altogether. Along the way A,B and C help in understanding a complete different concept Z. It is similiar to a network.

So again to my problem is: I think a lot of ressources you can use to study such topics actualy begin with B and you have to look A up in another book. Sometimes it is even worse and the whole topic begins with C or even Z. So I would like to ask the people here:

	1. Can you relate to this? Do you have experienced similiar things.
	2. Do you mabye have any ressources, books or similiar which can show you were you are and thus guide you somehow?
	3. In what way did you learn statistics?
	4. And lastly do you think it´s possible to create such a ""network"" with maybe a common starting point like basic mathematical knowledge and then guide you through the whole process of mastering a topic (In the way of ""my current knowledge seems to mostly be at those topics, so I should actually start with this and that topic, instead of X, which is my actual goal"").

",0,"Hello r/statistics 😃
I´m taking the not so recent post https://www.reddit.com/r/statistics/comments/85xp2c/this_sub_is_a_microcosm_of_the_field_of/?utm_content=title&utm_medium=hot&utm_source=reddit&utm_name=statistics as an opportunity for a discussion thread that I am thinking about for a while now. 

First of all: I am a total statistics newbie. I may know more than many other people, but still not nearly enough and want to changen this. But I want to do it well, thorough and quick, so that my time does not go to waste. One thing I realized recently is that often when reading more complicated textbooks, which give either a broade overview of the whole topic or goes into a specific topic, the introductory chapters and thus explaining basic concepts feel incerdible difficult to understand at first. Don´t get me wrong. It´s not like I am understanding nothing. It´s just that It felt like key concepts didn´t quite get into my mind, that while I understand them, they are not intuitively there. I hate this feeling. The feeling of not understanding deeply enough what you are learning.

But I discovered along the way that often, reading and studying certain different topics helps a lot of getting a intuitive understanding of other topics, that I actually want to study, right away. Like understanding more about the basics of the normality curve to understand parametric and then non parametric test, or when reading mathematical symbols directly knowing them and not having to look them up helps in understanding what they try to say. At this point many will likely say of course that´s a thing. You have to learn the basics first. And well..of course you have. This is why things are thaught in a certain order in school.
My point is rather that especially when self teaching a lot of the different ressources (online or textbook) never tell what you should actually know in order to get a complete understanding of the topic and the only way I know of is to spend hours and hours behind dozens of different textbooks while occasionaly googling. 

The closest thing to what I mean I ever saw is the tree view of the working topics in Rosalind.info. The only problem I have with it is that reality is not really like a strict tree. Your more likely learn A to get a better head start to B, B in turn get relatively easy. Then you can learn C with relative ease and understand it, but then C helps a tremendous amount in understanding A, and so creating an even deeper understanding of A and the whole topic altogether. Along the way A,B and C help in understanding a complete different concept Z. It is similiar to a network.

So again to my problem is: I think a lot of ressources you can use to study such topics actualy begin with B and you have to look A up in another book. Sometimes it is even worse and the whole topic begins with C or even Z. So I would like to ask the people here:

	1. Can you relate to this? Do you have experienced similiar things.
	2. Do you mabye have any ressources, books or similiar which can show you were you are and thus guide you somehow?
	3. In what way did you learn statistics?
	4. And lastly do you think it´s possible to create such a ""network"" with maybe a common starting point like basic mathematical knowledge and then guide you through the whole process of mastering a topic (In the way of ""my current knowledge seems to mostly be at those topics, so I should actually start with this and that topic, instead of X, which is my actual goal"").

",16,statistics,54935,,Some thoughts on learning statistics from a beginner.,https://www.reddit.com/r/statistics/comments/8bsp13/some_thoughts_on_learning_statistics_from_a/,all_ads,2018-04-12 23:00:47,51 days 02:34:47.036584000,
I am in Tulsa Oklahoma where there are literally no colleges that offer this degree under a master's level.  Anybody know of a legit college that offers an online BS in statistics,18,1523596971.0,8btsoc,False,I am in Tulsa Oklahoma where there are literally no colleges that offer this degree under a master's level.  Anybody know of a legit college that offers an online BS in statistics,0,I am in Tulsa Oklahoma where there are literally no colleges that offer this degree under a master's level.  Anybody know of a legit college that offers an online BS in statistics,8,statistics,54935,,No local colleges for a BS in statistics,https://www.reddit.com/r/statistics/comments/8btsoc/no_local_colleges_for_a_bs_in_statistics/,all_ads,2018-04-13 01:22:51,51 days 00:12:43.036584000,
"I am currently working on a project for Army Basic Training that is analyzing Army Physical Fitness Test (APFT) scores for Basic Trainees. My goal is to be able to predict performance on their final test given their performance on two diagnostic tests. In short, each Trainee takes three tests, each about three weeks apart. The final test is the one that decides if they graduate or not. Each test consists of 2 minutes of pushups, two minutes of sit-ups, and a two mile run. Each trainee is required to get a score of 60 points in each category to pass. The points are based on repetitions, age, and sex. In theory, we would like to be able to identify those that are at risk of not passing the final test, and give them additional training. I have two data sets, one of about 1700 and the other of about 700. Using R, I ran multiple regression on the first data set and found no significance between APFT 1 and APFT 3, but strong correlation between APFT 2 and APFT 3. I then did the same on the next data set and found strong correlation and significance between all three test. It must be noted that the first set of 1700 has some issues, but I thought the robustness of the data would account for them. The second data set of 700 is more ""pure"". If anyone has any advice or help on the following, it would be greatly appreciated; Where do I go from here? I have two data sets with different hypotheses, and data will be slowly trickling in, but I do not have anymore at this time. Secondly, with APFT 3 being the dependent variable, the equation I used needs the scores from APFT 1 and APFT 2 to predict the third. If I want to make a decision on a Trainee after APFT 1, how do I do that with this model without APFT 2? Currently, I've been using the average increase in score from APFT 1 to APFT 2 as the scores for APFT 2. For example if a Trainee scored 30 points on APFT 1 pushups, the average increase for a male is 11 points. So for APFT 2, I've used a score of 41 points to predict what APFT 3 will be. I have other data not directly related to Physical Fitness, like Age, Marksmanship Score, Job in the Army, etc. I would like to see if these have any significance in predicting performance. I have run p test on Marksmanship and found no significance, but I still think there may be. Finally, How would I see if job title has significance. For example, those that are going to be an Army mechanic tend to do better than say a Human Resources specialist? Any help, critiques, or guidance would be greatly appreciated!",14,1523567515.0,8bq11o,False,"I am currently working on a project for Army Basic Training that is analyzing Army Physical Fitness Test (APFT) scores for Basic Trainees. My goal is to be able to predict performance on their final test given their performance on two diagnostic tests. In short, each Trainee takes three tests, each about three weeks apart. The final test is the one that decides if they graduate or not. Each test consists of 2 minutes of pushups, two minutes of sit-ups, and a two mile run. Each trainee is required to get a score of 60 points in each category to pass. The points are based on repetitions, age, and sex. In theory, we would like to be able to identify those that are at risk of not passing the final test, and give them additional training. I have two data sets, one of about 1700 and the other of about 700. Using R, I ran multiple regression on the first data set and found no significance between APFT 1 and APFT 3, but strong correlation between APFT 2 and APFT 3. I then did the same on the next data set and found strong correlation and significance between all three test. It must be noted that the first set of 1700 has some issues, but I thought the robustness of the data would account for them. The second data set of 700 is more ""pure"". If anyone has any advice or help on the following, it would be greatly appreciated; Where do I go from here? I have two data sets with different hypotheses, and data will be slowly trickling in, but I do not have anymore at this time. Secondly, with APFT 3 being the dependent variable, the equation I used needs the scores from APFT 1 and APFT 2 to predict the third. If I want to make a decision on a Trainee after APFT 1, how do I do that with this model without APFT 2? Currently, I've been using the average increase in score from APFT 1 to APFT 2 as the scores for APFT 2. For example if a Trainee scored 30 points on APFT 1 pushups, the average increase for a male is 11 points. So for APFT 2, I've used a score of 41 points to predict what APFT 3 will be. I have other data not directly related to Physical Fitness, like Age, Marksmanship Score, Job in the Army, etc. I would like to see if these have any significance in predicting performance. I have run p test on Marksmanship and found no significance, but I still think there may be. Finally, How would I see if job title has significance. For example, those that are going to be an Army mechanic tend to do better than say a Human Resources specialist? Any help, critiques, or guidance would be greatly appreciated!",0,"I am currently working on a project for Army Basic Training that is analyzing Army Physical Fitness Test (APFT) scores for Basic Trainees. My goal is to be able to predict performance on their final test given their performance on two diagnostic tests. In short, each Trainee takes three tests, each about three weeks apart. The final test is the one that decides if they graduate or not. Each test consists of 2 minutes of pushups, two minutes of sit-ups, and a two mile run. Each trainee is required to get a score of 60 points in each category to pass. The points are based on repetitions, age, and sex. In theory, we would like to be able to identify those that are at risk of not passing the final test, and give them additional training. I have two data sets, one of about 1700 and the other of about 700. Using R, I ran multiple regression on the first data set and found no significance between APFT 1 and APFT 3, but strong correlation between APFT 2 and APFT 3. I then did the same on the next data set and found strong correlation and significance between all three test. It must be noted that the first set of 1700 has some issues, but I thought the robustness of the data would account for them. The second data set of 700 is more ""pure"". If anyone has any advice or help on the following, it would be greatly appreciated; Where do I go from here? I have two data sets with different hypotheses, and data will be slowly trickling in, but I do not have anymore at this time. Secondly, with APFT 3 being the dependent variable, the equation I used needs the scores from APFT 1 and APFT 2 to predict the third. If I want to make a decision on a Trainee after APFT 1, how do I do that with this model without APFT 2? Currently, I've been using the average increase in score from APFT 1 to APFT 2 as the scores for APFT 2. For example if a Trainee scored 30 points on APFT 1 pushups, the average increase for a male is 11 points. So for APFT 2, I've used a score of 41 points to predict what APFT 3 will be. I have other data not directly related to Physical Fitness, like Age, Marksmanship Score, Job in the Army, etc. I would like to see if these have any significance in predicting performance. I have run p test on Marksmanship and found no significance, but I still think there may be. Finally, How would I see if job title has significance. For example, those that are going to be an Army mechanic tend to do better than say a Human Resources specialist? Any help, critiques, or guidance would be greatly appreciated!",24,statistics,54935,,Multiple Regression and the Army Physical Fitness Test,https://www.reddit.com/r/statistics/comments/8bq11o/multiple_regression_and_the_army_physical_fitness/,all_ads,2018-04-12 17:11:55,51 days 08:23:39.036584000,
"Hi everyone,

I'm using a bootstrap method to find the 95% confidence interval for Sen's slope, but I'm not sure if my methodology is correct. My steps are as follows:

    #create data.frame
    set.seed(123)
    df <- data.frame(x = 1:100,
             y = rnorm(100))

    #Sen's slope

    slope <- zyp.sen(y~x,data = df)
    slope <- slope$coef[1] + slope$coef[2]*df$x

    #bootstrap the tau statistic of zyp.trend.vector (pre-whitening step)

    pw_tau_func <- function(z) zyp.trend.vector(z, method = ""yuepilon"")[5]
    boot.out <- tsboot(df$y, pw_tau_func, R=999, l=5, sim=""fixed"")

    #find confidence intervals from boot object using boot.ci

    ci_upr <- boot.ci(boot.out, type=""norm"")$normal[3]
    ci_upr <- slope + abs(ci_upr)
    ci_lwr <- boot.ci(boot.out, type=""norm"")$normal[2]
    ci_lwr <- slope - abs(ci_lwr)

    #plot output

    plot(slope, type = ""l"", ylab = ""Simulated Data"", ylim = c(-1,1))
    points(ci_upr, col = ""red"")
    points(ci_lwr, col = ""red"")

Is bootstrapping the tau statistic the correct method for generating confidence intervals for Sen's slope? I've seen conflicting information around the internet.",0,1523595670.0,8btmso,False,"Hi everyone,

I'm using a bootstrap method to find the 95% confidence interval for Sen's slope, but I'm not sure if my methodology is correct. My steps are as follows:

    #create data.frame
    set.seed(123)
    df <- data.frame(x = 1:100,
             y = rnorm(100))

    #Sen's slope

    slope <- zyp.sen(y~x,data = df)
    slope <- slope$coef[1] + slope$coef[2]*df$x

    #bootstrap the tau statistic of zyp.trend.vector (pre-whitening step)

    pw_tau_func <- function(z) zyp.trend.vector(z, method = ""yuepilon"")[5]
    boot.out <- tsboot(df$y, pw_tau_func, R=999, l=5, sim=""fixed"")

    #find confidence intervals from boot object using boot.ci

    ci_upr <- boot.ci(boot.out, type=""norm"")$normal[3]
    ci_upr <- slope + abs(ci_upr)
    ci_lwr <- boot.ci(boot.out, type=""norm"")$normal[2]
    ci_lwr <- slope - abs(ci_lwr)

    #plot output

    plot(slope, type = ""l"", ylab = ""Simulated Data"", ylim = c(-1,1))
    points(ci_upr, col = ""red"")
    points(ci_lwr, col = ""red"")

Is bootstrapping the tau statistic the correct method for generating confidence intervals for Sen's slope? I've seen conflicting information around the internet.",0,"Hi everyone,

I'm using a bootstrap method to find the 95% confidence interval for Sen's slope, but I'm not sure if my methodology is correct. My steps are as follows:

    #create data.frame
    set.seed(123)
    df <- data.frame(x = 1:100,
             y = rnorm(100))

    #Sen's slope

    slope <- zyp.sen(y~x,data = df)
    slope <- slope$coef[1] + slope$coef[2]*df$x

    #bootstrap the tau statistic of zyp.trend.vector (pre-whitening step)

    pw_tau_func <- function(z) zyp.trend.vector(z, method = ""yuepilon"")[5]
    boot.out <- tsboot(df$y, pw_tau_func, R=999, l=5, sim=""fixed"")

    #find confidence intervals from boot object using boot.ci

    ci_upr <- boot.ci(boot.out, type=""norm"")$normal[3]
    ci_upr <- slope + abs(ci_upr)
    ci_lwr <- boot.ci(boot.out, type=""norm"")$normal[2]
    ci_lwr <- slope - abs(ci_lwr)

    #plot output

    plot(slope, type = ""l"", ylab = ""Simulated Data"", ylim = c(-1,1))
    points(ci_upr, col = ""red"")
    points(ci_lwr, col = ""red"")

Is bootstrapping the tau statistic the correct method for generating confidence intervals for Sen's slope? I've seen conflicting information around the internet.",5,statistics,54935,,Correct way to bootstrap nonparametric confidence intervals,https://www.reddit.com/r/statistics/comments/8btmso/correct_way_to_bootstrap_nonparametric_confidence/,all_ads,2018-04-13 01:01:10,51 days 00:34:24.036584000,
"Hello there. 

We have a simple dataset folowing pre-transplant patients and I would like to plot something like [this](http://circ.ahajournals.org/content/circulationaha/126/11/1401/F1.large.jpg).

I've managed to plot both a survival curve and an event-free survival curve (with the survival package) and also the curves for our competing outcomes (transplant and death, with the cmprsk package). However, I have found no way to perform an analysis such as the one I intended.

Do I need to run an analysis with mstate, setting each state? I tried following the tutorial for it, but it gets waaay into covariates and adjustments and I would simply like the unadjusted analysis (I've gone so far as setting the states and the transition matrix, but didn't advance from there).

Any help? Sorry, I almost never deal with primary level data and my trusted statistician is unavailable this week. Thanks everyone in advance.

EDIT: OK, turns out there's no command to plot both survival and event lines, you just need to use par(new=t) to plot one thing over the other. For anyone reading this in the future, remember that you need to align axes and that you must be absolutely sure your curves are actually mutually exclusive.

Mstate *would* be able to handle more advanced analyses, but just for two competing events and event-free survival curves it's not necessary (says my statistician).",2,1523587616.0,8bsl6z,False,"Hello there. 

We have a simple dataset folowing pre-transplant patients and I would like to plot something like [this](http://circ.ahajournals.org/content/circulationaha/126/11/1401/F1.large.jpg).

I've managed to plot both a survival curve and an event-free survival curve (with the survival package) and also the curves for our competing outcomes (transplant and death, with the cmprsk package). However, I have found no way to perform an analysis such as the one I intended.

Do I need to run an analysis with mstate, setting each state? I tried following the tutorial for it, but it gets waaay into covariates and adjustments and I would simply like the unadjusted analysis (I've gone so far as setting the states and the transition matrix, but didn't advance from there).

Any help? Sorry, I almost never deal with primary level data and my trusted statistician is unavailable this week. Thanks everyone in advance.

EDIT: OK, turns out there's no command to plot both survival and event lines, you just need to use par(new=t) to plot one thing over the other. For anyone reading this in the future, remember that you need to align axes and that you must be absolutely sure your curves are actually mutually exclusive.

Mstate *would* be able to handle more advanced analyses, but just for two competing events and event-free survival curves it's not necessary (says my statistician).",0,"Hello there. 

We have a simple dataset folowing pre-transplant patients and I would like to plot something like [this](http://circ.ahajournals.org/content/circulationaha/126/11/1401/F1.large.jpg).

I've managed to plot both a survival curve and an event-free survival curve (with the survival package) and also the curves for our competing outcomes (transplant and death, with the cmprsk package). However, I have found no way to perform an analysis such as the one I intended.

Do I need to run an analysis with mstate, setting each state? I tried following the tutorial for it, but it gets waaay into covariates and adjustments and I would simply like the unadjusted analysis (I've gone so far as setting the states and the transition matrix, but didn't advance from there).

Any help? Sorry, I almost never deal with primary level data and my trusted statistician is unavailable this week. Thanks everyone in advance.

EDIT: OK, turns out there's no command to plot both survival and event lines, you just need to use par(new=t) to plot one thing over the other. For anyone reading this in the future, remember that you need to align axes and that you must be absolutely sure your curves are actually mutually exclusive.

Mstate *would* be able to handle more advanced analyses, but just for two competing events and event-free survival curves it's not necessary (says my statistician).",7,statistics,54935,,Competing outcomes plot in r,https://www.reddit.com/r/statistics/comments/8bsl6z/competing_outcomes_plot_in_r/,all_ads,2018-04-12 22:46:56,51 days 02:48:38.036584000,
"Hi,

its my understanding that a u-statistic can be use to make inference from a small sample to a bigger sample, so in this regard is similar to a bootstrap in the sense for getting a better estimation.
Is this correct? What is a good book/paper/etc where i can learn about this class of statistic (aside from the original from Hoeffding) , hopefully with examples and all that?

Many thxs.
",1,1523615980.0,8bvwzt,False,"Hi,

its my understanding that a u-statistic can be use to make inference from a small sample to a bigger sample, so in this regard is similar to a bootstrap in the sense for getting a better estimation.
Is this correct? What is a good book/paper/etc where i can learn about this class of statistic (aside from the original from Hoeffding) , hopefully with examples and all that?

Many thxs.
",0,"Hi,

its my understanding that a u-statistic can be use to make inference from a small sample to a bigger sample, so in this regard is similar to a bootstrap in the sense for getting a better estimation.
Is this correct? What is a good book/paper/etc where i can learn about this class of statistic (aside from the original from Hoeffding) , hopefully with examples and all that?

Many thxs.
",0,statistics,54935,,U-statistics complementing bootstrap?,https://www.reddit.com/r/statistics/comments/8bvwzt/ustatistics_complementing_bootstrap/,all_ads,2018-04-13 06:39:40,50 days 18:55:54.036584000,
"Hi all!  If this better belongs in /r/AskStatistics please let me know.

I do finance work, a friend asked me for my thoughts and this is really out of my field of expertise:

**The data:**

Three columns:  

1. %  Pay to Market Rate (roughly ranges from 50% to 120%)
2. Years of experience (1 year, 2 year, 3 year, 4 year, 5 year)  (each employee is grouped in one of these 5 categories)
3. Still with company (either Yes or No)

2000 rows, where each row is an employee.

**The Goal:**

To represent/analyze the data in such a way that one could see at what % or what year of experience are most people leaving, so that you one theoretically pay those that are in that 'danger zone' a higher salary to retain them?


**My Attempt:**

If I had the exact amount of time each person had been with the company (so the X axis might be 60 months instead of just 5 years), I could use a scatter plot where the X axis is years of experience and Y axis is % Pay, and I could make the dots either different colors or different sizes to indicate the binary choice of with company or not.  But the problem is that I only have 5 possible X axis choices so a scatter-plot wouldn't work (would just be 5 vertical lines of dots).

And now I realize that this data while pretty simple probably isn't best analyzed using a single chart, but rather some kind of statistical analysis could be done first?  Not sure, any ideas?

Thanks!
",2,1523563957.0,8bpnv2,False,"Hi all!  If this better belongs in /r/AskStatistics please let me know.

I do finance work, a friend asked me for my thoughts and this is really out of my field of expertise:

**The data:**

Three columns:  

1. %  Pay to Market Rate (roughly ranges from 50% to 120%)
2. Years of experience (1 year, 2 year, 3 year, 4 year, 5 year)  (each employee is grouped in one of these 5 categories)
3. Still with company (either Yes or No)

2000 rows, where each row is an employee.

**The Goal:**

To represent/analyze the data in such a way that one could see at what % or what year of experience are most people leaving, so that you one theoretically pay those that are in that 'danger zone' a higher salary to retain them?


**My Attempt:**

If I had the exact amount of time each person had been with the company (so the X axis might be 60 months instead of just 5 years), I could use a scatter plot where the X axis is years of experience and Y axis is % Pay, and I could make the dots either different colors or different sizes to indicate the binary choice of with company or not.  But the problem is that I only have 5 possible X axis choices so a scatter-plot wouldn't work (would just be 5 vertical lines of dots).

And now I realize that this data while pretty simple probably isn't best analyzed using a single chart, but rather some kind of statistical analysis could be done first?  Not sure, any ideas?

Thanks!
",0,"Hi all!  If this better belongs in /r/AskStatistics please let me know.

I do finance work, a friend asked me for my thoughts and this is really out of my field of expertise:

**The data:**

Three columns:  

1. %  Pay to Market Rate (roughly ranges from 50% to 120%)
2. Years of experience (1 year, 2 year, 3 year, 4 year, 5 year)  (each employee is grouped in one of these 5 categories)
3. Still with company (either Yes or No)

2000 rows, where each row is an employee.

**The Goal:**

To represent/analyze the data in such a way that one could see at what % or what year of experience are most people leaving, so that you one theoretically pay those that are in that 'danger zone' a higher salary to retain them?


**My Attempt:**

If I had the exact amount of time each person had been with the company (so the X axis might be 60 months instead of just 5 years), I could use a scatter plot where the X axis is years of experience and Y axis is % Pay, and I could make the dots either different colors or different sizes to indicate the binary choice of with company or not.  But the problem is that I only have 5 possible X axis choices so a scatter-plot wouldn't work (would just be 5 vertical lines of dots).

And now I realize that this data while pretty simple probably isn't best analyzed using a single chart, but rather some kind of statistical analysis could be done first?  Not sure, any ideas?

Thanks!
",13,statistics,54935,,Any thoughts on how to represent this data?,https://www.reddit.com/r/statistics/comments/8bpnv2/any_thoughts_on_how_to_represent_this_data/,all_ads,2018-04-12 16:12:37,51 days 09:22:57.036584000,
"Hello!  I have started getting into mining Twitter using [rtweet](http://rtweet.info/).  For anyone that has worked with this before...notice the US map on the webpage linked above (scroll down, it's the 2nd illustration)?  Is it just me, or are there just not enough individual points to visually represent the distribution?  In the lines of R code listed above the map, you can see that it is pulling 10,000 Tweets of the English language in the US.  I can understand if there are multiple points being plotted on top of each other, but I cannot make sense of the giant spacing between points on some areas of the map, especially with so much data!  Of course, if the data were to be plotted the way I'd expect, smaller plotpoints would be needed...or perhaps some type of [heat map](https://en.wikipedia.org/wiki/Heat_map)?

Thoughts? Thanks in advance!",3,1523572000.0,8bqju4,False,"Hello!  I have started getting into mining Twitter using [rtweet](http://rtweet.info/).  For anyone that has worked with this before...notice the US map on the webpage linked above (scroll down, it's the 2nd illustration)?  Is it just me, or are there just not enough individual points to visually represent the distribution?  In the lines of R code listed above the map, you can see that it is pulling 10,000 Tweets of the English language in the US.  I can understand if there are multiple points being plotted on top of each other, but I cannot make sense of the giant spacing between points on some areas of the map, especially with so much data!  Of course, if the data were to be plotted the way I'd expect, smaller plotpoints would be needed...or perhaps some type of [heat map](https://en.wikipedia.org/wiki/Heat_map)?

Thoughts? Thanks in advance!",0,"Hello!  I have started getting into mining Twitter using [rtweet](http://rtweet.info/).  For anyone that has worked with this before...notice the US map on the webpage linked above (scroll down, it's the 2nd illustration)?  Is it just me, or are there just not enough individual points to visually represent the distribution?  In the lines of R code listed above the map, you can see that it is pulling 10,000 Tweets of the English language in the US.  I can understand if there are multiple points being plotted on top of each other, but I cannot make sense of the giant spacing between points on some areas of the map, especially with so much data!  Of course, if the data were to be plotted the way I'd expect, smaller plotpoints would be needed...or perhaps some type of [heat map](https://en.wikipedia.org/wiki/Heat_map)?

Thoughts? Thanks in advance!",7,statistics,54935,,Twitter Mining,https://www.reddit.com/r/statistics/comments/8bqju4/twitter_mining/,all_ads,2018-04-12 18:26:40,51 days 07:08:54.036584000,
What can you do with a statistics degree other than working in a business and analyze data regarding money? Are these positions hard to access? I really like the demographic side of statistics ,6,1523574098.0,8bqtiw,False,What can you do with a statistics degree other than working in a business and analyze data regarding money? Are these positions hard to access? I really like the demographic side of statistics ,0,What can you do with a statistics degree other than working in a business and analyze data regarding money? Are these positions hard to access? I really like the demographic side of statistics ,5,statistics,54935,,What are other options other than working in a business?,https://www.reddit.com/r/statistics/comments/8bqtiw/what_are_other_options_other_than_working_in_a/,all_ads,2018-04-12 19:01:38,51 days 06:33:56.036584000,
Having a hard time defining it,1,1523608152.0,8bv3en,False,Having a hard time defining it,0,Having a hard time defining it,0,statistics,54935,,how does one create and include an interaction term in a regression model,https://www.reddit.com/r/statistics/comments/8bv3en/how_does_one_create_and_include_an_interaction/,all_ads,2018-04-13 04:29:12,50 days 21:06:22.036584000,
"Hi all,
I have an assignment due, and one of the questions asks me to choose an inferential test to perform on some data, including a reason why, whether it is one tailed or two tailed, and the chosen confidence interval.

What I don’t understand is that I’ve been given an 2 populations of data, albeit, only 15 pairs of data. I’ve already calculated the population mean and standard deviation for both sets. In this case, why would I need to use an inferential test? We’ve learned about the tTest specifically, so I’m presuming he wants me to use this, but I’ve only ever seen examples where samples are used, and I’ve seen arguments that it isn’t really necessary because what can I infer when I already know the population means. Secondly, should I just go ahead with this test regardless? And alter the equation to use population mean rather than sample? 

Thanks a lot, I really appreciate it.

Edit:
Some information about the data-
Two people are basically comparing algorithms run times and they run the test 15 times each to compare.",6,1523605564.0,8but5c,False,"Hi all,
I have an assignment due, and one of the questions asks me to choose an inferential test to perform on some data, including a reason why, whether it is one tailed or two tailed, and the chosen confidence interval.

What I don’t understand is that I’ve been given an 2 populations of data, albeit, only 15 pairs of data. I’ve already calculated the population mean and standard deviation for both sets. In this case, why would I need to use an inferential test? We’ve learned about the tTest specifically, so I’m presuming he wants me to use this, but I’ve only ever seen examples where samples are used, and I’ve seen arguments that it isn’t really necessary because what can I infer when I already know the population means. Secondly, should I just go ahead with this test regardless? And alter the equation to use population mean rather than sample? 

Thanks a lot, I really appreciate it.

Edit:
Some information about the data-
Two people are basically comparing algorithms run times and they run the test 15 times each to compare.",0,"Hi all,
I have an assignment due, and one of the questions asks me to choose an inferential test to perform on some data, including a reason why, whether it is one tailed or two tailed, and the chosen confidence interval.

What I don’t understand is that I’ve been given an 2 populations of data, albeit, only 15 pairs of data. I’ve already calculated the population mean and standard deviation for both sets. In this case, why would I need to use an inferential test? We’ve learned about the tTest specifically, so I’m presuming he wants me to use this, but I’ve only ever seen examples where samples are used, and I’ve seen arguments that it isn’t really necessary because what can I infer when I already know the population means. Secondly, should I just go ahead with this test regardless? And alter the equation to use population mean rather than sample? 

Thanks a lot, I really appreciate it.

Edit:
Some information about the data-
Two people are basically comparing algorithms run times and they run the test 15 times each to compare.",0,statistics,54935,,What inferential test for this?,https://www.reddit.com/r/statistics/comments/8but5c/what_inferential_test_for_this/,all_ads,2018-04-13 03:46:04,50 days 21:49:30.036584000,
"Q:https://imgur.com/a/tde8S

Hi, for this question I am not sure why the sample size is 16 instead of 32 since the populations are involved so why is the total sample size not used? 

Also when the H0 is rejected, it is mathematically correct to say H1 is accepted? Or are you not allowed to make this assumption, so you can only say there is sufficient evidence to reject null hypothesis?

Thanks",10,1523604602.0,8bupck,False,"Q:https://imgur.com/a/tde8S

Hi, for this question I am not sure why the sample size is 16 instead of 32 since the populations are involved so why is the total sample size not used? 

Also when the H0 is rejected, it is mathematically correct to say H1 is accepted? Or are you not allowed to make this assumption, so you can only say there is sufficient evidence to reject null hypothesis?

Thanks",0,"Q:https://imgur.com/a/tde8S

Hi, for this question I am not sure why the sample size is 16 instead of 32 since the populations are involved so why is the total sample size not used? 

Also when the H0 is rejected, it is mathematically correct to say H1 is accepted? Or are you not allowed to make this assumption, so you can only say there is sufficient evidence to reject null hypothesis?

Thanks",0,statistics,54935,,Hypothesis testing question in statistics,https://www.reddit.com/r/statistics/comments/8bupck/hypothesis_testing_question_in_statistics/,all_ads,2018-04-13 03:30:02,50 days 22:05:32.036584000,
"More specifically, is it always better to use as high a number of simulations as possible? (e.g. is sims=10000 better than sims=1000)",8,1523576584.0,8br536,False,"More specifically, is it always better to use as high a number of simulations as possible? (e.g. is sims=10000 better than sims=1000)",0,"More specifically, is it always better to use as high a number of simulations as possible? (e.g. is sims=10000 better than sims=1000)",4,statistics,54935,,"Can the number of simulations in a model affect the p-value, and how?",https://www.reddit.com/r/statistics/comments/8br536/can_the_number_of_simulations_in_a_model_affect/,all_ads,2018-04-12 19:43:04,51 days 05:52:30.036584000,
"I am trying desperately to recall the name/theory for a statstical trend in surveys.

Essentially, for questions like ""Have you ever heard of Delaware?"" the response rate of ""no"" is always vastly higher than the percent of people who have heard of Delaware.

As I recall, it is the effect of people answering the survey who may have done a number of things:

- offered a random answer
- deliberately answer wrong
- got confused or did not understand the question
- etc

But the end result is something like that the studies done on this effect have shown that there will always be a larger than actual ratio of ""no"" type answers for simple questions that everyone knows the answer to.

Plzhelp.",3,1523603255.0,8buk58,False,"I am trying desperately to recall the name/theory for a statstical trend in surveys.

Essentially, for questions like ""Have you ever heard of Delaware?"" the response rate of ""no"" is always vastly higher than the percent of people who have heard of Delaware.

As I recall, it is the effect of people answering the survey who may have done a number of things:

- offered a random answer
- deliberately answer wrong
- got confused or did not understand the question
- etc

But the end result is something like that the studies done on this effect have shown that there will always be a larger than actual ratio of ""no"" type answers for simple questions that everyone knows the answer to.

Plzhelp.",0,"I am trying desperately to recall the name/theory for a statstical trend in surveys.

Essentially, for questions like ""Have you ever heard of Delaware?"" the response rate of ""no"" is always vastly higher than the percent of people who have heard of Delaware.

As I recall, it is the effect of people answering the survey who may have done a number of things:

- offered a random answer
- deliberately answer wrong
- got confused or did not understand the question
- etc

But the end result is something like that the studies done on this effect have shown that there will always be a larger than actual ratio of ""no"" type answers for simple questions that everyone knows the answer to.

Plzhelp.",0,statistics,54935,,Help me Identify Statistics Phenomenon?,https://www.reddit.com/r/statistics/comments/8buk58/help_me_identify_statistics_phenomenon/,all_ads,2018-04-13 03:07:35,50 days 22:27:59.036584000,
"I mostly enjoyed my stats classes, but when it comes to actually working, I'm really worried. Sitting on a computer googling how to manipulate data, what my error messages mean, how to use functions and packages, it makes me  flustered/confused/frustrated all day, plus constantly worried that my questions annoy coworkers. 2-3x per hour I have to get up and walk around, my body always pretends to be hungry when it's really just bored, and I feel tired all the time. Since these things only make me even slower, I try to stay productive by ""working"" 10+ hours a day.

Maybe this will get better over time since I really didn't learn how to use statistical software in school (they tried to cover 4 languages and it was too much for, me I only survived with massive help from friends). I tried a friend's adderall and that helps immensely, but probably not a long-term or healthy solution. 

As I slowly, slowly get better at programming I imagine I'll get faster at figuring how to use new packages and diagnosing errors. Will I be okay then? Or will I just be faster but still doing the same thing (i.e. being annoyed/confused/flustered all day?) I'm worried that I'm supposed to be *enjoying* ""figuring stuff out"" rather than hating it, if so it seems I chose the wrong career.",4,1523601888.0,8bueh2,False,"I mostly enjoyed my stats classes, but when it comes to actually working, I'm really worried. Sitting on a computer googling how to manipulate data, what my error messages mean, how to use functions and packages, it makes me  flustered/confused/frustrated all day, plus constantly worried that my questions annoy coworkers. 2-3x per hour I have to get up and walk around, my body always pretends to be hungry when it's really just bored, and I feel tired all the time. Since these things only make me even slower, I try to stay productive by ""working"" 10+ hours a day.

Maybe this will get better over time since I really didn't learn how to use statistical software in school (they tried to cover 4 languages and it was too much for, me I only survived with massive help from friends). I tried a friend's adderall and that helps immensely, but probably not a long-term or healthy solution. 

As I slowly, slowly get better at programming I imagine I'll get faster at figuring how to use new packages and diagnosing errors. Will I be okay then? Or will I just be faster but still doing the same thing (i.e. being annoyed/confused/flustered all day?) I'm worried that I'm supposed to be *enjoying* ""figuring stuff out"" rather than hating it, if so it seems I chose the wrong career.",0,"I mostly enjoyed my stats classes, but when it comes to actually working, I'm really worried. Sitting on a computer googling how to manipulate data, what my error messages mean, how to use functions and packages, it makes me  flustered/confused/frustrated all day, plus constantly worried that my questions annoy coworkers. 2-3x per hour I have to get up and walk around, my body always pretends to be hungry when it's really just bored, and I feel tired all the time. Since these things only make me even slower, I try to stay productive by ""working"" 10+ hours a day.

Maybe this will get better over time since I really didn't learn how to use statistical software in school (they tried to cover 4 languages and it was too much for, me I only survived with massive help from friends). I tried a friend's adderall and that helps immensely, but probably not a long-term or healthy solution. 

As I slowly, slowly get better at programming I imagine I'll get faster at figuring how to use new packages and diagnosing errors. Will I be okay then? Or will I just be faster but still doing the same thing (i.e. being annoyed/confused/flustered all day?) I'm worried that I'm supposed to be *enjoying* ""figuring stuff out"" rather than hating it, if so it seems I chose the wrong career.",1,statistics,54935,,First stats job: it okay to feel flustered to the point of complete inefficiency / constant sleepiness in the beginning?,https://www.reddit.com/r/statistics/comments/8bueh2/first_stats_job_it_okay_to_feel_flustered_to_the/,all_ads,2018-04-13 02:44:48,50 days 22:50:46.036584000,
"A quote that's been on my mind the past few days. 

It seems to me that Statistics and English students have more in common than you'd think. Both have to take a step back and analyze all corners of society or research, and not just a small slice.

In the English world I think of people like George Orwell, who chronicled the lives of the working poor in England and France, by living among them. In Statistics I think of Sudhir Venkatesh, who investigated the underground economy with Steven Levitt, to understand the life of those living in inner city projects, as well as the market for drug-trafficking. ",24,1523523449.0,8bm8qg,False,"A quote that's been on my mind the past few days. 

It seems to me that Statistics and English students have more in common than you'd think. Both have to take a step back and analyze all corners of society or research, and not just a small slice.

In the English world I think of people like George Orwell, who chronicled the lives of the working poor in England and France, by living among them. In Statistics I think of Sudhir Venkatesh, who investigated the underground economy with Steven Levitt, to understand the life of those living in inner city projects, as well as the market for drug-trafficking. ",0,"A quote that's been on my mind the past few days. 

It seems to me that Statistics and English students have more in common than you'd think. Both have to take a step back and analyze all corners of society or research, and not just a small slice.

In the English world I think of people like George Orwell, who chronicled the lives of the working poor in England and France, by living among them. In Statistics I think of Sudhir Venkatesh, who investigated the underground economy with Steven Levitt, to understand the life of those living in inner city projects, as well as the market for drug-trafficking. ",52,statistics,54935,,"“We are not concerned with the very poor. They are unthinkable, and only to be approached by the statistician or the poet.” - E.M. Forster",https://www.reddit.com/r/statistics/comments/8bm8qg/we_are_not_concerned_with_the_very_poor_they_are/,all_ads,2018-04-12 04:57:29,51 days 20:38:05.036584000,
"I recently went down a rabbit hole of apportionment methods, how to round percentages so the sum is 100, and allocating seats to the US House of Representatives. You might be interested in my notes on the matter:

https://medium.com/@cyounkins/rounding-percentages-and-the-house-of-representatives-87ff4587f163",0,1523586804.0,8bshim,False,"I recently went down a rabbit hole of apportionment methods, how to round percentages so the sum is 100, and allocating seats to the US House of Representatives. You might be interested in my notes on the matter:

https://medium.com/@cyounkins/rounding-percentages-and-the-house-of-representatives-87ff4587f163",0,"I recently went down a rabbit hole of apportionment methods, how to round percentages so the sum is 100, and allocating seats to the US House of Representatives. You might be interested in my notes on the matter:

https://medium.com/@cyounkins/rounding-percentages-and-the-house-of-representatives-87ff4587f163",3,statistics,54935,,Rounding Percentages and the House of Representatives,https://www.reddit.com/r/statistics/comments/8bshim/rounding_percentages_and_the_house_of/,all_ads,2018-04-12 22:33:24,51 days 03:02:10.036584000,
"hey all,
so I’m having a little trouble estimating effect size to find a sample size or power. In one of my reference books, it says this info is usually “known” or estimated by what’s already in the literature but I’m not sure if they’re too much data on what im looking at. Basically the research question I’m looking at is trying to test how accurately 2 different instruments work compared to goldstandard tool and then look at the difference in instrument 1 and instrument 2. I suggested a repeated measures ANOVA as all three instruments will be tested on the same group of people. Just wondering how I can get a sense of estimating effect size (within group variance and correlation). Thanks!",1,1523589255.0,8bssss,False,"hey all,
so I’m having a little trouble estimating effect size to find a sample size or power. In one of my reference books, it says this info is usually “known” or estimated by what’s already in the literature but I’m not sure if they’re too much data on what im looking at. Basically the research question I’m looking at is trying to test how accurately 2 different instruments work compared to goldstandard tool and then look at the difference in instrument 1 and instrument 2. I suggested a repeated measures ANOVA as all three instruments will be tested on the same group of people. Just wondering how I can get a sense of estimating effect size (within group variance and correlation). Thanks!",0,"hey all,
so I’m having a little trouble estimating effect size to find a sample size or power. In one of my reference books, it says this info is usually “known” or estimated by what’s already in the literature but I’m not sure if they’re too much data on what im looking at. Basically the research question I’m looking at is trying to test how accurately 2 different instruments work compared to goldstandard tool and then look at the difference in instrument 1 and instrument 2. I suggested a repeated measures ANOVA as all three instruments will be tested on the same group of people. Just wondering how I can get a sense of estimating effect size (within group variance and correlation). Thanks!",0,statistics,54935,,Having a little trouble with effect size for a repeated measures ANOVA,https://www.reddit.com/r/statistics/comments/8bssss/having_a_little_trouble_with_effect_size_for_a/,all_ads,2018-04-12 23:14:15,51 days 02:21:19.036584000,
"Engineering graduate here. I want to study it for myself since I want to work with data analysis in the long term, so I am on my first stepsc in Python and now I want to know some Statistics.

However, when I tried once I found it unpractical since there is a lot of data to do the exercises. What I mean is that writing down charts takes a LOT of time, as well writing all the formulas like we do in College and using a calculator...I am not interesting studying it in the traditional way because it takes a lot of time to build it, time that I want to use to work on it to learn.

So how can I do it?
Should I do all the analysis in Excel?
",4,1523580613.0,8bro3g,False,"Engineering graduate here. I want to study it for myself since I want to work with data analysis in the long term, so I am on my first stepsc in Python and now I want to know some Statistics.

However, when I tried once I found it unpractical since there is a lot of data to do the exercises. What I mean is that writing down charts takes a LOT of time, as well writing all the formulas like we do in College and using a calculator...I am not interesting studying it in the traditional way because it takes a lot of time to build it, time that I want to use to work on it to learn.

So how can I do it?
Should I do all the analysis in Excel?
",0,"Engineering graduate here. I want to study it for myself since I want to work with data analysis in the long term, so I am on my first stepsc in Python and now I want to know some Statistics.

However, when I tried once I found it unpractical since there is a lot of data to do the exercises. What I mean is that writing down charts takes a LOT of time, as well writing all the formulas like we do in College and using a calculator...I am not interesting studying it in the traditional way because it takes a lot of time to build it, time that I want to use to work on it to learn.

So how can I do it?
Should I do all the analysis in Excel?
",0,statistics,54935,,How can I study statistics efficiently?,https://www.reddit.com/r/statistics/comments/8bro3g/how_can_i_study_statistics_efficiently/,all_ads,2018-04-12 20:50:13,51 days 04:45:21.036584000,
"I wanna hear some opinions on this topic.

The main reason for asking is because I was given a data set to analyze for a class and we are given the freedom to use any method of analysis. 

I don't have much experience with missing observations or dealing with imbalanced data. 

I was thinking about using some estimation method to make up for the missing observation and then I thought about removing the variable since this variable has a lot of missing observations. If i remove this variable and undersample the rest, then the data would be balanced.

***Edit: Made some edits in the text, and probably should've asked ""What are your opinions on removing observations?""

",14,1523525267.0,8bmfph,False,"I wanna hear some opinions on this topic.

The main reason for asking is because I was given a data set to analyze for a class and we are given the freedom to use any method of analysis. 

I don't have much experience with missing observations or dealing with imbalanced data. 

I was thinking about using some estimation method to make up for the missing observation and then I thought about removing the variable since this variable has a lot of missing observations. If i remove this variable and undersample the rest, then the data would be balanced.

***Edit: Made some edits in the text, and probably should've asked ""What are your opinions on removing observations?""

",0,"I wanna hear some opinions on this topic.

The main reason for asking is because I was given a data set to analyze for a class and we are given the freedom to use any method of analysis. 

I don't have much experience with missing observations or dealing with imbalanced data. 

I was thinking about using some estimation method to make up for the missing observation and then I thought about removing the variable since this variable has a lot of missing observations. If i remove this variable and undersample the rest, then the data would be balanced.

***Edit: Made some edits in the text, and probably should've asked ""What are your opinions on removing observations?""

",9,statistics,54935,,What are your opinions on removing data?,https://www.reddit.com/r/statistics/comments/8bmfph/what_are_your_opinions_on_removing_data/,all_ads,2018-04-12 05:27:47,51 days 20:07:47.036584000,
"I am not sure exactly where to begin this nor do I know exactly what I am after, but essentially I would like to use statistics to assist the animal shelter I volunteer at. 

I do not have the strongest background in stats, but I did study math at university; I understand probability theory quite well, and took econometrics and game theory courses but do not remember  the material. Furthermore, I do not have experience applying these concepts in the world, my interaction with these concepts has been through class and theory. I guess I should stop here and ask: Do I have the skill set necessary to bring some useful insight to the shelter? If not, am I reasonably close to having skills that could provide useful in the near future?

I have tried Googling to find a project that somebody has done or is working, but I did not find any useful pointers/direction. It seems the rather broad consulting industry may have some of these tools, but they are not the most eager to share their methods openly. The shelter itself only does the most rudimentary statistics: averages, percent change sort of thing, but they do have a bit of data and programs that are impacting the raw numbers. Raw numbers are fine, but I would like to had some sort of useful analysis. Generally speaking, I would like to take the data and help staff make better decisions or better allocate resources to improve the shelter. Since I do not have any experience nor have I ever held a ""technical"" position, I do not really know the next step from here. Here are some topics I might like to explore:
1. Help with euthanasia decisions
2. Better manage feral cat population. The shelter has been doing a trap, fix release program for years and there has been a remarkable decrease in cat intakes (and hence euthanasia), but I feel there is maybe a more efficient way of targeting areas to trap cats or something like that.
3. Identifying animals that may need more work or extra attention in order to get a positive outcome. Alternatively, maybe promoting the animals more likely to get adopted so they spend less time at the shelter and their space becomes open sooner.
4. work with the staff to improve data entry methods -  allowing for more uniformity and easier to run future analysis

I do realize what I am asking is a bit of an undertaking, but I am personally tired of not using my brain enough and not having a technical job. I figure a project aiding a place I care about would be a great focus and way to develop skills, but if you have other ideas I would love to hear them! Any comments would be appreciated. Thanks for your time!",13,1523502335.0,8bjo2z,False,"I am not sure exactly where to begin this nor do I know exactly what I am after, but essentially I would like to use statistics to assist the animal shelter I volunteer at. 

I do not have the strongest background in stats, but I did study math at university; I understand probability theory quite well, and took econometrics and game theory courses but do not remember  the material. Furthermore, I do not have experience applying these concepts in the world, my interaction with these concepts has been through class and theory. I guess I should stop here and ask: Do I have the skill set necessary to bring some useful insight to the shelter? If not, am I reasonably close to having skills that could provide useful in the near future?

I have tried Googling to find a project that somebody has done or is working, but I did not find any useful pointers/direction. It seems the rather broad consulting industry may have some of these tools, but they are not the most eager to share their methods openly. The shelter itself only does the most rudimentary statistics: averages, percent change sort of thing, but they do have a bit of data and programs that are impacting the raw numbers. Raw numbers are fine, but I would like to had some sort of useful analysis. Generally speaking, I would like to take the data and help staff make better decisions or better allocate resources to improve the shelter. Since I do not have any experience nor have I ever held a ""technical"" position, I do not really know the next step from here. Here are some topics I might like to explore:
1. Help with euthanasia decisions
2. Better manage feral cat population. The shelter has been doing a trap, fix release program for years and there has been a remarkable decrease in cat intakes (and hence euthanasia), but I feel there is maybe a more efficient way of targeting areas to trap cats or something like that.
3. Identifying animals that may need more work or extra attention in order to get a positive outcome. Alternatively, maybe promoting the animals more likely to get adopted so they spend less time at the shelter and their space becomes open sooner.
4. work with the staff to improve data entry methods -  allowing for more uniformity and easier to run future analysis

I do realize what I am asking is a bit of an undertaking, but I am personally tired of not using my brain enough and not having a technical job. I figure a project aiding a place I care about would be a great focus and way to develop skills, but if you have other ideas I would love to hear them! Any comments would be appreciated. Thanks for your time!",0,"I am not sure exactly where to begin this nor do I know exactly what I am after, but essentially I would like to use statistics to assist the animal shelter I volunteer at. 

I do not have the strongest background in stats, but I did study math at university; I understand probability theory quite well, and took econometrics and game theory courses but do not remember  the material. Furthermore, I do not have experience applying these concepts in the world, my interaction with these concepts has been through class and theory. I guess I should stop here and ask: Do I have the skill set necessary to bring some useful insight to the shelter? If not, am I reasonably close to having skills that could provide useful in the near future?

I have tried Googling to find a project that somebody has done or is working, but I did not find any useful pointers/direction. It seems the rather broad consulting industry may have some of these tools, but they are not the most eager to share their methods openly. The shelter itself only does the most rudimentary statistics: averages, percent change sort of thing, but they do have a bit of data and programs that are impacting the raw numbers. Raw numbers are fine, but I would like to had some sort of useful analysis. Generally speaking, I would like to take the data and help staff make better decisions or better allocate resources to improve the shelter. Since I do not have any experience nor have I ever held a ""technical"" position, I do not really know the next step from here. Here are some topics I might like to explore:
1. Help with euthanasia decisions
2. Better manage feral cat population. The shelter has been doing a trap, fix release program for years and there has been a remarkable decrease in cat intakes (and hence euthanasia), but I feel there is maybe a more efficient way of targeting areas to trap cats or something like that.
3. Identifying animals that may need more work or extra attention in order to get a positive outcome. Alternatively, maybe promoting the animals more likely to get adopted so they spend less time at the shelter and their space becomes open sooner.
4. work with the staff to improve data entry methods -  allowing for more uniformity and easier to run future analysis

I do realize what I am asking is a bit of an undertaking, but I am personally tired of not using my brain enough and not having a technical job. I figure a project aiding a place I care about would be a great focus and way to develop skills, but if you have other ideas I would love to hear them! Any comments would be appreciated. Thanks for your time!",24,statistics,54935,,Using Statistics to Help an Animal Shelter,https://www.reddit.com/r/statistics/comments/8bjo2z/using_statistics_to_help_an_animal_shelter/,all_ads,2018-04-11 23:05:35,52 days 02:29:59.036584000,
"Hi all.

I designed my experiment in a way that I thought would present me with options for analysis, but I'm starting to wonder if one of the options would be considered pseudoreplication.

The experiment was like this:

I work with 2 mussel species (Z and Q).

I put 4 of each species in a pipe in the river (each of the four got their own little chamber inside).

Each pipe had a baffle on the front and back to change water flow into and out of the pipe

I made four baffle types (A, B, C, D), with the intent that each one would allow faster/more water flow than the previous (ie A<B<C<D).

I grouped one of each pipe type together to make a replicate (right?) and attached these replicates to a cinderblock

There were 10 cinderblocks total

I put these underwater, cleaned them every two weeks, and took pre- and post- measurements of mussel body size, and also got attachment strength.

[The pipes looked like this](https://mokacytle.tumblr.com/post/172843351632)

***So, option 1 is:***

I could just analyze the data as a a 4x2 ANOVA, with the pipe type (A, B, C, D) vs. species (Z vs Q) with something like body length as the response variable (I have others), and Cinderblocks as a random blocking factor.

**BUT!** in each pipe, I also included a hard plaster block that dissolved over time under water, ostensibly with higher dissolution due to increased water flow. I checked to see if the pipe baffle had any affect with a one way ANOVA (response variable = amount dissolved; four pipe types) and I found that the baffles basically did what I wanted them to (A<B<C<D), but not as strongly as I had hoped (Only pipes A and D are significantly different).

***Therefore option 2 is:***

I could do an regression/ANCOVA using these plaster blocks, with species as the factor, plaster mass lost as the covariate, and body length as the response variable, with cinderblocks again acting as a random blocking factor.

My questions are:

1) is the second option psuedoreplication, because there is only one measure of flow per pipe, but 4 mussels of each species per pipe?.

2) if it's not psuedoreplication, how do I tell if I should go with the ANOVA or ANCOVA?


I'm using JMP for the analysis, which can't do GLMM unfortunately. Any help would be appreciated.",0,1523522731.0,8bm5xz,False,"Hi all.

I designed my experiment in a way that I thought would present me with options for analysis, but I'm starting to wonder if one of the options would be considered pseudoreplication.

The experiment was like this:

I work with 2 mussel species (Z and Q).

I put 4 of each species in a pipe in the river (each of the four got their own little chamber inside).

Each pipe had a baffle on the front and back to change water flow into and out of the pipe

I made four baffle types (A, B, C, D), with the intent that each one would allow faster/more water flow than the previous (ie A<B<C<D).

I grouped one of each pipe type together to make a replicate (right?) and attached these replicates to a cinderblock

There were 10 cinderblocks total

I put these underwater, cleaned them every two weeks, and took pre- and post- measurements of mussel body size, and also got attachment strength.

[The pipes looked like this](https://mokacytle.tumblr.com/post/172843351632)

***So, option 1 is:***

I could just analyze the data as a a 4x2 ANOVA, with the pipe type (A, B, C, D) vs. species (Z vs Q) with something like body length as the response variable (I have others), and Cinderblocks as a random blocking factor.

**BUT!** in each pipe, I also included a hard plaster block that dissolved over time under water, ostensibly with higher dissolution due to increased water flow. I checked to see if the pipe baffle had any affect with a one way ANOVA (response variable = amount dissolved; four pipe types) and I found that the baffles basically did what I wanted them to (A<B<C<D), but not as strongly as I had hoped (Only pipes A and D are significantly different).

***Therefore option 2 is:***

I could do an regression/ANCOVA using these plaster blocks, with species as the factor, plaster mass lost as the covariate, and body length as the response variable, with cinderblocks again acting as a random blocking factor.

My questions are:

1) is the second option psuedoreplication, because there is only one measure of flow per pipe, but 4 mussels of each species per pipe?.

2) if it's not psuedoreplication, how do I tell if I should go with the ANOVA or ANCOVA?


I'm using JMP for the analysis, which can't do GLMM unfortunately. Any help would be appreciated.",0,"Hi all.

I designed my experiment in a way that I thought would present me with options for analysis, but I'm starting to wonder if one of the options would be considered pseudoreplication.

The experiment was like this:

I work with 2 mussel species (Z and Q).

I put 4 of each species in a pipe in the river (each of the four got their own little chamber inside).

Each pipe had a baffle on the front and back to change water flow into and out of the pipe

I made four baffle types (A, B, C, D), with the intent that each one would allow faster/more water flow than the previous (ie A<B<C<D).

I grouped one of each pipe type together to make a replicate (right?) and attached these replicates to a cinderblock

There were 10 cinderblocks total

I put these underwater, cleaned them every two weeks, and took pre- and post- measurements of mussel body size, and also got attachment strength.

[The pipes looked like this](https://mokacytle.tumblr.com/post/172843351632)

***So, option 1 is:***

I could just analyze the data as a a 4x2 ANOVA, with the pipe type (A, B, C, D) vs. species (Z vs Q) with something like body length as the response variable (I have others), and Cinderblocks as a random blocking factor.

**BUT!** in each pipe, I also included a hard plaster block that dissolved over time under water, ostensibly with higher dissolution due to increased water flow. I checked to see if the pipe baffle had any affect with a one way ANOVA (response variable = amount dissolved; four pipe types) and I found that the baffles basically did what I wanted them to (A<B<C<D), but not as strongly as I had hoped (Only pipes A and D are significantly different).

***Therefore option 2 is:***

I could do an regression/ANCOVA using these plaster blocks, with species as the factor, plaster mass lost as the covariate, and body length as the response variable, with cinderblocks again acting as a random blocking factor.

My questions are:

1) is the second option psuedoreplication, because there is only one measure of flow per pipe, but 4 mussels of each species per pipe?.

2) if it's not psuedoreplication, how do I tell if I should go with the ANOVA or ANCOVA?


I'm using JMP for the analysis, which can't do GLMM unfortunately. Any help would be appreciated.",4,statistics,54935,,Does this analytical choice commit pseudoreplication?,https://www.reddit.com/r/statistics/comments/8bm5xz/does_this_analytical_choice_commit/,all_ads,2018-04-12 04:45:31,51 days 20:50:03.036584000,
"There are many popular science books (example: *Bad Science*) that go into various dodgy ways that stats can be fiddled with. Examples: misleading y-axes, p-hacking, too-low sample size, experiments with inadequate controls, cherry-picking data, etc. Anecdotally, it seems like many of these are making their way into the public consciousness too, which is a welcome development. Even some more esoteric ones like Simpson's Paradox have [youtube videos](https://www.youtube.com/watch?v=ebEkn-BiW5k) with high viewcounts.

What are some more obscure statistical/experimental fallacies and tricks that only experts would really know about?",19,1523496396.0,8bivdm,False,"There are many popular science books (example: *Bad Science*) that go into various dodgy ways that stats can be fiddled with. Examples: misleading y-axes, p-hacking, too-low sample size, experiments with inadequate controls, cherry-picking data, etc. Anecdotally, it seems like many of these are making their way into the public consciousness too, which is a welcome development. Even some more esoteric ones like Simpson's Paradox have [youtube videos](https://www.youtube.com/watch?v=ebEkn-BiW5k) with high viewcounts.

What are some more obscure statistical/experimental fallacies and tricks that only experts would really know about?",0,"There are many popular science books (example: *Bad Science*) that go into various dodgy ways that stats can be fiddled with. Examples: misleading y-axes, p-hacking, too-low sample size, experiments with inadequate controls, cherry-picking data, etc. Anecdotally, it seems like many of these are making their way into the public consciousness too, which is a welcome development. Even some more esoteric ones like Simpson's Paradox have [youtube videos](https://www.youtube.com/watch?v=ebEkn-BiW5k) with high viewcounts.

What are some more obscure statistical/experimental fallacies and tricks that only experts would really know about?",10,statistics,54935,,What are some less well-known misleading statistical tricks?,https://www.reddit.com/r/statistics/comments/8bivdm/what_are_some_less_wellknown_misleading/,all_ads,2018-04-11 21:26:36,52 days 04:08:58.036584000,
"Hi all,

I have regression models, ANOVAS and ANCOVAS that included blocking (RCB) because of the way I set up my field studies. Without going into all the details (I can provide them later if you want), is it acceptable to remove the random factor if REML variance estimates show a non-significant Wald p-value, and a percentage of the total variation in the 1-10% range?

If it's acceptable, is there a cutoff (ie 5% like a 0.05 p value)?
",2,1523507749.0,8bkdjs,False,"Hi all,

I have regression models, ANOVAS and ANCOVAS that included blocking (RCB) because of the way I set up my field studies. Without going into all the details (I can provide them later if you want), is it acceptable to remove the random factor if REML variance estimates show a non-significant Wald p-value, and a percentage of the total variation in the 1-10% range?

If it's acceptable, is there a cutoff (ie 5% like a 0.05 p value)?
",0,"Hi all,

I have regression models, ANOVAS and ANCOVAS that included blocking (RCB) because of the way I set up my field studies. Without going into all the details (I can provide them later if you want), is it acceptable to remove the random factor if REML variance estimates show a non-significant Wald p-value, and a percentage of the total variation in the 1-10% range?

If it's acceptable, is there a cutoff (ie 5% like a 0.05 p value)?
",5,statistics,54935,,Can you remove the random factor of blocking if it contributes a small amount variation to the model? (REML),https://www.reddit.com/r/statistics/comments/8bkdjs/can_you_remove_the_random_factor_of_blocking_if/,all_ads,2018-04-12 00:35:49,52 days 00:59:45.036584000,
"I have been able to find little literature related to the topic online (e.g., [A note on the Expected Value of an Inverse Matrix](https://www.researchgate.net/profile/Theodore_Groves/publication/31400997_A_Note_on_the_Expected_Value_of_an_Inverse_Matrix/links/02bfe511db35022767000000/A-Note-on-the-Expected-Value-of-an-Inverse-Matrix.pdf) such that E($\frac{1}{X})$ $\geq$ ($\frac{1}{E(X)}$\), though, vaguely, I am working on a project in which an inequality for expectations of matrices with powers other than the inverse might come in handy.

While my search led me to a particular paper which had seemingly solved this problem, it had apparently been redacted (see: [Editor's Note: A Note on the Expected Values of Powers of a Matrix](http://www.jstor.org.uml.idm.oclc.org/stable/3314700)). Has anyone come across any useful literature on the topic of random matrix powers?",1,1523532321.0,8bn6iq,False,"I have been able to find little literature related to the topic online (e.g., [A note on the Expected Value of an Inverse Matrix](https://www.researchgate.net/profile/Theodore_Groves/publication/31400997_A_Note_on_the_Expected_Value_of_an_Inverse_Matrix/links/02bfe511db35022767000000/A-Note-on-the-Expected-Value-of-an-Inverse-Matrix.pdf) such that E($\frac{1}{X})$ $\geq$ ($\frac{1}{E(X)}$\), though, vaguely, I am working on a project in which an inequality for expectations of matrices with powers other than the inverse might come in handy.

While my search led me to a particular paper which had seemingly solved this problem, it had apparently been redacted (see: [Editor's Note: A Note on the Expected Values of Powers of a Matrix](http://www.jstor.org.uml.idm.oclc.org/stable/3314700)). Has anyone come across any useful literature on the topic of random matrix powers?",0,"I have been able to find little literature related to the topic online (e.g., [A note on the Expected Value of an Inverse Matrix](https://www.researchgate.net/profile/Theodore_Groves/publication/31400997_A_Note_on_the_Expected_Value_of_an_Inverse_Matrix/links/02bfe511db35022767000000/A-Note-on-the-Expected-Value-of-an-Inverse-Matrix.pdf) such that E($\frac{1}{X})$ $\geq$ ($\frac{1}{E(X)}$\), though, vaguely, I am working on a project in which an inequality for expectations of matrices with powers other than the inverse might come in handy.

While my search led me to a particular paper which had seemingly solved this problem, it had apparently been redacted (see: [Editor's Note: A Note on the Expected Values of Powers of a Matrix](http://www.jstor.org.uml.idm.oclc.org/stable/3314700)). Has anyone come across any useful literature on the topic of random matrix powers?",1,statistics,54935,,Theorems for the Expectation of Matrices Taken to a Power,https://www.reddit.com/r/statistics/comments/8bn6iq/theorems_for_the_expectation_of_matrices_taken_to/,all_ads,2018-04-12 07:25:21,51 days 18:10:13.036584000,
"Hi, I'm a medical resident with no formal training in statistics, though I've enjoyed dipping into it for clinical research projects before.  I'm wondering if I can ask for some advice on a clinical project I've got going on now.

The basic gist is that I have a lot of predictor variables.  Most of them are binary except for the experimental category variable, of which I have four levels including a control group.  I also have a few continuous response variables, though if it's easier I can smash them all together into a single averaged-out response variable.  I'm trying to figure out what kind of analysis to run - basically, I want to see if my experimental groups have significantly different effects on the response variable when compared to a control group, while also controlling for all the other binary predictor variables.  

I've read that I can use a multiple regression with dummy variables to force the yes/no variables to work with it, but is there another way to do it?

Thanks very much!",16,1523517743.0,8blm06,False,"Hi, I'm a medical resident with no formal training in statistics, though I've enjoyed dipping into it for clinical research projects before.  I'm wondering if I can ask for some advice on a clinical project I've got going on now.

The basic gist is that I have a lot of predictor variables.  Most of them are binary except for the experimental category variable, of which I have four levels including a control group.  I also have a few continuous response variables, though if it's easier I can smash them all together into a single averaged-out response variable.  I'm trying to figure out what kind of analysis to run - basically, I want to see if my experimental groups have significantly different effects on the response variable when compared to a control group, while also controlling for all the other binary predictor variables.  

I've read that I can use a multiple regression with dummy variables to force the yes/no variables to work with it, but is there another way to do it?

Thanks very much!",0,"Hi, I'm a medical resident with no formal training in statistics, though I've enjoyed dipping into it for clinical research projects before.  I'm wondering if I can ask for some advice on a clinical project I've got going on now.

The basic gist is that I have a lot of predictor variables.  Most of them are binary except for the experimental category variable, of which I have four levels including a control group.  I also have a few continuous response variables, though if it's easier I can smash them all together into a single averaged-out response variable.  I'm trying to figure out what kind of analysis to run - basically, I want to see if my experimental groups have significantly different effects on the response variable when compared to a control group, while also controlling for all the other binary predictor variables.  

I've read that I can use a multiple regression with dummy variables to force the yes/no variables to work with it, but is there another way to do it?

Thanks very much!",2,statistics,54935,,Which analysis to run?,https://www.reddit.com/r/statistics/comments/8blm06/which_analysis_to_run/,all_ads,2018-04-12 03:22:23,51 days 22:13:11.036584000,
"I play a lot of live backgammon. Invariably, as a player, you can't help but think about the probability of dice rolls. And also, of course, you start to see patterns or question probability based on observations. In particular I have a question about repeating numbers and the odds of rolling sequences.

So in backgammon you roll two dice, and when recording rolls they are noted as 2-digit sequences. If I roll a 4 on the first die and a 3 on the second, that is a 43. This is just to clarify how the rolls are identified.

So let's say you're playing a game, and you roll a 54. Make your move pick up your dice, then the next time you roll and it's a 54 again. Let's say now that it happens a third time, 54 again. The way I understand it, the odds of rolling any particular combination is 1/36 (about 2.8%). And if I understand what I think I do, then the odds of a following 54 are (1/36)*(1/36). That's like 0.08% chance if my math is correct. And then the third would be just another 1/36 thrown in, and that gets us to 0.01% chance.

So what I'm wondering is whether or not the odds of rolling a 54, 54, 54 are really any different than  rolling a 64, 12, 23. Rolling any combination (aside from doubles) is the same, so it shouldn't be any different, right? We just don't notice the rolls when they're different numbers. I'm looking for some insight, and also would love a link to any kind of study or report about how bad humans are at identifying true randomness. I have heard about the [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler%27s_fallacy), but that isn't really what I'm looking for. Thanks for reading!",5,1523530845.0,8bn16g,False,"I play a lot of live backgammon. Invariably, as a player, you can't help but think about the probability of dice rolls. And also, of course, you start to see patterns or question probability based on observations. In particular I have a question about repeating numbers and the odds of rolling sequences.

So in backgammon you roll two dice, and when recording rolls they are noted as 2-digit sequences. If I roll a 4 on the first die and a 3 on the second, that is a 43. This is just to clarify how the rolls are identified.

So let's say you're playing a game, and you roll a 54. Make your move pick up your dice, then the next time you roll and it's a 54 again. Let's say now that it happens a third time, 54 again. The way I understand it, the odds of rolling any particular combination is 1/36 (about 2.8%). And if I understand what I think I do, then the odds of a following 54 are (1/36)*(1/36). That's like 0.08% chance if my math is correct. And then the third would be just another 1/36 thrown in, and that gets us to 0.01% chance.

So what I'm wondering is whether or not the odds of rolling a 54, 54, 54 are really any different than  rolling a 64, 12, 23. Rolling any combination (aside from doubles) is the same, so it shouldn't be any different, right? We just don't notice the rolls when they're different numbers. I'm looking for some insight, and also would love a link to any kind of study or report about how bad humans are at identifying true randomness. I have heard about the [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler%27s_fallacy), but that isn't really what I'm looking for. Thanks for reading!",0,"I play a lot of live backgammon. Invariably, as a player, you can't help but think about the probability of dice rolls. And also, of course, you start to see patterns or question probability based on observations. In particular I have a question about repeating numbers and the odds of rolling sequences.

So in backgammon you roll two dice, and when recording rolls they are noted as 2-digit sequences. If I roll a 4 on the first die and a 3 on the second, that is a 43. This is just to clarify how the rolls are identified.

So let's say you're playing a game, and you roll a 54. Make your move pick up your dice, then the next time you roll and it's a 54 again. Let's say now that it happens a third time, 54 again. The way I understand it, the odds of rolling any particular combination is 1/36 (about 2.8%). And if I understand what I think I do, then the odds of a following 54 are (1/36)*(1/36). That's like 0.08% chance if my math is correct. And then the third would be just another 1/36 thrown in, and that gets us to 0.01% chance.

So what I'm wondering is whether or not the odds of rolling a 54, 54, 54 are really any different than  rolling a 64, 12, 23. Rolling any combination (aside from doubles) is the same, so it shouldn't be any different, right? We just don't notice the rolls when they're different numbers. I'm looking for some insight, and also would love a link to any kind of study or report about how bad humans are at identifying true randomness. I have heard about the [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler%27s_fallacy), but that isn't really what I'm looking for. Thanks for reading!",0,statistics,54935,,Understanding dice rolls in backgammon.,https://www.reddit.com/r/statistics/comments/8bn16g/understanding_dice_rolls_in_backgammon/,all_ads,2018-04-12 07:00:45,51 days 18:34:49.036584000,
"Hello, my question is basically about combining two datasets which have similar features, but were generated via different processes in order to model a customer satisfaction score and get an idea of which features are most important to higher scores. Then ideally, the company will work on improving the identified areas where possible. I am working on this project for a very old company where many (my bosses) are not exceptionally knowledgeable of stats, and methodologies have previously been nothing more advanced than graphs/OLS.

The data I am attempting to model the customer score of is compiled from two sets: one generated via phone interactions and one via web interactions, so there are similar features, but also mutually exclusive ones (leading to quite a bit of NAs). I don't think this is the correct procedure, and believe we should investigate the processes separately. But.. this is my first actual career out of school, and technically I was hired for a reporting role but I've been able to wiggle my way into this project and am very much hoping this project will help facilitate my transition into a more statistical role, so I want to be sure I'm correct in my thought process.

My plan after sorting the data issues is to run some ordinal regression model(s) via logistic methods, in addition to some ordinal-specific random forest implementations in order to try and tease out the most useful features.

Any input is very much appreciated, especially if anyone may have done similar types of analysis, thank you.",3,1523491094.0,8bi6ce,False,"Hello, my question is basically about combining two datasets which have similar features, but were generated via different processes in order to model a customer satisfaction score and get an idea of which features are most important to higher scores. Then ideally, the company will work on improving the identified areas where possible. I am working on this project for a very old company where many (my bosses) are not exceptionally knowledgeable of stats, and methodologies have previously been nothing more advanced than graphs/OLS.

The data I am attempting to model the customer score of is compiled from two sets: one generated via phone interactions and one via web interactions, so there are similar features, but also mutually exclusive ones (leading to quite a bit of NAs). I don't think this is the correct procedure, and believe we should investigate the processes separately. But.. this is my first actual career out of school, and technically I was hired for a reporting role but I've been able to wiggle my way into this project and am very much hoping this project will help facilitate my transition into a more statistical role, so I want to be sure I'm correct in my thought process.

My plan after sorting the data issues is to run some ordinal regression model(s) via logistic methods, in addition to some ordinal-specific random forest implementations in order to try and tease out the most useful features.

Any input is very much appreciated, especially if anyone may have done similar types of analysis, thank you.",0,"Hello, my question is basically about combining two datasets which have similar features, but were generated via different processes in order to model a customer satisfaction score and get an idea of which features are most important to higher scores. Then ideally, the company will work on improving the identified areas where possible. I am working on this project for a very old company where many (my bosses) are not exceptionally knowledgeable of stats, and methodologies have previously been nothing more advanced than graphs/OLS.

The data I am attempting to model the customer score of is compiled from two sets: one generated via phone interactions and one via web interactions, so there are similar features, but also mutually exclusive ones (leading to quite a bit of NAs). I don't think this is the correct procedure, and believe we should investigate the processes separately. But.. this is my first actual career out of school, and technically I was hired for a reporting role but I've been able to wiggle my way into this project and am very much hoping this project will help facilitate my transition into a more statistical role, so I want to be sure I'm correct in my thought process.

My plan after sorting the data issues is to run some ordinal regression model(s) via logistic methods, in addition to some ordinal-specific random forest implementations in order to try and tease out the most useful features.

Any input is very much appreciated, especially if anyone may have done similar types of analysis, thank you.",6,statistics,54935,,Combining data generated via two different processes for modeling?,https://www.reddit.com/r/statistics/comments/8bi6ce/combining_data_generated_via_two_different/,all_ads,2018-04-11 19:58:14,52 days 05:37:20.036584000,
"At the moment, I am using ridge and lasso regression for an inferential task. However, both techniques introduce bias on their coefficients deliberately in order to minimize the test error, which can make inferential tasks confusing. 

Is it reasonable to directly assess the lasso/ridge coefficients as effects on the response? ",9,1523487521.0,8bhpjk,False,"At the moment, I am using ridge and lasso regression for an inferential task. However, both techniques introduce bias on their coefficients deliberately in order to minimize the test error, which can make inferential tasks confusing. 

Is it reasonable to directly assess the lasso/ridge coefficients as effects on the response? ",0,"At the moment, I am using ridge and lasso regression for an inferential task. However, both techniques introduce bias on their coefficients deliberately in order to minimize the test error, which can make inferential tasks confusing. 

Is it reasonable to directly assess the lasso/ridge coefficients as effects on the response? ",5,statistics,54935,,Penalized Regression Coefficients: Are they interpretable?,https://www.reddit.com/r/statistics/comments/8bhpjk/penalized_regression_coefficients_are_they/,all_ads,2018-04-11 18:58:41,52 days 06:36:53.036584000,
"In this premise, we aren't taking the square root of the sum (or sums, rather).",10,1523490089.0,8bi1j9,False,"In this premise, we aren't taking the square root of the sum (or sums, rather).",0,"In this premise, we aren't taking the square root of the sum (or sums, rather).",2,statistics,54935,,"Are there any advantages to using the sum of squares (to, say, estimate error) as opposed to using the sum of absolute values?",https://www.reddit.com/r/statistics/comments/8bi1j9/are_there_any_advantages_to_using_the_sum_of/,all_ads,2018-04-11 19:41:29,52 days 05:54:05.036584000,
"Hi folks, I hope you are doing well. I am a Statistics undergraduate student. A Stats graduate student at my university has recently talked about her internship in which she used Statistics and Bioinformatics to help patients with stroke. I got inspired a lot by that, and want to seriously consider if Bioinformatics could be an area of application in my career as a Statistician/Data Scientist. I have a couple of questions, and I hope you can help with them.

1. Could you talk a little bit about your background? That is, speaking of your first job involved Bioinformatics, what was your main area of knowledge and level of education? 

2. What are some highlights of your current work? In terms of involved projects and satisfaction.

3. Although I have a good computational foundation, with a degree in Statistics, I would love to do more statistical data analysis. What are some areas in Bioinformatics that I can apply/learn statistical skills?

4. I also plan to do a Masters in Statistics. Would it be a good move? Do you have any advice on how I should prepare for the profession? I am based in Silicon Valley, so I hope the location may help a bit. 

Although the title says Statistician/Data Scientists, I will just equally appreciate if anyone who has experience can help me. All inputs are welcomed. Thank you so much.",12,1523448104.0,8be9hi,False,"Hi folks, I hope you are doing well. I am a Statistics undergraduate student. A Stats graduate student at my university has recently talked about her internship in which she used Statistics and Bioinformatics to help patients with stroke. I got inspired a lot by that, and want to seriously consider if Bioinformatics could be an area of application in my career as a Statistician/Data Scientist. I have a couple of questions, and I hope you can help with them.

1. Could you talk a little bit about your background? That is, speaking of your first job involved Bioinformatics, what was your main area of knowledge and level of education? 

2. What are some highlights of your current work? In terms of involved projects and satisfaction.

3. Although I have a good computational foundation, with a degree in Statistics, I would love to do more statistical data analysis. What are some areas in Bioinformatics that I can apply/learn statistical skills?

4. I also plan to do a Masters in Statistics. Would it be a good move? Do you have any advice on how I should prepare for the profession? I am based in Silicon Valley, so I hope the location may help a bit. 

Although the title says Statistician/Data Scientists, I will just equally appreciate if anyone who has experience can help me. All inputs are welcomed. Thank you so much.",0,"Hi folks, I hope you are doing well. I am a Statistics undergraduate student. A Stats graduate student at my university has recently talked about her internship in which she used Statistics and Bioinformatics to help patients with stroke. I got inspired a lot by that, and want to seriously consider if Bioinformatics could be an area of application in my career as a Statistician/Data Scientist. I have a couple of questions, and I hope you can help with them.

1. Could you talk a little bit about your background? That is, speaking of your first job involved Bioinformatics, what was your main area of knowledge and level of education? 

2. What are some highlights of your current work? In terms of involved projects and satisfaction.

3. Although I have a good computational foundation, with a degree in Statistics, I would love to do more statistical data analysis. What are some areas in Bioinformatics that I can apply/learn statistical skills?

4. I also plan to do a Masters in Statistics. Would it be a good move? Do you have any advice on how I should prepare for the profession? I am based in Silicon Valley, so I hope the location may help a bit. 

Although the title says Statistician/Data Scientists, I will just equally appreciate if anyone who has experience can help me. All inputs are welcomed. Thank you so much.",13,statistics,54935,,"Statisticians/Data Scientists working in Bioinformatics, could you help me with some questions?",https://www.reddit.com/r/statistics/comments/8be9hi/statisticiansdata_scientists_working_in/,all_ads,2018-04-11 08:01:44,52 days 17:33:50.036584000,
"How do you test for spurious regression? Is it just using the DW test statistic?
",2,1523468644.0,8bftic,False,"How do you test for spurious regression? Is it just using the DW test statistic?
",0,"How do you test for spurious regression? Is it just using the DW test statistic?
",1,statistics,54935,,How to test for spurious regression?,https://www.reddit.com/r/statistics/comments/8bftic/how_to_test_for_spurious_regression/,all_ads,2018-04-11 13:44:04,52 days 11:51:30.036584000,
"https://jamanetwork.com/journals/jama/article-abstract/2676503?redirect=true

Do you all have differing views of this possible paradigm shift? What fields might it effect most negatively and positively?",21,1523412597.0,8b9vj9,False,"https://jamanetwork.com/journals/jama/article-abstract/2676503?redirect=true

Do you all have differing views of this possible paradigm shift? What fields might it effect most negatively and positively?",0,"https://jamanetwork.com/journals/jama/article-abstract/2676503?redirect=true

Do you all have differing views of this possible paradigm shift? What fields might it effect most negatively and positively?",9,statistics,54935,,Lowering P-Value Threshold Standard?,https://www.reddit.com/r/statistics/comments/8b9vj9/lowering_pvalue_threshold_standard/,all_ads,2018-04-10 22:09:57,53 days 03:25:37.036584000,
"Hi everyone!

As a totally personal project over the last few days I have been working on a small - *small* - package for Julia that does some basic regressions (think STATA):

**[Alistair.jl](https://github.com/giob1994/Alistair.jl)**

I recognize that it is quite the amateur work, but I am so far quite proud of the results (this is my first serious work developing a package in Julia, to be honest) and I wanted to share! 

The feature that this library aims for is *speed*: it executes all the main linear regression tasks (OLS, FGLS, Iterated GLS) quite fast, and works directly with Julia Arrays (no pesky abstractions). It even supports (in prototype) non-linear regressions through Optim.jl!

I am thinking of adding some more features, so I'd definitely like if more experienced folks took a look and gave me an honest feedback, spotting the errors/pain points I most certainly put in there. Suggestions are obviously welcome. Finally, if somebody wants to help it would be great!

Thanks!",7,1523387020.0,8b6r5n,False,"Hi everyone!

As a totally personal project over the last few days I have been working on a small - *small* - package for Julia that does some basic regressions (think STATA):

**[Alistair.jl](https://github.com/giob1994/Alistair.jl)**

I recognize that it is quite the amateur work, but I am so far quite proud of the results (this is my first serious work developing a package in Julia, to be honest) and I wanted to share! 

The feature that this library aims for is *speed*: it executes all the main linear regression tasks (OLS, FGLS, Iterated GLS) quite fast, and works directly with Julia Arrays (no pesky abstractions). It even supports (in prototype) non-linear regressions through Optim.jl!

I am thinking of adding some more features, so I'd definitely like if more experienced folks took a look and gave me an honest feedback, spotting the errors/pain points I most certainly put in there. Suggestions are obviously welcome. Finally, if somebody wants to help it would be great!

Thanks!",0,"Hi everyone!

As a totally personal project over the last few days I have been working on a small - *small* - package for Julia that does some basic regressions (think STATA):

**[Alistair.jl](https://github.com/giob1994/Alistair.jl)**

I recognize that it is quite the amateur work, but I am so far quite proud of the results (this is my first serious work developing a package in Julia, to be honest) and I wanted to share! 

The feature that this library aims for is *speed*: it executes all the main linear regression tasks (OLS, FGLS, Iterated GLS) quite fast, and works directly with Julia Arrays (no pesky abstractions). It even supports (in prototype) non-linear regressions through Optim.jl!

I am thinking of adding some more features, so I'd definitely like if more experienced folks took a look and gave me an honest feedback, spotting the errors/pain points I most certainly put in there. Suggestions are obviously welcome. Finally, if somebody wants to help it would be great!

Thanks!",27,statistics,54935,,A minimal regression library for Julia,https://www.reddit.com/r/statistics/comments/8b6r5n/a_minimal_regression_library_for_julia/,all_ads,2018-04-10 15:03:40,53 days 10:31:54.036584000,
"So I have no experience in statistics and while I'm learning a bit here and there, I'm looking for some good resources to teach myself at this time. Do you self-taught guys have any recommendations for books or sites? I ultimately want to try and go the data science or analyst route and all the roles I'm looking at have stats as a requirement. I would appreciate any help.",8,1523424835.0,8bbmsn,False,"So I have no experience in statistics and while I'm learning a bit here and there, I'm looking for some good resources to teach myself at this time. Do you self-taught guys have any recommendations for books or sites? I ultimately want to try and go the data science or analyst route and all the roles I'm looking at have stats as a requirement. I would appreciate any help.",0,"So I have no experience in statistics and while I'm learning a bit here and there, I'm looking for some good resources to teach myself at this time. Do you self-taught guys have any recommendations for books or sites? I ultimately want to try and go the data science or analyst route and all the roles I'm looking at have stats as a requirement. I would appreciate any help.",3,statistics,54935,,"I currently work in market research handling insights for ad campaigns and I'm trying to learn statistics, what are some good books/resources?",https://www.reddit.com/r/statistics/comments/8bbmsn/i_currently_work_in_market_research_handling/,all_ads,2018-04-11 01:33:55,53 days 00:01:39.036584000,
"Let me start off by saying I fucked up. I have a research project due in two days and I started it a while ago but haven’t finished. (This was a group project but my partners withdrew from the class so I’m on my own)

Topic: what affects the price of a call option on a growth stock?

I collected historical option prices the same call option over 44 consecutive days. Jan/19 is the expiration date, and I used the same strike price.

The independent variables I collected were: implied volatility, Volume, whether or not there was an earnings report within 5 days(dummy variable) and the price of the stock.

I didn’t take into account that my sample wasn’t random cross sectional, but time series instead. So of course I just used OLS to pump out my equation and descriptive statistics in r.

Today I was explaining to a friend the LSAs, and as I said iid, I realized I fucked up.

We haven’t done anything with time series analysis in my class so I have no idea about that, I don’t have time to get new data/change my project.

What is required for the report: A) intro B)Data C)Plots of Independent & dependent variables D)model specification & regression results E)model fitting F)inference & interpretations of significant coeff estimates G) summary

What’re my options since OLS is out of the window, must use a GLM. Is MLE the way to go?
",4,1523438495.0,8bdb53,False,"Let me start off by saying I fucked up. I have a research project due in two days and I started it a while ago but haven’t finished. (This was a group project but my partners withdrew from the class so I’m on my own)

Topic: what affects the price of a call option on a growth stock?

I collected historical option prices the same call option over 44 consecutive days. Jan/19 is the expiration date, and I used the same strike price.

The independent variables I collected were: implied volatility, Volume, whether or not there was an earnings report within 5 days(dummy variable) and the price of the stock.

I didn’t take into account that my sample wasn’t random cross sectional, but time series instead. So of course I just used OLS to pump out my equation and descriptive statistics in r.

Today I was explaining to a friend the LSAs, and as I said iid, I realized I fucked up.

We haven’t done anything with time series analysis in my class so I have no idea about that, I don’t have time to get new data/change my project.

What is required for the report: A) intro B)Data C)Plots of Independent & dependent variables D)model specification & regression results E)model fitting F)inference & interpretations of significant coeff estimates G) summary

What’re my options since OLS is out of the window, must use a GLM. Is MLE the way to go?
",0,"Let me start off by saying I fucked up. I have a research project due in two days and I started it a while ago but haven’t finished. (This was a group project but my partners withdrew from the class so I’m on my own)

Topic: what affects the price of a call option on a growth stock?

I collected historical option prices the same call option over 44 consecutive days. Jan/19 is the expiration date, and I used the same strike price.

The independent variables I collected were: implied volatility, Volume, whether or not there was an earnings report within 5 days(dummy variable) and the price of the stock.

I didn’t take into account that my sample wasn’t random cross sectional, but time series instead. So of course I just used OLS to pump out my equation and descriptive statistics in r.

Today I was explaining to a friend the LSAs, and as I said iid, I realized I fucked up.

We haven’t done anything with time series analysis in my class so I have no idea about that, I don’t have time to get new data/change my project.

What is required for the report: A) intro B)Data C)Plots of Independent & dependent variables D)model specification & regression results E)model fitting F)inference & interpretations of significant coeff estimates G) summary

What’re my options since OLS is out of the window, must use a GLM. Is MLE the way to go?
",2,statistics,54935,,Help with a research project,https://www.reddit.com/r/statistics/comments/8bdb53/help_with_a_research_project/,all_ads,2018-04-11 05:21:35,52 days 20:13:59.036584000,
"In the stick-breaking construction of Dirichlet (let me base things on Sethuraman's construction - slide 6 of [this][1]) do we sample one $\phi$ vector from the base distribution $H$ and use it for sampling $\phi_k$ at each step $k$? This is what I ""think"" they actually do. 

But I am puzzled, as in their paper they mention $H$ is a symmetric Dirichlet distribution over the vocabulary (see the third paragraph of section 2 of [this][2]). Which means the dimensionality of $\phi$ would be the size of the vocabulary. Then it is not clear how the index $k$ of $\phi_k$s are mapped to the index of $\beta_k$s. Basically sampling $\beta_k$ will stop if no more stick is left, which means the $k$ index of $\beta_k$ can potentially be much smaller than the size of the $\phi$ vector of size $|\text{Vocabulary}|$ sampled from $H$.

My guess is that they first sample $\phi_k$s, but all they keep from the sample is its corresponding index in $\phi$. Then the sampled $\beta_k$ is just a weight associated with that index. This ways it is clear what each $\beta_k$ corresponds to.

  [1]: http://videolectures.net/aistats2011_wang_online/?q=stick-breaking
  [2]: http://proceedings.mlr.press/v15/wang11a/wang11a.pdf",3,1523404983.0,8b8qzy,False,"In the stick-breaking construction of Dirichlet (let me base things on Sethuraman's construction - slide 6 of [this][1]) do we sample one $\phi$ vector from the base distribution $H$ and use it for sampling $\phi_k$ at each step $k$? This is what I ""think"" they actually do. 

But I am puzzled, as in their paper they mention $H$ is a symmetric Dirichlet distribution over the vocabulary (see the third paragraph of section 2 of [this][2]). Which means the dimensionality of $\phi$ would be the size of the vocabulary. Then it is not clear how the index $k$ of $\phi_k$s are mapped to the index of $\beta_k$s. Basically sampling $\beta_k$ will stop if no more stick is left, which means the $k$ index of $\beta_k$ can potentially be much smaller than the size of the $\phi$ vector of size $|\text{Vocabulary}|$ sampled from $H$.

My guess is that they first sample $\phi_k$s, but all they keep from the sample is its corresponding index in $\phi$. Then the sampled $\beta_k$ is just a weight associated with that index. This ways it is clear what each $\beta_k$ corresponds to.

  [1]: http://videolectures.net/aistats2011_wang_online/?q=stick-breaking
  [2]: http://proceedings.mlr.press/v15/wang11a/wang11a.pdf",0,"In the stick-breaking construction of Dirichlet (let me base things on Sethuraman's construction - slide 6 of [this][1]) do we sample one $\phi$ vector from the base distribution $H$ and use it for sampling $\phi_k$ at each step $k$? This is what I ""think"" they actually do. 

But I am puzzled, as in their paper they mention $H$ is a symmetric Dirichlet distribution over the vocabulary (see the third paragraph of section 2 of [this][2]). Which means the dimensionality of $\phi$ would be the size of the vocabulary. Then it is not clear how the index $k$ of $\phi_k$s are mapped to the index of $\beta_k$s. Basically sampling $\beta_k$ will stop if no more stick is left, which means the $k$ index of $\beta_k$ can potentially be much smaller than the size of the $\phi$ vector of size $|\text{Vocabulary}|$ sampled from $H$.

My guess is that they first sample $\phi_k$s, but all they keep from the sample is its corresponding index in $\phi$. Then the sampled $\beta_k$ is just a weight associated with that index. This ways it is clear what each $\beta_k$ corresponds to.

  [1]: http://videolectures.net/aistats2011_wang_online/?q=stick-breaking
  [2]: http://proceedings.mlr.press/v15/wang11a/wang11a.pdf",5,statistics,54935,,Stick-breaking construction of Dirichlet process,https://www.reddit.com/r/statistics/comments/8b8qzy/stickbreaking_construction_of_dirichlet_process/,all_ads,2018-04-10 20:03:03,53 days 05:32:31.036584000,
"So I have a table of values in excel on bird sightings over the past 7 years. Each row is a year, and each column a month in the breeding season. So the rows are 2010, 2011, etc. And the columns are Feb, Mar, April, etc. The table is filled with values on bird sightings of a species (20, 100, etc). 

I want to see if bird sightings have changed between years (comparing 2010 to 2017 for example), so I computed averages for each year by row(since each row of values is a year), and then an a 1-factor ANOVA on it to get a p-value. 

**The Main Question:**

But I also want to compare on a month-by-month basis regardless of what year(row) they are in. Do I also compute the averages for each month(like all March values from 2010 to 2017) and use those for a 1-factor ANOVA too? Or do I do something different?

Here is a mockup table of my values of what I am doing roughly in Excel.
https://pastebin.com/uZaAiJyr",11,1523425517.0,8bbpxo,False,"So I have a table of values in excel on bird sightings over the past 7 years. Each row is a year, and each column a month in the breeding season. So the rows are 2010, 2011, etc. And the columns are Feb, Mar, April, etc. The table is filled with values on bird sightings of a species (20, 100, etc). 

I want to see if bird sightings have changed between years (comparing 2010 to 2017 for example), so I computed averages for each year by row(since each row of values is a year), and then an a 1-factor ANOVA on it to get a p-value. 

**The Main Question:**

But I also want to compare on a month-by-month basis regardless of what year(row) they are in. Do I also compute the averages for each month(like all March values from 2010 to 2017) and use those for a 1-factor ANOVA too? Or do I do something different?

Here is a mockup table of my values of what I am doing roughly in Excel.
https://pastebin.com/uZaAiJyr",0,"So I have a table of values in excel on bird sightings over the past 7 years. Each row is a year, and each column a month in the breeding season. So the rows are 2010, 2011, etc. And the columns are Feb, Mar, April, etc. The table is filled with values on bird sightings of a species (20, 100, etc). 

I want to see if bird sightings have changed between years (comparing 2010 to 2017 for example), so I computed averages for each year by row(since each row of values is a year), and then an a 1-factor ANOVA on it to get a p-value. 

**The Main Question:**

But I also want to compare on a month-by-month basis regardless of what year(row) they are in. Do I also compute the averages for each month(like all March values from 2010 to 2017) and use those for a 1-factor ANOVA too? Or do I do something different?

Here is a mockup table of my values of what I am doing roughly in Excel.
https://pastebin.com/uZaAiJyr",1,statistics,54935,,is 1-factor ANOVA right to use in this situation(Excel),https://www.reddit.com/r/statistics/comments/8bbpxo/is_1factor_anova_right_to_use_in_this/,all_ads,2018-04-11 01:45:17,52 days 23:50:17.036584000,
"Hello,

I'm looking at Cook's distance for checking my leverage points in my data and I don't quite follow the intuition for using beta parameters in the calculations while the fitted values work i.e. the matrix notation D_i=(yhat_(i)-y)^T (yhat_(i)-y) = (Bhat_(i)-Bhat)x^T x(Bhat_(i)-Bhat).

I understand that cook's distance offers a geometric description of the Euclidean distance in the fitted values matrix form, but the beta notation doesn't offer the same ease of understanding. Does someone have a better understanding of this beta matrix notation and can provide a reason why it's equivalent?",2,1523423045.0,8bbdtg,False,"Hello,

I'm looking at Cook's distance for checking my leverage points in my data and I don't quite follow the intuition for using beta parameters in the calculations while the fitted values work i.e. the matrix notation D_i=(yhat_(i)-y)^T (yhat_(i)-y) = (Bhat_(i)-Bhat)x^T x(Bhat_(i)-Bhat).

I understand that cook's distance offers a geometric description of the Euclidean distance in the fitted values matrix form, but the beta notation doesn't offer the same ease of understanding. Does someone have a better understanding of this beta matrix notation and can provide a reason why it's equivalent?",0,"Hello,

I'm looking at Cook's distance for checking my leverage points in my data and I don't quite follow the intuition for using beta parameters in the calculations while the fitted values work i.e. the matrix notation D_i=(yhat_(i)-y)^T (yhat_(i)-y) = (Bhat_(i)-Bhat)x^T x(Bhat_(i)-Bhat).

I understand that cook's distance offers a geometric description of the Euclidean distance in the fitted values matrix form, but the beta notation doesn't offer the same ease of understanding. Does someone have a better understanding of this beta matrix notation and can provide a reason why it's equivalent?",1,statistics,54935,,Why can cook's distance be found using fitted values AND beta parameters? The intuition only makes sense for fitted values,https://www.reddit.com/r/statistics/comments/8bbdtg/why_can_cooks_distance_be_found_using_fitted/,all_ads,2018-04-11 01:04:05,53 days 00:31:29.036584000,
"Hello, after a few hours searching for information without results I thought I would just ask.  I am comparing bin counts between two conditions.  Normal values range from ~2-15 and there are plenty of zeros scattered throughout.  My goal is to determine the influence of initial value in one condition on the ratio obtained between two conditions.  

The problem is that the zeros of course can't return normal ratios, and I feel that to just throw out ratios where a bin changed from x to 0 or vice versa would be a serious problem as it would skew the overall results.  My first thought is to transform the data by adding 1 to all bins, this eliminates the first problem, but of course changes the ratios dramatically for the smaller numbers.  

Are there any perhaps better transformations I could apply to the data?  Or is there a statistical method that might conserve some information from the nan or inf bins?",3,1523420987.0,8bb33m,False,"Hello, after a few hours searching for information without results I thought I would just ask.  I am comparing bin counts between two conditions.  Normal values range from ~2-15 and there are plenty of zeros scattered throughout.  My goal is to determine the influence of initial value in one condition on the ratio obtained between two conditions.  

The problem is that the zeros of course can't return normal ratios, and I feel that to just throw out ratios where a bin changed from x to 0 or vice versa would be a serious problem as it would skew the overall results.  My first thought is to transform the data by adding 1 to all bins, this eliminates the first problem, but of course changes the ratios dramatically for the smaller numbers.  

Are there any perhaps better transformations I could apply to the data?  Or is there a statistical method that might conserve some information from the nan or inf bins?",0,"Hello, after a few hours searching for information without results I thought I would just ask.  I am comparing bin counts between two conditions.  Normal values range from ~2-15 and there are plenty of zeros scattered throughout.  My goal is to determine the influence of initial value in one condition on the ratio obtained between two conditions.  

The problem is that the zeros of course can't return normal ratios, and I feel that to just throw out ratios where a bin changed from x to 0 or vice versa would be a serious problem as it would skew the overall results.  My first thought is to transform the data by adding 1 to all bins, this eliminates the first problem, but of course changes the ratios dramatically for the smaller numbers.  

Are there any perhaps better transformations I could apply to the data?  Or is there a statistical method that might conserve some information from the nan or inf bins?",0,statistics,54935,,Analyzing distribution of ratios when some values are NaN or Inf (i.e. X/0 or 0/X)?,https://www.reddit.com/r/statistics/comments/8bb33m/analyzing_distribution_of_ratios_when_some_values/,all_ads,2018-04-11 00:29:47,53 days 01:05:47.036584000,
"We are from Penn State University and are conducting a study for research purposes.  We are asking you to complete a brief survey (about 5-10 minutes) on your perceptions of sexual partner selection.  You must be age 18 and older to participate.  Although we can offer no compensation, we appreciate your contribution to our project. [This is a survey](https://pennstate.qualtrics.com/jfe/form/SV_bmumVB9cPv0M80B)",0,1523445262.0,8be05h,False,"We are from Penn State University and are conducting a study for research purposes.  We are asking you to complete a brief survey (about 5-10 minutes) on your perceptions of sexual partner selection.  You must be age 18 and older to participate.  Although we can offer no compensation, we appreciate your contribution to our project. [This is a survey](https://pennstate.qualtrics.com/jfe/form/SV_bmumVB9cPv0M80B)",0,"We are from Penn State University and are conducting a study for research purposes.  We are asking you to complete a brief survey (about 5-10 minutes) on your perceptions of sexual partner selection.  You must be age 18 and older to participate.  Although we can offer no compensation, we appreciate your contribution to our project. [This is a survey](https://pennstate.qualtrics.com/jfe/form/SV_bmumVB9cPv0M80B)",0,statistics,54935,,[Academic]Perceptions of relationship partners (US18+),https://www.reddit.com/r/statistics/comments/8be05h/academicperceptions_of_relationship_partners_us18/,all_ads,2018-04-11 07:14:22,52 days 18:21:12.036584000,
"Apologies if this has been beaten to death in this sub, I'm an engineer not a statistician. I'm just trying to understand why we draw a line at 0.05 or 0.10. 

I'm usually looking to show that two things are equivalent. We make a change to a product, but we want to make sure the output is the same. 

So if I got a P-Value of, say, 0.5, that's WAY higher than the accepted ""significance line"" of 0.05. So most would say ""no significant difference"" and move on. But if you said ""there's a 50% chance the difference you saw was due to experimental factors,"" I'd probably want to run that experiment a couple more times just to make sure it was randomness. 

Does that make sense? I'm not sure if I'm thinking about this right, I'd appreciate some help! ",25,1523412299.0,8b9tv4,False,"Apologies if this has been beaten to death in this sub, I'm an engineer not a statistician. I'm just trying to understand why we draw a line at 0.05 or 0.10. 

I'm usually looking to show that two things are equivalent. We make a change to a product, but we want to make sure the output is the same. 

So if I got a P-Value of, say, 0.5, that's WAY higher than the accepted ""significance line"" of 0.05. So most would say ""no significant difference"" and move on. But if you said ""there's a 50% chance the difference you saw was due to experimental factors,"" I'd probably want to run that experiment a couple more times just to make sure it was randomness. 

Does that make sense? I'm not sure if I'm thinking about this right, I'd appreciate some help! ",0,"Apologies if this has been beaten to death in this sub, I'm an engineer not a statistician. I'm just trying to understand why we draw a line at 0.05 or 0.10. 

I'm usually looking to show that two things are equivalent. We make a change to a product, but we want to make sure the output is the same. 

So if I got a P-Value of, say, 0.5, that's WAY higher than the accepted ""significance line"" of 0.05. So most would say ""no significant difference"" and move on. But if you said ""there's a 50% chance the difference you saw was due to experimental factors,"" I'd probably want to run that experiment a couple more times just to make sure it was randomness. 

Does that make sense? I'm not sure if I'm thinking about this right, I'd appreciate some help! ",1,statistics,54935,,Why even draw a 'line of significance'?,https://www.reddit.com/r/statistics/comments/8b9tv4/why_even_draw_a_line_of_significance/,all_ads,2018-04-10 22:04:59,53 days 03:30:35.036584000,
"A coin is tossed, and then a six sided die is rolled. Which of the following defines the sample space S?

I answered S = {H,T,1,2,3,4,5,6}

But the correct answer is S = {H1, H2, H3... T1, T2, T3... all the way to 6}

Can anyone explain why?",3,1523411572.0,8b9pyf,False,"A coin is tossed, and then a six sided die is rolled. Which of the following defines the sample space S?

I answered S = {H,T,1,2,3,4,5,6}

But the correct answer is S = {H1, H2, H3... T1, T2, T3... all the way to 6}

Can anyone explain why?",0,"A coin is tossed, and then a six sided die is rolled. Which of the following defines the sample space S?

I answered S = {H,T,1,2,3,4,5,6}

But the correct answer is S = {H1, H2, H3... T1, T2, T3... all the way to 6}

Can anyone explain why?",0,statistics,54935,,Sample space help?,https://www.reddit.com/r/statistics/comments/8b9pyf/sample_space_help/,all_ads,2018-04-10 21:52:52,53 days 03:42:42.036584000,
"When testing one or multiple quantitative variables to assess the predictive power for a logistic regression model, one calculates the variable's information value.

I recently learned that the values in the quantitative values should be binned/grouped.

#Questions

1. How does one choose the number of the bins?

2. Does the number of bins effect the IV value?

3. Will all quantitative variables need to share the same amount of bins?",0,1523409665.0,8b9fgd,False,"When testing one or multiple quantitative variables to assess the predictive power for a logistic regression model, one calculates the variable's information value.

I recently learned that the values in the quantitative values should be binned/grouped.

#Questions

1. How does one choose the number of the bins?

2. Does the number of bins effect the IV value?

3. Will all quantitative variables need to share the same amount of bins?",0,"When testing one or multiple quantitative variables to assess the predictive power for a logistic regression model, one calculates the variable's information value.

I recently learned that the values in the quantitative values should be binned/grouped.

#Questions

1. How does one choose the number of the bins?

2. Does the number of bins effect the IV value?

3. Will all quantitative variables need to share the same amount of bins?",1,statistics,54935,,Information value bin size,https://www.reddit.com/r/statistics/comments/8b9fgd/information_value_bin_size/,all_ads,2018-04-10 21:21:05,53 days 04:14:29.036584000,
"I've been mentally kicking around this Gelman post for a few months:
http://andrewgelman.com/2015/07/09/hey-guess-what-there-really-is-a-hot-hand/

The relevant quote is 

> Jack takes a coin from his pocket and decides that he will flip it 4 times in a row, writing down the outcome of each flip on a scrap of paper. After he is done flipping, he will look at the flips that immediately followed an outcome of heads, and compute the relative frequency of heads on those flips. Because the coin is fair, Jack of course expects this conditional relative frequency to be equal to the probability of flipping a heads: 0.5. Shockingly, Jack is wrong. If he were to sample 1 million fair coins and flip each coin 4 times, observing the conditional relative frequency for each coin, on average the relative frequency would be approximately 0.4.

Umm. okay. I read the paper and decide that there's something to this hot hand thing -- It must be TRUE. But I'm conflicted. My old stats professor in college would go on and on about the hot hand fallacy -- It must be FALSE. Another professor says that we will always go back and forth on the existence of hot hand.

So what do you think?",14,1523340869.0,8b2ipj,False,"I've been mentally kicking around this Gelman post for a few months:
http://andrewgelman.com/2015/07/09/hey-guess-what-there-really-is-a-hot-hand/

The relevant quote is 

> Jack takes a coin from his pocket and decides that he will flip it 4 times in a row, writing down the outcome of each flip on a scrap of paper. After he is done flipping, he will look at the flips that immediately followed an outcome of heads, and compute the relative frequency of heads on those flips. Because the coin is fair, Jack of course expects this conditional relative frequency to be equal to the probability of flipping a heads: 0.5. Shockingly, Jack is wrong. If he were to sample 1 million fair coins and flip each coin 4 times, observing the conditional relative frequency for each coin, on average the relative frequency would be approximately 0.4.

Umm. okay. I read the paper and decide that there's something to this hot hand thing -- It must be TRUE. But I'm conflicted. My old stats professor in college would go on and on about the hot hand fallacy -- It must be FALSE. Another professor says that we will always go back and forth on the existence of hot hand.

So what do you think?",0,"I've been mentally kicking around this Gelman post for a few months:
http://andrewgelman.com/2015/07/09/hey-guess-what-there-really-is-a-hot-hand/

The relevant quote is 

> Jack takes a coin from his pocket and decides that he will flip it 4 times in a row, writing down the outcome of each flip on a scrap of paper. After he is done flipping, he will look at the flips that immediately followed an outcome of heads, and compute the relative frequency of heads on those flips. Because the coin is fair, Jack of course expects this conditional relative frequency to be equal to the probability of flipping a heads: 0.5. Shockingly, Jack is wrong. If he were to sample 1 million fair coins and flip each coin 4 times, observing the conditional relative frequency for each coin, on average the relative frequency would be approximately 0.4.

Umm. okay. I read the paper and decide that there's something to this hot hand thing -- It must be TRUE. But I'm conflicted. My old stats professor in college would go on and on about the hot hand fallacy -- It must be FALSE. Another professor says that we will always go back and forth on the existence of hot hand.

So what do you think?",32,statistics,54935,,Hot Hand is fake. Or is it? What is the current state of this debate?,https://www.reddit.com/r/statistics/comments/8b2ipj/hot_hand_is_fake_or_is_it_what_is_the_current/,all_ads,2018-04-10 02:14:29,53 days 23:21:05.036584000,
"In a simple linear regression model , I was taught in class that I need to check for constant variance and independence of residuals. 

I understand what to look for in data output but not the theoretical meaning behind it.

What does independent residuals that show constant variance actually say about the model? What are they independent to and why do they need to be ? ",2,1523393774.0,8b7e68,False,"In a simple linear regression model , I was taught in class that I need to check for constant variance and independence of residuals. 

I understand what to look for in data output but not the theoretical meaning behind it.

What does independent residuals that show constant variance actually say about the model? What are they independent to and why do they need to be ? ",0,"In a simple linear regression model , I was taught in class that I need to check for constant variance and independence of residuals. 

I understand what to look for in data output but not the theoretical meaning behind it.

What does independent residuals that show constant variance actually say about the model? What are they independent to and why do they need to be ? ",1,statistics,54935,,What does independent residuals actually mean ?,https://www.reddit.com/r/statistics/comments/8b7e68/what_does_independent_residuals_actually_mean/,all_ads,2018-04-10 16:56:14,53 days 08:39:20.036584000,
"Obgyns now recommending baby aspirin to high risk women without defining what high risk is.  It seems that no more than 10% of women are at risk to begin with... but also that baby aspirin only reduces risk by 25%... meaning only a 2.5% reduction in overall risk...

Yet, simultaneously you trade that 2.5% “benefit” for a guaranteed 10% (our greater) risk for the side effects of the baby aspirin... plus unknown side effects to baby since they don’t study baby for more than 18 months after birth.  There is currently data that baby aspirin is in fact dangerous if taken in 3rd trimester.

There don’t seem to be more than a few studies on the benefits of baby aspirin, but I can’t even understand how they even conclude a 25% reduction in the first place unless they’re studying only women with previous Pre-eclampsia.  In other words, how do they collect data from proving a negative?  More so, how do they then apply for this “proof” to a more general set of women whom have no history of Pre-eclampsia?

One study (Exeter) talked about samples ranging from 1.1-4.3% but given there was no way for them to guarantee that Pre-eclampsia would develop, I wonder also what the margin of error would be.

I’m not a “doctor skeptic” so not looking to be confirmed, but just trying to better understand.  After all my wife has zero history of preeclampsia, historically low pressure which is the opposite of what they worry about.  One healthy delivery prior.  The only reason they seem to claim she is high risk is because she’s 42 and has had Graves’ disease on and off.  Actually had graves during last pregnancy which was normal.  Currently no graves.  At this moment the only thing that qualifies her to be at any risk is she’s 42. 

When I look at the data with untrained eyes I get the impression that her situation was never part of the initial experiment which seems dubious to begin with.  Experiment seems designed and conducted on women with a history of preeclampsia.

I think how doctors used to say breastfeeding was bad for the baby and now they say otherwise.  Not to mention the other varied examples of poorly conducted experiments.

Worst of all, I get the impression the doctors may know less about the statistics than I do.

Any thoughts would be greatly appreciated!

",4,1523393311.0,8b7chg,False,"Obgyns now recommending baby aspirin to high risk women without defining what high risk is.  It seems that no more than 10% of women are at risk to begin with... but also that baby aspirin only reduces risk by 25%... meaning only a 2.5% reduction in overall risk...

Yet, simultaneously you trade that 2.5% “benefit” for a guaranteed 10% (our greater) risk for the side effects of the baby aspirin... plus unknown side effects to baby since they don’t study baby for more than 18 months after birth.  There is currently data that baby aspirin is in fact dangerous if taken in 3rd trimester.

There don’t seem to be more than a few studies on the benefits of baby aspirin, but I can’t even understand how they even conclude a 25% reduction in the first place unless they’re studying only women with previous Pre-eclampsia.  In other words, how do they collect data from proving a negative?  More so, how do they then apply for this “proof” to a more general set of women whom have no history of Pre-eclampsia?

One study (Exeter) talked about samples ranging from 1.1-4.3% but given there was no way for them to guarantee that Pre-eclampsia would develop, I wonder also what the margin of error would be.

I’m not a “doctor skeptic” so not looking to be confirmed, but just trying to better understand.  After all my wife has zero history of preeclampsia, historically low pressure which is the opposite of what they worry about.  One healthy delivery prior.  The only reason they seem to claim she is high risk is because she’s 42 and has had Graves’ disease on and off.  Actually had graves during last pregnancy which was normal.  Currently no graves.  At this moment the only thing that qualifies her to be at any risk is she’s 42. 

When I look at the data with untrained eyes I get the impression that her situation was never part of the initial experiment which seems dubious to begin with.  Experiment seems designed and conducted on women with a history of preeclampsia.

I think how doctors used to say breastfeeding was bad for the baby and now they say otherwise.  Not to mention the other varied examples of poorly conducted experiments.

Worst of all, I get the impression the doctors may know less about the statistics than I do.

Any thoughts would be greatly appreciated!

",0,"Obgyns now recommending baby aspirin to high risk women without defining what high risk is.  It seems that no more than 10% of women are at risk to begin with... but also that baby aspirin only reduces risk by 25%... meaning only a 2.5% reduction in overall risk...

Yet, simultaneously you trade that 2.5% “benefit” for a guaranteed 10% (our greater) risk for the side effects of the baby aspirin... plus unknown side effects to baby since they don’t study baby for more than 18 months after birth.  There is currently data that baby aspirin is in fact dangerous if taken in 3rd trimester.

There don’t seem to be more than a few studies on the benefits of baby aspirin, but I can’t even understand how they even conclude a 25% reduction in the first place unless they’re studying only women with previous Pre-eclampsia.  In other words, how do they collect data from proving a negative?  More so, how do they then apply for this “proof” to a more general set of women whom have no history of Pre-eclampsia?

One study (Exeter) talked about samples ranging from 1.1-4.3% but given there was no way for them to guarantee that Pre-eclampsia would develop, I wonder also what the margin of error would be.

I’m not a “doctor skeptic” so not looking to be confirmed, but just trying to better understand.  After all my wife has zero history of preeclampsia, historically low pressure which is the opposite of what they worry about.  One healthy delivery prior.  The only reason they seem to claim she is high risk is because she’s 42 and has had Graves’ disease on and off.  Actually had graves during last pregnancy which was normal.  Currently no graves.  At this moment the only thing that qualifies her to be at any risk is she’s 42. 

When I look at the data with untrained eyes I get the impression that her situation was never part of the initial experiment which seems dubious to begin with.  Experiment seems designed and conducted on women with a history of preeclampsia.

I think how doctors used to say breastfeeding was bad for the baby and now they say otherwise.  Not to mention the other varied examples of poorly conducted experiments.

Worst of all, I get the impression the doctors may know less about the statistics than I do.

Any thoughts would be greatly appreciated!

",0,statistics,54935,,Baby Aspirin and Pre-eclampsia?,https://www.reddit.com/r/statistics/comments/8b7chg/baby_aspirin_and_preeclampsia/,all_ads,2018-04-10 16:48:31,53 days 08:47:03.036584000,
Hi I am trying to test the extent of lying in a truth-telling experiment. People had to pick out a chip from a bag with either £5 or £1 on it. The actual distribution of chips in the bag was 30% £1 chips and 70% £5 chips. They could report any number they wanted to (didnt depend on what they picked out). I have so far conducted a chi squared test comparing the actual distribution of reports against the distribution given everyone was honest (70% reporting £5 chips). Should I instead do a binomial test? Thanks,20,1523341360.0,8b2kqv,False,Hi I am trying to test the extent of lying in a truth-telling experiment. People had to pick out a chip from a bag with either £5 or £1 on it. The actual distribution of chips in the bag was 30% £1 chips and 70% £5 chips. They could report any number they wanted to (didnt depend on what they picked out). I have so far conducted a chi squared test comparing the actual distribution of reports against the distribution given everyone was honest (70% reporting £5 chips). Should I instead do a binomial test? Thanks,0,Hi I am trying to test the extent of lying in a truth-telling experiment. People had to pick out a chip from a bag with either £5 or £1 on it. The actual distribution of chips in the bag was 30% £1 chips and 70% £5 chips. They could report any number they wanted to (didnt depend on what they picked out). I have so far conducted a chi squared test comparing the actual distribution of reports against the distribution given everyone was honest (70% reporting £5 chips). Should I instead do a binomial test? Thanks,7,statistics,54935,,Chi Squared or Binomial?,https://www.reddit.com/r/statistics/comments/8b2kqv/chi_squared_or_binomial/,all_ads,2018-04-10 02:22:40,53 days 23:12:54.036584000,
"So, I have a problem where I want to cluster data on Likert scale, which is basically ordinal categorical data (I know that 6<7 and 7<8, but it's very possible, that the difference between 7 and 8 is much greater than the difference between 7 and 6.).

So to transform the data into space where it would be reasonable to use Euclidean distance I came up with this algorithm:

1. Assume that there is some continious hidden variable X with distribution F(x).

2. The answers of a questionare are a function T(x) of this random variable:

T(x)= 1, if x<t_1; 2, if t_1<=x<t_2; ... ; 10, if x>=t_9.

Basically, if domain of X is partitioned into 10 intervals, and answer of the questionare indicates which interval the hidden variable belongs to.

If we use this assumption, it is fairly easy to estimate the bounds of each interval using the sample. This is done using the formula: F(t\_i)-F(t_{i-1})={probability of answer i}.

Then I would convert each ordinal variable (answer to questionare) to the expected value of X on condition that x is in the given interval ( (t\_i+t_{i-1})/2 in case where X is uniform).

My reasoning is that such transformation should account for differences between different answers, while still using the fact that data is ordinal, thus allowing me to be more confident when using methods that utilize euclidean distance.

Do you think it makes sense? Is there something similar but better? If there is, how is it called and where could I get more information about it?
",4,1523373078.0,8b5r72,False,"So, I have a problem where I want to cluster data on Likert scale, which is basically ordinal categorical data (I know that 6<7 and 7<8, but it's very possible, that the difference between 7 and 8 is much greater than the difference between 7 and 6.).

So to transform the data into space where it would be reasonable to use Euclidean distance I came up with this algorithm:

1. Assume that there is some continious hidden variable X with distribution F(x).

2. The answers of a questionare are a function T(x) of this random variable:

T(x)= 1, if x<t_1; 2, if t_1<=x<t_2; ... ; 10, if x>=t_9.

Basically, if domain of X is partitioned into 10 intervals, and answer of the questionare indicates which interval the hidden variable belongs to.

If we use this assumption, it is fairly easy to estimate the bounds of each interval using the sample. This is done using the formula: F(t\_i)-F(t_{i-1})={probability of answer i}.

Then I would convert each ordinal variable (answer to questionare) to the expected value of X on condition that x is in the given interval ( (t\_i+t_{i-1})/2 in case where X is uniform).

My reasoning is that such transformation should account for differences between different answers, while still using the fact that data is ordinal, thus allowing me to be more confident when using methods that utilize euclidean distance.

Do you think it makes sense? Is there something similar but better? If there is, how is it called and where could I get more information about it?
",0,"So, I have a problem where I want to cluster data on Likert scale, which is basically ordinal categorical data (I know that 6<7 and 7<8, but it's very possible, that the difference between 7 and 8 is much greater than the difference between 7 and 6.).

So to transform the data into space where it would be reasonable to use Euclidean distance I came up with this algorithm:

1. Assume that there is some continious hidden variable X with distribution F(x).

2. The answers of a questionare are a function T(x) of this random variable:

T(x)= 1, if x<t_1; 2, if t_1<=x<t_2; ... ; 10, if x>=t_9.

Basically, if domain of X is partitioned into 10 intervals, and answer of the questionare indicates which interval the hidden variable belongs to.

If we use this assumption, it is fairly easy to estimate the bounds of each interval using the sample. This is done using the formula: F(t\_i)-F(t_{i-1})={probability of answer i}.

Then I would convert each ordinal variable (answer to questionare) to the expected value of X on condition that x is in the given interval ( (t\_i+t_{i-1})/2 in case where X is uniform).

My reasoning is that such transformation should account for differences between different answers, while still using the fact that data is ordinal, thus allowing me to be more confident when using methods that utilize euclidean distance.

Do you think it makes sense? Is there something similar but better? If there is, how is it called and where could I get more information about it?
",1,statistics,54935,,How is this procedure I came up with called?,https://www.reddit.com/r/statistics/comments/8b5r72/how_is_this_procedure_i_came_up_with_called/,all_ads,2018-04-10 11:11:18,53 days 14:24:16.036584000,
"I am completely unaware of what a mixture model is. I have only ever used regressions. I was referred to mixture models as a way of analyzing a set of data (X items of four different types were rated on Y dimensions; told to run a mixture model without identifying type first, and then to run a second one in which type is identified, the comparison of models will help answer the question of whether these different types are indeed rated differently). 

However, I'm having the hardest time finding a *basic* explanation of what mixture models are. Every piece of material I come across presents them in the midst of material on machine learning or another larger method that I'm unfamiliar with, so it's been very difficult to get a basic understanding of what these models are.

Thanks!",20,1523346200.0,8b34qo,False,"I am completely unaware of what a mixture model is. I have only ever used regressions. I was referred to mixture models as a way of analyzing a set of data (X items of four different types were rated on Y dimensions; told to run a mixture model without identifying type first, and then to run a second one in which type is identified, the comparison of models will help answer the question of whether these different types are indeed rated differently). 

However, I'm having the hardest time finding a *basic* explanation of what mixture models are. Every piece of material I come across presents them in the midst of material on machine learning or another larger method that I'm unfamiliar with, so it's been very difficult to get a basic understanding of what these models are.

Thanks!",0,"I am completely unaware of what a mixture model is. I have only ever used regressions. I was referred to mixture models as a way of analyzing a set of data (X items of four different types were rated on Y dimensions; told to run a mixture model without identifying type first, and then to run a second one in which type is identified, the comparison of models will help answer the question of whether these different types are indeed rated differently). 

However, I'm having the hardest time finding a *basic* explanation of what mixture models are. Every piece of material I come across presents them in the midst of material on machine learning or another larger method that I'm unfamiliar with, so it's been very difficult to get a basic understanding of what these models are.

Thanks!",3,statistics,54935,,ELI5: What is a mixture model?,https://www.reddit.com/r/statistics/comments/8b34qo/eli5_what_is_a_mixture_model/,all_ads,2018-04-10 03:43:20,53 days 21:52:14.036584000,
"I'm trying to come up with a metric that assesses the general ""health"" associated with some system. I don't have the right data calculated to come up with a classification model or anything like that.

My idea was, for a given important metric related to this system, develop a control chart-type mechanism.

Here's what I'm thinking:

1. Track this metric for each distinct subcomponent in the system (each of which has its own normal operating level)
2. Draw 1- and 2-stddev upper control limits for each subcomponent
3. Integrate to find the area under each upper control limit and the mean... The greater the area, the less ""healthy"" that process is.
4. Take 3) a step further by penalizing sharp upward spikes in each process. Maybe an exponential scale on the rate of change between each time period would be useful here.

Can someone sanity-check me here?",0,1523354825.0,8b438m,False,"I'm trying to come up with a metric that assesses the general ""health"" associated with some system. I don't have the right data calculated to come up with a classification model or anything like that.

My idea was, for a given important metric related to this system, develop a control chart-type mechanism.

Here's what I'm thinking:

1. Track this metric for each distinct subcomponent in the system (each of which has its own normal operating level)
2. Draw 1- and 2-stddev upper control limits for each subcomponent
3. Integrate to find the area under each upper control limit and the mean... The greater the area, the less ""healthy"" that process is.
4. Take 3) a step further by penalizing sharp upward spikes in each process. Maybe an exponential scale on the rate of change between each time period would be useful here.

Can someone sanity-check me here?",0,"I'm trying to come up with a metric that assesses the general ""health"" associated with some system. I don't have the right data calculated to come up with a classification model or anything like that.

My idea was, for a given important metric related to this system, develop a control chart-type mechanism.

Here's what I'm thinking:

1. Track this metric for each distinct subcomponent in the system (each of which has its own normal operating level)
2. Draw 1- and 2-stddev upper control limits for each subcomponent
3. Integrate to find the area under each upper control limit and the mean... The greater the area, the less ""healthy"" that process is.
4. Take 3) a step further by penalizing sharp upward spikes in each process. Maybe an exponential scale on the rate of change between each time period would be useful here.

Can someone sanity-check me here?",0,statistics,54935,,Area between upper control limit and mean... Is it meaningful?,https://www.reddit.com/r/statistics/comments/8b438m/area_between_upper_control_limit_and_mean_is_it/,all_ads,2018-04-10 06:07:05,53 days 19:28:29.036584000,
"So I am looking at a couple studies for medications. I am trying to quantify exactly the percentages increase happens when people taking these medications. I think 1.20 means a 20% increase for example in this study https://www.ncbi.nlm.nih.gov/pubmed/26580313


""Treatment with a single antibiotic course was associated with higher risk for depression with all antibiotic groups, with an adjusted OR (AOR) of 1.23 for penicillins (95% CI, 1.18-1.29) and 1.25 (95% CI, 1.15-1.35) for quinolones.""

So does these decimals they use, just get transferred over into percentages? Like 1.23 would be an increase chance by 23%? I suck so hard at math. Any help would be nice. ",8,1523352813.0,8b3vgs,False,"So I am looking at a couple studies for medications. I am trying to quantify exactly the percentages increase happens when people taking these medications. I think 1.20 means a 20% increase for example in this study https://www.ncbi.nlm.nih.gov/pubmed/26580313


""Treatment with a single antibiotic course was associated with higher risk for depression with all antibiotic groups, with an adjusted OR (AOR) of 1.23 for penicillins (95% CI, 1.18-1.29) and 1.25 (95% CI, 1.15-1.35) for quinolones.""

So does these decimals they use, just get transferred over into percentages? Like 1.23 would be an increase chance by 23%? I suck so hard at math. Any help would be nice. ",0,"So I am looking at a couple studies for medications. I am trying to quantify exactly the percentages increase happens when people taking these medications. I think 1.20 means a 20% increase for example in this study https://www.ncbi.nlm.nih.gov/pubmed/26580313


""Treatment with a single antibiotic course was associated with higher risk for depression with all antibiotic groups, with an adjusted OR (AOR) of 1.23 for penicillins (95% CI, 1.18-1.29) and 1.25 (95% CI, 1.15-1.35) for quinolones.""

So does these decimals they use, just get transferred over into percentages? Like 1.23 would be an increase chance by 23%? I suck so hard at math. Any help would be nice. ",0,statistics,54935,,Can someone help me understand basic statistics in research articles?,https://www.reddit.com/r/statistics/comments/8b3vgs/can_someone_help_me_understand_basic_statistics/,all_ads,2018-04-10 05:33:33,53 days 20:02:01.036584000,
"For example if you're only given a property of a specific person, how many people does there need to be out of a total population before it is considered statistically unlikely that you can identify that individual with the given property (e.g. gender, age, reddit usage time).",37,1523283547.0,8awdka,False,"For example if you're only given a property of a specific person, how many people does there need to be out of a total population before it is considered statistically unlikely that you can identify that individual with the given property (e.g. gender, age, reddit usage time).",0,"For example if you're only given a property of a specific person, how many people does there need to be out of a total population before it is considered statistically unlikely that you can identify that individual with the given property (e.g. gender, age, reddit usage time).",31,statistics,54935,,"How big does a population have to be before an individual in that population is considered ""anonymous""?",https://www.reddit.com/r/statistics/comments/8awdka/how_big_does_a_population_have_to_be_before_an/,all_ads,2018-04-09 10:19:07,54 days 15:16:27.036584000,
"I've been unsuccessful in my search but are there ways to estimate parameters that incorporates an indicator variable for the parameters? 

For example if my parameters (x,y) lived in R^2 but I knew that the parameters were really in the 1st quadrant. So I put a check on them such as if x or y is negative those parameters are ""bad"" and if both are positive then those parameters are ""good"". Is there a way to not waste any time with the bad parameters? I know I could use box constraints for this example but when the ""good"" or ""bad"" depend on some function of the parameters, an indicator would be more helpful.",0,1523345869.0,8b33ga,False,"I've been unsuccessful in my search but are there ways to estimate parameters that incorporates an indicator variable for the parameters? 

For example if my parameters (x,y) lived in R^2 but I knew that the parameters were really in the 1st quadrant. So I put a check on them such as if x or y is negative those parameters are ""bad"" and if both are positive then those parameters are ""good"". Is there a way to not waste any time with the bad parameters? I know I could use box constraints for this example but when the ""good"" or ""bad"" depend on some function of the parameters, an indicator would be more helpful.",0,"I've been unsuccessful in my search but are there ways to estimate parameters that incorporates an indicator variable for the parameters? 

For example if my parameters (x,y) lived in R^2 but I knew that the parameters were really in the 1st quadrant. So I put a check on them such as if x or y is negative those parameters are ""bad"" and if both are positive then those parameters are ""good"". Is there a way to not waste any time with the bad parameters? I know I could use box constraints for this example but when the ""good"" or ""bad"" depend on some function of the parameters, an indicator would be more helpful.",1,statistics,54935,,Parameter estimation but with indicator variable?,https://www.reddit.com/r/statistics/comments/8b33ga/parameter_estimation_but_with_indicator_variable/,all_ads,2018-04-10 03:37:49,53 days 21:57:45.036584000,
"Can anyone give me some tips on learning about bootstrapping? I’ve done light google searches, but I haven’t really explored it. Any light books recommendations on bootstrapping?",2,1523341259.0,8b2kc6,False,"Can anyone give me some tips on learning about bootstrapping? I’ve done light google searches, but I haven’t really explored it. Any light books recommendations on bootstrapping?",0,"Can anyone give me some tips on learning about bootstrapping? I’ve done light google searches, but I haven’t really explored it. Any light books recommendations on bootstrapping?",0,statistics,54935,,Learning about bootstrapping,https://www.reddit.com/r/statistics/comments/8b2kc6/learning_about_bootstrapping/,all_ads,2018-04-10 02:20:59,53 days 23:14:35.036584000,
"This question is really bothering me because I have no clue how to set it up. 

A coin is about to be tossed multiple times. Assume the coin is fair, probability of heads and tails are both .5. If the coin is tossed 60 times, what is the probability that less than 1/3 of the tosses are heads?

Appreciate any advice on how to setup or solve this problem. 


",11,1523335633.0,8b1uq5,False,"This question is really bothering me because I have no clue how to set it up. 

A coin is about to be tossed multiple times. Assume the coin is fair, probability of heads and tails are both .5. If the coin is tossed 60 times, what is the probability that less than 1/3 of the tosses are heads?

Appreciate any advice on how to setup or solve this problem. 


",0,"This question is really bothering me because I have no clue how to set it up. 

A coin is about to be tossed multiple times. Assume the coin is fair, probability of heads and tails are both .5. If the coin is tossed 60 times, what is the probability that less than 1/3 of the tosses are heads?

Appreciate any advice on how to setup or solve this problem. 


",1,statistics,54935,,Coin toss question,https://www.reddit.com/r/statistics/comments/8b1uq5/coin_toss_question/,all_ads,2018-04-10 00:47:13,54 days 00:48:21.036584000,
"Q: https://imgur.com/a/c3Vz4


Hi, I'd like some help on part b please. I don't get why the combination method to work out the new probability are 5C2 and 3C2. Why does 6 decrease to 5 and 4 decreases to 3? ?  

Thanks
",2,1523348506.0,8b3e0c,False,"Q: https://imgur.com/a/c3Vz4


Hi, I'd like some help on part b please. I don't get why the combination method to work out the new probability are 5C2 and 3C2. Why does 6 decrease to 5 and 4 decreases to 3? ?  

Thanks
",0,"Q: https://imgur.com/a/c3Vz4


Hi, I'd like some help on part b please. I don't get why the combination method to work out the new probability are 5C2 and 3C2. Why does 6 decrease to 5 and 4 decreases to 3? ?  

Thanks
",0,statistics,54935,,Probability question,https://www.reddit.com/r/statistics/comments/8b3e0c/probability_question/,all_ads,2018-04-10 04:21:46,53 days 21:13:48.036584000,
"I have a simulation which spits out data in the form of a spreadsheet containing the frequency density of each value. Whilst this is useful for creating a histogram I need to do further statistical tests on it and I think using SPSS would be the easiest way to do this. However I can't manually import the data to SPSS as I have several samples each containing 100,000 simulations. Is there a way to perform statistical tests on this kind of data without having to do it manually?",0,1523302470.0,8axqcg,False,"I have a simulation which spits out data in the form of a spreadsheet containing the frequency density of each value. Whilst this is useful for creating a histogram I need to do further statistical tests on it and I think using SPSS would be the easiest way to do this. However I can't manually import the data to SPSS as I have several samples each containing 100,000 simulations. Is there a way to perform statistical tests on this kind of data without having to do it manually?",0,"I have a simulation which spits out data in the form of a spreadsheet containing the frequency density of each value. Whilst this is useful for creating a histogram I need to do further statistical tests on it and I think using SPSS would be the easiest way to do this. However I can't manually import the data to SPSS as I have several samples each containing 100,000 simulations. Is there a way to perform statistical tests on this kind of data without having to do it manually?",4,statistics,54935,,Importing frequency density data into SPSS,https://www.reddit.com/r/statistics/comments/8axqcg/importing_frequency_density_data_into_spss/,all_ads,2018-04-09 15:34:30,54 days 10:01:04.036584000,
"I am comparing disease risk scores (non normal data) within the range of antelope which changes during

1.Summer 2.Spring and 3.Winter (my 3 samples)

They antelope are in different counties during each season. 32 counties in summer , 33 in spring and 35 in winter. Each county has a score based on disease risk. I am comparing these scores.

Now looking into a test called Skillings.Mack. Know anything about this? Or any other tests I could use?/ ways of solving this problem.

Thanks in advance   ",2,1523302446.0,8axq8z,False,"I am comparing disease risk scores (non normal data) within the range of antelope which changes during

1.Summer 2.Spring and 3.Winter (my 3 samples)

They antelope are in different counties during each season. 32 counties in summer , 33 in spring and 35 in winter. Each county has a score based on disease risk. I am comparing these scores.

Now looking into a test called Skillings.Mack. Know anything about this? Or any other tests I could use?/ ways of solving this problem.

Thanks in advance   ",0,"I am comparing disease risk scores (non normal data) within the range of antelope which changes during

1.Summer 2.Spring and 3.Winter (my 3 samples)

They antelope are in different counties during each season. 32 counties in summer , 33 in spring and 35 in winter. Each county has a score based on disease risk. I am comparing these scores.

Now looking into a test called Skillings.Mack. Know anything about this? Or any other tests I could use?/ ways of solving this problem.

Thanks in advance   ",4,statistics,54935,,"Can anybody help me choose a non parametric test? I have 3 samples of slightly different sizes (32, 33, 35)",https://www.reddit.com/r/statistics/comments/8axq8z/can_anybody_help_me_choose_a_non_parametric_test/,all_ads,2018-04-09 15:34:06,54 days 10:01:28.036584000,
"I am curious if anyone has any insight into the intuition behind Sverdrup's Lemma (e.g. GENERALIZATION OF SVERDRUP'S LEMMA AND ITS APPLICATIONS TO MULTIVARIATE DISTRIBUTION THEORY by Kabe), unfortunately the original paper by Sverdrup is behind a paywall I can't get past.  Sverdrup's Lemma is saying presumably saying something about how to construct multivariate distributions, and my hunch is that it is a result about change of variables, but the formula is quite hard to parse, and there is little explanation in the paper (all caps).  Thanks.",3,1523327740.0,8b0sil,False,"I am curious if anyone has any insight into the intuition behind Sverdrup's Lemma (e.g. GENERALIZATION OF SVERDRUP'S LEMMA AND ITS APPLICATIONS TO MULTIVARIATE DISTRIBUTION THEORY by Kabe), unfortunately the original paper by Sverdrup is behind a paywall I can't get past.  Sverdrup's Lemma is saying presumably saying something about how to construct multivariate distributions, and my hunch is that it is a result about change of variables, but the formula is quite hard to parse, and there is little explanation in the paper (all caps).  Thanks.",0,"I am curious if anyone has any insight into the intuition behind Sverdrup's Lemma (e.g. GENERALIZATION OF SVERDRUP'S LEMMA AND ITS APPLICATIONS TO MULTIVARIATE DISTRIBUTION THEORY by Kabe), unfortunately the original paper by Sverdrup is behind a paywall I can't get past.  Sverdrup's Lemma is saying presumably saying something about how to construct multivariate distributions, and my hunch is that it is a result about change of variables, but the formula is quite hard to parse, and there is little explanation in the paper (all caps).  Thanks.",1,statistics,54935,,Understanding Sverdrup's Lemma,https://www.reddit.com/r/statistics/comments/8b0sil/understanding_sverdrups_lemma/,all_ads,2018-04-09 22:35:40,54 days 02:59:54.036584000,
"I wrote a blog on how to implement Multinomial Naive Bayes classifier (from scratch) using Python for categorizing news papers:

https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67",2,1523322488.0,8b02qk,False,"I wrote a blog on how to implement Multinomial Naive Bayes classifier (from scratch) using Python for categorizing news papers:

https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67",0,"I wrote a blog on how to implement Multinomial Naive Bayes classifier (from scratch) using Python for categorizing news papers:

https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67",0,statistics,54935,,Multinomial Naive Bayes Classifier for Text Analysis,https://www.reddit.com/r/statistics/comments/8b02qk/multinomial_naive_bayes_classifier_for_text/,all_ads,2018-04-09 21:08:08,54 days 04:27:26.036584000,
"Hi, I have a big doubt when applying multi-testing to bootstrap p-values. I now have a problem where I want to know if my observed value (**X**) is significantly higher or lower than spected by chance.

We're using the bootstrap distribution to determine the probability of observing a value higher or lower than **X**. So if **X** leaves a probability of **p-value=0.15** at the right of that distribution, I would say that at the significance level alpha = 0.05, it is not higher than expected by chance, nor lower. 

Let's say that I know want to apply some multi-testing model, for example FDR or Bonferroni, so I input my pvalue = 0.15, and I get a corrected **p-value = 1.0**.

However, I have a difficult understanding this corrected value. The previous pvalue = 0.15 had a correspondence with the observed value of **X** in the bootstrap distribution, but now the pvalue = 1.0 would have a correspondence with.... what? I don't understand it. I a one-tailed test it could be the lowest possible value, and I'm not sure that this is a valid reasoning, but any way this a a two-tailed test, so, this pvalue=1.0 what does mean exaclty in the bootstrap distribution??
",0,1523317065.0,8azch9,False,"Hi, I have a big doubt when applying multi-testing to bootstrap p-values. I now have a problem where I want to know if my observed value (**X**) is significantly higher or lower than spected by chance.

We're using the bootstrap distribution to determine the probability of observing a value higher or lower than **X**. So if **X** leaves a probability of **p-value=0.15** at the right of that distribution, I would say that at the significance level alpha = 0.05, it is not higher than expected by chance, nor lower. 

Let's say that I know want to apply some multi-testing model, for example FDR or Bonferroni, so I input my pvalue = 0.15, and I get a corrected **p-value = 1.0**.

However, I have a difficult understanding this corrected value. The previous pvalue = 0.15 had a correspondence with the observed value of **X** in the bootstrap distribution, but now the pvalue = 1.0 would have a correspondence with.... what? I don't understand it. I a one-tailed test it could be the lowest possible value, and I'm not sure that this is a valid reasoning, but any way this a a two-tailed test, so, this pvalue=1.0 what does mean exaclty in the bootstrap distribution??
",0,"Hi, I have a big doubt when applying multi-testing to bootstrap p-values. I now have a problem where I want to know if my observed value (**X**) is significantly higher or lower than spected by chance.

We're using the bootstrap distribution to determine the probability of observing a value higher or lower than **X**. So if **X** leaves a probability of **p-value=0.15** at the right of that distribution, I would say that at the significance level alpha = 0.05, it is not higher than expected by chance, nor lower. 

Let's say that I know want to apply some multi-testing model, for example FDR or Bonferroni, so I input my pvalue = 0.15, and I get a corrected **p-value = 1.0**.

However, I have a difficult understanding this corrected value. The previous pvalue = 0.15 had a correspondence with the observed value of **X** in the bootstrap distribution, but now the pvalue = 1.0 would have a correspondence with.... what? I don't understand it. I a one-tailed test it could be the lowest possible value, and I'm not sure that this is a valid reasoning, but any way this a a two-tailed test, so, this pvalue=1.0 what does mean exaclty in the bootstrap distribution??
",1,statistics,54935,,Doubt regarding bootstrap p-values and multi-test,https://www.reddit.com/r/statistics/comments/8azch9/doubt_regarding_bootstrap_pvalues_and_multitest/,all_ads,2018-04-09 19:37:45,54 days 05:57:49.036584000,
"Hi all. 

I'm performing a GLM testing the effects of 3 angles and speed on duty factor (% time spent with foot off the ground compared to on the ground). 

I am trying to find the equations of the 3 lines and I am having trouble. Any help?

Here's the output: https://imgur.com/222363C

Here's the graph: https://imgur.com/vYsiUPG

EDIT: using SPSS",3,1523307085.0,8ay60d,False,"Hi all. 

I'm performing a GLM testing the effects of 3 angles and speed on duty factor (% time spent with foot off the ground compared to on the ground). 

I am trying to find the equations of the 3 lines and I am having trouble. Any help?

Here's the output: https://imgur.com/222363C

Here's the graph: https://imgur.com/vYsiUPG

EDIT: using SPSS",0,"Hi all. 

I'm performing a GLM testing the effects of 3 angles and speed on duty factor (% time spent with foot off the ground compared to on the ground). 

I am trying to find the equations of the 3 lines and I am having trouble. Any help?

Here's the output: https://imgur.com/222363C

Here's the graph: https://imgur.com/vYsiUPG

EDIT: using SPSS",0,statistics,54935,,Help with interpreting parameter estimates of a GLM,https://www.reddit.com/r/statistics/comments/8ay60d/help_with_interpreting_parameter_estimates_of_a/,all_ads,2018-04-09 16:51:25,54 days 08:44:09.036584000,
"I have a basic understanding of statistics and different analysis methods, but each video I listen to about this topic is hard to follow and difficult to learn off.

If someone could explain this topic as if I was 5... that would be incredible!

Thanks in advance",8,1523282791.0,8awbdw,False,"I have a basic understanding of statistics and different analysis methods, but each video I listen to about this topic is hard to follow and difficult to learn off.

If someone could explain this topic as if I was 5... that would be incredible!

Thanks in advance",0,"I have a basic understanding of statistics and different analysis methods, but each video I listen to about this topic is hard to follow and difficult to learn off.

If someone could explain this topic as if I was 5... that would be incredible!

Thanks in advance",3,statistics,54935,,[Q] Can someone please ELI5 Multivariate Statistics Analysis?,https://www.reddit.com/r/statistics/comments/8awbdw/q_can_someone_please_eli5_multivariate_statistics/,all_ads,2018-04-09 10:06:31,54 days 15:29:03.036584000,
"Hi,

I am going to teach statistics to a bunch of students who chose data science as an optional subject. I am not able to find a good book that will spark curiosity as well as teach them basics of statistics. The school kids belong to middle level (grades 7-9). Please help.",9,1523282383.0,8awa6u,False,"Hi,

I am going to teach statistics to a bunch of students who chose data science as an optional subject. I am not able to find a good book that will spark curiosity as well as teach them basics of statistics. The school kids belong to middle level (grades 7-9). Please help.",0,"Hi,

I am going to teach statistics to a bunch of students who chose data science as an optional subject. I am not able to find a good book that will spark curiosity as well as teach them basics of statistics. The school kids belong to middle level (grades 7-9). Please help.",3,statistics,54935,,Need a good book to teach kids statistics in a fun way,https://www.reddit.com/r/statistics/comments/8awa6u/need_a_good_book_to_teach_kids_statistics_in_a/,all_ads,2018-04-09 09:59:43,54 days 15:35:51.036584000,
"Hi everyone, I am doing a project on linear regression using Automobile dataset from UCI Machine Learning Repository. So far, I have had multiple linear regression, ridge regression and the lasso. The dataset only has about more than 200 records, whereas there are 4 binary variables and 7 multiple-level categorical variables. The variable with the most factors has 22 levels. For multiple linear regression, R complains that some new levels introduced. For ridge regression and the lasso, R notified that X and Y mismatched dimensions. 

I think why the problem arose is understandable. The test set is just a small part of the original data, so it is easily unable to cover all factors. I am quite stuck at this point. I hope those of who have experience in this problem can help me. Thank you so much in advance! ",13,1523264166.0,8aukff,False,"Hi everyone, I am doing a project on linear regression using Automobile dataset from UCI Machine Learning Repository. So far, I have had multiple linear regression, ridge regression and the lasso. The dataset only has about more than 200 records, whereas there are 4 binary variables and 7 multiple-level categorical variables. The variable with the most factors has 22 levels. For multiple linear regression, R complains that some new levels introduced. For ridge regression and the lasso, R notified that X and Y mismatched dimensions. 

I think why the problem arose is understandable. The test set is just a small part of the original data, so it is easily unable to cover all factors. I am quite stuck at this point. I hope those of who have experience in this problem can help me. Thank you so much in advance! ",0,"Hi everyone, I am doing a project on linear regression using Automobile dataset from UCI Machine Learning Repository. So far, I have had multiple linear regression, ridge regression and the lasso. The dataset only has about more than 200 records, whereas there are 4 binary variables and 7 multiple-level categorical variables. The variable with the most factors has 22 levels. For multiple linear regression, R complains that some new levels introduced. For ridge regression and the lasso, R notified that X and Y mismatched dimensions. 

I think why the problem arose is understandable. The test set is just a small part of the original data, so it is easily unable to cover all factors. I am quite stuck at this point. I hope those of who have experience in this problem can help me. Thank you so much in advance! ",7,statistics,54935,,Cross-validation failed when regression model contains many multiple-level categorical variables. What should I do?,https://www.reddit.com/r/statistics/comments/8aukff/crossvalidation_failed_when_regression_model/,all_ads,2018-04-09 04:56:06,54 days 20:39:28.036584000,
Tried Friedman Test but an assumption of this appears to be having equal sample sizes. Not entirely sure what I should do. Thanks for any help. ,4,1523299021.0,8axgcl,False,Tried Friedman Test but an assumption of this appears to be having equal sample sizes. Not entirely sure what I should do. Thanks for any help. ,0,Tried Friedman Test but an assumption of this appears to be having equal sample sizes. Not entirely sure what I should do. Thanks for any help. ,0,statistics,54935,,"Non Normal Data with slightly uneven sample sizes (32, 31,34). How can I compare the difference?",https://www.reddit.com/r/statistics/comments/8axgcl/non_normal_data_with_slightly_uneven_sample_sizes/,all_ads,2018-04-09 14:37:01,54 days 10:58:33.036584000,
"Feller gives a simple formula for the Ruin problem for winning or losing 1 at each trial. Is there a similar formula for the case where the winning and losing amounts are unequal?

IE, how can one choose between a small probability of loss or a small amount of loss? ",3,1523284893.0,8awh33,False,"Feller gives a simple formula for the Ruin problem for winning or losing 1 at each trial. Is there a similar formula for the case where the winning and losing amounts are unequal?

IE, how can one choose between a small probability of loss or a small amount of loss? ",0,"Feller gives a simple formula for the Ruin problem for winning or losing 1 at each trial. Is there a similar formula for the case where the winning and losing amounts are unequal?

IE, how can one choose between a small probability of loss or a small amount of loss? ",2,statistics,54935,,Ruin problem with unequal payouts,https://www.reddit.com/r/statistics/comments/8awh33/ruin_problem_with_unequal_payouts/,all_ads,2018-04-09 10:41:33,54 days 14:54:01.036584000,
"I've been trying to wrap my head around the concept of sufficiency and I thought of this. Is this true? That your data itself can be a sufficient statistic. I'm personally leaning towards no. As depending on what parameters you have, the distribution of the data of the data will depend on the parameter. Am I correct?",5,1523296848.0,8axag3,False,"I've been trying to wrap my head around the concept of sufficiency and I thought of this. Is this true? That your data itself can be a sufficient statistic. I'm personally leaning towards no. As depending on what parameters you have, the distribution of the data of the data will depend on the parameter. Am I correct?",0,"I've been trying to wrap my head around the concept of sufficiency and I thought of this. Is this true? That your data itself can be a sufficient statistic. I'm personally leaning towards no. As depending on what parameters you have, the distribution of the data of the data will depend on the parameter. Am I correct?",1,statistics,54935,,Can your data itself be a sufficient statistic?,https://www.reddit.com/r/statistics/comments/8axag3/can_your_data_itself_be_a_sufficient_statistic/,all_ads,2018-04-09 14:00:48,54 days 11:34:46.036584000,
"I'm using Dedoose for qualitative research.  I have a complete codebook, and 720 conversations to code.  What I want in the end is the complete count of each code within each conversation, and I'll use SPSS for the actual analysis.

I'm having a hard time figuring out how to do two things:
1.  measure interrater reliability
2.  collapse the codes applied by multiple raters on a single conversation so the codes aren't double-counted

I might have more questions as I go along.  I'd love it if anyone who has experience with Dedoose can respond to this thread.  Thank you!",0,1523264804.0,8aumu1,False,"I'm using Dedoose for qualitative research.  I have a complete codebook, and 720 conversations to code.  What I want in the end is the complete count of each code within each conversation, and I'll use SPSS for the actual analysis.

I'm having a hard time figuring out how to do two things:
1.  measure interrater reliability
2.  collapse the codes applied by multiple raters on a single conversation so the codes aren't double-counted

I might have more questions as I go along.  I'd love it if anyone who has experience with Dedoose can respond to this thread.  Thank you!",0,"I'm using Dedoose for qualitative research.  I have a complete codebook, and 720 conversations to code.  What I want in the end is the complete count of each code within each conversation, and I'll use SPSS for the actual analysis.

I'm having a hard time figuring out how to do two things:
1.  measure interrater reliability
2.  collapse the codes applied by multiple raters on a single conversation so the codes aren't double-counted

I might have more questions as I go along.  I'd love it if anyone who has experience with Dedoose can respond to this thread.  Thank you!",5,statistics,54935,,Who's Familiar with Dedoose Software?,https://www.reddit.com/r/statistics/comments/8aumu1/whos_familiar_with_dedoose_software/,all_ads,2018-04-09 05:06:44,54 days 20:28:50.036584000,
Got into 2 MS in Statistics programs: George Washington U and Villanova. I want to go into industry right out of the program. To me they both feel like awesome programs but cost aside what should I be making my decision based on? GWU is super pricy which is why I’m shying away from it. Anyone know anything about either of these programs? ,4,1523260902.0,8au80s,False,Got into 2 MS in Statistics programs: George Washington U and Villanova. I want to go into industry right out of the program. To me they both feel like awesome programs but cost aside what should I be making my decision based on? GWU is super pricy which is why I’m shying away from it. Anyone know anything about either of these programs? ,0,Got into 2 MS in Statistics programs: George Washington U and Villanova. I want to go into industry right out of the program. To me they both feel like awesome programs but cost aside what should I be making my decision based on? GWU is super pricy which is why I’m shying away from it. Anyone know anything about either of these programs? ,6,statistics,54935,,Gotta make a grad school decision....,https://www.reddit.com/r/statistics/comments/8au80s/gotta_make_a_grad_school_decision/,all_ads,2018-04-09 04:01:42,54 days 21:33:52.036584000,
"I have posted a question on stackexchange regarding comparison of distributions [here](https://stats.stackexchange.com/questions/339313/comparing-distributions-bayesian-decision-analysis).

Rather than repeat the post, Ill just give you the gist...

I am trying to compare distributions of two measurements. The two distributions are from a ""positive"" and ""negative"" unobserved variable, and I want to test if the observed variable differs between the two groups. I have sampled from the posteror and computed p(diff_means > 0) = 0.3. 

However, since one distribtion has a longer tail, is it possible to threshold to select/ deselect a population with higher values?

There is a figure in the link to help explain!

",8,1523232215.0,8aquwi,False,"I have posted a question on stackexchange regarding comparison of distributions [here](https://stats.stackexchange.com/questions/339313/comparing-distributions-bayesian-decision-analysis).

Rather than repeat the post, Ill just give you the gist...

I am trying to compare distributions of two measurements. The two distributions are from a ""positive"" and ""negative"" unobserved variable, and I want to test if the observed variable differs between the two groups. I have sampled from the posteror and computed p(diff_means > 0) = 0.3. 

However, since one distribtion has a longer tail, is it possible to threshold to select/ deselect a population with higher values?

There is a figure in the link to help explain!

",0,"I have posted a question on stackexchange regarding comparison of distributions [here](https://stats.stackexchange.com/questions/339313/comparing-distributions-bayesian-decision-analysis).

Rather than repeat the post, Ill just give you the gist...

I am trying to compare distributions of two measurements. The two distributions are from a ""positive"" and ""negative"" unobserved variable, and I want to test if the observed variable differs between the two groups. I have sampled from the posteror and computed p(diff_means > 0) = 0.3. 

However, since one distribtion has a longer tail, is it possible to threshold to select/ deselect a population with higher values?

There is a figure in the link to help explain!

",15,statistics,54935,,comparing distributions - bayesian decision analysis (X-cross-validated),https://www.reddit.com/r/statistics/comments/8aquwi/comparing_distributions_bayesian_decision/,all_ads,2018-04-08 20:03:35,55 days 05:31:59.036584000,
"Dear Colleagues,

I notice a page for determining sample sizes (http://web1.sph.emory.edu/cdckms/sample%20size%202%20grps%20cohort.htm) cite formulas in the second edition of Fleiss's  Statistical Methods for Rates and Proportions book. What I would like to know is, what formulas do these correspond to in Fleiss's third edition of the book, as I dont have access to a copy of the second edition.

Many thanks.",0,1523279813.0,8aw2uw,False,"Dear Colleagues,

I notice a page for determining sample sizes (http://web1.sph.emory.edu/cdckms/sample%20size%202%20grps%20cohort.htm) cite formulas in the second edition of Fleiss's  Statistical Methods for Rates and Proportions book. What I would like to know is, what formulas do these correspond to in Fleiss's third edition of the book, as I dont have access to a copy of the second edition.

Many thanks.",0,"Dear Colleagues,

I notice a page for determining sample sizes (http://web1.sph.emory.edu/cdckms/sample%20size%202%20grps%20cohort.htm) cite formulas in the second edition of Fleiss's  Statistical Methods for Rates and Proportions book. What I would like to know is, what formulas do these correspond to in Fleiss's third edition of the book, as I dont have access to a copy of the second edition.

Many thanks.",1,statistics,54935,,"Formulas 3.18 & 3.19 in Fleiss JL et al. Statistical Methods for Rates and Proportions, second edition. Wiley. 1981.",https://www.reddit.com/r/statistics/comments/8aw2uw/formulas_318_319_in_fleiss_jl_et_al_statistical/,all_ads,2018-04-09 09:16:53,54 days 16:18:41.036584000,
I have a chi squared GOF test with 3912 degrees of freedom and a chi squared value of 3780.92. How do I get the p value? (I'd be fine if you just told me what it is as well :P),6,1523278113.0,8avxif,False,I have a chi squared GOF test with 3912 degrees of freedom and a chi squared value of 3780.92. How do I get the p value? (I'd be fine if you just told me what it is as well :P),0,I have a chi squared GOF test with 3912 degrees of freedom and a chi squared value of 3780.92. How do I get the p value? (I'd be fine if you just told me what it is as well :P),0,statistics,54935,,How do I get the p value for a chi squared test with a high df?,https://www.reddit.com/r/statistics/comments/8avxif/how_do_i_get_the_p_value_for_a_chi_squared_test/,all_ads,2018-04-09 08:48:33,54 days 16:47:01.036584000,
"I'm working on a project where I am comparing disease rates, I have little to no statistical background, save a few intermediate classes and youtube.Do you guys know of a company that I can pay in order to help me properly analyze the data? ",0,1523277504.0,8avvji,False,"I'm working on a project where I am comparing disease rates, I have little to no statistical background, save a few intermediate classes and youtube.Do you guys know of a company that I can pay in order to help me properly analyze the data? ",0,"I'm working on a project where I am comparing disease rates, I have little to no statistical background, save a few intermediate classes and youtube.Do you guys know of a company that I can pay in order to help me properly analyze the data? ",0,statistics,54935,,Need help properly analyzing and configuring study data,https://www.reddit.com/r/statistics/comments/8avvji/need_help_properly_analyzing_and_configuring/,all_ads,2018-04-09 08:38:24,54 days 16:57:10.036584000,
,0,1523252815.0,8atbvx,False,,0,,0,statistics,54935,,Graphing Help,https://www.reddit.com/r/MathHelp/comments/8at7fs/graphing_help/,all_ads,2018-04-09 01:46:55,54 days 23:48:39.036584000,19600.0
Just finished collecting data on a project I’ve been working on for a while and I’m having a brain fart on how to process it. I’ve been working to see if empathetic imitation extends to complex motor movements. I had participants watch videos that included models on a screen writing at points in them. Videos were ranked ordinaly based on the amount of the target motion visible over the duration of the video. The Dv is the amount of writing if any. Participants had their hands on a pencil to paper while watching and the length of the farthest point from the starting middle point is measured to determine how much movement occurred. I believe this would be ratio due to it being possible no movement occurred. How then would I analyze this data. It’s probably something simple but I can’t for the life of me remember what my original plan was.,8,1523230319.0,8aqnft,False,Just finished collecting data on a project I’ve been working on for a while and I’m having a brain fart on how to process it. I’ve been working to see if empathetic imitation extends to complex motor movements. I had participants watch videos that included models on a screen writing at points in them. Videos were ranked ordinaly based on the amount of the target motion visible over the duration of the video. The Dv is the amount of writing if any. Participants had their hands on a pencil to paper while watching and the length of the farthest point from the starting middle point is measured to determine how much movement occurred. I believe this would be ratio due to it being possible no movement occurred. How then would I analyze this data. It’s probably something simple but I can’t for the life of me remember what my original plan was.,0,Just finished collecting data on a project I’ve been working on for a while and I’m having a brain fart on how to process it. I’ve been working to see if empathetic imitation extends to complex motor movements. I had participants watch videos that included models on a screen writing at points in them. Videos were ranked ordinaly based on the amount of the target motion visible over the duration of the video. The Dv is the amount of writing if any. Participants had their hands on a pencil to paper while watching and the length of the farthest point from the starting middle point is measured to determine how much movement occurred. I believe this would be ratio due to it being possible no movement occurred. How then would I analyze this data. It’s probably something simple but I can’t for the life of me remember what my original plan was.,2,statistics,54935,,What sort of test should I be doing to analyze this data?,https://www.reddit.com/r/statistics/comments/8aqnft/what_sort_of_test_should_i_be_doing_to_analyze/,all_ads,2018-04-08 19:31:59,55 days 06:03:35.036584000,
How do I do anova with heteroscedastic data?,9,1523192185.0,8anvwq,False,How do I do anova with heteroscedastic data?,0,How do I do anova with heteroscedastic data?,8,statistics,54935,,"2 way anova, what do I do when Levene’s test fails?",https://www.reddit.com/r/statistics/comments/8anvwq/2_way_anova_what_do_i_do_when_levenes_test_fails/,all_ads,2018-04-08 08:56:25,55 days 16:39:09.036584000,
"Is the presence or absence of something considered ordinal data? In my case, I'm grouping participants on whether they played a game with violence or a game without violence. Since one group would essentially be ""zero violence"" and the other would be ""more than zero violence"", would this represent an ordering that indicates ordinal data? Or is it just nominal?",11,1523181537.0,8amy2e,False,"Is the presence or absence of something considered ordinal data? In my case, I'm grouping participants on whether they played a game with violence or a game without violence. Since one group would essentially be ""zero violence"" and the other would be ""more than zero violence"", would this represent an ordering that indicates ordinal data? Or is it just nominal?",0,"Is the presence or absence of something considered ordinal data? In my case, I'm grouping participants on whether they played a game with violence or a game without violence. Since one group would essentially be ""zero violence"" and the other would be ""more than zero violence"", would this represent an ordering that indicates ordinal data? Or is it just nominal?",3,statistics,54935,,Is this nominal or ordinal data?,https://www.reddit.com/r/statistics/comments/8amy2e/is_this_nominal_or_ordinal_data/,all_ads,2018-04-08 05:58:57,55 days 19:36:37.036584000,
"So i have data with massive amounts of metadata.  What are the best practices for reading in, aggregating, and displaying it?  I can do it,  but it isn't very elegant.  More specifically i end up with dozens of subsetted data frames, plots,  etc.

For example,  i have a gigantic table of enviro samples.  To aggregate or process data i end up subsetting to smaller more specific frames.  These start to take up memory. So, i rm() when im done with it. Is there a more memory efficient way or elegant way to do this? 

I am going to be working with dna sets with 200+ million observations over hundreds of sets.  My current approach isn't going to work unless i have access to a computing cluster,  which i don't.",14,1523159746.0,8aklvb,False,"So i have data with massive amounts of metadata.  What are the best practices for reading in, aggregating, and displaying it?  I can do it,  but it isn't very elegant.  More specifically i end up with dozens of subsetted data frames, plots,  etc.

For example,  i have a gigantic table of enviro samples.  To aggregate or process data i end up subsetting to smaller more specific frames.  These start to take up memory. So, i rm() when im done with it. Is there a more memory efficient way or elegant way to do this? 

I am going to be working with dna sets with 200+ million observations over hundreds of sets.  My current approach isn't going to work unless i have access to a computing cluster,  which i don't.",0,"So i have data with massive amounts of metadata.  What are the best practices for reading in, aggregating, and displaying it?  I can do it,  but it isn't very elegant.  More specifically i end up with dozens of subsetted data frames, plots,  etc.

For example,  i have a gigantic table of enviro samples.  To aggregate or process data i end up subsetting to smaller more specific frames.  These start to take up memory. So, i rm() when im done with it. Is there a more memory efficient way or elegant way to do this? 

I am going to be working with dna sets with 200+ million observations over hundreds of sets.  My current approach isn't going to work unless i have access to a computing cluster,  which i don't.",4,statistics,54935,,R elegance and scaling,https://www.reddit.com/r/statistics/comments/8aklvb/r_elegance_and_scaling/,all_ads,2018-04-07 23:55:46,56 days 01:39:48.036584000,
"Hi guys, I must test stationarity of a time series.
I use the function adf.test(timeSeriesData) {aTSA}, this function return  a result the contains 3 components:
 
* type1 (a matrix with three columns: lag, ADF, p.value, where ADF is the Augmented Dickey-Fuller test statistic)
* type2 (same as above for the second type of linear model)
* type3 (same as above for the second type of linear model)
 
If p.value < 0.05 the time series is stationarity.
My problem is: which is the difference between type1, 2 and 3? In which case do I use the type X?

Thanks for your answers!

EDIT CODE:
 
    result <- adf.test(signals[[nameOfSignal]]$data , output = FALSE)
    if(result$type1[,3]< 0.05){
        print(""Is stationary"")",3,1523122924.0,8ah2ac,False,"Hi guys, I must test stationarity of a time series.
I use the function adf.test(timeSeriesData) {aTSA}, this function return  a result the contains 3 components:
 
* type1 (a matrix with three columns: lag, ADF, p.value, where ADF is the Augmented Dickey-Fuller test statistic)
* type2 (same as above for the second type of linear model)
* type3 (same as above for the second type of linear model)
 
If p.value < 0.05 the time series is stationarity.
My problem is: which is the difference between type1, 2 and 3? In which case do I use the type X?

Thanks for your answers!

EDIT CODE:
 
    result <- adf.test(signals[[nameOfSignal]]$data , output = FALSE)
    if(result$type1[,3]< 0.05){
        print(""Is stationary"")",0,"Hi guys, I must test stationarity of a time series.
I use the function adf.test(timeSeriesData) {aTSA}, this function return  a result the contains 3 components:
 
* type1 (a matrix with three columns: lag, ADF, p.value, where ADF is the Augmented Dickey-Fuller test statistic)
* type2 (same as above for the second type of linear model)
* type3 (same as above for the second type of linear model)
 
If p.value < 0.05 the time series is stationarity.
My problem is: which is the difference between type1, 2 and 3? In which case do I use the type X?

Thanks for your answers!

EDIT CODE:
 
    result <- adf.test(signals[[nameOfSignal]]$data , output = FALSE)
    if(result$type1[,3]< 0.05){
        print(""Is stationary"")",21,statistics,54935,,How to check if time series is stationary? (with R programming language),https://www.reddit.com/r/statistics/comments/8ah2ac/how_to_check_if_time_series_is_stationary_with_r/,all_ads,2018-04-07 13:42:04,56 days 11:53:30.036584000,
"when you have F 1/(α/2,v1,v2) and F α/2,v2,v1 do you get two different numbers when using an F chart? the rows/columns on an F chart are still labeled V1,V2 so if v1 was 5 and v2 was 2 would you just use the same outcome from the chart twice or would you flip the degrees of freedom and get two separate outcomes?",5,1523178641.0,8amnw0,False,"when you have F 1/(α/2,v1,v2) and F α/2,v2,v1 do you get two different numbers when using an F chart? the rows/columns on an F chart are still labeled V1,V2 so if v1 was 5 and v2 was 2 would you just use the same outcome from the chart twice or would you flip the degrees of freedom and get two separate outcomes?",0,"when you have F 1/(α/2,v1,v2) and F α/2,v2,v1 do you get two different numbers when using an F chart? the rows/columns on an F chart are still labeled V1,V2 so if v1 was 5 and v2 was 2 would you just use the same outcome from the chart twice or would you flip the degrees of freedom and get two separate outcomes?",1,statistics,54935,,why do the degrees of freedom flip in F test equations?,https://www.reddit.com/r/statistics/comments/8amnw0/why_do_the_degrees_of_freedom_flip_in_f_test/,all_ads,2018-04-08 05:10:41,55 days 20:24:53.036584000,
"Hi! Have any of you met a textbook which states the dependent variable (y) is supposed to be normally distrubuted as an assumption for linear regression model? (I know it's not neccessery, for the sake of normality - only residuals. But finding this not-so-correct statement in any textbook will help me with a bet ; ). )",4,1523197273.0,8ao9hg,False,"Hi! Have any of you met a textbook which states the dependent variable (y) is supposed to be normally distrubuted as an assumption for linear regression model? (I know it's not neccessery, for the sake of normality - only residuals. But finding this not-so-correct statement in any textbook will help me with a bet ; ). )",0,"Hi! Have any of you met a textbook which states the dependent variable (y) is supposed to be normally distrubuted as an assumption for linear regression model? (I know it's not neccessery, for the sake of normality - only residuals. But finding this not-so-correct statement in any textbook will help me with a bet ; ). )",0,statistics,54935,,assumptions of linear regression.,https://www.reddit.com/r/statistics/comments/8ao9hg/assumptions_of_linear_regression/,all_ads,2018-04-08 10:21:13,55 days 15:14:21.036584000,
"So I'm currently critiquing two journals as part of many in my systematic-type review. Both display similar results, but one is a RCT parallel study whilst the other is a RCT crossover study. All the other journals have used SD and the parallel uses SD. However, the crossover reports results in a SEM format. 

Now from my personal research given my basic understanding of statistics, SEM presents the 'mean of the means' of multiple samples in the population with the variance of this, and SD is the reported data with mean presented and SD as variation across the results of the population. Now also in the research countless biomedical journals on reporting SD or SEM have conflicting advice on that in descriptive statistics (like this which is weights, serum biochemical markers, calories) SD should be used and SEM should not as it's more for inferential statistics and provides a potentially misleading table of results as readers will think it is smaller variation than it actually is. 

So my query is I can't understand why they have used SEM. The details of the study are:

Initially A is control, 
B is intervention 

Data collected

A 6 weeks 

B 6 weeks

Reassessment and data collection

No washout 

Crossover

A becomes control 6 weeks 

B becomes intervention 6 weeks

Final data gather.

Now I thought perhaps because it is a crossover study SEM could be used, but then I thought there wouldn't be multiple samples collected as the participants would all have only gone through the intervention once. So no need to use SEM and SD would be more appropriate. Is this assumption correct? Sorry if this is confusing I can clarify anything needed or provide the study if you want to see it.",7,1523167047.0,8alfw7,False,"So I'm currently critiquing two journals as part of many in my systematic-type review. Both display similar results, but one is a RCT parallel study whilst the other is a RCT crossover study. All the other journals have used SD and the parallel uses SD. However, the crossover reports results in a SEM format. 

Now from my personal research given my basic understanding of statistics, SEM presents the 'mean of the means' of multiple samples in the population with the variance of this, and SD is the reported data with mean presented and SD as variation across the results of the population. Now also in the research countless biomedical journals on reporting SD or SEM have conflicting advice on that in descriptive statistics (like this which is weights, serum biochemical markers, calories) SD should be used and SEM should not as it's more for inferential statistics and provides a potentially misleading table of results as readers will think it is smaller variation than it actually is. 

So my query is I can't understand why they have used SEM. The details of the study are:

Initially A is control, 
B is intervention 

Data collected

A 6 weeks 

B 6 weeks

Reassessment and data collection

No washout 

Crossover

A becomes control 6 weeks 

B becomes intervention 6 weeks

Final data gather.

Now I thought perhaps because it is a crossover study SEM could be used, but then I thought there wouldn't be multiple samples collected as the participants would all have only gone through the intervention once. So no need to use SEM and SD would be more appropriate. Is this assumption correct? Sorry if this is confusing I can clarify anything needed or provide the study if you want to see it.",0,"So I'm currently critiquing two journals as part of many in my systematic-type review. Both display similar results, but one is a RCT parallel study whilst the other is a RCT crossover study. All the other journals have used SD and the parallel uses SD. However, the crossover reports results in a SEM format. 

Now from my personal research given my basic understanding of statistics, SEM presents the 'mean of the means' of multiple samples in the population with the variance of this, and SD is the reported data with mean presented and SD as variation across the results of the population. Now also in the research countless biomedical journals on reporting SD or SEM have conflicting advice on that in descriptive statistics (like this which is weights, serum biochemical markers, calories) SD should be used and SEM should not as it's more for inferential statistics and provides a potentially misleading table of results as readers will think it is smaller variation than it actually is. 

So my query is I can't understand why they have used SEM. The details of the study are:

Initially A is control, 
B is intervention 

Data collected

A 6 weeks 

B 6 weeks

Reassessment and data collection

No washout 

Crossover

A becomes control 6 weeks 

B becomes intervention 6 weeks

Final data gather.

Now I thought perhaps because it is a crossover study SEM could be used, but then I thought there wouldn't be multiple samples collected as the participants would all have only gone through the intervention once. So no need to use SEM and SD would be more appropriate. Is this assumption correct? Sorry if this is confusing I can clarify anything needed or provide the study if you want to see it.",0,statistics,54935,,Why would a report use standard error of means (SEM) instead of standard deviation (SD)?,https://www.reddit.com/r/statistics/comments/8alfw7/why_would_a_report_use_standard_error_of_means/,all_ads,2018-04-08 01:57:27,55 days 23:38:07.036584000,
"Hello, so I am trying to figure out what statistical test I should use for this situation. 

I want to see if the data I collected is significantly below a certain number (Lets say 10). Let's say I collected 25 datasets and the average came out to 6.02 +/- 2.23. How can I prove this is statistically below 10 using lets say a p-value? Thank you. ",6,1523154623.0,8ak0js,False,"Hello, so I am trying to figure out what statistical test I should use for this situation. 

I want to see if the data I collected is significantly below a certain number (Lets say 10). Let's say I collected 25 datasets and the average came out to 6.02 +/- 2.23. How can I prove this is statistically below 10 using lets say a p-value? Thank you. ",0,"Hello, so I am trying to figure out what statistical test I should use for this situation. 

I want to see if the data I collected is significantly below a certain number (Lets say 10). Let's say I collected 25 datasets and the average came out to 6.02 +/- 2.23. How can I prove this is statistically below 10 using lets say a p-value? Thank you. ",0,statistics,54935,,What test should I use for this situation?,https://www.reddit.com/r/statistics/comments/8ak0js/what_test_should_i_use_for_this_situation/,all_ads,2018-04-07 22:30:23,56 days 03:05:11.036584000,
"It's been big in the news for the last few years that a lot of published papers don't have reproducible results with much of the blame falling on p=0.05 and very small effect sizes randomly deemed significant.  Are there statistical techniques that these (usually unexpert in statistics) researchers could use that would be stronger?  Something like bootstrapping or rerunning with all combinations of n-1 data points from their data to get a sense of how fragile their result was, except an idea generated by someone who knows statistics better than I do.  ",41,1523070768.0,8ac8mc,False,"It's been big in the news for the last few years that a lot of published papers don't have reproducible results with much of the blame falling on p=0.05 and very small effect sizes randomly deemed significant.  Are there statistical techniques that these (usually unexpert in statistics) researchers could use that would be stronger?  Something like bootstrapping or rerunning with all combinations of n-1 data points from their data to get a sense of how fragile their result was, except an idea generated by someone who knows statistics better than I do.  ",0,"It's been big in the news for the last few years that a lot of published papers don't have reproducible results with much of the blame falling on p=0.05 and very small effect sizes randomly deemed significant.  Are there statistical techniques that these (usually unexpert in statistics) researchers could use that would be stronger?  Something like bootstrapping or rerunning with all combinations of n-1 data points from their data to get a sense of how fragile their result was, except an idea generated by someone who knows statistics better than I do.  ",35,statistics,54935,,What techniques could researchers be using to reduce reproducibility problems?,https://www.reddit.com/r/statistics/comments/8ac8mc/what_techniques_could_researchers_be_using_to/,all_ads,2018-04-06 23:12:48,57 days 02:22:46.036584000,
"I hope this is allowed. I have to analyse some data and all I have been told is to do a GLM in R. I have installed R and R studio and looked at online guides and youtube videos but I'm at a loss, I can't understand anything. The only stats I've done before have been on GraphPad Prism and this was very basic. Any advice, or if you know a good source that I can use to learn GLM and maybe practice it, would be greatly appreciated! Thank you. ",15,1523151334.0,8ajn4o,False,"I hope this is allowed. I have to analyse some data and all I have been told is to do a GLM in R. I have installed R and R studio and looked at online guides and youtube videos but I'm at a loss, I can't understand anything. The only stats I've done before have been on GraphPad Prism and this was very basic. Any advice, or if you know a good source that I can use to learn GLM and maybe practice it, would be greatly appreciated! Thank you. ",0,"I hope this is allowed. I have to analyse some data and all I have been told is to do a GLM in R. I have installed R and R studio and looked at online guides and youtube videos but I'm at a loss, I can't understand anything. The only stats I've done before have been on GraphPad Prism and this was very basic. Any advice, or if you know a good source that I can use to learn GLM and maybe practice it, would be greatly appreciated! Thank you. ",0,statistics,54935,,"Please help! No prior knowledge of R, but need to do a GLM on data with 3 independent variables and 8 dependent ones",https://www.reddit.com/r/statistics/comments/8ajn4o/please_help_no_prior_knowledge_of_r_but_need_to/,all_ads,2018-04-07 21:35:34,56 days 04:00:00.036584000,
"I’m working as an actuary currently and pretty far thorough the exam process, and I have a bachelors in math.  Actuarial work has become a bit boring for me and I don’t have passion for what I’m doing at all. 

I love doing analysis and am decent at programming, I just don’t care about the insurance industry and would rather apply my skills to something more interesting or meaningful. I know I’d have some learning to do on my own before switching jobs, but I was hoping to hear from others who came from a similar background as me if you have any advice! ",10,1523094679.0,8aexho,False,"I’m working as an actuary currently and pretty far thorough the exam process, and I have a bachelors in math.  Actuarial work has become a bit boring for me and I don’t have passion for what I’m doing at all. 

I love doing analysis and am decent at programming, I just don’t care about the insurance industry and would rather apply my skills to something more interesting or meaningful. I know I’d have some learning to do on my own before switching jobs, but I was hoping to hear from others who came from a similar background as me if you have any advice! ",0,"I’m working as an actuary currently and pretty far thorough the exam process, and I have a bachelors in math.  Actuarial work has become a bit boring for me and I don’t have passion for what I’m doing at all. 

I love doing analysis and am decent at programming, I just don’t care about the insurance industry and would rather apply my skills to something more interesting or meaningful. I know I’d have some learning to do on my own before switching jobs, but I was hoping to hear from others who came from a similar background as me if you have any advice! ",6,statistics,54935,,Any former actuaries out there who switched to a statistics role?,https://www.reddit.com/r/statistics/comments/8aexho/any_former_actuaries_out_there_who_switched_to_a/,all_ads,2018-04-07 05:51:19,56 days 19:44:15.036584000,
"Hi, I need to *prepare* for a test, and this is one of the questions on a practice paper. I am not cheating - I merely want to prepare for my test. Hopefully this qualifies here.

> 3. (15 points) Sheldon, Leonard, Raj and Howard have decided to become silent partners in Stuarts comic book empire. One of their ﬁrst decisions is about opening hours at the weekend. Over one Saturday they have counted the number of customers in the shop in regular intervals of 10 minutes. Can we assume that the frequency of visitors follows a Poisson distribution? Here are the observations:

> Number of customers xi 0 1 2 3 4 5 6+ 

> Number of intervals with xi customers 5 7 10 11 5 6 4

I'm clueless here - what do I do? Don't solve it - just tell me what to do. Does this thing follow a poisson distribution? I checked online and the questions I found all have something like an average rate of whatever/time. How do I find that out here? The last number is 6+(6 and more) so I can't find any average rates here, can I?

Any help is much appreciated.",4,1523114500.0,8agjvr,False,"Hi, I need to *prepare* for a test, and this is one of the questions on a practice paper. I am not cheating - I merely want to prepare for my test. Hopefully this qualifies here.

> 3. (15 points) Sheldon, Leonard, Raj and Howard have decided to become silent partners in Stuarts comic book empire. One of their ﬁrst decisions is about opening hours at the weekend. Over one Saturday they have counted the number of customers in the shop in regular intervals of 10 minutes. Can we assume that the frequency of visitors follows a Poisson distribution? Here are the observations:

> Number of customers xi 0 1 2 3 4 5 6+ 

> Number of intervals with xi customers 5 7 10 11 5 6 4

I'm clueless here - what do I do? Don't solve it - just tell me what to do. Does this thing follow a poisson distribution? I checked online and the questions I found all have something like an average rate of whatever/time. How do I find that out here? The last number is 6+(6 and more) so I can't find any average rates here, can I?

Any help is much appreciated.",0,"Hi, I need to *prepare* for a test, and this is one of the questions on a practice paper. I am not cheating - I merely want to prepare for my test. Hopefully this qualifies here.

> 3. (15 points) Sheldon, Leonard, Raj and Howard have decided to become silent partners in Stuarts comic book empire. One of their ﬁrst decisions is about opening hours at the weekend. Over one Saturday they have counted the number of customers in the shop in regular intervals of 10 minutes. Can we assume that the frequency of visitors follows a Poisson distribution? Here are the observations:

> Number of customers xi 0 1 2 3 4 5 6+ 

> Number of intervals with xi customers 5 7 10 11 5 6 4

I'm clueless here - what do I do? Don't solve it - just tell me what to do. Does this thing follow a poisson distribution? I checked online and the questions I found all have something like an average rate of whatever/time. How do I find that out here? The last number is 6+(6 and more) so I can't find any average rates here, can I?

Any help is much appreciated.",0,statistics,54935,,Not a homework/cheating question,https://www.reddit.com/r/statistics/comments/8agjvr/not_a_homeworkcheating_question/,all_ads,2018-04-07 11:21:40,56 days 14:13:54.036584000,
"Video: https://www.youtube.com/watch?v=acsSIyDugP0
Visualizations created using the tool: https://github.com/ryu577/pyray",0,1523112212.0,8age94,False,"Video: https://www.youtube.com/watch?v=acsSIyDugP0
Visualizations created using the tool: https://github.com/ryu577/pyray",0,"Video: https://www.youtube.com/watch?v=acsSIyDugP0
Visualizations created using the tool: https://github.com/ryu577/pyray",0,statistics,54935,,Check out this visualization of Newtons method working in multiple dimensions: https://www.youtube.com/watch?v=acsSIyDugP0,https://www.reddit.com/r/statistics/comments/8age94/check_out_this_visualization_of_newtons_method/,all_ads,2018-04-07 10:43:32,56 days 14:52:02.036584000,
"If you could get a large tattoo of a theorem, proof, or distribution, what would you get? 

I have Bayes under my bicep but I want a sleeve length on the shoulder/tricep/forearm. Anyone do something similar? ",21,1523087557.0,8ae87m,False,"If you could get a large tattoo of a theorem, proof, or distribution, what would you get? 

I have Bayes under my bicep but I want a sleeve length on the shoulder/tricep/forearm. Anyone do something similar? ",0,"If you could get a large tattoo of a theorem, proof, or distribution, what would you get? 

I have Bayes under my bicep but I want a sleeve length on the shoulder/tricep/forearm. Anyone do something similar? ",2,statistics,54935,,Tattoo Ideas?,https://www.reddit.com/r/statistics/comments/8ae87m/tattoo_ideas/,all_ads,2018-04-07 03:52:37,56 days 21:42:57.036584000,
In multiple regression. Is it 3? ,5,1523080437.0,8adg7j,False,In multiple regression. Is it 3? ,0,In multiple regression. Is it 3? ,0,statistics,54935,,The number of interactions possible with three predictor variables,https://www.reddit.com/r/statistics/comments/8adg7j/the_number_of_interactions_possible_with_three/,all_ads,2018-04-07 01:53:57,56 days 23:41:37.036584000,
,18,1523008040.0,8a5n6j,False,,0,,28,statistics,54935,,What is your favorite software for flashy/sexy visualizations and good data entry?,https://www.reddit.com/r/AskStatistics/comments/8a459b/what_is_your_favorite_software_for_flashysexy/,all_ads,2018-04-06 05:47:20,57 days 19:48:14.036584000,19600.0
"Since the coefficient of the interaction term stays the same after centering, the effect of X on Z will be (B1+B3*Y). Before centering, Y will always be positive, but after centering, Y can be negative. So what I'm confused on is that the interaction effect can change signs after centering?",6,1523067822.0,8abut8,False,"Since the coefficient of the interaction term stays the same after centering, the effect of X on Z will be (B1+B3*Y). Before centering, Y will always be positive, but after centering, Y can be negative. So what I'm confused on is that the interaction effect can change signs after centering?",0,"Since the coefficient of the interaction term stays the same after centering, the effect of X on Z will be (B1+B3*Y). Before centering, Y will always be positive, but after centering, Y can be negative. So what I'm confused on is that the interaction effect can change signs after centering?",0,statistics,54935,,Confused on interaction term multiple regression,https://www.reddit.com/r/statistics/comments/8abut8/confused_on_interaction_term_multiple_regression/,all_ads,2018-04-06 22:23:42,57 days 03:11:52.036584000,
"I am a university student doing a market analysis of Vietnam's cosmetics industry and I found a company, Q&Me that has lots of stats about Vietnam. the cosmetics statistics are only with a sample size of 500 people. So I am wondering if this is large enough or if anybody has used this companies research before. I do not want to use it if it is a poor sample. I apologize if there was things wrong with my post, I have been working on this for many hours now and It is getting late in my neck of the woods. Thanks in advance.",0,1523054670.0,8aa2fy,False,"I am a university student doing a market analysis of Vietnam's cosmetics industry and I found a company, Q&Me that has lots of stats about Vietnam. the cosmetics statistics are only with a sample size of 500 people. So I am wondering if this is large enough or if anybody has used this companies research before. I do not want to use it if it is a poor sample. I apologize if there was things wrong with my post, I have been working on this for many hours now and It is getting late in my neck of the woods. Thanks in advance.",0,"I am a university student doing a market analysis of Vietnam's cosmetics industry and I found a company, Q&Me that has lots of stats about Vietnam. the cosmetics statistics are only with a sample size of 500 people. So I am wondering if this is large enough or if anybody has used this companies research before. I do not want to use it if it is a poor sample. I apologize if there was things wrong with my post, I have been working on this for many hours now and It is getting late in my neck of the woods. Thanks in advance.",1,statistics,54935,,Anybody used Q&Me?,https://www.reddit.com/r/statistics/comments/8aa2fy/anybody_used_qme/,all_ads,2018-04-06 18:44:30,57 days 06:51:04.036584000,
"I did a study where participants fell into 1 of 2 groups: Group A and Group B. I measured the walking speed of participants at 10 different time points (i.e. 10 sec mark, 20 sec mark, 30 sec mark, etc). My hypothesis is that participants in Group A will increase their speed at an earlier time point than will Group B. I also hypothesize that the subsequent decrease in speed will come at an earlier time point for participants in Group A than Group B.

Would the right way to test this be with a 2x10 mixed measures ANOVA? If so, how would I be able to prove the earlier increase and earlier decrease for Group A? Would it be finding that there is an interaction? But where would I go from there?",6,1523007698.0,8a5lrj,False,"I did a study where participants fell into 1 of 2 groups: Group A and Group B. I measured the walking speed of participants at 10 different time points (i.e. 10 sec mark, 20 sec mark, 30 sec mark, etc). My hypothesis is that participants in Group A will increase their speed at an earlier time point than will Group B. I also hypothesize that the subsequent decrease in speed will come at an earlier time point for participants in Group A than Group B.

Would the right way to test this be with a 2x10 mixed measures ANOVA? If so, how would I be able to prove the earlier increase and earlier decrease for Group A? Would it be finding that there is an interaction? But where would I go from there?",0,"I did a study where participants fell into 1 of 2 groups: Group A and Group B. I measured the walking speed of participants at 10 different time points (i.e. 10 sec mark, 20 sec mark, 30 sec mark, etc). My hypothesis is that participants in Group A will increase their speed at an earlier time point than will Group B. I also hypothesize that the subsequent decrease in speed will come at an earlier time point for participants in Group A than Group B.

Would the right way to test this be with a 2x10 mixed measures ANOVA? If so, how would I be able to prove the earlier increase and earlier decrease for Group A? Would it be finding that there is an interaction? But where would I go from there?",1,statistics,54935,,Statistical test for time points?,https://www.reddit.com/r/statistics/comments/8a5lrj/statistical_test_for_time_points/,all_ads,2018-04-06 05:41:38,57 days 19:53:56.036584000,
"p(X)= Sum Y??? of p(X,Y)

I don't understand what the summation is supposed to be, usually it's a series of things? the Y is written underneath the big E with nothing else on top",10,1523003751.0,8a55eu,False,"p(X)= Sum Y??? of p(X,Y)

I don't understand what the summation is supposed to be, usually it's a series of things? the Y is written underneath the big E with nothing else on top",0,"p(X)= Sum Y??? of p(X,Y)

I don't understand what the summation is supposed to be, usually it's a series of things? the Y is written underneath the big E with nothing else on top",1,statistics,54935,,what is the sum rule?,https://www.reddit.com/r/statistics/comments/8a55eu/what_is_the_sum_rule/,all_ads,2018-04-06 04:35:51,57 days 20:59:43.036584000,
"I thought some of you could relate to what Norvig (director of research at Google) is saying here: http://norvig.com/21-days.html

>Researchers have shown it takes about ten years to develop expertise in any of a wide variety of areas, including chess playing, music composition, telegraph operation, painting, piano playing, swimming, tennis, and research in neuropsychology and topology. The key is deliberative practice

>Get interested in programming, and do some because it is fun. Make sure that it keeps being enough fun so that you will be willing to put in your ten years/10,000 hours.

>Program. The best kind of learning is learning by doing. 

>Talk with other programmers; read other programs. This is more important than any book or training course.

>If you want, put in four years at a college (or more at a graduate school).

>Work on projects with other programmers. Be the best programmer on some projects; be the worst on some others. 

>Work on projects after other programmers. Understand a program written by someone else. 

*****

>So go ahead and buy that Java/Ruby/Javascript/PHP book; you'll probably get some use out of it. But you won't change your life, or your real overall expertise as a programmer in 24 hours or 21 days. How about working hard to continually improve over 24 months? Well, now you're starting to get somewhere...

",7,1522915065.0,89uxiz,False,"I thought some of you could relate to what Norvig (director of research at Google) is saying here: http://norvig.com/21-days.html

>Researchers have shown it takes about ten years to develop expertise in any of a wide variety of areas, including chess playing, music composition, telegraph operation, painting, piano playing, swimming, tennis, and research in neuropsychology and topology. The key is deliberative practice

>Get interested in programming, and do some because it is fun. Make sure that it keeps being enough fun so that you will be willing to put in your ten years/10,000 hours.

>Program. The best kind of learning is learning by doing. 

>Talk with other programmers; read other programs. This is more important than any book or training course.

>If you want, put in four years at a college (or more at a graduate school).

>Work on projects with other programmers. Be the best programmer on some projects; be the worst on some others. 

>Work on projects after other programmers. Understand a program written by someone else. 

*****

>So go ahead and buy that Java/Ruby/Javascript/PHP book; you'll probably get some use out of it. But you won't change your life, or your real overall expertise as a programmer in 24 hours or 21 days. How about working hard to continually improve over 24 months? Well, now you're starting to get somewhere...

",0,"I thought some of you could relate to what Norvig (director of research at Google) is saying here: http://norvig.com/21-days.html

>Researchers have shown it takes about ten years to develop expertise in any of a wide variety of areas, including chess playing, music composition, telegraph operation, painting, piano playing, swimming, tennis, and research in neuropsychology and topology. The key is deliberative practice

>Get interested in programming, and do some because it is fun. Make sure that it keeps being enough fun so that you will be willing to put in your ten years/10,000 hours.

>Program. The best kind of learning is learning by doing. 

>Talk with other programmers; read other programs. This is more important than any book or training course.

>If you want, put in four years at a college (or more at a graduate school).

>Work on projects with other programmers. Be the best programmer on some projects; be the worst on some others. 

>Work on projects after other programmers. Understand a program written by someone else. 

*****

>So go ahead and buy that Java/Ruby/Javascript/PHP book; you'll probably get some use out of it. But you won't change your life, or your real overall expertise as a programmer in 24 hours or 21 days. How about working hard to continually improve over 24 months? Well, now you're starting to get somewhere...

",82,statistics,54935,,"""Teach Yourself Programming in Ten Years"" by Peter Norvig (very relevant to statistics)",https://www.reddit.com/r/statistics/comments/89uxiz/teach_yourself_programming_in_ten_years_by_peter/,all_ads,2018-04-05 03:57:45,58 days 21:37:49.036584000,
"Disclosure - not a statistician, a high school teacher attempting to teach myself how to undertake a meta analysis.

I am using OpenMEE to assemble my data and complete the meta analysis (all fine so far).  When I actually run the meta analysis, I can chose the ""random effects model"" from a list:

(1) Hedges-Olkin
(2) Sidik-Jonkman
(3) DerSimonian-Laird
(4) Maximum Likelihood
(5) Restricted Maximum Likelihood
(6) Empirical Bayes

As far as I understand it, these different methods account for the weighting of each individual effect size in my study, based on variance (and other factors) - and in some way account for the ""random"" nature of the overall study.

Can someone ELI5 the specific difference between each method -- I have Googled it and I cant find anything out there that hits the spot.

That - or a link to a book / reference that will help me....

Cheers all.
",3,1522947740.0,89y5u8,False,"Disclosure - not a statistician, a high school teacher attempting to teach myself how to undertake a meta analysis.

I am using OpenMEE to assemble my data and complete the meta analysis (all fine so far).  When I actually run the meta analysis, I can chose the ""random effects model"" from a list:

(1) Hedges-Olkin
(2) Sidik-Jonkman
(3) DerSimonian-Laird
(4) Maximum Likelihood
(5) Restricted Maximum Likelihood
(6) Empirical Bayes

As far as I understand it, these different methods account for the weighting of each individual effect size in my study, based on variance (and other factors) - and in some way account for the ""random"" nature of the overall study.

Can someone ELI5 the specific difference between each method -- I have Googled it and I cant find anything out there that hits the spot.

That - or a link to a book / reference that will help me....

Cheers all.
",0,"Disclosure - not a statistician, a high school teacher attempting to teach myself how to undertake a meta analysis.

I am using OpenMEE to assemble my data and complete the meta analysis (all fine so far).  When I actually run the meta analysis, I can chose the ""random effects model"" from a list:

(1) Hedges-Olkin
(2) Sidik-Jonkman
(3) DerSimonian-Laird
(4) Maximum Likelihood
(5) Restricted Maximum Likelihood
(6) Empirical Bayes

As far as I understand it, these different methods account for the weighting of each individual effect size in my study, based on variance (and other factors) - and in some way account for the ""random"" nature of the overall study.

Can someone ELI5 the specific difference between each method -- I have Googled it and I cant find anything out there that hits the spot.

That - or a link to a book / reference that will help me....

Cheers all.
",10,statistics,54935,,ELI5 Request: Random effects method when undertaking a meta analysis,https://www.reddit.com/r/statistics/comments/89y5u8/eli5_request_random_effects_method_when/,all_ads,2018-04-05 13:02:20,58 days 12:33:14.036584000,
"I've got two bar charts on 1 graph, showing condition 1 vs condition 2, but I've done Tukey tests between conditions so like my chart has 16 bars and 13 significant differences to show, I tried the bracket system but it's super messy, with all the bars crossing over. Is there another way to showcase the Tukey test results?",5,1522965324.0,89zvve,False,"I've got two bar charts on 1 graph, showing condition 1 vs condition 2, but I've done Tukey tests between conditions so like my chart has 16 bars and 13 significant differences to show, I tried the bracket system but it's super messy, with all the bars crossing over. Is there another way to showcase the Tukey test results?",0,"I've got two bar charts on 1 graph, showing condition 1 vs condition 2, but I've done Tukey tests between conditions so like my chart has 16 bars and 13 significant differences to show, I tried the bracket system but it's super messy, with all the bars crossing over. Is there another way to showcase the Tukey test results?",3,statistics,54935,,How to show significant differences on bar chart with lots of bars?,https://www.reddit.com/r/statistics/comments/89zvve/how_to_show_significant_differences_on_bar_chart/,all_ads,2018-04-05 17:55:24,58 days 07:40:10.036584000,
https://medium.com/@sadatnazrul/clustering-based-unsupervised-learning-8d705298ae51,4,1522993746.0,8a3w7j,False,https://medium.com/@sadatnazrul/clustering-based-unsupervised-learning-8d705298ae51,0,https://medium.com/@sadatnazrul/clustering-based-unsupervised-learning-8d705298ae51,0,statistics,54935,,Clustering Based Unsupervised Learning,https://www.reddit.com/r/statistics/comments/8a3w7j/clustering_based_unsupervised_learning/,all_ads,2018-04-06 01:49:06,57 days 23:46:28.036584000,
,4,1522978247.0,8a1p7h,False,,0,,0,statistics,54935,,What are the pros and cons of using least absolution deviations vs least squares in estimating parameters?,https://www.reddit.com/r/statistics/comments/8a1p7h/what_are_the_pros_and_cons_of_using_least/,all_ads,2018-04-05 21:30:47,58 days 04:04:47.036584000,
"I will be graduating with my bachelors in mathematics May 2019, and hope to enter a MS biostatistics program the following fall. I got an A in calc one, B in calc 2 and 3 and decent math grades beyond that. In the fall the relevant class I will be taking are a regression analysis class, an intro to biostatistics class, and a project based mathematical epidemiology class. I am doing an internship in Singapore this summer, and I am still in the placement process but I’m hoping I get placed in a research setting at a university, but beyond that I will have no research experience unless I can figure out something for the fall beyond my project. I should have around a 3.4 GPA or when I apply, and I haven’t taken the GRE yet. I also just have absolutely 0 idea of what to expect about admissions. I go to a state school in Kansas, and would like to enter our biostatistics program. Do I have a chance?",3,1522967842.0,8a07xu,False,"I will be graduating with my bachelors in mathematics May 2019, and hope to enter a MS biostatistics program the following fall. I got an A in calc one, B in calc 2 and 3 and decent math grades beyond that. In the fall the relevant class I will be taking are a regression analysis class, an intro to biostatistics class, and a project based mathematical epidemiology class. I am doing an internship in Singapore this summer, and I am still in the placement process but I’m hoping I get placed in a research setting at a university, but beyond that I will have no research experience unless I can figure out something for the fall beyond my project. I should have around a 3.4 GPA or when I apply, and I haven’t taken the GRE yet. I also just have absolutely 0 idea of what to expect about admissions. I go to a state school in Kansas, and would like to enter our biostatistics program. Do I have a chance?",0,"I will be graduating with my bachelors in mathematics May 2019, and hope to enter a MS biostatistics program the following fall. I got an A in calc one, B in calc 2 and 3 and decent math grades beyond that. In the fall the relevant class I will be taking are a regression analysis class, an intro to biostatistics class, and a project based mathematical epidemiology class. I am doing an internship in Singapore this summer, and I am still in the placement process but I’m hoping I get placed in a research setting at a university, but beyond that I will have no research experience unless I can figure out something for the fall beyond my project. I should have around a 3.4 GPA or when I apply, and I haven’t taken the GRE yet. I also just have absolutely 0 idea of what to expect about admissions. I go to a state school in Kansas, and would like to enter our biostatistics program. Do I have a chance?",1,statistics,54935,,Will I be able to get into a biostatistics M.S. program?,https://www.reddit.com/r/statistics/comments/8a07xu/will_i_be_able_to_get_into_a_biostatistics_ms/,all_ads,2018-04-05 18:37:22,58 days 06:58:12.036584000,
"This almost sounds like the intro to a terrible joke, so sorry if you were expecting a punchline.

I am doing my Ph.D in biostats and came from applied math. I sense that my approach to the material is different than my classmate's approaches just based on the questions I ask in class and what I feel is interesting about the material. I was hoping to hear from professors, or people who came into stats from math, what they feel the difference is (if one exists at all).",3,1522923352.0,89vx4t,False,"This almost sounds like the intro to a terrible joke, so sorry if you were expecting a punchline.

I am doing my Ph.D in biostats and came from applied math. I sense that my approach to the material is different than my classmate's approaches just based on the questions I ask in class and what I feel is interesting about the material. I was hoping to hear from professors, or people who came into stats from math, what they feel the difference is (if one exists at all).",0,"This almost sounds like the intro to a terrible joke, so sorry if you were expecting a punchline.

I am doing my Ph.D in biostats and came from applied math. I sense that my approach to the material is different than my classmate's approaches just based on the questions I ask in class and what I feel is interesting about the material. I was hoping to hear from professors, or people who came into stats from math, what they feel the difference is (if one exists at all).",8,statistics,54935,,What are the differences between a statistics student studying statistics and a math student studying statistics?,https://www.reddit.com/r/statistics/comments/89vx4t/what_are_the_differences_between_a_statistics/,all_ads,2018-04-05 06:15:52,58 days 19:19:42.036584000,
"My professor says this is the most important theorem in inferential statistics.

Was just curious about your thoughts on that statement?",50,1522894385.0,89rzev,False,"My professor says this is the most important theorem in inferential statistics.

Was just curious about your thoughts on that statement?",0,"My professor says this is the most important theorem in inferential statistics.

Was just curious about your thoughts on that statement?",26,statistics,54935,,The central limit theorem,https://www.reddit.com/r/statistics/comments/89rzev/the_central_limit_theorem/,all_ads,2018-04-04 22:13:05,59 days 03:22:29.036584000,
"I have a very large dataset with 5 columns:

* Contagious_date
* Measurements_date
* Measurement_1
* Measurement_2
* Death_date

I want to find out which measurement is better. To do so, I am using time-dependent ROC curves.

Prior knowledge:

* Death will happen a year **after the contagion**, maybe a year minus or plus two weeks. Death in fact distributes like a thin normal around the year.

* I will not know for sure when the contagion happend in most cases (but I do have a subset of animals that were administered the disease on purpose so for them, I do have the exact date).

* Both measurements are indicators of disease advancement are made at the same time for each subject, and are a blood concentration of substance X. I only have one measure of each kind for each subject at one particular time.

* In theory, both measurements are more correlated with contagion date than to death date, because death date can be more influenced by external factors. I rather want a contagious date than a death date. **More than when the subject is going to die, I want to know when the concentration of substance X is going to be at particular number, and it always happen a year after the contagion, ""independently"" of when the subject dies.**

* I do have censored data, when subjects die for any other causes.

I have succesfully done the ROC curves for both measurements to estimate the death date, they are acceptable but not good. My question is, can I perform a *reverse* time-dependant ROC curve? 
I talk about trying to estimate the contagion date instead of the death date, I just need to reverse the measurements and see how good they are estimating contagion date with that subset of my sample that has those exact dates. I couldn't find any examples in the literature so far...

Also, when I say ""can I"" I mean if I should... I have already done it and results are better than when I do it with death date, but I don't know if it is an acceptable metodology... Have you ever heard of doing this? Do you know if it is right? Does it sound intuitive? I have a masters in biostatistics but my background is Biology so while it does sound right to me, I am absolutely lost in the math, and can't really know if this violates some vital part of the time dependant ROC.

Thank you very much in advance!
",0,1522952265.0,89yill,False,"I have a very large dataset with 5 columns:

* Contagious_date
* Measurements_date
* Measurement_1
* Measurement_2
* Death_date

I want to find out which measurement is better. To do so, I am using time-dependent ROC curves.

Prior knowledge:

* Death will happen a year **after the contagion**, maybe a year minus or plus two weeks. Death in fact distributes like a thin normal around the year.

* I will not know for sure when the contagion happend in most cases (but I do have a subset of animals that were administered the disease on purpose so for them, I do have the exact date).

* Both measurements are indicators of disease advancement are made at the same time for each subject, and are a blood concentration of substance X. I only have one measure of each kind for each subject at one particular time.

* In theory, both measurements are more correlated with contagion date than to death date, because death date can be more influenced by external factors. I rather want a contagious date than a death date. **More than when the subject is going to die, I want to know when the concentration of substance X is going to be at particular number, and it always happen a year after the contagion, ""independently"" of when the subject dies.**

* I do have censored data, when subjects die for any other causes.

I have succesfully done the ROC curves for both measurements to estimate the death date, they are acceptable but not good. My question is, can I perform a *reverse* time-dependant ROC curve? 
I talk about trying to estimate the contagion date instead of the death date, I just need to reverse the measurements and see how good they are estimating contagion date with that subset of my sample that has those exact dates. I couldn't find any examples in the literature so far...

Also, when I say ""can I"" I mean if I should... I have already done it and results are better than when I do it with death date, but I don't know if it is an acceptable metodology... Have you ever heard of doing this? Do you know if it is right? Does it sound intuitive? I have a masters in biostatistics but my background is Biology so while it does sound right to me, I am absolutely lost in the math, and can't really know if this violates some vital part of the time dependant ROC.

Thank you very much in advance!
",0,"I have a very large dataset with 5 columns:

* Contagious_date
* Measurements_date
* Measurement_1
* Measurement_2
* Death_date

I want to find out which measurement is better. To do so, I am using time-dependent ROC curves.

Prior knowledge:

* Death will happen a year **after the contagion**, maybe a year minus or plus two weeks. Death in fact distributes like a thin normal around the year.

* I will not know for sure when the contagion happend in most cases (but I do have a subset of animals that were administered the disease on purpose so for them, I do have the exact date).

* Both measurements are indicators of disease advancement are made at the same time for each subject, and are a blood concentration of substance X. I only have one measure of each kind for each subject at one particular time.

* In theory, both measurements are more correlated with contagion date than to death date, because death date can be more influenced by external factors. I rather want a contagious date than a death date. **More than when the subject is going to die, I want to know when the concentration of substance X is going to be at particular number, and it always happen a year after the contagion, ""independently"" of when the subject dies.**

* I do have censored data, when subjects die for any other causes.

I have succesfully done the ROC curves for both measurements to estimate the death date, they are acceptable but not good. My question is, can I perform a *reverse* time-dependant ROC curve? 
I talk about trying to estimate the contagion date instead of the death date, I just need to reverse the measurements and see how good they are estimating contagion date with that subset of my sample that has those exact dates. I couldn't find any examples in the literature so far...

Also, when I say ""can I"" I mean if I should... I have already done it and results are better than when I do it with death date, but I don't know if it is an acceptable metodology... Have you ever heard of doing this? Do you know if it is right? Does it sound intuitive? I have a masters in biostatistics but my background is Biology so while it does sound right to me, I am absolutely lost in the math, and can't really know if this violates some vital part of the time dependant ROC.

Thank you very much in advance!
",1,statistics,54935,,Can I reverse the metodology for time-dependent ROC curves?,https://www.reddit.com/r/statistics/comments/89yill/can_i_reverse_the_metodology_for_timedependent/,all_ads,2018-04-05 14:17:45,58 days 11:17:49.036584000,
I can seem to find any sources online. Can you guys recommend some?,1,1522949786.0,89ybfr,False,I can seem to find any sources online. Can you guys recommend some?,0,I can seem to find any sources online. Can you guys recommend some?,0,statistics,54935,,Where can I find a step-by-step lesson/lecture on manually calculating Cronbach's alpha?,https://www.reddit.com/r/statistics/comments/89ybfr/where_can_i_find_a_stepbystep_lessonlecture_on/,all_ads,2018-04-05 13:36:26,58 days 11:59:08.036584000,
I got accepted into a master's program in Statistics that I'm going to start in August. I'm just coming out of undergrad and I understand graduate school is a big transition. What are some things I could do to make sure I'm ready over the summer? I met the minimum math requirements but I'm a bit hazy on some calculus and linear algebra. Anything in particular I should take a look at over the summer to make sure I'm well prepared?,7,1522906142.0,89tpdn,False,I got accepted into a master's program in Statistics that I'm going to start in August. I'm just coming out of undergrad and I understand graduate school is a big transition. What are some things I could do to make sure I'm ready over the summer? I met the minimum math requirements but I'm a bit hazy on some calculus and linear algebra. Anything in particular I should take a look at over the summer to make sure I'm well prepared?,0,I got accepted into a master's program in Statistics that I'm going to start in August. I'm just coming out of undergrad and I understand graduate school is a big transition. What are some things I could do to make sure I'm ready over the summer? I met the minimum math requirements but I'm a bit hazy on some calculus and linear algebra. Anything in particular I should take a look at over the summer to make sure I'm well prepared?,9,statistics,54935,,Prospective graduate student (in Statistics),https://www.reddit.com/r/statistics/comments/89tpdn/prospective_graduate_student_in_statistics/,all_ads,2018-04-05 01:29:02,59 days 00:06:32.036584000,
"I'm devising problems where students use different kinds of modeling & simulation methods to study problems of interest.  I'm not a statistician but I have an idea for a problem that would use Monte Carlo simulation to demonstrate the CLT.  

Reading responses on this post today, /r/statistics/comments/89rzev/the_central_limit_theorem/, it's clear there could be some confusion about the CLT (and I'm not completely certain myself).

I'd like advice from /r/statistics to make sure what I'm proposing actually demonstrates the CLT - or if I should run away out of concern of teaching people bad statistics.. or teaching statistics badly.

I would propose that the problem like this:

    1. pick any distribution that's not already normal (e.g. exponential, binomial, something bimodal, etc) 
    2. create a large population (10,000s) - store in an array
    3. take a small (uniform) random sample from that population and compute and store the mean
    4. repeat 3 a ""bunch"" of times.
    5. analyze your collection of means from those samples

Am I correct that the CLT says those means (from #5) should appear to be normal (even if the underlying population is not normally distributed), especially as the number of the samplings goes up?

Would this be a good problem for demonstrating the CLT while also showing the use of Monte Carlo simulation?

Edit:  One thing I'm not sure about next is ""what does this mean""?  Because the means of your sets of samples is normally distributed, you can now ____?  Because I've taken a bunch of sets of samples, which have a mean that is normally distributed, does the CLT allow me to say something about the underlying population?",14,1522911250.0,89ug0l,False,"I'm devising problems where students use different kinds of modeling & simulation methods to study problems of interest.  I'm not a statistician but I have an idea for a problem that would use Monte Carlo simulation to demonstrate the CLT.  

Reading responses on this post today, /r/statistics/comments/89rzev/the_central_limit_theorem/, it's clear there could be some confusion about the CLT (and I'm not completely certain myself).

I'd like advice from /r/statistics to make sure what I'm proposing actually demonstrates the CLT - or if I should run away out of concern of teaching people bad statistics.. or teaching statistics badly.

I would propose that the problem like this:

    1. pick any distribution that's not already normal (e.g. exponential, binomial, something bimodal, etc) 
    2. create a large population (10,000s) - store in an array
    3. take a small (uniform) random sample from that population and compute and store the mean
    4. repeat 3 a ""bunch"" of times.
    5. analyze your collection of means from those samples

Am I correct that the CLT says those means (from #5) should appear to be normal (even if the underlying population is not normally distributed), especially as the number of the samplings goes up?

Would this be a good problem for demonstrating the CLT while also showing the use of Monte Carlo simulation?

Edit:  One thing I'm not sure about next is ""what does this mean""?  Because the means of your sets of samples is normally distributed, you can now ____?  Because I've taken a bunch of sets of samples, which have a mean that is normally distributed, does the CLT allow me to say something about the underlying population?",0,"I'm devising problems where students use different kinds of modeling & simulation methods to study problems of interest.  I'm not a statistician but I have an idea for a problem that would use Monte Carlo simulation to demonstrate the CLT.  

Reading responses on this post today, /r/statistics/comments/89rzev/the_central_limit_theorem/, it's clear there could be some confusion about the CLT (and I'm not completely certain myself).

I'd like advice from /r/statistics to make sure what I'm proposing actually demonstrates the CLT - or if I should run away out of concern of teaching people bad statistics.. or teaching statistics badly.

I would propose that the problem like this:

    1. pick any distribution that's not already normal (e.g. exponential, binomial, something bimodal, etc) 
    2. create a large population (10,000s) - store in an array
    3. take a small (uniform) random sample from that population and compute and store the mean
    4. repeat 3 a ""bunch"" of times.
    5. analyze your collection of means from those samples

Am I correct that the CLT says those means (from #5) should appear to be normal (even if the underlying population is not normally distributed), especially as the number of the samplings goes up?

Would this be a good problem for demonstrating the CLT while also showing the use of Monte Carlo simulation?

Edit:  One thing I'm not sure about next is ""what does this mean""?  Because the means of your sets of samples is normally distributed, you can now ____?  Because I've taken a bunch of sets of samples, which have a mean that is normally distributed, does the CLT allow me to say something about the underlying population?",4,statistics,54935,,Using Monte Carlo simulation to demonstrate the Central Limit Theorem?,https://www.reddit.com/r/statistics/comments/89ug0l/using_monte_carlo_simulation_to_demonstrate_the/,all_ads,2018-04-05 02:54:10,58 days 22:41:24.036584000,
"I've asked quite a few questions today and I apologize, but I see this site as a good resource to get some guidance when I can't ask my professor.

The question reads:

**If you draw an M&M candy at random you will have one of six colors. The probability depends on the proportion of each color among all candies made. Assume these are the probabilities:


Color|Brown|Red|Yellow|Green|Orange|Blue
Prob|0.3|0.3|?|0.1|0.1|0.1

If you select two m&ms and the colors are independent, then what is the probability both are the same color?**

My thinking for this problem is that because they are independent I need to multiply, but because I select two colors that are the same I need to add my products from the multiplication. Am I on the right track?

",16,1522917236.0,89v78r,False,"I've asked quite a few questions today and I apologize, but I see this site as a good resource to get some guidance when I can't ask my professor.

The question reads:

**If you draw an M&M candy at random you will have one of six colors. The probability depends on the proportion of each color among all candies made. Assume these are the probabilities:


Color|Brown|Red|Yellow|Green|Orange|Blue
Prob|0.3|0.3|?|0.1|0.1|0.1

If you select two m&ms and the colors are independent, then what is the probability both are the same color?**

My thinking for this problem is that because they are independent I need to multiply, but because I select two colors that are the same I need to add my products from the multiplication. Am I on the right track?

",0,"I've asked quite a few questions today and I apologize, but I see this site as a good resource to get some guidance when I can't ask my professor.

The question reads:

**If you draw an M&M candy at random you will have one of six colors. The probability depends on the proportion of each color among all candies made. Assume these are the probabilities:


Color|Brown|Red|Yellow|Green|Orange|Blue
Prob|0.3|0.3|?|0.1|0.1|0.1

If you select two m&ms and the colors are independent, then what is the probability both are the same color?**

My thinking for this problem is that because they are independent I need to multiply, but because I select two colors that are the same I need to add my products from the multiplication. Am I on the right track?

",3,statistics,54935,,A probability question regarding M&Ms,https://www.reddit.com/r/statistics/comments/89v78r/a_probability_question_regarding_mms/,all_ads,2018-04-05 04:33:56,58 days 21:01:38.036584000,
"I graduated with a CS degree a few years ago so I forgot almost everything from intro to statistics. I have an idea for a cool side-project that involves taking a few data points about a basketball player and see if there's a correlation between them. The data points are:

1.) The basketball player's game efficiency score for all 82 games this season (some opponents are better, some opponents are worse and some games are played in more important parts of the season than others. this game efficiency score isn't really adjusted for anything and I feel like there really isn't an optimal way to just project a game score given so many variables)

2.) For all 82 games, the closing line (e.g. team A is favored by 6 points over team B) at Pinnacle, the most reputable sports book and the actual outcome of the game (e.g. team A performs below expectations and only wins by 1 point). I feel that this is the best way I can A/B the difference even though a single player doesn't really have 100% impact on a game

3.) My non-basketball statistic I'm looking for a correlation with. Could be anything from watching all their games and measuring the number of televised high fives they give to what jersey color they're wearing that day.



What topics do you guys suggested I look into to determine whether there is a correlation with my different data points? Also, does anyone have any suggestions about how I can improve my data points given my questions about #1+2 above? Thanks a bunch!",1,1522938012.0,89xele,False,"I graduated with a CS degree a few years ago so I forgot almost everything from intro to statistics. I have an idea for a cool side-project that involves taking a few data points about a basketball player and see if there's a correlation between them. The data points are:

1.) The basketball player's game efficiency score for all 82 games this season (some opponents are better, some opponents are worse and some games are played in more important parts of the season than others. this game efficiency score isn't really adjusted for anything and I feel like there really isn't an optimal way to just project a game score given so many variables)

2.) For all 82 games, the closing line (e.g. team A is favored by 6 points over team B) at Pinnacle, the most reputable sports book and the actual outcome of the game (e.g. team A performs below expectations and only wins by 1 point). I feel that this is the best way I can A/B the difference even though a single player doesn't really have 100% impact on a game

3.) My non-basketball statistic I'm looking for a correlation with. Could be anything from watching all their games and measuring the number of televised high fives they give to what jersey color they're wearing that day.



What topics do you guys suggested I look into to determine whether there is a correlation with my different data points? Also, does anyone have any suggestions about how I can improve my data points given my questions about #1+2 above? Thanks a bunch!",0,"I graduated with a CS degree a few years ago so I forgot almost everything from intro to statistics. I have an idea for a cool side-project that involves taking a few data points about a basketball player and see if there's a correlation between them. The data points are:

1.) The basketball player's game efficiency score for all 82 games this season (some opponents are better, some opponents are worse and some games are played in more important parts of the season than others. this game efficiency score isn't really adjusted for anything and I feel like there really isn't an optimal way to just project a game score given so many variables)

2.) For all 82 games, the closing line (e.g. team A is favored by 6 points over team B) at Pinnacle, the most reputable sports book and the actual outcome of the game (e.g. team A performs below expectations and only wins by 1 point). I feel that this is the best way I can A/B the difference even though a single player doesn't really have 100% impact on a game

3.) My non-basketball statistic I'm looking for a correlation with. Could be anything from watching all their games and measuring the number of televised high fives they give to what jersey color they're wearing that day.



What topics do you guys suggested I look into to determine whether there is a correlation with my different data points? Also, does anyone have any suggestions about how I can improve my data points given my questions about #1+2 above? Thanks a bunch!",1,statistics,54935,,What statistics topics should I be looking into to determine if there's a correlation between a few data points?,https://www.reddit.com/r/statistics/comments/89xele/what_statistics_topics_should_i_be_looking_into/,all_ads,2018-04-05 10:20:12,58 days 15:15:22.036584000,
"So long story short we have to memorize this formula for my chem class using factorials to find arrangements, but I cannot get it through my system cz I am the kind of person who wants to know the source of the equation/ logical methodology used to reach the equation. I am already researching these concepts but I would also love if you guys provide me with some sources of your own that tackle these topics well. Thank you in advance.",4,1522934851.0,89x4os,False,"So long story short we have to memorize this formula for my chem class using factorials to find arrangements, but I cannot get it through my system cz I am the kind of person who wants to know the source of the equation/ logical methodology used to reach the equation. I am already researching these concepts but I would also love if you guys provide me with some sources of your own that tackle these topics well. Thank you in advance.",0,"So long story short we have to memorize this formula for my chem class using factorials to find arrangements, but I cannot get it through my system cz I am the kind of person who wants to know the source of the equation/ logical methodology used to reach the equation. I am already researching these concepts but I would also love if you guys provide me with some sources of your own that tackle these topics well. Thank you in advance.",1,statistics,54935,,"Combinations, permutations, factorial help!",https://www.reddit.com/r/statistics/comments/89x4os/combinations_permutations_factorial_help/,all_ads,2018-04-05 09:27:31,58 days 16:08:03.036584000,
Is it by main effects or the standardized model? Unstandardized. I'm weighing one or the other but can't seem to pin point which it is,1,1522920932.0,89vmrr,False,Is it by main effects or the standardized model? Unstandardized. I'm weighing one or the other but can't seem to pin point which it is,0,Is it by main effects or the standardized model? Unstandardized. I'm weighing one or the other but can't seem to pin point which it is,2,statistics,54935,,How can slope differences between groups be tested in regression?,https://www.reddit.com/r/statistics/comments/89vmrr/how_can_slope_differences_between_groups_be/,all_ads,2018-04-05 05:35:32,58 days 20:00:02.036584000,
"I could say that, Ideally, giving hard questions more weight can be justified because 1) if they get the hard question correct, then that's the knowledge equivalent of a bunch of small easy questions or 2) as a way to make the effort ""worth their while"".    
But what about easy questions?  If a learner is missing the low-hanging fruit, shouldn't that should be stronger evidence that they don't understand the material and are just writing stuff down?",19,1522934043.0,89x1z0,False,"I could say that, Ideally, giving hard questions more weight can be justified because 1) if they get the hard question correct, then that's the knowledge equivalent of a bunch of small easy questions or 2) as a way to make the effort ""worth their while"".    
But what about easy questions?  If a learner is missing the low-hanging fruit, shouldn't that should be stronger evidence that they don't understand the material and are just writing stuff down?",0,"I could say that, Ideally, giving hard questions more weight can be justified because 1) if they get the hard question correct, then that's the knowledge equivalent of a bunch of small easy questions or 2) as a way to make the effort ""worth their while"".    
But what about easy questions?  If a learner is missing the low-hanging fruit, shouldn't that should be stronger evidence that they don't understand the material and are just writing stuff down?",0,statistics,54935,,Is it really better to give hard questions more weight?,https://www.reddit.com/r/statistics/comments/89x1z0/is_it_really_better_to_give_hard_questions_more/,all_ads,2018-04-05 09:14:03,58 days 16:21:31.036584000,
"I am looking at how pay raises affect employee tenure.  I have 3 variables I would like to consider:

*Received_Raise (""YES""/""NO"")
*Raise_Amount (numeric)
*Raise_% (numeric)

So far, Received_Raise is most important, but I can't imagine that the amount of the raise is unimportant.  But, a linear model with just Received_Raise (by itself) performs 50% better than a linear model with just Raise_Amount or Raise_% (by themselves respectively).

So, my question is whether I can include the interaction of Received_Raise*Raise_%?

My hesitations are:

*If Received_Raise is ""NO"", the Raise_Amount/% will also be 0
*If Raise_Amount = 0 where Received_Raise = ""NO"", then why doesn't Raise_Amount implicitly hold more information and perform better than Received_Raise?

The final thing I should mention is that Received_Raise performs much better when predicting actual tenure in months using linear regression.  However, Raise_% is not much worse than Received_Raise when simply predicting whether an employee stays at least 5 months using logistic regression.

My conclusion is that:

*I should only use Raise_% as it normalizes the amount by the employee's wage and implicitly holds all the information of Received_Raise
*There is some other covariate/moderator/mediator that impacts tenures above 6 months

Thoughts?",4,1522896994.0,89sdvj,False,"I am looking at how pay raises affect employee tenure.  I have 3 variables I would like to consider:

*Received_Raise (""YES""/""NO"")
*Raise_Amount (numeric)
*Raise_% (numeric)

So far, Received_Raise is most important, but I can't imagine that the amount of the raise is unimportant.  But, a linear model with just Received_Raise (by itself) performs 50% better than a linear model with just Raise_Amount or Raise_% (by themselves respectively).

So, my question is whether I can include the interaction of Received_Raise*Raise_%?

My hesitations are:

*If Received_Raise is ""NO"", the Raise_Amount/% will also be 0
*If Raise_Amount = 0 where Received_Raise = ""NO"", then why doesn't Raise_Amount implicitly hold more information and perform better than Received_Raise?

The final thing I should mention is that Received_Raise performs much better when predicting actual tenure in months using linear regression.  However, Raise_% is not much worse than Received_Raise when simply predicting whether an employee stays at least 5 months using logistic regression.

My conclusion is that:

*I should only use Raise_% as it normalizes the amount by the employee's wage and implicitly holds all the information of Received_Raise
*There is some other covariate/moderator/mediator that impacts tenures above 6 months

Thoughts?",0,"I am looking at how pay raises affect employee tenure.  I have 3 variables I would like to consider:

*Received_Raise (""YES""/""NO"")
*Raise_Amount (numeric)
*Raise_% (numeric)

So far, Received_Raise is most important, but I can't imagine that the amount of the raise is unimportant.  But, a linear model with just Received_Raise (by itself) performs 50% better than a linear model with just Raise_Amount or Raise_% (by themselves respectively).

So, my question is whether I can include the interaction of Received_Raise*Raise_%?

My hesitations are:

*If Received_Raise is ""NO"", the Raise_Amount/% will also be 0
*If Raise_Amount = 0 where Received_Raise = ""NO"", then why doesn't Raise_Amount implicitly hold more information and perform better than Received_Raise?

The final thing I should mention is that Received_Raise performs much better when predicting actual tenure in months using linear regression.  However, Raise_% is not much worse than Received_Raise when simply predicting whether an employee stays at least 5 months using logistic regression.

My conclusion is that:

*I should only use Raise_% as it normalizes the amount by the employee's wage and implicitly holds all the information of Received_Raise
*There is some other covariate/moderator/mediator that impacts tenures above 6 months

Thoughts?",3,statistics,54935,,Can I model this as an interaction or will there be collinearity?,https://www.reddit.com/r/statistics/comments/89sdvj/can_i_model_this_as_an_interaction_or_will_there/,all_ads,2018-04-04 22:56:34,59 days 02:39:00.036584000,
"High school girls average 80 text messages daily. Assume the population standard deviation is 15 text messages. Suppose a random sample of 100 high school girls is taken.

What is the probability that the sample mean is more than 90?
What is the probability that the sample mean is less than 85?
What is the probability the sample mean is in between 85 and 90

just lost with this whole problem",1,1522916977.0,89v61h,False,"High school girls average 80 text messages daily. Assume the population standard deviation is 15 text messages. Suppose a random sample of 100 high school girls is taken.

What is the probability that the sample mean is more than 90?
What is the probability that the sample mean is less than 85?
What is the probability the sample mean is in between 85 and 90

just lost with this whole problem",0,"High school girls average 80 text messages daily. Assume the population standard deviation is 15 text messages. Suppose a random sample of 100 high school girls is taken.

What is the probability that the sample mean is more than 90?
What is the probability that the sample mean is less than 85?
What is the probability the sample mean is in between 85 and 90

just lost with this whole problem",0,statistics,54935,,Need help with a problem regarding sample mean,https://www.reddit.com/r/statistics/comments/89v61h/need_help_with_a_problem_regarding_sample_mean/,all_ads,2018-04-05 04:29:37,58 days 21:05:57.036584000,
"I am going to run an analysis using a default prior outlined by [Rouder and Morey \(2012](http://pcl.missouri.edu/sites/default/files/Rouder.Morey_.MBR_.2012.pdf); i.e., cauchy distribution centred at 0 and scale of 1; with the eventual goal of running a sensitivity analysis that covers various scale parameters of the prior); however, I am unclear how exactly to prepare the data.

As a quick overview, I have a continuous DV and a dichotomous predictor.

The section on reparameterization appears the bottom of page 883.

""The first property, that the Bayes factor should be invariant to the units of measurement, is met by reparameterizing the model in terms of a standardized effect measure. Model M1 may be rewritten as [Formula 4](https://imgur.com/a/vG3lo) where Sx is the (population) standard deviation of x and BETA is the standardized effect given by [Formula 5](https://imgur.com/a/cmkIG). It is straightforward to show that Equations (2) and (4) are reparameterizations of the same model. The parameter BETA describes how much a change in standard deviation units of x affects a change in standard deviation units of y... This standardization should not be confused with the more conventional standardization where data and covariates are transformed so that the slope is constrained to be between 1 and 1 (Kutner, Nachtsheim, Neter, & Li, 2004)."" (pp. 883-884, Rouder & Morey, 2012)

I'm left a bit unclear as exactly how to how exactly to prepare my data for this prior. Am I right to interpret this as me having to center my predictor and then divide it by its standard deviation? I'm also unclear if this changes when the predictor is a dichotomous, rather than continuous, variable?

Edit: centred at not mean of

Edit 2: Okay, I'm pretty sure that what I'm supposed to do is standardize the predictor and outcome. However, I'm not sure now what to do in the case of a categorical predictor?",2,1522916490.0,89v3vz,False,"I am going to run an analysis using a default prior outlined by [Rouder and Morey \(2012](http://pcl.missouri.edu/sites/default/files/Rouder.Morey_.MBR_.2012.pdf); i.e., cauchy distribution centred at 0 and scale of 1; with the eventual goal of running a sensitivity analysis that covers various scale parameters of the prior); however, I am unclear how exactly to prepare the data.

As a quick overview, I have a continuous DV and a dichotomous predictor.

The section on reparameterization appears the bottom of page 883.

""The first property, that the Bayes factor should be invariant to the units of measurement, is met by reparameterizing the model in terms of a standardized effect measure. Model M1 may be rewritten as [Formula 4](https://imgur.com/a/vG3lo) where Sx is the (population) standard deviation of x and BETA is the standardized effect given by [Formula 5](https://imgur.com/a/cmkIG). It is straightforward to show that Equations (2) and (4) are reparameterizations of the same model. The parameter BETA describes how much a change in standard deviation units of x affects a change in standard deviation units of y... This standardization should not be confused with the more conventional standardization where data and covariates are transformed so that the slope is constrained to be between 1 and 1 (Kutner, Nachtsheim, Neter, & Li, 2004)."" (pp. 883-884, Rouder & Morey, 2012)

I'm left a bit unclear as exactly how to how exactly to prepare my data for this prior. Am I right to interpret this as me having to center my predictor and then divide it by its standard deviation? I'm also unclear if this changes when the predictor is a dichotomous, rather than continuous, variable?

Edit: centred at not mean of

Edit 2: Okay, I'm pretty sure that what I'm supposed to do is standardize the predictor and outcome. However, I'm not sure now what to do in the case of a categorical predictor?",0,"I am going to run an analysis using a default prior outlined by [Rouder and Morey \(2012](http://pcl.missouri.edu/sites/default/files/Rouder.Morey_.MBR_.2012.pdf); i.e., cauchy distribution centred at 0 and scale of 1; with the eventual goal of running a sensitivity analysis that covers various scale parameters of the prior); however, I am unclear how exactly to prepare the data.

As a quick overview, I have a continuous DV and a dichotomous predictor.

The section on reparameterization appears the bottom of page 883.

""The first property, that the Bayes factor should be invariant to the units of measurement, is met by reparameterizing the model in terms of a standardized effect measure. Model M1 may be rewritten as [Formula 4](https://imgur.com/a/vG3lo) where Sx is the (population) standard deviation of x and BETA is the standardized effect given by [Formula 5](https://imgur.com/a/cmkIG). It is straightforward to show that Equations (2) and (4) are reparameterizations of the same model. The parameter BETA describes how much a change in standard deviation units of x affects a change in standard deviation units of y... This standardization should not be confused with the more conventional standardization where data and covariates are transformed so that the slope is constrained to be between 1 and 1 (Kutner, Nachtsheim, Neter, & Li, 2004)."" (pp. 883-884, Rouder & Morey, 2012)

I'm left a bit unclear as exactly how to how exactly to prepare my data for this prior. Am I right to interpret this as me having to center my predictor and then divide it by its standard deviation? I'm also unclear if this changes when the predictor is a dichotomous, rather than continuous, variable?

Edit: centred at not mean of

Edit 2: Okay, I'm pretty sure that what I'm supposed to do is standardize the predictor and outcome. However, I'm not sure now what to do in the case of a categorical predictor?",1,statistics,54935,,Having trouble interpreting a passage on reparameterization.,https://www.reddit.com/r/statistics/comments/89v3vz/having_trouble_interpreting_a_passage_on/,all_ads,2018-04-05 04:21:30,58 days 21:14:04.036584000,
"Did a grade 11 biology lab, requires stat test, don't know what each do. The lab was testing if there was a significant difference between population growth of Lemna (type of aquatic plant) and 5 different colors of light. I'm deciding between a chi square and paired t test.",4,1522916319.0,89v36b,False,"Did a grade 11 biology lab, requires stat test, don't know what each do. The lab was testing if there was a significant difference between population growth of Lemna (type of aquatic plant) and 5 different colors of light. I'm deciding between a chi square and paired t test.",0,"Did a grade 11 biology lab, requires stat test, don't know what each do. The lab was testing if there was a significant difference between population growth of Lemna (type of aquatic plant) and 5 different colors of light. I'm deciding between a chi square and paired t test.",1,statistics,54935,,What type of stat test should I use?,https://www.reddit.com/r/statistics/comments/89v36b/what_type_of_stat_test_should_i_use/,all_ads,2018-04-05 04:18:39,58 days 21:16:55.036584000,
"I was really confused on why the interaction effect changes after centering the variables. The coefficient stays the same, but before centering the variables, let's say that all the values of the IV was positive. However after centering, some of them become negative. So before centering, all values would have a positive interaction effect while after centering, some values would have a positive interaction effect while some would have a negative. Can anyone please tell me why and if I did something wrong?",0,1522916190.0,89v2ka,False,"I was really confused on why the interaction effect changes after centering the variables. The coefficient stays the same, but before centering the variables, let's say that all the values of the IV was positive. However after centering, some of them become negative. So before centering, all values would have a positive interaction effect while after centering, some values would have a positive interaction effect while some would have a negative. Can anyone please tell me why and if I did something wrong?",0,"I was really confused on why the interaction effect changes after centering the variables. The coefficient stays the same, but before centering the variables, let's say that all the values of the IV was positive. However after centering, some of them become negative. So before centering, all values would have a positive interaction effect while after centering, some values would have a positive interaction effect while some would have a negative. Can anyone please tell me why and if I did something wrong?",0,statistics,54935,,Need help with mean-centering with interaction term,https://www.reddit.com/r/statistics/comments/89v2ka/need_help_with_meancentering_with_interaction_term/,all_ads,2018-04-05 04:16:30,58 days 21:19:04.036584000,
"Cohen's f^2 = R^2 / (1 - R^2 ).  
Can I calculate f^2 as R^2Change / (1 - R^2Change ) to indicate whether my model was significant once the moderator was entered? (Moderated multiple regression analyses)  
Edit: I wonder if the way to calculate this would be R^2 Change / (1 - R^2 ) to determine the effect size of the moderator. Maybe?",0,1522914595.0,89uve6,False,"Cohen's f^2 = R^2 / (1 - R^2 ).  
Can I calculate f^2 as R^2Change / (1 - R^2Change ) to indicate whether my model was significant once the moderator was entered? (Moderated multiple regression analyses)  
Edit: I wonder if the way to calculate this would be R^2 Change / (1 - R^2 ) to determine the effect size of the moderator. Maybe?",0,"Cohen's f^2 = R^2 / (1 - R^2 ).  
Can I calculate f^2 as R^2Change / (1 - R^2Change ) to indicate whether my model was significant once the moderator was entered? (Moderated multiple regression analyses)  
Edit: I wonder if the way to calculate this would be R^2 Change / (1 - R^2 ) to determine the effect size of the moderator. Maybe?",1,statistics,54935,,Can R^2 change be substituted for R^2 in Cohen's f^2 formula?,https://www.reddit.com/r/statistics/comments/89uve6/can_r2_change_be_substituted_for_r2_in_cohens_f2/,all_ads,2018-04-05 03:49:55,58 days 21:45:39.036584000,
https://medium.com/@sadatnazrul/data-science-interview-guide-4ee9f5dc778,20,1522829997.0,89kdl9,False,https://medium.com/@sadatnazrul/data-science-interview-guide-4ee9f5dc778,0,https://medium.com/@sadatnazrul/data-science-interview-guide-4ee9f5dc778,76,statistics,54935,,Data Science Interview Guide,https://www.reddit.com/r/statistics/comments/89kdl9/data_science_interview_guide/,all_ads,2018-04-04 04:19:57,59 days 21:15:37.036584000,
"These are my questions:
When you mean-center a model with an interaction term, how come the coefficients of the IV's change, but not the interaction term?
Also when interpreting the model, let's say the mean of an IV is 5. So when you plug in numbers into the model, let's say the value of the IV was 4, would you plug in -1 or 4?",2,1522909493.0,89u7pl,False,"These are my questions:
When you mean-center a model with an interaction term, how come the coefficients of the IV's change, but not the interaction term?
Also when interpreting the model, let's say the mean of an IV is 5. So when you plug in numbers into the model, let's say the value of the IV was 4, would you plug in -1 or 4?",0,"These are my questions:
When you mean-center a model with an interaction term, how come the coefficients of the IV's change, but not the interaction term?
Also when interpreting the model, let's say the mean of an IV is 5. So when you plug in numbers into the model, let's say the value of the IV was 4, would you plug in -1 or 4?",1,statistics,54935,,A few questions about mean-centering,https://www.reddit.com/r/statistics/comments/89u7pl/a_few_questions_about_meancentering/,all_ads,2018-04-05 02:24:53,58 days 23:10:41.036584000,
"I ran a pilot survey where I asked several multiple choice questions where participants were permitted to select multiple options.  The data set also includes some ordinal variables (Likert scales) and demographic data (age, gender, orientation, etc.).

If I want to use one of the multiple select variables as a dependent variable, what methods are available to me?

**More details**

When I designed the survey, my plan was to use chi^2 cross tables on most variables (df ranging from 2-16 in each direction), since these are non-parametric and can handle categorical dependent variables.  I am using Cramer's V to assess effect size and adjusted residual tables to interpret which relationships are meaningful.  I have not yet used the chi^2 contribution tables, but I plan to use these in my next round of analysis.

For multiple select variables, I planned to use descriptive statistics only.  However, if I can do something else with it, I would like to.

I imagine there is some generalized case of cross tables that can make use of multiple-select data, but I don't know.  One person who saw the preliminary analysis suggested using Latent Profile Analysis for some of my un-analyzed research questions.  I have yet to study up on that, but I imagine that approach will still run into similar issues.",0,1522890779.0,89rf5q,False,"I ran a pilot survey where I asked several multiple choice questions where participants were permitted to select multiple options.  The data set also includes some ordinal variables (Likert scales) and demographic data (age, gender, orientation, etc.).

If I want to use one of the multiple select variables as a dependent variable, what methods are available to me?

**More details**

When I designed the survey, my plan was to use chi^2 cross tables on most variables (df ranging from 2-16 in each direction), since these are non-parametric and can handle categorical dependent variables.  I am using Cramer's V to assess effect size and adjusted residual tables to interpret which relationships are meaningful.  I have not yet used the chi^2 contribution tables, but I plan to use these in my next round of analysis.

For multiple select variables, I planned to use descriptive statistics only.  However, if I can do something else with it, I would like to.

I imagine there is some generalized case of cross tables that can make use of multiple-select data, but I don't know.  One person who saw the preliminary analysis suggested using Latent Profile Analysis for some of my un-analyzed research questions.  I have yet to study up on that, but I imagine that approach will still run into similar issues.",0,"I ran a pilot survey where I asked several multiple choice questions where participants were permitted to select multiple options.  The data set also includes some ordinal variables (Likert scales) and demographic data (age, gender, orientation, etc.).

If I want to use one of the multiple select variables as a dependent variable, what methods are available to me?

**More details**

When I designed the survey, my plan was to use chi^2 cross tables on most variables (df ranging from 2-16 in each direction), since these are non-parametric and can handle categorical dependent variables.  I am using Cramer's V to assess effect size and adjusted residual tables to interpret which relationships are meaningful.  I have not yet used the chi^2 contribution tables, but I plan to use these in my next round of analysis.

For multiple select variables, I planned to use descriptive statistics only.  However, if I can do something else with it, I would like to.

I imagine there is some generalized case of cross tables that can make use of multiple-select data, but I don't know.  One person who saw the preliminary analysis suggested using Latent Profile Analysis for some of my un-analyzed research questions.  I have yet to study up on that, but I imagine that approach will still run into similar issues.",2,statistics,54935,,Advice Needed: Methods for analyzing a multiple-select categorical dependent variable?,https://www.reddit.com/r/statistics/comments/89rf5q/advice_needed_methods_for_analyzing_a/,all_ads,2018-04-04 21:12:59,59 days 04:22:35.036584000,
"Hello,

I'm working with statistical data sets that define individuals that have made purchases based on their audience behavior, which defines the likelihood they are one race or another. 

For example:



PRODUCT | White | Black | Hispanic | Asian | Other
---|---|----|----|----|----
Product | 88.29 | 6.41 | 4.1 | 3.99 | 4.91
Product | 52.05 | 31.1 | 25.56 | 7.75 | 14.73

We also get this style of reporting for income:



PRODUCT | <20K | 20K-40K | 40K-50K | 50L-75K | 75K-100K | 100K+
---|---|----|----|----|----|----
Product | 0 | 0 | 0 | 0 | 100 | 0
Product | 8.77 | 34.52 | 20.91 | 34.78 | 1.01 | 0


My understanding is this kind of breakdown is much more accurate than just taking the highest result and saying ""they're white"", but I'm having trouble wrapping my head around how to summarize the demographics. 

Does anyone A) have experience with this kind of ethnicity presentation and have any advice on how you tackle it, and B) does  anyone know what that kind of statistical modeling is called so I can be more efficient researching the subject if I have more questions that aren't addressed in this thread?

This style of breakouts is throwing me off my game, so any advice is greatly appreciated!

",0,1522902657.0,89t6wt,False,"Hello,

I'm working with statistical data sets that define individuals that have made purchases based on their audience behavior, which defines the likelihood they are one race or another. 

For example:



PRODUCT | White | Black | Hispanic | Asian | Other
---|---|----|----|----|----
Product | 88.29 | 6.41 | 4.1 | 3.99 | 4.91
Product | 52.05 | 31.1 | 25.56 | 7.75 | 14.73

We also get this style of reporting for income:



PRODUCT | <20K | 20K-40K | 40K-50K | 50L-75K | 75K-100K | 100K+
---|---|----|----|----|----|----
Product | 0 | 0 | 0 | 0 | 100 | 0
Product | 8.77 | 34.52 | 20.91 | 34.78 | 1.01 | 0


My understanding is this kind of breakdown is much more accurate than just taking the highest result and saying ""they're white"", but I'm having trouble wrapping my head around how to summarize the demographics. 

Does anyone A) have experience with this kind of ethnicity presentation and have any advice on how you tackle it, and B) does  anyone know what that kind of statistical modeling is called so I can be more efficient researching the subject if I have more questions that aren't addressed in this thread?

This style of breakouts is throwing me off my game, so any advice is greatly appreciated!

",0,"Hello,

I'm working with statistical data sets that define individuals that have made purchases based on their audience behavior, which defines the likelihood they are one race or another. 

For example:



PRODUCT | White | Black | Hispanic | Asian | Other
---|---|----|----|----|----
Product | 88.29 | 6.41 | 4.1 | 3.99 | 4.91
Product | 52.05 | 31.1 | 25.56 | 7.75 | 14.73

We also get this style of reporting for income:



PRODUCT | <20K | 20K-40K | 40K-50K | 50L-75K | 75K-100K | 100K+
---|---|----|----|----|----|----
Product | 0 | 0 | 0 | 0 | 100 | 0
Product | 8.77 | 34.52 | 20.91 | 34.78 | 1.01 | 0


My understanding is this kind of breakdown is much more accurate than just taking the highest result and saying ""they're white"", but I'm having trouble wrapping my head around how to summarize the demographics. 

Does anyone A) have experience with this kind of ethnicity presentation and have any advice on how you tackle it, and B) does  anyone know what that kind of statistical modeling is called so I can be more efficient researching the subject if I have more questions that aren't addressed in this thread?

This style of breakouts is throwing me off my game, so any advice is greatly appreciated!

",0,statistics,54935,,Ethnicity and Audiences - What's the best summary approach?,https://www.reddit.com/r/statistics/comments/89t6wt/ethnicity_and_audiences_whats_the_best_summary/,all_ads,2018-04-05 00:30:57,59 days 01:04:37.036584000,
"I'm trying to optimize something; I believe to do this, I should solve an equation that results in the mean of the normal distribution function being equal to a lower tolerance limit.  Can this be solved?

Integrate from a to infinity (or a to 5sigma should be close enough) the function x*N(mu, sigma) = T, where N is the normal distribution function and T is the lower tolerance limit.  


I've worked this out a few times and I end up with something that looks like a numerical analysis problem (find the approximate result with an algorithm) but when I do this, I get a result that I am not confident works out.  Am I overlooking something obvious about why this cannot be done?

Thanks,
KK",7,1522893077.0,89rs2v,False,"I'm trying to optimize something; I believe to do this, I should solve an equation that results in the mean of the normal distribution function being equal to a lower tolerance limit.  Can this be solved?

Integrate from a to infinity (or a to 5sigma should be close enough) the function x*N(mu, sigma) = T, where N is the normal distribution function and T is the lower tolerance limit.  


I've worked this out a few times and I end up with something that looks like a numerical analysis problem (find the approximate result with an algorithm) but when I do this, I get a result that I am not confident works out.  Am I overlooking something obvious about why this cannot be done?

Thanks,
KK",0,"I'm trying to optimize something; I believe to do this, I should solve an equation that results in the mean of the normal distribution function being equal to a lower tolerance limit.  Can this be solved?

Integrate from a to infinity (or a to 5sigma should be close enough) the function x*N(mu, sigma) = T, where N is the normal distribution function and T is the lower tolerance limit.  


I've worked this out a few times and I end up with something that looks like a numerical analysis problem (find the approximate result with an algorithm) but when I do this, I get a result that I am not confident works out.  Am I overlooking something obvious about why this cannot be done?

Thanks,
KK",0,statistics,54935,,Mean of a Section of Normal Distribution Function,https://www.reddit.com/r/statistics/comments/89rs2v/mean_of_a_section_of_normal_distribution_function/,all_ads,2018-04-04 21:51:17,59 days 03:44:17.036584000,
"Problem:

""A plant breeder wishes to study the effects of soil drainage and variety of tulip bulbs on flower production. Twelve 3 m by 10 m experimental sites are available in a garden. Each site is a trench 0.5 m deep. Soil drainage is changed by adding varying amounts of sand to a clay soil (more sand improves drainage), mixing the two well, and placing the mixture in the trench. The bulbs are then planted in the soils, and flower production is measured the following spring. It is felt that four different levels of soil drainage would suffice, and there are fifteen tulip varieties that need to be studied.

(15 pts) Describe the experimental design you would use for this study and provide a skeleton ANOVA table (Sources of variation and degrees of freedom).""


I'm just trying to figure out the correct test to apply to this problem so I can create the skeleton ANOVA table. I initially thought it was a completely randomized two-way ANOVA but my professor shot that down. Is it a split-plot ANOVA?
",1,1522887614.0,89qxlf,False,"Problem:

""A plant breeder wishes to study the effects of soil drainage and variety of tulip bulbs on flower production. Twelve 3 m by 10 m experimental sites are available in a garden. Each site is a trench 0.5 m deep. Soil drainage is changed by adding varying amounts of sand to a clay soil (more sand improves drainage), mixing the two well, and placing the mixture in the trench. The bulbs are then planted in the soils, and flower production is measured the following spring. It is felt that four different levels of soil drainage would suffice, and there are fifteen tulip varieties that need to be studied.

(15 pts) Describe the experimental design you would use for this study and provide a skeleton ANOVA table (Sources of variation and degrees of freedom).""


I'm just trying to figure out the correct test to apply to this problem so I can create the skeleton ANOVA table. I initially thought it was a completely randomized two-way ANOVA but my professor shot that down. Is it a split-plot ANOVA?
",0,"Problem:

""A plant breeder wishes to study the effects of soil drainage and variety of tulip bulbs on flower production. Twelve 3 m by 10 m experimental sites are available in a garden. Each site is a trench 0.5 m deep. Soil drainage is changed by adding varying amounts of sand to a clay soil (more sand improves drainage), mixing the two well, and placing the mixture in the trench. The bulbs are then planted in the soils, and flower production is measured the following spring. It is felt that four different levels of soil drainage would suffice, and there are fifteen tulip varieties that need to be studied.

(15 pts) Describe the experimental design you would use for this study and provide a skeleton ANOVA table (Sources of variation and degrees of freedom).""


I'm just trying to figure out the correct test to apply to this problem so I can create the skeleton ANOVA table. I initially thought it was a completely randomized two-way ANOVA but my professor shot that down. Is it a split-plot ANOVA?
",1,statistics,54935,,Determining the proper test to apply to this problem,https://www.reddit.com/r/statistics/comments/89qxlf/determining_the_proper_test_to_apply_to_this/,all_ads,2018-04-04 20:20:14,59 days 05:15:20.036584000,
"I am a statistics major and going to have an internship over the summer where I am going to be a part of forecasting team. Sadly, I'm going to have the graduate course on forecasting next semester, after the internship. What are some good materials (books, programming tutorials, videos) that will introduce me to forecasting?",3,1522828024.0,89k2lx,False,"I am a statistics major and going to have an internship over the summer where I am going to be a part of forecasting team. Sadly, I'm going to have the graduate course on forecasting next semester, after the internship. What are some good materials (books, programming tutorials, videos) that will introduce me to forecasting?",0,"I am a statistics major and going to have an internship over the summer where I am going to be a part of forecasting team. Sadly, I'm going to have the graduate course on forecasting next semester, after the internship. What are some good materials (books, programming tutorials, videos) that will introduce me to forecasting?",18,statistics,54935,,Forecasting and time-series,https://www.reddit.com/r/statistics/comments/89k2lx/forecasting_and_timeseries/,all_ads,2018-04-04 03:47:04,59 days 21:48:30.036584000,
"I extracted the posts of a specific subreddit and used a word score list from Finn Arup Nielsen to find sentiment scores. The data will be aggregated by the month.

I intend to normalize the sentiment score against the count of words with an assigned score. When I do this, I can see several options on how to do this:

* Option 1 is to normalize the scores per post, then aggregate the data by the month

* Option 2 is to aggregate the score and word count, then normalize

I opted for Option 1 since it allows me to present the data in other aggregation levels if I wish, is this the right move?

Edit: Goal is to be able to measure how a certain subreddit/community feels about changes applied to the community/subreddit's interest. Measurable to the point wherein if changes applied to the interest causes the community/subreddit's sentiment score to go below a certain point, someone should look into what's pissing people off.

More Edit: I normalized against the word count to take into account the possibility where the score for a certain month may only be higher because of more posts being made, instead of the actual sentiment. I wanted to make the results less affected by the total number of posts on the subreddit.",7,1522873002.0,89oxeb,False,"I extracted the posts of a specific subreddit and used a word score list from Finn Arup Nielsen to find sentiment scores. The data will be aggregated by the month.

I intend to normalize the sentiment score against the count of words with an assigned score. When I do this, I can see several options on how to do this:

* Option 1 is to normalize the scores per post, then aggregate the data by the month

* Option 2 is to aggregate the score and word count, then normalize

I opted for Option 1 since it allows me to present the data in other aggregation levels if I wish, is this the right move?

Edit: Goal is to be able to measure how a certain subreddit/community feels about changes applied to the community/subreddit's interest. Measurable to the point wherein if changes applied to the interest causes the community/subreddit's sentiment score to go below a certain point, someone should look into what's pissing people off.

More Edit: I normalized against the word count to take into account the possibility where the score for a certain month may only be higher because of more posts being made, instead of the actual sentiment. I wanted to make the results less affected by the total number of posts on the subreddit.",0,"I extracted the posts of a specific subreddit and used a word score list from Finn Arup Nielsen to find sentiment scores. The data will be aggregated by the month.

I intend to normalize the sentiment score against the count of words with an assigned score. When I do this, I can see several options on how to do this:

* Option 1 is to normalize the scores per post, then aggregate the data by the month

* Option 2 is to aggregate the score and word count, then normalize

I opted for Option 1 since it allows me to present the data in other aggregation levels if I wish, is this the right move?

Edit: Goal is to be able to measure how a certain subreddit/community feels about changes applied to the community/subreddit's interest. Measurable to the point wherein if changes applied to the interest causes the community/subreddit's sentiment score to go below a certain point, someone should look into what's pissing people off.

More Edit: I normalized against the word count to take into account the possibility where the score for a certain month may only be higher because of more posts being made, instead of the actual sentiment. I wanted to make the results less affected by the total number of posts on the subreddit.",0,statistics,54935,,How to normalize sentiment analysis data of a specific subreddit?,https://www.reddit.com/r/statistics/comments/89oxeb/how_to_normalize_sentiment_analysis_data_of_a/,all_ads,2018-04-04 16:16:42,59 days 09:18:52.036584000,
"I understand mathematically, but struggle intuitively. Standard deviation means something to me immediately, but I find it hard to wrap my head around variance. 

For example, if I'm told the standard deviation of some dataset is 5, then I immediately know that ~95% of the data is within ±10. I also immediately know from Chebyshev's inequality that a minimum of 75% of the dataset is within ±10. 


But when I am given the variance of a dataset, it doesn't really *""mean""* anything to me in terms of an intuitive understanding of the dataset... If you know what I mean. I can square root the variance and only then will it mean something to me.  I know that Variance its the square of the standard deviation. But... so what? 

Could someone provide an intuitive explanation, thanks!
",18,1522808452.0,89gssw,False,"I understand mathematically, but struggle intuitively. Standard deviation means something to me immediately, but I find it hard to wrap my head around variance. 

For example, if I'm told the standard deviation of some dataset is 5, then I immediately know that ~95% of the data is within ±10. I also immediately know from Chebyshev's inequality that a minimum of 75% of the dataset is within ±10. 


But when I am given the variance of a dataset, it doesn't really *""mean""* anything to me in terms of an intuitive understanding of the dataset... If you know what I mean. I can square root the variance and only then will it mean something to me.  I know that Variance its the square of the standard deviation. But... so what? 

Could someone provide an intuitive explanation, thanks!
",0,"I understand mathematically, but struggle intuitively. Standard deviation means something to me immediately, but I find it hard to wrap my head around variance. 

For example, if I'm told the standard deviation of some dataset is 5, then I immediately know that ~95% of the data is within ±10. I also immediately know from Chebyshev's inequality that a minimum of 75% of the dataset is within ±10. 


But when I am given the variance of a dataset, it doesn't really *""mean""* anything to me in terms of an intuitive understanding of the dataset... If you know what I mean. I can square root the variance and only then will it mean something to me.  I know that Variance its the square of the standard deviation. But... so what? 

Could someone provide an intuitive explanation, thanks!
",28,statistics,54935,,Understanding the intuitive meaning behind variance,https://www.reddit.com/r/statistics/comments/89gssw/understanding_the_intuitive_meaning_behind/,all_ads,2018-04-03 22:20:52,60 days 03:14:42.036584000,
"https://hbswk.hbs.edu/item/could-a-new-business-model-make-clinical-drug-trials-more-accessible-to-patients

>Stern: Frequentist trials are those that basically set everything up in advance, and in a sense let it run, let it rip. Traditional randomized controlled trials are frequentist trials. We do some statistical calculations up front, and we say, ""Look, if we're expecting this type of effect from this drug, this is roughly how many patients we'll need to give it to, in order to observe a statistically significant difference between treatments and control groups.""

>Bayesian trials, which are the basis for adaptive platform trials, are a little bit different. What Bayesian trials do is incorporate information as it accumulates. I'll give you an example. Say patients with a certain tumor type are doing incredibly well on a given therapy. We can incorporate that information into how we assign patients to receive that therapy, and that'll work into the adaptive platform design.

*****

>What the platform trial does builds on this insight that if we can get it together in advance to coordinate on a lot of aspects of the trial infrastructure, we can actually share a control arm...Now, without doing anything else, we have just gone from needing 160 patients in total for us both to get statistical significance, to needing 120 patients. The sample size has decreased. We're seeing efficiencies, especially in uncommon cancer where it's often hard to find these patients; 40 percent of cancer trials never fully enroll.

*****

>Then, we can layer on top of that setup what we call Bayesian adaptive randomization....We can use our pre-specified algorithm to now preferentially assign that type of patient to your arm of the trial. Patients love this. As the trial progresses, patients are not only being used efficiently, because of the shared control arm component of the trial design, but we've also got the patients that are coming in being preferentially assigned to the treatment arm of the trial they're most likely to benefit from.",3,1522840673.0,89lu8m,False,"https://hbswk.hbs.edu/item/could-a-new-business-model-make-clinical-drug-trials-more-accessible-to-patients

>Stern: Frequentist trials are those that basically set everything up in advance, and in a sense let it run, let it rip. Traditional randomized controlled trials are frequentist trials. We do some statistical calculations up front, and we say, ""Look, if we're expecting this type of effect from this drug, this is roughly how many patients we'll need to give it to, in order to observe a statistically significant difference between treatments and control groups.""

>Bayesian trials, which are the basis for adaptive platform trials, are a little bit different. What Bayesian trials do is incorporate information as it accumulates. I'll give you an example. Say patients with a certain tumor type are doing incredibly well on a given therapy. We can incorporate that information into how we assign patients to receive that therapy, and that'll work into the adaptive platform design.

*****

>What the platform trial does builds on this insight that if we can get it together in advance to coordinate on a lot of aspects of the trial infrastructure, we can actually share a control arm...Now, without doing anything else, we have just gone from needing 160 patients in total for us both to get statistical significance, to needing 120 patients. The sample size has decreased. We're seeing efficiencies, especially in uncommon cancer where it's often hard to find these patients; 40 percent of cancer trials never fully enroll.

*****

>Then, we can layer on top of that setup what we call Bayesian adaptive randomization....We can use our pre-specified algorithm to now preferentially assign that type of patient to your arm of the trial. Patients love this. As the trial progresses, patients are not only being used efficiently, because of the shared control arm component of the trial design, but we've also got the patients that are coming in being preferentially assigned to the treatment arm of the trial they're most likely to benefit from.",0,"https://hbswk.hbs.edu/item/could-a-new-business-model-make-clinical-drug-trials-more-accessible-to-patients

>Stern: Frequentist trials are those that basically set everything up in advance, and in a sense let it run, let it rip. Traditional randomized controlled trials are frequentist trials. We do some statistical calculations up front, and we say, ""Look, if we're expecting this type of effect from this drug, this is roughly how many patients we'll need to give it to, in order to observe a statistically significant difference between treatments and control groups.""

>Bayesian trials, which are the basis for adaptive platform trials, are a little bit different. What Bayesian trials do is incorporate information as it accumulates. I'll give you an example. Say patients with a certain tumor type are doing incredibly well on a given therapy. We can incorporate that information into how we assign patients to receive that therapy, and that'll work into the adaptive platform design.

*****

>What the platform trial does builds on this insight that if we can get it together in advance to coordinate on a lot of aspects of the trial infrastructure, we can actually share a control arm...Now, without doing anything else, we have just gone from needing 160 patients in total for us both to get statistical significance, to needing 120 patients. The sample size has decreased. We're seeing efficiencies, especially in uncommon cancer where it's often hard to find these patients; 40 percent of cancer trials never fully enroll.

*****

>Then, we can layer on top of that setup what we call Bayesian adaptive randomization....We can use our pre-specified algorithm to now preferentially assign that type of patient to your arm of the trial. Patients love this. As the trial progresses, patients are not only being used efficiently, because of the shared control arm component of the trial design, but we've also got the patients that are coming in being preferentially assigned to the treatment arm of the trial they're most likely to benefit from.",3,statistics,54935,,Clinical Drugs Trials are becoming inaccessible and underpowered - could Bayesian adaptive platform trials change the game?,https://www.reddit.com/r/statistics/comments/89lu8m/clinical_drugs_trials_are_becoming_inaccessible/,all_ads,2018-04-04 07:17:53,59 days 18:17:41.036584000,
"I previously accepted my offer to UNC's biostatistic's master's program (currently ranked #2). However, this would be out-of-state and I only received loans in financial aid. Meanwhile, tuition would be free for me at the UT Health Science Center, though this is ranked around ~20. My question is: is it worth taking out nearly $60k in loans for a master's from UNC as opposed to the UT Health Science Center? Does rank really matter that much? I would like to keep the option of a PhD open, but I am not planning to pursue this at the moment. ",7,1522825487.0,89jppw,False,"I previously accepted my offer to UNC's biostatistic's master's program (currently ranked #2). However, this would be out-of-state and I only received loans in financial aid. Meanwhile, tuition would be free for me at the UT Health Science Center, though this is ranked around ~20. My question is: is it worth taking out nearly $60k in loans for a master's from UNC as opposed to the UT Health Science Center? Does rank really matter that much? I would like to keep the option of a PhD open, but I am not planning to pursue this at the moment. ",0,"I previously accepted my offer to UNC's biostatistic's master's program (currently ranked #2). However, this would be out-of-state and I only received loans in financial aid. Meanwhile, tuition would be free for me at the UT Health Science Center, though this is ranked around ~20. My question is: is it worth taking out nearly $60k in loans for a master's from UNC as opposed to the UT Health Science Center? Does rank really matter that much? I would like to keep the option of a PhD open, but I am not planning to pursue this at the moment. ",5,statistics,54935,,UNC Chapel Hill vs UT Health Science Master's,https://www.reddit.com/r/statistics/comments/89jppw/unc_chapel_hill_vs_ut_health_science_masters/,all_ads,2018-04-04 03:04:47,59 days 22:30:47.036584000,
"I'm using stata and I am using an interaction term, so the vifs are very high. After mean-centering and running the regression again, the vifs didn't change at all. As my model overall is statistically significant yet none of the variables are, this is a problem. ",1,1522851911.0,89n24f,False,"I'm using stata and I am using an interaction term, so the vifs are very high. After mean-centering and running the regression again, the vifs didn't change at all. As my model overall is statistically significant yet none of the variables are, this is a problem. ",0,"I'm using stata and I am using an interaction term, so the vifs are very high. After mean-centering and running the regression again, the vifs didn't change at all. As my model overall is statistically significant yet none of the variables are, this is a problem. ",1,statistics,54935,,Mean-centering didn't remove multicollinearity,https://www.reddit.com/r/statistics/comments/89n24f/meancentering_didnt_remove_multicollinearity/,all_ads,2018-04-04 10:25:11,59 days 15:10:23.036584000,
"So I took a sample of stock picks and calculated their returns over 1 year and 2 years and did tests to see if they beat the general market at the 95% confidence level. I also calculated if 1 year and 2 year returns were significantly different from eachother. 

My results were that 1 year returns DID beat the market at the 95% confidence level, 2 year returns DID NOT beat the market at the 95% confidence level, and that 1 year and 2 year returns WERE NOT significantly different at the 95% level.

The thing I'm confused about is how is it ok that 1 year returns beat the market and 2 year returns didn't, but somehow 1 year and 2 year returns are not significantly different from eachother?
Is this technically a conflict? How should I interpret it? Should I say something like I might have a false positive for 1 year returns/false negative for 2 year returns? Or am I missing something?
",10,1522844491.0,89mart,False,"So I took a sample of stock picks and calculated their returns over 1 year and 2 years and did tests to see if they beat the general market at the 95% confidence level. I also calculated if 1 year and 2 year returns were significantly different from eachother. 

My results were that 1 year returns DID beat the market at the 95% confidence level, 2 year returns DID NOT beat the market at the 95% confidence level, and that 1 year and 2 year returns WERE NOT significantly different at the 95% level.

The thing I'm confused about is how is it ok that 1 year returns beat the market and 2 year returns didn't, but somehow 1 year and 2 year returns are not significantly different from eachother?
Is this technically a conflict? How should I interpret it? Should I say something like I might have a false positive for 1 year returns/false negative for 2 year returns? Or am I missing something?
",0,"So I took a sample of stock picks and calculated their returns over 1 year and 2 years and did tests to see if they beat the general market at the 95% confidence level. I also calculated if 1 year and 2 year returns were significantly different from eachother. 

My results were that 1 year returns DID beat the market at the 95% confidence level, 2 year returns DID NOT beat the market at the 95% confidence level, and that 1 year and 2 year returns WERE NOT significantly different at the 95% level.

The thing I'm confused about is how is it ok that 1 year returns beat the market and 2 year returns didn't, but somehow 1 year and 2 year returns are not significantly different from eachother?
Is this technically a conflict? How should I interpret it? Should I say something like I might have a false positive for 1 year returns/false negative for 2 year returns? Or am I missing something?
",0,statistics,54935,,Basic bitch statistics question,https://www.reddit.com/r/statistics/comments/89mart/basic_bitch_statistics_question/,all_ads,2018-04-04 08:21:31,59 days 17:14:03.036584000,
"Hi all,

Does anyone know of a method for determining how many peaks are present in a distribution of points on the unit circle?  My data contains many different sets of points.  Each point is an angular value (i.e. a value on [-pi, pi]).  Some of these sets clearly have a single peak in the distribution, while others have multiple peaks.  In SOME cases, the single/multiple peaks in question could be reasonably well modeled as a von Mises distribution (if single peak) or mixture of von Mises distributions (if multiple peaks).  In other cases, however, the shape(s) of the peak(s) deviates substantially from the shape you would expect in a von Mises distribution (e.g. some peaks may be sawtooth-like, or close to rectangular), and so I am hoping to find a method that can count peaks in a circular distribution without assuming a particular peak shape.",5,1522802888.0,89ft08,False,"Hi all,

Does anyone know of a method for determining how many peaks are present in a distribution of points on the unit circle?  My data contains many different sets of points.  Each point is an angular value (i.e. a value on [-pi, pi]).  Some of these sets clearly have a single peak in the distribution, while others have multiple peaks.  In SOME cases, the single/multiple peaks in question could be reasonably well modeled as a von Mises distribution (if single peak) or mixture of von Mises distributions (if multiple peaks).  In other cases, however, the shape(s) of the peak(s) deviates substantially from the shape you would expect in a von Mises distribution (e.g. some peaks may be sawtooth-like, or close to rectangular), and so I am hoping to find a method that can count peaks in a circular distribution without assuming a particular peak shape.",0,"Hi all,

Does anyone know of a method for determining how many peaks are present in a distribution of points on the unit circle?  My data contains many different sets of points.  Each point is an angular value (i.e. a value on [-pi, pi]).  Some of these sets clearly have a single peak in the distribution, while others have multiple peaks.  In SOME cases, the single/multiple peaks in question could be reasonably well modeled as a von Mises distribution (if single peak) or mixture of von Mises distributions (if multiple peaks).  In other cases, however, the shape(s) of the peak(s) deviates substantially from the shape you would expect in a von Mises distribution (e.g. some peaks may be sawtooth-like, or close to rectangular), and so I am hoping to find a method that can count peaks in a circular distribution without assuming a particular peak shape.",6,statistics,54935,,Circular statistics question: Non-parametric method for determining number of peaks in circular distribution?,https://www.reddit.com/r/statistics/comments/89ft08/circular_statistics_question_nonparametric_method/,all_ads,2018-04-03 20:48:08,60 days 04:47:26.036584000,
"Suppose a sample is drawn x_1, ... x_n. Suppose the distribution of x is at least continuous. Is there any reasonable way to test that x_1 ... x_n are independent?  It seems unlikely to me unless a parametric assumption is made, but maybe I'm wrong. Any thoughts from the community?",6,1522835499.0,89l5ed,False,"Suppose a sample is drawn x_1, ... x_n. Suppose the distribution of x is at least continuous. Is there any reasonable way to test that x_1 ... x_n are independent?  It seems unlikely to me unless a parametric assumption is made, but maybe I'm wrong. Any thoughts from the community?",0,"Suppose a sample is drawn x_1, ... x_n. Suppose the distribution of x is at least continuous. Is there any reasonable way to test that x_1 ... x_n are independent?  It seems unlikely to me unless a parametric assumption is made, but maybe I'm wrong. Any thoughts from the community?",1,statistics,54935,,Tests of independence,https://www.reddit.com/r/statistics/comments/89l5ed/tests_of_independence/,all_ads,2018-04-04 05:51:39,59 days 19:43:55.036584000,
"I have what feels like a bad idea:  I have a process that returns a real number.  I have built two separate models to make a prediction for what value is expected at each point.  What I'd like to do is use (one of the two) model predictions as a Bayesian prior, and will blend this with the observed data to get a posterior estimate.  The problem I have is one of model selection:  Which of the two models should I use.  I'm playing with the idea of letting the model choice vary with each point:  Pick the model that's closest to the observed data.  Has anyone got any leads about how to analyze or think about this sort of approach?  Intuitively, down-side is that it allows one to chase noise in the data.  But the plus-side is that there are cases where I know one model is working better than the other and vise-versa, and taking this approach might allow me to automate the fix in those cases.",6,1522833099.0,89ktlg,False,"I have what feels like a bad idea:  I have a process that returns a real number.  I have built two separate models to make a prediction for what value is expected at each point.  What I'd like to do is use (one of the two) model predictions as a Bayesian prior, and will blend this with the observed data to get a posterior estimate.  The problem I have is one of model selection:  Which of the two models should I use.  I'm playing with the idea of letting the model choice vary with each point:  Pick the model that's closest to the observed data.  Has anyone got any leads about how to analyze or think about this sort of approach?  Intuitively, down-side is that it allows one to chase noise in the data.  But the plus-side is that there are cases where I know one model is working better than the other and vise-versa, and taking this approach might allow me to automate the fix in those cases.",0,"I have what feels like a bad idea:  I have a process that returns a real number.  I have built two separate models to make a prediction for what value is expected at each point.  What I'd like to do is use (one of the two) model predictions as a Bayesian prior, and will blend this with the observed data to get a posterior estimate.  The problem I have is one of model selection:  Which of the two models should I use.  I'm playing with the idea of letting the model choice vary with each point:  Pick the model that's closest to the observed data.  Has anyone got any leads about how to analyze or think about this sort of approach?  Intuitively, down-side is that it allows one to chase noise in the data.  But the plus-side is that there are cases where I know one model is working better than the other and vise-versa, and taking this approach might allow me to automate the fix in those cases.",1,statistics,54935,,Selecting closest of two models,https://www.reddit.com/r/statistics/comments/89ktlg/selecting_closest_of_two_models/,all_ads,2018-04-04 05:11:39,59 days 20:23:55.036584000,
"I'm trying to convert some processing time data to log-normal. I'm using equations 5.56 and 5.57 from [this stat book](http://uc.tc/u/l7ccj7.jpg) I had.

Let's say my normal mean is 15 and my normal standard deviation is 7. If I use these equations, I get my log-normal mean as 108254987.8 and standard deviation as 1.28399E+19 which seam way too large.

I am not extremely familiar with log-normal but I do know the variables will likely be much larger than typical normal distributions. Am I doing something wrong or is this actually correct?

Any help is appreciated. ",5,1522832518.0,89kqt2,False,"I'm trying to convert some processing time data to log-normal. I'm using equations 5.56 and 5.57 from [this stat book](http://uc.tc/u/l7ccj7.jpg) I had.

Let's say my normal mean is 15 and my normal standard deviation is 7. If I use these equations, I get my log-normal mean as 108254987.8 and standard deviation as 1.28399E+19 which seam way too large.

I am not extremely familiar with log-normal but I do know the variables will likely be much larger than typical normal distributions. Am I doing something wrong or is this actually correct?

Any help is appreciated. ",0,"I'm trying to convert some processing time data to log-normal. I'm using equations 5.56 and 5.57 from [this stat book](http://uc.tc/u/l7ccj7.jpg) I had.

Let's say my normal mean is 15 and my normal standard deviation is 7. If I use these equations, I get my log-normal mean as 108254987.8 and standard deviation as 1.28399E+19 which seam way too large.

I am not extremely familiar with log-normal but I do know the variables will likely be much larger than typical normal distributions. Am I doing something wrong or is this actually correct?

Any help is appreciated. ",0,statistics,54935,,Converting normal mean and stdev to log-normal mean and stdev.,https://www.reddit.com/r/statistics/comments/89kqt2/converting_normal_mean_and_stdev_to_lognormal/,all_ads,2018-04-04 05:01:58,59 days 20:33:36.036584000,
"So I am trying to determine the probability that one player beats another player in a set of a series of games in the most accurate way possible. So in a best of 5/first to 3 set scenario (or sometimes best of 3/first to 2 scenario), each round the players play a game against each other. In the game there is a point system and the first to get 2 points wins. I have past data I can use on all of this to help me predict the future outcomes.

One of the ways I think I would do this is to use the past data to directly calculate the probabilities that someone wins in the future. What I mean by this is if Player A has won 2 sets and lost 1 set to Player B(2-1 record) then based on this record I can say that the next time they play Player A has a ~67% chance of winning. Then continuously update the probability every time they play. The problem with this is, is there a way to determine the probability that one player beats another player if they have an undefeated record, so if Player A is 2-0 against Player B, is there anyway I can estimate the probability the next time Player A goes against Player B? At worst for Player A, the probability would be 67% since the next time they play Player B could win and make it 2-1, right? So I have a range of [67%,100%) but any way to narrow it down? Also, is there a good way to determine the probability that a player wins over another player if they haven't ever played each other before so there is no past data? The only thing I can think of is to use their performances against other players, but I wouldn't know what to do with that information.

I think I could use a hypergeometric distribution to find probability a player wins the set by using first to get 3 game wins in a set out of 5 draws. But in this case N (the total games played) would be changing since if someone 3-0s or 3-1s their opponent that's only 3 or 4 games instead of 5 since you wouldn't play out the rest of the games.

Also, I would ideally have a weight to favor more recent results over older results. So Player A has a 2-1 set advantage over Player B, but the 2 wins Player A got were 6 months ago and the 1 Player B got was last week, so I'd want to favor Player B's win more than just 33%. Any ideas on what a good way to determine what the weight should be?

Thoughts? Are these valid ways to go about it? Is there a better way? Ideas on how to go about undefeated records or instances where no past data is present?",4,1522816537.0,89ia8s,False,"So I am trying to determine the probability that one player beats another player in a set of a series of games in the most accurate way possible. So in a best of 5/first to 3 set scenario (or sometimes best of 3/first to 2 scenario), each round the players play a game against each other. In the game there is a point system and the first to get 2 points wins. I have past data I can use on all of this to help me predict the future outcomes.

One of the ways I think I would do this is to use the past data to directly calculate the probabilities that someone wins in the future. What I mean by this is if Player A has won 2 sets and lost 1 set to Player B(2-1 record) then based on this record I can say that the next time they play Player A has a ~67% chance of winning. Then continuously update the probability every time they play. The problem with this is, is there a way to determine the probability that one player beats another player if they have an undefeated record, so if Player A is 2-0 against Player B, is there anyway I can estimate the probability the next time Player A goes against Player B? At worst for Player A, the probability would be 67% since the next time they play Player B could win and make it 2-1, right? So I have a range of [67%,100%) but any way to narrow it down? Also, is there a good way to determine the probability that a player wins over another player if they haven't ever played each other before so there is no past data? The only thing I can think of is to use their performances against other players, but I wouldn't know what to do with that information.

I think I could use a hypergeometric distribution to find probability a player wins the set by using first to get 3 game wins in a set out of 5 draws. But in this case N (the total games played) would be changing since if someone 3-0s or 3-1s their opponent that's only 3 or 4 games instead of 5 since you wouldn't play out the rest of the games.

Also, I would ideally have a weight to favor more recent results over older results. So Player A has a 2-1 set advantage over Player B, but the 2 wins Player A got were 6 months ago and the 1 Player B got was last week, so I'd want to favor Player B's win more than just 33%. Any ideas on what a good way to determine what the weight should be?

Thoughts? Are these valid ways to go about it? Is there a better way? Ideas on how to go about undefeated records or instances where no past data is present?",0,"So I am trying to determine the probability that one player beats another player in a set of a series of games in the most accurate way possible. So in a best of 5/first to 3 set scenario (or sometimes best of 3/first to 2 scenario), each round the players play a game against each other. In the game there is a point system and the first to get 2 points wins. I have past data I can use on all of this to help me predict the future outcomes.

One of the ways I think I would do this is to use the past data to directly calculate the probabilities that someone wins in the future. What I mean by this is if Player A has won 2 sets and lost 1 set to Player B(2-1 record) then based on this record I can say that the next time they play Player A has a ~67% chance of winning. Then continuously update the probability every time they play. The problem with this is, is there a way to determine the probability that one player beats another player if they have an undefeated record, so if Player A is 2-0 against Player B, is there anyway I can estimate the probability the next time Player A goes against Player B? At worst for Player A, the probability would be 67% since the next time they play Player B could win and make it 2-1, right? So I have a range of [67%,100%) but any way to narrow it down? Also, is there a good way to determine the probability that a player wins over another player if they haven't ever played each other before so there is no past data? The only thing I can think of is to use their performances against other players, but I wouldn't know what to do with that information.

I think I could use a hypergeometric distribution to find probability a player wins the set by using first to get 3 game wins in a set out of 5 draws. But in this case N (the total games played) would be changing since if someone 3-0s or 3-1s their opponent that's only 3 or 4 games instead of 5 since you wouldn't play out the rest of the games.

Also, I would ideally have a weight to favor more recent results over older results. So Player A has a 2-1 set advantage over Player B, but the 2 wins Player A got were 6 months ago and the 1 Player B got was last week, so I'd want to favor Player B's win more than just 33%. Any ideas on what a good way to determine what the weight should be?

Thoughts? Are these valid ways to go about it? Is there a better way? Ideas on how to go about undefeated records or instances where no past data is present?",2,statistics,54935,,Probability of wins/loss in Bo5/Bo3 set using past data,https://www.reddit.com/r/statistics/comments/89ia8s/probability_of_winsloss_in_bo5bo3_set_using_past/,all_ads,2018-04-04 00:35:37,60 days 00:59:57.036584000,
"For my thesis, I found that the potential mediator was not significant (Using Baron and Kenny's way) after conducting regressions, even though mediation was found in a younger population in a previous study.
For the discussion I am trying to say that mediation should not have been expected because the independent variable (Adult attachment) and dependent variable (close friendship quality) are conceptually almost the same thing and there does not need to be a mechanism through which attachment works to influence friendship quality because they are measuring the same thing. 

SO my question is, is that a sound argument? Is there any research explaining how different two variables must be in order for mediation to make sense?  ",2,1522804759.0,89g559,False,"For my thesis, I found that the potential mediator was not significant (Using Baron and Kenny's way) after conducting regressions, even though mediation was found in a younger population in a previous study.
For the discussion I am trying to say that mediation should not have been expected because the independent variable (Adult attachment) and dependent variable (close friendship quality) are conceptually almost the same thing and there does not need to be a mechanism through which attachment works to influence friendship quality because they are measuring the same thing. 

SO my question is, is that a sound argument? Is there any research explaining how different two variables must be in order for mediation to make sense?  ",0,"For my thesis, I found that the potential mediator was not significant (Using Baron and Kenny's way) after conducting regressions, even though mediation was found in a younger population in a previous study.
For the discussion I am trying to say that mediation should not have been expected because the independent variable (Adult attachment) and dependent variable (close friendship quality) are conceptually almost the same thing and there does not need to be a mechanism through which attachment works to influence friendship quality because they are measuring the same thing. 

SO my question is, is that a sound argument? Is there any research explaining how different two variables must be in order for mediation to make sense?  ",4,statistics,54935,,"If IV and DV are conceptually similar, is there no need for a mediator?",https://www.reddit.com/r/statistics/comments/89g559/if_iv_and_dv_are_conceptually_similar_is_there_no/,all_ads,2018-04-03 21:19:19,60 days 04:16:15.036584000,
"I am an EMT, not a statistician.   I have a patient that we visit regularly and for many years.   I have the date/time of every visit, roughly 450 events.    Empirically, I think that the interval in days between visits is interesting and my hope is that I with some analysis I can find a way to improve his care.

Specifically, I think there are times when he has issues that cause our visits to be more frequent and other times he has a longer durations between visits (baseline behavior).   Just for illustration, suppose the baseline group was when we see him once every 25 days, and the ""frequent"" group is when we see him twice friday, once saturday, monday and then tuesday.    That grouping is more overt that his actual data.   He definitely has clusters exactly as above.     But his ""baseline"" might be more like 8 days not 25.   It's very unusual to go 10 days without seeing him.

I suspect that his ""frequent"" group might have a common cause (is it correct to say they are correlated?) like if he has a fever or maybe some other behavior.   If I can find a way to identify that he's in ""frequent"" mode, maybe I can get him to recognize it to make a short term change and then avoid 1 or 2 of my ""frequent"" visits.

What kind of test should I do to identify a cluster of correlated events in the baseline of uncorrelated events?

",4,1522818082.0,89ikf4,False,"I am an EMT, not a statistician.   I have a patient that we visit regularly and for many years.   I have the date/time of every visit, roughly 450 events.    Empirically, I think that the interval in days between visits is interesting and my hope is that I with some analysis I can find a way to improve his care.

Specifically, I think there are times when he has issues that cause our visits to be more frequent and other times he has a longer durations between visits (baseline behavior).   Just for illustration, suppose the baseline group was when we see him once every 25 days, and the ""frequent"" group is when we see him twice friday, once saturday, monday and then tuesday.    That grouping is more overt that his actual data.   He definitely has clusters exactly as above.     But his ""baseline"" might be more like 8 days not 25.   It's very unusual to go 10 days without seeing him.

I suspect that his ""frequent"" group might have a common cause (is it correct to say they are correlated?) like if he has a fever or maybe some other behavior.   If I can find a way to identify that he's in ""frequent"" mode, maybe I can get him to recognize it to make a short term change and then avoid 1 or 2 of my ""frequent"" visits.

What kind of test should I do to identify a cluster of correlated events in the baseline of uncorrelated events?

",0,"I am an EMT, not a statistician.   I have a patient that we visit regularly and for many years.   I have the date/time of every visit, roughly 450 events.    Empirically, I think that the interval in days between visits is interesting and my hope is that I with some analysis I can find a way to improve his care.

Specifically, I think there are times when he has issues that cause our visits to be more frequent and other times he has a longer durations between visits (baseline behavior).   Just for illustration, suppose the baseline group was when we see him once every 25 days, and the ""frequent"" group is when we see him twice friday, once saturday, monday and then tuesday.    That grouping is more overt that his actual data.   He definitely has clusters exactly as above.     But his ""baseline"" might be more like 8 days not 25.   It's very unusual to go 10 days without seeing him.

I suspect that his ""frequent"" group might have a common cause (is it correct to say they are correlated?) like if he has a fever or maybe some other behavior.   If I can find a way to identify that he's in ""frequent"" mode, maybe I can get him to recognize it to make a short term change and then avoid 1 or 2 of my ""frequent"" visits.

What kind of test should I do to identify a cluster of correlated events in the baseline of uncorrelated events?

",0,statistics,54935,,Time Series Analysis of Patient Data?,https://www.reddit.com/r/statistics/comments/89ikf4/time_series_analysis_of_patient_data/,all_ads,2018-04-04 01:01:22,60 days 00:34:12.036584000,
"Let's say I have two event streams. An event is identified as a labeled interval or a point in time. The best I could come up with was to count number of co-occurrences based on whether there is any overlap (intersection is non-zero) of the two intervals. To check overlap, I allowed some margin of error, say delta, so an interval would actually be considered as (start +- delta, end +- delta). By same reasoning, I could assume each point also as an interval ( point +- delta, if delta is my margin of error).

I just wonder if there is a better way than just counting, or a better way to define co-occurrence than overlap. Thanks.",0,1522818018.0,89ijz0,False,"Let's say I have two event streams. An event is identified as a labeled interval or a point in time. The best I could come up with was to count number of co-occurrences based on whether there is any overlap (intersection is non-zero) of the two intervals. To check overlap, I allowed some margin of error, say delta, so an interval would actually be considered as (start +- delta, end +- delta). By same reasoning, I could assume each point also as an interval ( point +- delta, if delta is my margin of error).

I just wonder if there is a better way than just counting, or a better way to define co-occurrence than overlap. Thanks.",0,"Let's say I have two event streams. An event is identified as a labeled interval or a point in time. The best I could come up with was to count number of co-occurrences based on whether there is any overlap (intersection is non-zero) of the two intervals. To check overlap, I allowed some margin of error, say delta, so an interval would actually be considered as (start +- delta, end +- delta). By same reasoning, I could assume each point also as an interval ( point +- delta, if delta is my margin of error).

I just wonder if there is a better way than just counting, or a better way to define co-occurrence than overlap. Thanks.",1,statistics,54935,,Measuring co-occurence of events,https://www.reddit.com/r/statistics/comments/89ijz0/measuring_cooccurence_of_events/,all_ads,2018-04-04 01:00:18,60 days 00:35:16.036584000,
"So I have a spinner that spins 1,2, or 3. I want to spin a one 3 times. The odds of this happening are 1/27 with 3 rolls. How can I calculate the odds of this happening with 5 rolls? 10 rolls? Etc. Thanks. I’m really stumped how I would go about solving this.",4,1522802942.0,89ftdm,False,"So I have a spinner that spins 1,2, or 3. I want to spin a one 3 times. The odds of this happening are 1/27 with 3 rolls. How can I calculate the odds of this happening with 5 rolls? 10 rolls? Etc. Thanks. I’m really stumped how I would go about solving this.",0,"So I have a spinner that spins 1,2, or 3. I want to spin a one 3 times. The odds of this happening are 1/27 with 3 rolls. How can I calculate the odds of this happening with 5 rolls? 10 rolls? Etc. Thanks. I’m really stumped how I would go about solving this.",2,statistics,54935,,Find chance of success given odds and number of times rolled,https://www.reddit.com/r/statistics/comments/89ftdm/find_chance_of_success_given_odds_and_number_of/,all_ads,2018-04-03 20:49:02,60 days 04:46:32.036584000,
"Hi all, I'm trying to find a minor that could help me marketability and career wise. My school (UCLA) doesn't offer a CS or econ minor, and I really have little interest in doing a medicine based field as my career goals are more geared towards data science for economic forecasting. I was thinking a math minor, but maybe that's just redundant and not worth too much? I was originally an applied math major and just recently switched to stats. Anyways, any advice would help! My stats department and academic counselor said to just pick one based on your passions, and that it's not something you necessarily have to do (even though the department page ""highly recommends"" pairing the stats major with a minor).",11,1522810194.0,89h40p,False,"Hi all, I'm trying to find a minor that could help me marketability and career wise. My school (UCLA) doesn't offer a CS or econ minor, and I really have little interest in doing a medicine based field as my career goals are more geared towards data science for economic forecasting. I was thinking a math minor, but maybe that's just redundant and not worth too much? I was originally an applied math major and just recently switched to stats. Anyways, any advice would help! My stats department and academic counselor said to just pick one based on your passions, and that it's not something you necessarily have to do (even though the department page ""highly recommends"" pairing the stats major with a minor).",0,"Hi all, I'm trying to find a minor that could help me marketability and career wise. My school (UCLA) doesn't offer a CS or econ minor, and I really have little interest in doing a medicine based field as my career goals are more geared towards data science for economic forecasting. I was thinking a math minor, but maybe that's just redundant and not worth too much? I was originally an applied math major and just recently switched to stats. Anyways, any advice would help! My stats department and academic counselor said to just pick one based on your passions, and that it's not something you necessarily have to do (even though the department page ""highly recommends"" pairing the stats major with a minor).",1,statistics,54935,,Undergrad Advice: what are solid minors to pair with a stats major (and does it matter)?,https://www.reddit.com/r/statistics/comments/89h40p/undergrad_advice_what_are_solid_minors_to_pair/,all_ads,2018-04-03 22:49:54,60 days 02:45:40.036584000,
"So I'm working on a project looking at econ indicators in all the counties of the US. I need to design a macro that will flag a county if:
1. The county has has a really bad value based on that county's history.
2. The county has a really bad value compared to the nation.

The comparison within counties was easy, I just took the mean, divided by the standard deviation, and flagged observations over 1 SD away from the county mean.

But now I'm working on the national comparison, and it's a little trickier. Obviously I'm taking the difference between the county and national value, but to design the severity flag I need to standardize the results somehow.

So what I was thinking of doing is standardizing them using the standard deviation of the county.

So for example, let's say County A' s YoY Labor Force Participation growth is -5%, and it's SD  IS 3%. The National Rate is -1% with an SD of 1%. So my variable would be (-5--1)/3.

My reasoning is that this would show how far the national value is from the county's mean, based on the county's distribution.

Do you think this is a valid method/reasoning?",0,1522806582.0,89ggts,False,"So I'm working on a project looking at econ indicators in all the counties of the US. I need to design a macro that will flag a county if:
1. The county has has a really bad value based on that county's history.
2. The county has a really bad value compared to the nation.

The comparison within counties was easy, I just took the mean, divided by the standard deviation, and flagged observations over 1 SD away from the county mean.

But now I'm working on the national comparison, and it's a little trickier. Obviously I'm taking the difference between the county and national value, but to design the severity flag I need to standardize the results somehow.

So what I was thinking of doing is standardizing them using the standard deviation of the county.

So for example, let's say County A' s YoY Labor Force Participation growth is -5%, and it's SD  IS 3%. The National Rate is -1% with an SD of 1%. So my variable would be (-5--1)/3.

My reasoning is that this would show how far the national value is from the county's mean, based on the county's distribution.

Do you think this is a valid method/reasoning?",0,"So I'm working on a project looking at econ indicators in all the counties of the US. I need to design a macro that will flag a county if:
1. The county has has a really bad value based on that county's history.
2. The county has a really bad value compared to the nation.

The comparison within counties was easy, I just took the mean, divided by the standard deviation, and flagged observations over 1 SD away from the county mean.

But now I'm working on the national comparison, and it's a little trickier. Obviously I'm taking the difference between the county and national value, but to design the severity flag I need to standardize the results somehow.

So what I was thinking of doing is standardizing them using the standard deviation of the county.

So for example, let's say County A' s YoY Labor Force Participation growth is -5%, and it's SD  IS 3%. The National Rate is -1% with an SD of 1%. So my variable would be (-5--1)/3.

My reasoning is that this would show how far the national value is from the county's mean, based on the county's distribution.

Do you think this is a valid method/reasoning?",1,statistics,54935,,Measures of difference from a National Mean,https://www.reddit.com/r/statistics/comments/89ggts/measures_of_difference_from_a_national_mean/,all_ads,2018-04-03 21:49:42,60 days 03:45:52.036584000,
"Hello! I am planning my data analysis for my first meta-analysis. Some researchers have emphasized the utility of exclusively analyzing results of similarly designed studies. However, I have read a couple studies in which the researchers have combined results from many types of designs and transformed the effect sizes to be compared altogether. Which is the correct/better method?",9,1522720954.0,892g36,False,"Hello! I am planning my data analysis for my first meta-analysis. Some researchers have emphasized the utility of exclusively analyzing results of similarly designed studies. However, I have read a couple studies in which the researchers have combined results from many types of designs and transformed the effect sizes to be compared altogether. Which is the correct/better method?",0,"Hello! I am planning my data analysis for my first meta-analysis. Some researchers have emphasized the utility of exclusively analyzing results of similarly designed studies. However, I have read a couple studies in which the researchers have combined results from many types of designs and transformed the effect sizes to be compared altogether. Which is the correct/better method?",18,statistics,54935,,Can I use results from ANOVA studies and regression studies in one meta-analysis? Are meta-analyses more valid when all the studies use the same research design?,https://www.reddit.com/r/statistics/comments/892g36/can_i_use_results_from_anova_studies_and/,all_ads,2018-04-02 22:02:34,61 days 03:33:00.036584000,
"I don't plan on becoming a full-fledged statistician, but have been advised to consider pursuing a undergraduate degree in the subject given my ultimate career plans (which involve graduate school.) 

I'm completely game for this, but I like to know my options and plan for contingencies, so I'm trying to figure out how employable a BS in statistics is by itself--and I really have no idea. I figured you lovely people might be able to shed some light on the matter? How un/limited are prospects? What kinds of things might I be able to do with one? Would I be limited to junior-level positions?",26,1522701236.0,88yyfc,False,"I don't plan on becoming a full-fledged statistician, but have been advised to consider pursuing a undergraduate degree in the subject given my ultimate career plans (which involve graduate school.) 

I'm completely game for this, but I like to know my options and plan for contingencies, so I'm trying to figure out how employable a BS in statistics is by itself--and I really have no idea. I figured you lovely people might be able to shed some light on the matter? How un/limited are prospects? What kinds of things might I be able to do with one? Would I be limited to junior-level positions?",0,"I don't plan on becoming a full-fledged statistician, but have been advised to consider pursuing a undergraduate degree in the subject given my ultimate career plans (which involve graduate school.) 

I'm completely game for this, but I like to know my options and plan for contingencies, so I'm trying to figure out how employable a BS in statistics is by itself--and I really have no idea. I figured you lovely people might be able to shed some light on the matter? How un/limited are prospects? What kinds of things might I be able to do with one? Would I be limited to junior-level positions?",27,statistics,54935,,How employable is a BS in statistics by itself?,https://www.reddit.com/r/statistics/comments/88yyfc/how_employable_is_a_bs_in_statistics_by_itself/,all_ads,2018-04-02 16:33:56,61 days 09:01:38.036584000,
"ArXiv link: https://arxiv.org/abs/1803.10568 (not my paper)

TL;DR: A way to ask polling questions that helps give anonymity to the pollees in multiple choice (such as multi-party elections) questions. 

This is straightforwardly implementable as a Bayesian problem and solved with MCMC as well, if you're looking for a simple exercise. ",6,1522704648.0,88zawf,False,"ArXiv link: https://arxiv.org/abs/1803.10568 (not my paper)

TL;DR: A way to ask polling questions that helps give anonymity to the pollees in multiple choice (such as multi-party elections) questions. 

This is straightforwardly implementable as a Bayesian problem and solved with MCMC as well, if you're looking for a simple exercise. ",0,"ArXiv link: https://arxiv.org/abs/1803.10568 (not my paper)

TL;DR: A way to ask polling questions that helps give anonymity to the pollees in multiple choice (such as multi-party elections) questions. 

This is straightforwardly implementable as a Bayesian problem and solved with MCMC as well, if you're looking for a simple exercise. ",3,statistics,54935,,How to ask sensitive multiple choice questions,https://www.reddit.com/r/statistics/comments/88zawf/how_to_ask_sensitive_multiple_choice_questions/,all_ads,2018-04-02 17:30:48,61 days 08:04:46.036584000,
"Hey Stats,

I'm considering a class next semester for Univariate Time Series Analyses, but I'm wondering how difficult you would say the concepts are if you took a similar course in grad school?  Did you jump in and swim or does it get pretty deep, pretty quick?  

(For context, I'm employed as an analyst with an MPH in Applied Biostatistics.  I'm not uncomfortable with advanced topics, but my concern is that if the course turns out to be way more theoretical than applied, or just calculus heavy, it's going to be a slog and I was really hoping to just to get acclimated.  This would be my first course from a stats department as opposed to PH Epi/Biostats where everything is application based)

Thanks in advance for any input.  
  ",2,1522729239.0,894qqg,False,"Hey Stats,

I'm considering a class next semester for Univariate Time Series Analyses, but I'm wondering how difficult you would say the concepts are if you took a similar course in grad school?  Did you jump in and swim or does it get pretty deep, pretty quick?  

(For context, I'm employed as an analyst with an MPH in Applied Biostatistics.  I'm not uncomfortable with advanced topics, but my concern is that if the course turns out to be way more theoretical than applied, or just calculus heavy, it's going to be a slog and I was really hoping to just to get acclimated.  This would be my first course from a stats department as opposed to PH Epi/Biostats where everything is application based)

Thanks in advance for any input.  
  ",0,"Hey Stats,

I'm considering a class next semester for Univariate Time Series Analyses, but I'm wondering how difficult you would say the concepts are if you took a similar course in grad school?  Did you jump in and swim or does it get pretty deep, pretty quick?  

(For context, I'm employed as an analyst with an MPH in Applied Biostatistics.  I'm not uncomfortable with advanced topics, but my concern is that if the course turns out to be way more theoretical than applied, or just calculus heavy, it's going to be a slog and I was really hoping to just to get acclimated.  This would be my first course from a stats department as opposed to PH Epi/Biostats where everything is application based)

Thanks in advance for any input.  
  ",1,statistics,54935,,"Has anyone taking Univariate Time Series Analysis, comments?",https://www.reddit.com/r/statistics/comments/894qqg/has_anyone_taking_univariate_time_series_analysis/,all_ads,2018-04-03 00:20:39,61 days 01:14:55.036584000,
"I’ve been searching the internet for a few days for an answer but can’t find any so hope you all can help me with the following:

For my master’s thesis I’m investigating the influence on Earnings management but I couldn’t find a significant model. So my supervisor proposed to apply winsorize.

Now my question is: is it allowed to apply winsorization to only 1 independant variable of the data set or does al variables have to be winsorized if you choose to apply it?

Many thanks in advance!",7,1522726255.0,893y6r,False,"I’ve been searching the internet for a few days for an answer but can’t find any so hope you all can help me with the following:

For my master’s thesis I’m investigating the influence on Earnings management but I couldn’t find a significant model. So my supervisor proposed to apply winsorize.

Now my question is: is it allowed to apply winsorization to only 1 independant variable of the data set or does al variables have to be winsorized if you choose to apply it?

Many thanks in advance!",0,"I’ve been searching the internet for a few days for an answer but can’t find any so hope you all can help me with the following:

For my master’s thesis I’m investigating the influence on Earnings management but I couldn’t find a significant model. So my supervisor proposed to apply winsorize.

Now my question is: is it allowed to apply winsorization to only 1 independant variable of the data set or does al variables have to be winsorized if you choose to apply it?

Many thanks in advance!",0,statistics,54935,,Question about Winsorization,https://www.reddit.com/r/statistics/comments/893y6r/question_about_winsorization/,all_ads,2018-04-02 23:30:55,61 days 02:04:39.036584000,
"Hello, 

Can someone explain to me how to project data from level X to level X*3 so that its an accurate model. Simply multiplying the data by 3 is not a good representation of the spread. By this I mean that the original data set and new data set at a higher level will have the same standard deviation but in reality we would want the two data sets to have similar %RSD's. ",4,1522734264.0,895kzb,False,"Hello, 

Can someone explain to me how to project data from level X to level X*3 so that its an accurate model. Simply multiplying the data by 3 is not a good representation of the spread. By this I mean that the original data set and new data set at a higher level will have the same standard deviation but in reality we would want the two data sets to have similar %RSD's. ",0,"Hello, 

Can someone explain to me how to project data from level X to level X*3 so that its an accurate model. Simply multiplying the data by 3 is not a good representation of the spread. By this I mean that the original data set and new data set at a higher level will have the same standard deviation but in reality we would want the two data sets to have similar %RSD's. ",0,statistics,54935,,Projecting Data at Higher Levels,https://www.reddit.com/r/statistics/comments/895kzb/projecting_data_at_higher_levels/,all_ads,2018-04-03 01:44:24,60 days 23:51:10.036584000,
,25,1522629103.0,88s0su,False,,0,,47,statistics,54935,,What is your favourite theorem?,https://www.reddit.com/r/statistics/comments/88s0su/what_is_your_favourite_theorem/,all_ads,2018-04-01 20:31:43,62 days 05:03:51.036584000,
"So I've been less than impressed by our course textbook, it's overpriced, unclear notation and doesn't explain things that great. The topics we are covering throughout the semester are: Point estimation; Confidence Intervals, Bounds, and Regions; Hypothesis Testing; Asymptotic Analysis. Any suggestions will be greatly appreciated. ",5,1522667384.0,88wd6w,False,"So I've been less than impressed by our course textbook, it's overpriced, unclear notation and doesn't explain things that great. The topics we are covering throughout the semester are: Point estimation; Confidence Intervals, Bounds, and Regions; Hypothesis Testing; Asymptotic Analysis. Any suggestions will be greatly appreciated. ",0,"So I've been less than impressed by our course textbook, it's overpriced, unclear notation and doesn't explain things that great. The topics we are covering throughout the semester are: Point estimation; Confidence Intervals, Bounds, and Regions; Hypothesis Testing; Asymptotic Analysis. Any suggestions will be greatly appreciated. ",1,statistics,54935,,Suggestions for a great statistics book on these topics?,https://www.reddit.com/r/statistics/comments/88wd6w/suggestions_for_a_great_statistics_book_on_these/,all_ads,2018-04-02 07:09:44,61 days 18:25:50.036584000,
"Hi, I'm trying to compare two sets of data. Let's say it's some ct (cycle threshold) readings from various patients. First set is done when we set a baseline threshold, while the second set is done by regression. I would like to know how different the two sets are, and how different are they. Is that possible?",2,1522688966.0,88xzov,False,"Hi, I'm trying to compare two sets of data. Let's say it's some ct (cycle threshold) readings from various patients. First set is done when we set a baseline threshold, while the second set is done by regression. I would like to know how different the two sets are, and how different are they. Is that possible?",0,"Hi, I'm trying to compare two sets of data. Let's say it's some ct (cycle threshold) readings from various patients. First set is done when we set a baseline threshold, while the second set is done by regression. I would like to know how different the two sets are, and how different are they. Is that possible?",0,statistics,54935,,Help with stats,https://www.reddit.com/r/statistics/comments/88xzov/help_with_stats/,all_ads,2018-04-02 13:09:26,61 days 12:26:08.036584000,
"Hi everyone,

I am looking for some help with a study I am currently working on that is looking to see if training nurses lead to an overall decrease in patient blood glucose levels on day 3 of admission to a hospital and I will be using SPSS to test. I am having a hard time deciding what tests would be best to use since I am looking at overall change in pre / post patient data that is not tied a a specific nurse. I feel like the answer might be a simple one but am a bit rusty and feeling unsure of my self.  I would really appreciate advise on how to go about my analysis from all you lovely statisticians! 

Edit: adding this to my post from my comment for clarification on design. The blood glucose levels (BGL) are not tied to a specific patient.  Simply, I am looking at the BGL of all patience who were admitted to a specific wing of the hospital prior to training and 4 months after training. I have data for day 0 (admission date) through day 7 of admission on all patients. However, previous studies have identified that it is not until day 3 of treatment that a significant improvement can be seen. 33 or the 50 nurses went through training but there is no way to tell which nurse saw who. I have 2 variables I am looking at: 
1) BGL for all patients admitted from Nov 16 - Feb 17
2) BGL for all patients admitted from Nov 17 - Feb 18
I used the 4 moths after training and those same months the previous year as to control for seasonality of disease. If you have any other questions or need clarification, please let me know. ",6,1522665391.0,88w639,False,"Hi everyone,

I am looking for some help with a study I am currently working on that is looking to see if training nurses lead to an overall decrease in patient blood glucose levels on day 3 of admission to a hospital and I will be using SPSS to test. I am having a hard time deciding what tests would be best to use since I am looking at overall change in pre / post patient data that is not tied a a specific nurse. I feel like the answer might be a simple one but am a bit rusty and feeling unsure of my self.  I would really appreciate advise on how to go about my analysis from all you lovely statisticians! 

Edit: adding this to my post from my comment for clarification on design. The blood glucose levels (BGL) are not tied to a specific patient.  Simply, I am looking at the BGL of all patience who were admitted to a specific wing of the hospital prior to training and 4 months after training. I have data for day 0 (admission date) through day 7 of admission on all patients. However, previous studies have identified that it is not until day 3 of treatment that a significant improvement can be seen. 33 or the 50 nurses went through training but there is no way to tell which nurse saw who. I have 2 variables I am looking at: 
1) BGL for all patients admitted from Nov 16 - Feb 17
2) BGL for all patients admitted from Nov 17 - Feb 18
I used the 4 moths after training and those same months the previous year as to control for seasonality of disease. If you have any other questions or need clarification, please let me know. ",0,"Hi everyone,

I am looking for some help with a study I am currently working on that is looking to see if training nurses lead to an overall decrease in patient blood glucose levels on day 3 of admission to a hospital and I will be using SPSS to test. I am having a hard time deciding what tests would be best to use since I am looking at overall change in pre / post patient data that is not tied a a specific nurse. I feel like the answer might be a simple one but am a bit rusty and feeling unsure of my self.  I would really appreciate advise on how to go about my analysis from all you lovely statisticians! 

Edit: adding this to my post from my comment for clarification on design. The blood glucose levels (BGL) are not tied to a specific patient.  Simply, I am looking at the BGL of all patience who were admitted to a specific wing of the hospital prior to training and 4 months after training. I have data for day 0 (admission date) through day 7 of admission on all patients. However, previous studies have identified that it is not until day 3 of treatment that a significant improvement can be seen. 33 or the 50 nurses went through training but there is no way to tell which nurse saw who. I have 2 variables I am looking at: 
1) BGL for all patients admitted from Nov 16 - Feb 17
2) BGL for all patients admitted from Nov 17 - Feb 18
I used the 4 moths after training and those same months the previous year as to control for seasonality of disease. If you have any other questions or need clarification, please let me know. ",0,statistics,54935,,Help with tests for differences patient blood glucose levels after training nurses.,https://www.reddit.com/r/statistics/comments/88w639/help_with_tests_for_differences_patient_blood/,all_ads,2018-04-02 06:36:31,61 days 18:59:03.036584000,
"This semester, I've been trying a lot of new assignments to encourage reading and writing of statistical literature as part of a new class and in preparation for a course pack I am publishing soon. 

Here are two of the reading assignments and one of writing exercises that I tried this semester: ""Data and the Law"", ""Big Data in Healthcare"", and an exercise on writing good statistical instructions.

All of the required reading is open access.


 1. ""Data and the Law"" reading assignment, in which we look at some of the complications of copyright, scientific facts, and compilations.

This reading assignment pertains to “Data and the law: Beyond the sweat of the brow. Who owns published data? And what is data?” by Gerald van Belle and Leslie Ruiter, available at https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2014.00737.x





Q1) What are some things that the US Copyright Act denies protection to? Name at least three.



Q2) When was it ruled that telephone numbers were not subject to copyright?



Q3) Say that you wanted to use the information in Table 1 in your own publication. Give two other ways that Table 1 could be changed in order to meet the 'modicum of originality' requirement.



Q4) What are the restrictions, if any, on making a graph using someone else's data?



Q5) Which of the three, EU, USA, and Canada is the most restrictive on copyright law pertaining to data? Which is the least?




2. ""Big Data and Healthcare"" reading assignment, in which we look at some of the ways in which big data is changing how hospitals operate.


The following 8 questions can be answered by reading the article “Big data analytics in healthcare: promise and potential” by Wullianallur Raghupathi and Viju Raghupathi  in Health Information Science and Systems 2014, 2:3

Available at: http://www.hissjournal.com/content/2/1/3



Q1) What is more difficult in working with big data in healthcare? Why?

Q2) Give two examples of how big data analytics can lead to improved outcomes.

Q3) What are some developments or outcomes that can be predicted with big data?

Q4) What are the four V's of big data, pertaining to analytics in healthcare?

Q5) Give an example of a future application of real-time data.

Q6) Name three platform/tool options for conducting big data analytics?

Q7) What are the four steps of the methodology of big data analytics?

Q8) How did twitter tracking compare to official reports of cholera in Haiti in 2010?



3. ""Writing Instructional Material"" assignment, in which we try to describe the steps for performing a standard analysis.

In-Class Exercise: Write instructions for someone wishing to make a linear regression model and make some additional predictions from that model. Assume that the user has access to software like R, and has experience with it. Also assume that they don't know which statistics tests and plots are useful.



Yours should be about 200 words. The following example is more than 400 words, but shows a lot of what you could run into.



Example: Instructions for performing a one-way ANOVA, including diagnostics and post-hoc analysis.



1. Check if you data is of the proper type. The response/dependent variable should be a numerical quantity, and the explanatory/independent variable should be a categorical or grouping variable. If one of these isn't true, then ANOVA is not the analysis you want.



2. Check the extent of any missing data, if any. If there is some, you may want to remove cases with any missing data, or you may want to impute before continuing. Note that reliable imputation of the group or category may be impossible. Similar considerations may be needed for very small groups.



3. Check the residuals of the model from lm() for normality. Do this either by histogram, Q-Qplot, or a formal hypothesis test like Shapiro-Wilks or Anderson-Darling. If there is strong evidence of non-normality, follow the non-parametric route. Otherwise, follow the parametric route.





Parametric route

4. Use the lm() command to build a model of 'response ~ explanatory', and save that model. Use the anova() command on the saved model to get an anova table.



5. The p-value on the right of the ANOVA table is result of testing the hypothesis that all of the group means are equal. If this is small (smaller than some arbitrarily pre-selected alpha, such as 0.05), then continue to the post-hoc test in step 6.



6. Perform a Tukey test with the TukeyHSD() command. The differences in each pair of means will be shown in the output. The p-values for group differences are automatically adjusted to the number of groups and pairwise comparisons you have. The null hypotheses being tested in each case is if those two group means in the pair are equal. Any small p-values suggest 'honestly significant' differences between the means.





Non-Parametric route

4. Perform a Krusal-Walls test (a non-parametric ANOVA) on the data by using the kruskal.test() on 'response ~ explanatory' model. The null hypothesis being tested here is if the mean RANK of each group is the same.



5. The p-value given with the Krusal-Walls test indicates if there are any significant differences in the RANK mean of the groups. If this is small (smaller than some arbitrarily pre-selected alpha, such as 0.05), then continue to the post-hoc test in step 6.



6. Perform a Wilcoxon test on each pair of means and acquire a p-value for each. Compute an adjusted alpha from you initial family-wide alpha and a multiple testing adjustment such as the Bonferroni, Sidak correction. Alternatively, perform a non-parametric post-hoc test such as a Dunn's Test.

Blog mirror: http://www.stats-et-al.com/2018/03/assignments-for-statistical-literacy.html
",4,1522585437.0,88obry,False,"This semester, I've been trying a lot of new assignments to encourage reading and writing of statistical literature as part of a new class and in preparation for a course pack I am publishing soon. 

Here are two of the reading assignments and one of writing exercises that I tried this semester: ""Data and the Law"", ""Big Data in Healthcare"", and an exercise on writing good statistical instructions.

All of the required reading is open access.


 1. ""Data and the Law"" reading assignment, in which we look at some of the complications of copyright, scientific facts, and compilations.

This reading assignment pertains to “Data and the law: Beyond the sweat of the brow. Who owns published data? And what is data?” by Gerald van Belle and Leslie Ruiter, available at https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2014.00737.x





Q1) What are some things that the US Copyright Act denies protection to? Name at least three.



Q2) When was it ruled that telephone numbers were not subject to copyright?



Q3) Say that you wanted to use the information in Table 1 in your own publication. Give two other ways that Table 1 could be changed in order to meet the 'modicum of originality' requirement.



Q4) What are the restrictions, if any, on making a graph using someone else's data?



Q5) Which of the three, EU, USA, and Canada is the most restrictive on copyright law pertaining to data? Which is the least?




2. ""Big Data and Healthcare"" reading assignment, in which we look at some of the ways in which big data is changing how hospitals operate.


The following 8 questions can be answered by reading the article “Big data analytics in healthcare: promise and potential” by Wullianallur Raghupathi and Viju Raghupathi  in Health Information Science and Systems 2014, 2:3

Available at: http://www.hissjournal.com/content/2/1/3



Q1) What is more difficult in working with big data in healthcare? Why?

Q2) Give two examples of how big data analytics can lead to improved outcomes.

Q3) What are some developments or outcomes that can be predicted with big data?

Q4) What are the four V's of big data, pertaining to analytics in healthcare?

Q5) Give an example of a future application of real-time data.

Q6) Name three platform/tool options for conducting big data analytics?

Q7) What are the four steps of the methodology of big data analytics?

Q8) How did twitter tracking compare to official reports of cholera in Haiti in 2010?



3. ""Writing Instructional Material"" assignment, in which we try to describe the steps for performing a standard analysis.

In-Class Exercise: Write instructions for someone wishing to make a linear regression model and make some additional predictions from that model. Assume that the user has access to software like R, and has experience with it. Also assume that they don't know which statistics tests and plots are useful.



Yours should be about 200 words. The following example is more than 400 words, but shows a lot of what you could run into.



Example: Instructions for performing a one-way ANOVA, including diagnostics and post-hoc analysis.



1. Check if you data is of the proper type. The response/dependent variable should be a numerical quantity, and the explanatory/independent variable should be a categorical or grouping variable. If one of these isn't true, then ANOVA is not the analysis you want.



2. Check the extent of any missing data, if any. If there is some, you may want to remove cases with any missing data, or you may want to impute before continuing. Note that reliable imputation of the group or category may be impossible. Similar considerations may be needed for very small groups.



3. Check the residuals of the model from lm() for normality. Do this either by histogram, Q-Qplot, or a formal hypothesis test like Shapiro-Wilks or Anderson-Darling. If there is strong evidence of non-normality, follow the non-parametric route. Otherwise, follow the parametric route.





Parametric route

4. Use the lm() command to build a model of 'response ~ explanatory', and save that model. Use the anova() command on the saved model to get an anova table.



5. The p-value on the right of the ANOVA table is result of testing the hypothesis that all of the group means are equal. If this is small (smaller than some arbitrarily pre-selected alpha, such as 0.05), then continue to the post-hoc test in step 6.



6. Perform a Tukey test with the TukeyHSD() command. The differences in each pair of means will be shown in the output. The p-values for group differences are automatically adjusted to the number of groups and pairwise comparisons you have. The null hypotheses being tested in each case is if those two group means in the pair are equal. Any small p-values suggest 'honestly significant' differences between the means.





Non-Parametric route

4. Perform a Krusal-Walls test (a non-parametric ANOVA) on the data by using the kruskal.test() on 'response ~ explanatory' model. The null hypothesis being tested here is if the mean RANK of each group is the same.



5. The p-value given with the Krusal-Walls test indicates if there are any significant differences in the RANK mean of the groups. If this is small (smaller than some arbitrarily pre-selected alpha, such as 0.05), then continue to the post-hoc test in step 6.



6. Perform a Wilcoxon test on each pair of means and acquire a p-value for each. Compute an adjusted alpha from you initial family-wide alpha and a multiple testing adjustment such as the Bonferroni, Sidak correction. Alternatively, perform a non-parametric post-hoc test such as a Dunn's Test.

Blog mirror: http://www.stats-et-al.com/2018/03/assignments-for-statistical-literacy.html
",0,"This semester, I've been trying a lot of new assignments to encourage reading and writing of statistical literature as part of a new class and in preparation for a course pack I am publishing soon. 

Here are two of the reading assignments and one of writing exercises that I tried this semester: ""Data and the Law"", ""Big Data in Healthcare"", and an exercise on writing good statistical instructions.

All of the required reading is open access.


 1. ""Data and the Law"" reading assignment, in which we look at some of the complications of copyright, scientific facts, and compilations.

This reading assignment pertains to “Data and the law: Beyond the sweat of the brow. Who owns published data? And what is data?” by Gerald van Belle and Leslie Ruiter, available at https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2014.00737.x





Q1) What are some things that the US Copyright Act denies protection to? Name at least three.



Q2) When was it ruled that telephone numbers were not subject to copyright?



Q3) Say that you wanted to use the information in Table 1 in your own publication. Give two other ways that Table 1 could be changed in order to meet the 'modicum of originality' requirement.



Q4) What are the restrictions, if any, on making a graph using someone else's data?



Q5) Which of the three, EU, USA, and Canada is the most restrictive on copyright law pertaining to data? Which is the least?




2. ""Big Data and Healthcare"" reading assignment, in which we look at some of the ways in which big data is changing how hospitals operate.


The following 8 questions can be answered by reading the article “Big data analytics in healthcare: promise and potential” by Wullianallur Raghupathi and Viju Raghupathi  in Health Information Science and Systems 2014, 2:3

Available at: http://www.hissjournal.com/content/2/1/3



Q1) What is more difficult in working with big data in healthcare? Why?

Q2) Give two examples of how big data analytics can lead to improved outcomes.

Q3) What are some developments or outcomes that can be predicted with big data?

Q4) What are the four V's of big data, pertaining to analytics in healthcare?

Q5) Give an example of a future application of real-time data.

Q6) Name three platform/tool options for conducting big data analytics?

Q7) What are the four steps of the methodology of big data analytics?

Q8) How did twitter tracking compare to official reports of cholera in Haiti in 2010?



3. ""Writing Instructional Material"" assignment, in which we try to describe the steps for performing a standard analysis.

In-Class Exercise: Write instructions for someone wishing to make a linear regression model and make some additional predictions from that model. Assume that the user has access to software like R, and has experience with it. Also assume that they don't know which statistics tests and plots are useful.



Yours should be about 200 words. The following example is more than 400 words, but shows a lot of what you could run into.



Example: Instructions for performing a one-way ANOVA, including diagnostics and post-hoc analysis.



1. Check if you data is of the proper type. The response/dependent variable should be a numerical quantity, and the explanatory/independent variable should be a categorical or grouping variable. If one of these isn't true, then ANOVA is not the analysis you want.



2. Check the extent of any missing data, if any. If there is some, you may want to remove cases with any missing data, or you may want to impute before continuing. Note that reliable imputation of the group or category may be impossible. Similar considerations may be needed for very small groups.



3. Check the residuals of the model from lm() for normality. Do this either by histogram, Q-Qplot, or a formal hypothesis test like Shapiro-Wilks or Anderson-Darling. If there is strong evidence of non-normality, follow the non-parametric route. Otherwise, follow the parametric route.





Parametric route

4. Use the lm() command to build a model of 'response ~ explanatory', and save that model. Use the anova() command on the saved model to get an anova table.



5. The p-value on the right of the ANOVA table is result of testing the hypothesis that all of the group means are equal. If this is small (smaller than some arbitrarily pre-selected alpha, such as 0.05), then continue to the post-hoc test in step 6.



6. Perform a Tukey test with the TukeyHSD() command. The differences in each pair of means will be shown in the output. The p-values for group differences are automatically adjusted to the number of groups and pairwise comparisons you have. The null hypotheses being tested in each case is if those two group means in the pair are equal. Any small p-values suggest 'honestly significant' differences between the means.





Non-Parametric route

4. Perform a Krusal-Walls test (a non-parametric ANOVA) on the data by using the kruskal.test() on 'response ~ explanatory' model. The null hypothesis being tested here is if the mean RANK of each group is the same.



5. The p-value given with the Krusal-Walls test indicates if there are any significant differences in the RANK mean of the groups. If this is small (smaller than some arbitrarily pre-selected alpha, such as 0.05), then continue to the post-hoc test in step 6.



6. Perform a Wilcoxon test on each pair of means and acquire a p-value for each. Compute an adjusted alpha from you initial family-wide alpha and a multiple testing adjustment such as the Bonferroni, Sidak correction. Alternatively, perform a non-parametric post-hoc test such as a Dunn's Test.

Blog mirror: http://www.stats-et-al.com/2018/03/assignments-for-statistical-literacy.html
",38,statistics,54935,,"Assignments for statistical literacy: Big Data in Healthcare, Data and the Law, Manual Writing",https://www.reddit.com/r/statistics/comments/88obry/assignments_for_statistical_literacy_big_data_in/,all_ads,2018-04-01 08:23:57,62 days 17:11:37.036584000,
"I am a senior in college and am wanting to learn more stats. My school required me to take a basic stats course, and that was years ago and I remember not particularly enjoying the book. 

I have quite a bit of experience with proof writing, so I’m trying to find a book not too basic, but at the same time not trying to throw me into the deep end too quick. If this helps My interests in stats stems from looking into data science and machine learning as possible careers. I keep hearing linear algebra and stats are nearly inseparable the higher up you go, so if there are some stat books that highlight that relationship I would enjoy those too. Thanks. ",4,1522653444.0,88uwu9,False,"I am a senior in college and am wanting to learn more stats. My school required me to take a basic stats course, and that was years ago and I remember not particularly enjoying the book. 

I have quite a bit of experience with proof writing, so I’m trying to find a book not too basic, but at the same time not trying to throw me into the deep end too quick. If this helps My interests in stats stems from looking into data science and machine learning as possible careers. I keep hearing linear algebra and stats are nearly inseparable the higher up you go, so if there are some stat books that highlight that relationship I would enjoy those too. Thanks. ",0,"I am a senior in college and am wanting to learn more stats. My school required me to take a basic stats course, and that was years ago and I remember not particularly enjoying the book. 

I have quite a bit of experience with proof writing, so I’m trying to find a book not too basic, but at the same time not trying to throw me into the deep end too quick. If this helps My interests in stats stems from looking into data science and machine learning as possible careers. I keep hearing linear algebra and stats are nearly inseparable the higher up you go, so if there are some stat books that highlight that relationship I would enjoy those too. Thanks. ",2,statistics,54935,,Undergrad in math looking to learn stats for data science,https://www.reddit.com/r/statistics/comments/88uwu9/undergrad_in_math_looking_to_learn_stats_for_data/,all_ads,2018-04-02 03:17:24,61 days 22:18:10.036584000,
"Hey,

Getting myself mixed up here. 

1. I need to use a parametric statistical test to ""determine whether blood pressure is affected by caffeine dose"". Is this regression? Does regression ""determine"" whether there's a significant relationship between X and y or not, within one set of data?

2. I need to show this data on a graph. Y axis is blood pressure, X axis is caffeine given, in separate doses e.g. 45mg, 90mg, etc. Though these doses are quantitive data, i can't work out whether it's discrete or not (i.e. whether to do a histogram or a line graph!!!). 

I was thinking a histogram, with bars for SD - is this correct? 

Thanks so much!",6,1522614556.0,88qgpm,False,"Hey,

Getting myself mixed up here. 

1. I need to use a parametric statistical test to ""determine whether blood pressure is affected by caffeine dose"". Is this regression? Does regression ""determine"" whether there's a significant relationship between X and y or not, within one set of data?

2. I need to show this data on a graph. Y axis is blood pressure, X axis is caffeine given, in separate doses e.g. 45mg, 90mg, etc. Though these doses are quantitive data, i can't work out whether it's discrete or not (i.e. whether to do a histogram or a line graph!!!). 

I was thinking a histogram, with bars for SD - is this correct? 

Thanks so much!",0,"Hey,

Getting myself mixed up here. 

1. I need to use a parametric statistical test to ""determine whether blood pressure is affected by caffeine dose"". Is this regression? Does regression ""determine"" whether there's a significant relationship between X and y or not, within one set of data?

2. I need to show this data on a graph. Y axis is blood pressure, X axis is caffeine given, in separate doses e.g. 45mg, 90mg, etc. Though these doses are quantitive data, i can't work out whether it's discrete or not (i.e. whether to do a histogram or a line graph!!!). 

I was thinking a histogram, with bars for SD - is this correct? 

Thanks so much!",3,statistics,54935,,Basic biology stats help needed!,https://www.reddit.com/r/statistics/comments/88qgpm/basic_biology_stats_help_needed/,all_ads,2018-04-01 16:29:16,62 days 09:06:18.036584000,
"Hi,

I am learning about confidence interval at the moment (I am a beginner in statistics). When the question asks to find the confidence interval for the population proportion, I am kind of confused by this.

Thanks  ",6,1522625650.0,88rliq,False,"Hi,

I am learning about confidence interval at the moment (I am a beginner in statistics). When the question asks to find the confidence interval for the population proportion, I am kind of confused by this.

Thanks  ",0,"Hi,

I am learning about confidence interval at the moment (I am a beginner in statistics). When the question asks to find the confidence interval for the population proportion, I am kind of confused by this.

Thanks  ",0,statistics,54935,,What is population proportion?,https://www.reddit.com/r/statistics/comments/88rliq/what_is_population_proportion/,all_ads,2018-04-01 19:34:10,62 days 06:01:24.036584000,
"Hi, I'm a biostatistician. I work in clinical trials on sample calculations, analysis, maybe simulations once in a while.   
I see lots of interesting new developments in machine learning. And two things come to mind :  
-My job is probably going to be swallowed by big data analysis. *Not necessarily AI taking over my job, just RCT/observational studies becoming obsolete compared to the use of huge datasets and machine learning analysis methods*. Eg. **Google Verily, using technology and masses of data to identify positive health outcomes**. I imagine this will affect other stats fields also, not just medical.     
-It looks very exciting, compared to what I'm doing.   
   
As statisticians, are we able to transition into big data and machine learning? The skillset seems entirely different.",22,1522566598.0,88mhqw,False,"Hi, I'm a biostatistician. I work in clinical trials on sample calculations, analysis, maybe simulations once in a while.   
I see lots of interesting new developments in machine learning. And two things come to mind :  
-My job is probably going to be swallowed by big data analysis. *Not necessarily AI taking over my job, just RCT/observational studies becoming obsolete compared to the use of huge datasets and machine learning analysis methods*. Eg. **Google Verily, using technology and masses of data to identify positive health outcomes**. I imagine this will affect other stats fields also, not just medical.     
-It looks very exciting, compared to what I'm doing.   
   
As statisticians, are we able to transition into big data and machine learning? The skillset seems entirely different.",0,"Hi, I'm a biostatistician. I work in clinical trials on sample calculations, analysis, maybe simulations once in a while.   
I see lots of interesting new developments in machine learning. And two things come to mind :  
-My job is probably going to be swallowed by big data analysis. *Not necessarily AI taking over my job, just RCT/observational studies becoming obsolete compared to the use of huge datasets and machine learning analysis methods*. Eg. **Google Verily, using technology and masses of data to identify positive health outcomes**. I imagine this will affect other stats fields also, not just medical.     
-It looks very exciting, compared to what I'm doing.   
   
As statisticians, are we able to transition into big data and machine learning? The skillset seems entirely different.",18,statistics,54935,,Has anyone moved from statistics to AI? Is it even readable?,https://www.reddit.com/r/statistics/comments/88mhqw/has_anyone_moved_from_statistics_to_ai_is_it_even/,all_ads,2018-04-01 03:09:58,62 days 22:25:36.036584000,
"Got accepted to both. Which should I attend?

I currently work in food manufacturing in quality control, but I am interested in working in the energy sector. I'm mainly interested in optimization of production processes and formulations of products if my interests have any relevance.

My background is a dual BS in chemistry and biology. I got a minor in mathematics with an emphasis in modeling. I am familiar with C and have some superficial knowledge with C++.

Pros and Cons I see:
Texas A&M is significantly cheaper (my work will pay for about a third of it to 75%, depending on the credits I take in each semester).
Penn State is six credits shorter than A&M and no qualifying exam to stress over.
Texas A&M seems to have a larger variety of courses to study.
Penn State doesn't appear to make you focus on SAS like A&M does. I'm interested in learning R. My current work doesn't care what you use. But I can't predict what future companies will want.

Any pro tips for me?",19,1522546347.0,88k73s,False,"Got accepted to both. Which should I attend?

I currently work in food manufacturing in quality control, but I am interested in working in the energy sector. I'm mainly interested in optimization of production processes and formulations of products if my interests have any relevance.

My background is a dual BS in chemistry and biology. I got a minor in mathematics with an emphasis in modeling. I am familiar with C and have some superficial knowledge with C++.

Pros and Cons I see:
Texas A&M is significantly cheaper (my work will pay for about a third of it to 75%, depending on the credits I take in each semester).
Penn State is six credits shorter than A&M and no qualifying exam to stress over.
Texas A&M seems to have a larger variety of courses to study.
Penn State doesn't appear to make you focus on SAS like A&M does. I'm interested in learning R. My current work doesn't care what you use. But I can't predict what future companies will want.

Any pro tips for me?",0,"Got accepted to both. Which should I attend?

I currently work in food manufacturing in quality control, but I am interested in working in the energy sector. I'm mainly interested in optimization of production processes and formulations of products if my interests have any relevance.

My background is a dual BS in chemistry and biology. I got a minor in mathematics with an emphasis in modeling. I am familiar with C and have some superficial knowledge with C++.

Pros and Cons I see:
Texas A&M is significantly cheaper (my work will pay for about a third of it to 75%, depending on the credits I take in each semester).
Penn State is six credits shorter than A&M and no qualifying exam to stress over.
Texas A&M seems to have a larger variety of courses to study.
Penn State doesn't appear to make you focus on SAS like A&M does. I'm interested in learning R. My current work doesn't care what you use. But I can't predict what future companies will want.

Any pro tips for me?",11,statistics,54935,,Penn State vs Texas A&M for online MS degree.,https://www.reddit.com/r/statistics/comments/88k73s/penn_state_vs_texas_am_for_online_ms_degree/,all_ads,2018-03-31 21:32:27,63 days 04:03:07.036584000,
"Years ago I had this comic strip of a teacher (which IIRC was wearing glasses and a lab coat) that was fitting data using a rule, but there was a subset of outliers bellow the data he was fitting. He would then frown to the outliers, then erase it, then proceed to fit the rest of the data with his rule and a smile on his face.

I kinda of need it for a presentation I'm doing this week and I've been searching for this image for days on google with no luck. I've found some that I could use instead of it, but none has the same impact for me as this I just described. If you have it or know where I could find it I would be very grateful. Thanks in advance.",2,1522565194.0,88mca4,False,"Years ago I had this comic strip of a teacher (which IIRC was wearing glasses and a lab coat) that was fitting data using a rule, but there was a subset of outliers bellow the data he was fitting. He would then frown to the outliers, then erase it, then proceed to fit the rest of the data with his rule and a smile on his face.

I kinda of need it for a presentation I'm doing this week and I've been searching for this image for days on google with no luck. I've found some that I could use instead of it, but none has the same impact for me as this I just described. If you have it or know where I could find it I would be very grateful. Thanks in advance.",0,"Years ago I had this comic strip of a teacher (which IIRC was wearing glasses and a lab coat) that was fitting data using a rule, but there was a subset of outliers bellow the data he was fitting. He would then frown to the outliers, then erase it, then proceed to fit the rest of the data with his rule and a smile on his face.

I kinda of need it for a presentation I'm doing this week and I've been searching for this image for days on google with no luck. I've found some that I could use instead of it, but none has the same impact for me as this I just described. If you have it or know where I could find it I would be very grateful. Thanks in advance.",5,statistics,54935,,[Help] Searching for comic strip about erasing outliers,https://www.reddit.com/r/statistics/comments/88mca4/help_searching_for_comic_strip_about_erasing/,all_ads,2018-04-01 02:46:34,62 days 22:49:00.036584000,
"I'm helping my brother with his recovery and I need to arrange random testing for him, approximately once every two to three weeks. I have the marbles and a crown bag, but I no longer remember how to populate the draw pool (with replacement) such that I get a ""winner"" once in that interval. Gonna take a refresher come fall, but I need the calculation rather sooner. Can someone help me?",11,1522568284.0,88mo1a,False,"I'm helping my brother with his recovery and I need to arrange random testing for him, approximately once every two to three weeks. I have the marbles and a crown bag, but I no longer remember how to populate the draw pool (with replacement) such that I get a ""winner"" once in that interval. Gonna take a refresher come fall, but I need the calculation rather sooner. Can someone help me?",0,"I'm helping my brother with his recovery and I need to arrange random testing for him, approximately once every two to three weeks. I have the marbles and a crown bag, but I no longer remember how to populate the draw pool (with replacement) such that I get a ""winner"" once in that interval. Gonna take a refresher come fall, but I need the calculation rather sooner. Can someone help me?",3,statistics,54935,,Need to do random at-home drug testing and it's been almost 20 years since I took statistics.,https://www.reddit.com/r/statistics/comments/88mo1a/need_to_do_random_athome_drug_testing_and_its/,all_ads,2018-04-01 03:38:04,62 days 21:57:30.036584000,
"I'm a grad student studying biostatistics and currently taking a course titled Quantitative Methods. I have three semesters of basic statistics and Calculus I, II, and III under my belt. I know intro statistics like the back of my hand and have always gotten A's in my statistics courses. Quantitative Methods is a major part of my degree, which is why I'm so frustrated that I'm not catching on to the concepts. I've read all assigned sections of our textbook (Wackerly, Mendenhall, and Schaeffer) but when it comes to actual math problems I'm at a total loss of even where to begin. It seems as though some of the material my professor covers isn't covered in our textbook. Are there any supplemental textbooks, books, practice problems, online courses, etc. that would help me understand the material of this course?

To describe the course in more depth, these are the Learning Objectives:

1) Understand the definition of probability and how it relates to statistical analysis goals

2) Identify the appropriate probability distribution for a given outcome measure of interest

3) Understand the basic theory of estimation and statistical inference via the central limit 
theorem

4) Derive basic results for maximum likelihood estimation and inference and understand how it connects to the central limit theorem

5) Understand and apply the Neyman-Pearson theory of hypothesis tests to practical problems",9,1522562566.0,88m1xk,False,"I'm a grad student studying biostatistics and currently taking a course titled Quantitative Methods. I have three semesters of basic statistics and Calculus I, II, and III under my belt. I know intro statistics like the back of my hand and have always gotten A's in my statistics courses. Quantitative Methods is a major part of my degree, which is why I'm so frustrated that I'm not catching on to the concepts. I've read all assigned sections of our textbook (Wackerly, Mendenhall, and Schaeffer) but when it comes to actual math problems I'm at a total loss of even where to begin. It seems as though some of the material my professor covers isn't covered in our textbook. Are there any supplemental textbooks, books, practice problems, online courses, etc. that would help me understand the material of this course?

To describe the course in more depth, these are the Learning Objectives:

1) Understand the definition of probability and how it relates to statistical analysis goals

2) Identify the appropriate probability distribution for a given outcome measure of interest

3) Understand the basic theory of estimation and statistical inference via the central limit 
theorem

4) Derive basic results for maximum likelihood estimation and inference and understand how it connects to the central limit theorem

5) Understand and apply the Neyman-Pearson theory of hypothesis tests to practical problems",0,"I'm a grad student studying biostatistics and currently taking a course titled Quantitative Methods. I have three semesters of basic statistics and Calculus I, II, and III under my belt. I know intro statistics like the back of my hand and have always gotten A's in my statistics courses. Quantitative Methods is a major part of my degree, which is why I'm so frustrated that I'm not catching on to the concepts. I've read all assigned sections of our textbook (Wackerly, Mendenhall, and Schaeffer) but when it comes to actual math problems I'm at a total loss of even where to begin. It seems as though some of the material my professor covers isn't covered in our textbook. Are there any supplemental textbooks, books, practice problems, online courses, etc. that would help me understand the material of this course?

To describe the course in more depth, these are the Learning Objectives:

1) Understand the definition of probability and how it relates to statistical analysis goals

2) Identify the appropriate probability distribution for a given outcome measure of interest

3) Understand the basic theory of estimation and statistical inference via the central limit 
theorem

4) Derive basic results for maximum likelihood estimation and inference and understand how it connects to the central limit theorem

5) Understand and apply the Neyman-Pearson theory of hypothesis tests to practical problems",3,statistics,54935,,Struggling with graduate Quantitative Methods course,https://www.reddit.com/r/statistics/comments/88m1xk/struggling_with_graduate_quantitative_methods/,all_ads,2018-04-01 02:02:46,62 days 23:32:48.036584000,
"My question is what happens to modelers who work in industry as they get older. From my experience when I've seen my boss one or two levels up, some of them haven't done a regression in at least 10 years and I truly don't even respect their intelligence on pure stats and modelling. I doubt they've read a stats book in at least 20 years. Everyone at work sits in a cubicle or open desk seating and nobody even has books, nobody spends work time reading. It would look really weird if you did. Some of my bosses have told me comments like ""in my previous life when I did all this text book models"" ... but how/why did they move away from that???  It seems this happens to everyone. I don't understand what they do all day, yet now I fear I am coming to that same cross road as a mid 30 something modeler. I think they get bogged down in the business details and business functions, and use that knowledge to steer the modeling efforts of others. I used to not care at all about the business specifics, I always thought knowing the academic details was the hard part, picking up the business info as needed would be easy. 

But now I find I have a real lack of knowledge about the business, and with my fixed time limit at work (plus what I do outside of work) I just can't keep up with both. I can devote my time to learning more about the business, I can devote my time to not forgetting a base core of stats knowledge, or I can devote my time to learning more about stats. Pick 2. I've just come to accept that I can't compete with a 25 yo who just finished grad school, they will run me over in technical knowledge. The time it takes for me to even work through an old linear algebra book is a matter of weeks, and that's just to not forget stuff, let alone learn new stuff. And that takes away time I could spend learning more about the industry. There is just no way I can compete on technical knowledge, and that will only get worse as each year by year goes by. However, the college grads know nothing about the business, so I guess that is supposed to be the material I should focus on? I have seen all kinds of problematic modelling efforts and mistakes that show a mis-understanding of general business knowledge. I feel like I should just give up on my hope of staying sharp with textbook stats knowledge. But it feels kind of sad to give that up.

Anyone else face this before?",28,1522538025.0,88j8zi,False,"My question is what happens to modelers who work in industry as they get older. From my experience when I've seen my boss one or two levels up, some of them haven't done a regression in at least 10 years and I truly don't even respect their intelligence on pure stats and modelling. I doubt they've read a stats book in at least 20 years. Everyone at work sits in a cubicle or open desk seating and nobody even has books, nobody spends work time reading. It would look really weird if you did. Some of my bosses have told me comments like ""in my previous life when I did all this text book models"" ... but how/why did they move away from that???  It seems this happens to everyone. I don't understand what they do all day, yet now I fear I am coming to that same cross road as a mid 30 something modeler. I think they get bogged down in the business details and business functions, and use that knowledge to steer the modeling efforts of others. I used to not care at all about the business specifics, I always thought knowing the academic details was the hard part, picking up the business info as needed would be easy. 

But now I find I have a real lack of knowledge about the business, and with my fixed time limit at work (plus what I do outside of work) I just can't keep up with both. I can devote my time to learning more about the business, I can devote my time to not forgetting a base core of stats knowledge, or I can devote my time to learning more about stats. Pick 2. I've just come to accept that I can't compete with a 25 yo who just finished grad school, they will run me over in technical knowledge. The time it takes for me to even work through an old linear algebra book is a matter of weeks, and that's just to not forget stuff, let alone learn new stuff. And that takes away time I could spend learning more about the industry. There is just no way I can compete on technical knowledge, and that will only get worse as each year by year goes by. However, the college grads know nothing about the business, so I guess that is supposed to be the material I should focus on? I have seen all kinds of problematic modelling efforts and mistakes that show a mis-understanding of general business knowledge. I feel like I should just give up on my hope of staying sharp with textbook stats knowledge. But it feels kind of sad to give that up.

Anyone else face this before?",0,"My question is what happens to modelers who work in industry as they get older. From my experience when I've seen my boss one or two levels up, some of them haven't done a regression in at least 10 years and I truly don't even respect their intelligence on pure stats and modelling. I doubt they've read a stats book in at least 20 years. Everyone at work sits in a cubicle or open desk seating and nobody even has books, nobody spends work time reading. It would look really weird if you did. Some of my bosses have told me comments like ""in my previous life when I did all this text book models"" ... but how/why did they move away from that???  It seems this happens to everyone. I don't understand what they do all day, yet now I fear I am coming to that same cross road as a mid 30 something modeler. I think they get bogged down in the business details and business functions, and use that knowledge to steer the modeling efforts of others. I used to not care at all about the business specifics, I always thought knowing the academic details was the hard part, picking up the business info as needed would be easy. 

But now I find I have a real lack of knowledge about the business, and with my fixed time limit at work (plus what I do outside of work) I just can't keep up with both. I can devote my time to learning more about the business, I can devote my time to not forgetting a base core of stats knowledge, or I can devote my time to learning more about stats. Pick 2. I've just come to accept that I can't compete with a 25 yo who just finished grad school, they will run me over in technical knowledge. The time it takes for me to even work through an old linear algebra book is a matter of weeks, and that's just to not forget stuff, let alone learn new stuff. And that takes away time I could spend learning more about the industry. There is just no way I can compete on technical knowledge, and that will only get worse as each year by year goes by. However, the college grads know nothing about the business, so I guess that is supposed to be the material I should focus on? I have seen all kinds of problematic modelling efforts and mistakes that show a mis-understanding of general business knowledge. I feel like I should just give up on my hope of staying sharp with textbook stats knowledge. But it feels kind of sad to give that up.

Anyone else face this before?",14,statistics,54935,,Modelers age 40+ in a corporate setting,https://www.reddit.com/r/statistics/comments/88j8zi/modelers_age_40_in_a_corporate_setting/,all_ads,2018-03-31 19:13:45,63 days 06:21:49.036584000,
"Trying to do some simple data analysis with respect to credit card fraud detection. I was wondering what experts consider when trying to assess accuracy with highly unbalanced data set. 

The data set I am looking at has over 284k observations however < 0.2% are fraudulent. ",6,1522548452.0,88kgtd,False,"Trying to do some simple data analysis with respect to credit card fraud detection. I was wondering what experts consider when trying to assess accuracy with highly unbalanced data set. 

The data set I am looking at has over 284k observations however < 0.2% are fraudulent. ",0,"Trying to do some simple data analysis with respect to credit card fraud detection. I was wondering what experts consider when trying to assess accuracy with highly unbalanced data set. 

The data set I am looking at has over 284k observations however < 0.2% are fraudulent. ",6,statistics,54935,,"Question working with large data set, assessing accuracy:",https://www.reddit.com/r/statistics/comments/88kgtd/question_working_with_large_data_set_assessing/,all_ads,2018-03-31 22:07:32,63 days 03:28:02.036584000,
"Hi everyone.I am working on a lm that has two discrete variables.The first one takes discrete values from 4 to 16 and the other one also takes discrete values between 8 and 38.Should I model these variables as continous variables?, due to their wide space of states or should I use dummy variables instead?
I aprecciate any help :)",3,1522580224.0,88nu1l,False,"Hi everyone.I am working on a lm that has two discrete variables.The first one takes discrete values from 4 to 16 and the other one also takes discrete values between 8 and 38.Should I model these variables as continous variables?, due to their wide space of states or should I use dummy variables instead?
I aprecciate any help :)",0,"Hi everyone.I am working on a lm that has two discrete variables.The first one takes discrete values from 4 to 16 and the other one also takes discrete values between 8 and 38.Should I model these variables as continous variables?, due to their wide space of states or should I use dummy variables instead?
I aprecciate any help :)",1,statistics,54935,,Discrete variables on a linear regression model,https://www.reddit.com/r/statistics/comments/88nu1l/discrete_variables_on_a_linear_regression_model/,all_ads,2018-04-01 06:57:04,62 days 18:38:30.036584000,
"I am medical student and need to understand this table. What SE and B stand for? and how I can say with adjusted data shellfish consumption is associated with higher LDL. Thank you in advance.

TABLE: https://imgur.com/a/YihGE",2,1522600921.0,88pip7,False,"I am medical student and need to understand this table. What SE and B stand for? and how I can say with adjusted data shellfish consumption is associated with higher LDL. Thank you in advance.

TABLE: https://imgur.com/a/YihGE",0,"I am medical student and need to understand this table. What SE and B stand for? and how I can say with adjusted data shellfish consumption is associated with higher LDL. Thank you in advance.

TABLE: https://imgur.com/a/YihGE",0,statistics,54935,,[Help] I need help to read a table,https://www.reddit.com/r/statistics/comments/88pip7/help_i_need_help_to_read_a_table/,all_ads,2018-04-01 12:42:01,62 days 12:53:33.036584000,
"Recently in classes I've been learning about multicollinearity, and from what I'm understanding it's when independent variables in a regression have a high correlation with one another. Why would this be a problem, and could you maybe give a real life scenario example showing how this affects the data? Thank you!",7,1522580870.0,88nwdk,False,"Recently in classes I've been learning about multicollinearity, and from what I'm understanding it's when independent variables in a regression have a high correlation with one another. Why would this be a problem, and could you maybe give a real life scenario example showing how this affects the data? Thank you!",0,"Recently in classes I've been learning about multicollinearity, and from what I'm understanding it's when independent variables in a regression have a high correlation with one another. Why would this be a problem, and could you maybe give a real life scenario example showing how this affects the data? Thank you!",0,statistics,54935,,Question about multicollinearity,https://www.reddit.com/r/statistics/comments/88nwdk/question_about_multicollinearity/,all_ads,2018-04-01 07:07:50,62 days 18:27:44.036584000,
"I'm interested in discussing the validity of statements some people make about Frequentist results.

For example, in a 95% confidence interval, I calculate one interval and since 95% of the intervals I calculate using this procedure will contain the population mean, **there's a 95% chance my interval contains the mean** .

I've seen a lot of people argue that you can't make the emphasized statement because the CI isn't a Random Variable, it's already happened and therefore it either contains or doesn't contain the true population parameter.


What do you think?


Similarly do you think that calculating the False Discovery Rate of a study (say it's 10%) would allow you to say that there's a 10% chance that your rejection of the null is wrong.




I really enjoy hearing what people think about these things, as I feel like I'm constantly learning new insights about stats.",4,1522554841.0,88l70x,False,"I'm interested in discussing the validity of statements some people make about Frequentist results.

For example, in a 95% confidence interval, I calculate one interval and since 95% of the intervals I calculate using this procedure will contain the population mean, **there's a 95% chance my interval contains the mean** .

I've seen a lot of people argue that you can't make the emphasized statement because the CI isn't a Random Variable, it's already happened and therefore it either contains or doesn't contain the true population parameter.


What do you think?


Similarly do you think that calculating the False Discovery Rate of a study (say it's 10%) would allow you to say that there's a 10% chance that your rejection of the null is wrong.




I really enjoy hearing what people think about these things, as I feel like I'm constantly learning new insights about stats.",0,"I'm interested in discussing the validity of statements some people make about Frequentist results.

For example, in a 95% confidence interval, I calculate one interval and since 95% of the intervals I calculate using this procedure will contain the population mean, **there's a 95% chance my interval contains the mean** .

I've seen a lot of people argue that you can't make the emphasized statement because the CI isn't a Random Variable, it's already happened and therefore it either contains or doesn't contain the true population parameter.


What do you think?


Similarly do you think that calculating the False Discovery Rate of a study (say it's 10%) would allow you to say that there's a 10% chance that your rejection of the null is wrong.




I really enjoy hearing what people think about these things, as I feel like I'm constantly learning new insights about stats.",1,statistics,54935,,"P-values, alpha, Confidence Intervals and Probabilities...",https://www.reddit.com/r/statistics/comments/88l70x/pvalues_alpha_confidence_intervals_and/,all_ads,2018-03-31 23:54:01,63 days 01:41:33.036584000,
"I understand that if they were dependent, one could be defined by the other, but how does variables being independent makes things easier for us? I am currently studying regression analysis, in which disturbance term are assumed to be independent, this assumption further becomes the foundation for a lot of other proofs. I want to know what would have changed if disturbance were actually dependent?",13,1522520227.0,88hrxs,False,"I understand that if they were dependent, one could be defined by the other, but how does variables being independent makes things easier for us? I am currently studying regression analysis, in which disturbance term are assumed to be independent, this assumption further becomes the foundation for a lot of other proofs. I want to know what would have changed if disturbance were actually dependent?",0,"I understand that if they were dependent, one could be defined by the other, but how does variables being independent makes things easier for us? I am currently studying regression analysis, in which disturbance term are assumed to be independent, this assumption further becomes the foundation for a lot of other proofs. I want to know what would have changed if disturbance were actually dependent?",3,statistics,54935,,Why is variable being independent so important thing in statistics?,https://www.reddit.com/r/statistics/comments/88hrxs/why_is_variable_being_independent_so_important/,all_ads,2018-03-31 14:17:07,63 days 11:18:27.036584000,
"Okay, I am taking an introductory course in statistics and we are learning about hypothesis testing and student's t-test.

As a part of an assignment, I am supposed to verify whether t-test requires:

1. Homogenity of variance
2. Normal population
3. independence of data

How do I go about verifying these requirements experimentally?",1,1522553981.0,88l3n2,False,"Okay, I am taking an introductory course in statistics and we are learning about hypothesis testing and student's t-test.

As a part of an assignment, I am supposed to verify whether t-test requires:

1. Homogenity of variance
2. Normal population
3. independence of data

How do I go about verifying these requirements experimentally?",0,"Okay, I am taking an introductory course in statistics and we are learning about hypothesis testing and student's t-test.

As a part of an assignment, I am supposed to verify whether t-test requires:

1. Homogenity of variance
2. Normal population
3. independence of data

How do I go about verifying these requirements experimentally?",0,statistics,54935,,T-Test pre conditions,https://www.reddit.com/r/statistics/comments/88l3n2/ttest_pre_conditions/,all_ads,2018-03-31 23:39:41,63 days 01:55:53.036584000,
"I'm not entirely sure which tests to do, I have 8 sets of conditions, and I'm comparing average populations in 8 different locations based on these conditions. I can't tell if I should do t-tests or anova, or both?",22,1522521935.0,88hvti,False,"I'm not entirely sure which tests to do, I have 8 sets of conditions, and I'm comparing average populations in 8 different locations based on these conditions. I can't tell if I should do t-tests or anova, or both?",0,"I'm not entirely sure which tests to do, I have 8 sets of conditions, and I'm comparing average populations in 8 different locations based on these conditions. I can't tell if I should do t-tests or anova, or both?",0,statistics,54935,,ANOVA or T-test?,https://www.reddit.com/r/statistics/comments/88hvti/anova_or_ttest/,all_ads,2018-03-31 14:45:35,63 days 10:49:59.036584000,
"Julia Silge wrote an awesome blog post a few months back about creating something similar to word2vec, breaking it down in to easy to follow steps. 

The results are interesting and could be expanded following her workflow (perhaps in another /r/statistics post?).

Link: https://juliasilge.com/blog/tidy-word-vectors/",6,1522463484.0,88cl7m,False,"Julia Silge wrote an awesome blog post a few months back about creating something similar to word2vec, breaking it down in to easy to follow steps. 

The results are interesting and could be expanded following her workflow (perhaps in another /r/statistics post?).

Link: https://juliasilge.com/blog/tidy-word-vectors/",0,"Julia Silge wrote an awesome blog post a few months back about creating something similar to word2vec, breaking it down in to easy to follow steps. 

The results are interesting and could be expanded following her workflow (perhaps in another /r/statistics post?).

Link: https://juliasilge.com/blog/tidy-word-vectors/",19,statistics,54935,,Word Vectors with Tidy Data Principles,https://www.reddit.com/r/statistics/comments/88cl7m/word_vectors_with_tidy_data_principles/,all_ads,2018-03-30 22:31:24,64 days 03:04:10.036584000,
"I was curious how I would roll, say, a 20-sided die given nothing but my mind or a pencil or paper. Humans are notorious for being very skewed at guessing random numbers, and I couldn't find many good algorithms after a rudimentary Google search, so I decided to come up with one.

I turned to the most natural thing - language - and not requiring the user to know anything but the letters of the alphabet and a couple of words. Most mental-math random number generation algorithms require memorizing a specific set of numbers or steps - who has time for that?

Looking at the environment and ""counting the number of X"" doesn't work either, as that biases us towards objects that occur more than once, and doesn't provide an even distribution.

**The algorithm and results are detailed [here.](https://gist.github.com/tusing/3de5682785a34fbda67785a4a3256050)** Here's the algorithm if you're too lazy to read it:

1. Start with a seed (1). The seed serves to ""balance"" our output distribution, whereas the rest of the algorithm allows for a relatively random output order.

2. Start with a mod space (must be odd). If you want an even mod space, start with an odd one and take the first n-1 values.

3. Think of any normal English word or sentence. Lowercase letters only.

4. Add up all the letters in sentence (indexed by their order in the alphabet).
5. Increment the seed and add it to this sum.

6. Mod the sum by the space you want the random number to be in.

I feel a bit iffy about the idea of a seed and incrementing it, that's obviously the source of this ""even"" distribution we're getting as the results describe. After all, if you start with a number and increment it N times, mod m, you'll have an even N/m items in every bin! 

But with very predictable output. Thus, the rest of the algorithm serves to randomize the order of the output - i.e. where the simple incrementation algorithm would output [0...m-1] repeatedly, this would output something more random. And it does - I'm just not sure how to test how random this output is.


Anyways - what are your thoughts? Reasonably random for an algorithm that can be done mentally or on a small scrap of paper? 

What are some twists I could introduce to increase the randomness? Perhaps the seed could serve to permute the input in some fashion? If most of the structure comes from the order of the words and the order of the letters, I could define a seed that messes with each?",12,1522489307.0,88fhhr,False,"I was curious how I would roll, say, a 20-sided die given nothing but my mind or a pencil or paper. Humans are notorious for being very skewed at guessing random numbers, and I couldn't find many good algorithms after a rudimentary Google search, so I decided to come up with one.

I turned to the most natural thing - language - and not requiring the user to know anything but the letters of the alphabet and a couple of words. Most mental-math random number generation algorithms require memorizing a specific set of numbers or steps - who has time for that?

Looking at the environment and ""counting the number of X"" doesn't work either, as that biases us towards objects that occur more than once, and doesn't provide an even distribution.

**The algorithm and results are detailed [here.](https://gist.github.com/tusing/3de5682785a34fbda67785a4a3256050)** Here's the algorithm if you're too lazy to read it:

1. Start with a seed (1). The seed serves to ""balance"" our output distribution, whereas the rest of the algorithm allows for a relatively random output order.

2. Start with a mod space (must be odd). If you want an even mod space, start with an odd one and take the first n-1 values.

3. Think of any normal English word or sentence. Lowercase letters only.

4. Add up all the letters in sentence (indexed by their order in the alphabet).
5. Increment the seed and add it to this sum.

6. Mod the sum by the space you want the random number to be in.

I feel a bit iffy about the idea of a seed and incrementing it, that's obviously the source of this ""even"" distribution we're getting as the results describe. After all, if you start with a number and increment it N times, mod m, you'll have an even N/m items in every bin! 

But with very predictable output. Thus, the rest of the algorithm serves to randomize the order of the output - i.e. where the simple incrementation algorithm would output [0...m-1] repeatedly, this would output something more random. And it does - I'm just not sure how to test how random this output is.


Anyways - what are your thoughts? Reasonably random for an algorithm that can be done mentally or on a small scrap of paper? 

What are some twists I could introduce to increase the randomness? Perhaps the seed could serve to permute the input in some fashion? If most of the structure comes from the order of the words and the order of the letters, I could define a seed that messes with each?",0,"I was curious how I would roll, say, a 20-sided die given nothing but my mind or a pencil or paper. Humans are notorious for being very skewed at guessing random numbers, and I couldn't find many good algorithms after a rudimentary Google search, so I decided to come up with one.

I turned to the most natural thing - language - and not requiring the user to know anything but the letters of the alphabet and a couple of words. Most mental-math random number generation algorithms require memorizing a specific set of numbers or steps - who has time for that?

Looking at the environment and ""counting the number of X"" doesn't work either, as that biases us towards objects that occur more than once, and doesn't provide an even distribution.

**The algorithm and results are detailed [here.](https://gist.github.com/tusing/3de5682785a34fbda67785a4a3256050)** Here's the algorithm if you're too lazy to read it:

1. Start with a seed (1). The seed serves to ""balance"" our output distribution, whereas the rest of the algorithm allows for a relatively random output order.

2. Start with a mod space (must be odd). If you want an even mod space, start with an odd one and take the first n-1 values.

3. Think of any normal English word or sentence. Lowercase letters only.

4. Add up all the letters in sentence (indexed by their order in the alphabet).
5. Increment the seed and add it to this sum.

6. Mod the sum by the space you want the random number to be in.

I feel a bit iffy about the idea of a seed and incrementing it, that's obviously the source of this ""even"" distribution we're getting as the results describe. After all, if you start with a number and increment it N times, mod m, you'll have an even N/m items in every bin! 

But with very predictable output. Thus, the rest of the algorithm serves to randomize the order of the output - i.e. where the simple incrementation algorithm would output [0...m-1] repeatedly, this would output something more random. And it does - I'm just not sure how to test how random this output is.


Anyways - what are your thoughts? Reasonably random for an algorithm that can be done mentally or on a small scrap of paper? 

What are some twists I could introduce to increase the randomness? Perhaps the seed could serve to permute the input in some fashion? If most of the structure comes from the order of the words and the order of the letters, I could define a seed that messes with each?",5,statistics,54935,,"Question about how random my ""mental-math random number algorithm"" really is",https://www.reddit.com/r/statistics/comments/88fhhr/question_about_how_random_my_mentalmath_random/,all_ads,2018-03-31 05:41:47,63 days 19:53:47.036584000,
"Hopefully this doesn’t read like /r/woahdude, but I’m curious about the Fermi paradox. I’ve read about a heuristic that states when you have a sample of one you should assume that it is the most common example of the population. What other heuristics apply, and what guesses using these heuristics can we make about alien life using us as our single data point?",5,1522457634.0,88btlf,False,"Hopefully this doesn’t read like /r/woahdude, but I’m curious about the Fermi paradox. I’ve read about a heuristic that states when you have a sample of one you should assume that it is the most common example of the population. What other heuristics apply, and what guesses using these heuristics can we make about alien life using us as our single data point?",0,"Hopefully this doesn’t read like /r/woahdude, but I’m curious about the Fermi paradox. I’ve read about a heuristic that states when you have a sample of one you should assume that it is the most common example of the population. What other heuristics apply, and what guesses using these heuristics can we make about alien life using us as our single data point?",8,statistics,54935,,What are the most commonly used heuristics when you have to still estimate probability but have zero degrees of freedom?,https://www.reddit.com/r/statistics/comments/88btlf/what_are_the_most_commonly_used_heuristics_when/,all_ads,2018-03-30 20:53:54,64 days 04:41:40.036584000,
"The weights are inverse probability of treatment weights, not sampling weights or some other type, and the models being compared are multilevel negative binomial and poisson models. ",0,1522499507.0,88ge67,False,"The weights are inverse probability of treatment weights, not sampling weights or some other type, and the models being compared are multilevel negative binomial and poisson models. ",0,"The weights are inverse probability of treatment weights, not sampling weights or some other type, and the models being compared are multilevel negative binomial and poisson models. ",0,statistics,54935,,Is it appropriate to use the likelihood ratio test to compare models that are weighted?,https://www.reddit.com/r/statistics/comments/88ge67/is_it_appropriate_to_use_the_likelihood_ratio/,all_ads,2018-03-31 08:31:47,63 days 17:03:47.036584000,
"I'm doing research on review scores (from film to videogames and everthing in between)

I'm asking reviewers, industry people, etc

What online tool is best to use? (pref freeware)",4,1522491605.0,88fp7f,False,"I'm doing research on review scores (from film to videogames and everthing in between)

I'm asking reviewers, industry people, etc

What online tool is best to use? (pref freeware)",0,"I'm doing research on review scores (from film to videogames and everthing in between)

I'm asking reviewers, industry people, etc

What online tool is best to use? (pref freeware)",0,statistics,54935,,What's the best free online survey tool?,https://www.reddit.com/r/statistics/comments/88fp7f/whats_the_best_free_online_survey_tool/,all_ads,2018-03-31 06:20:05,63 days 19:15:29.036584000,
"For customers with more than 2 years tenure, I want to see if their product usage behavior is seasonal.

What else do I need to determine on top of a ACF with a stat sig lag(12)? (It's monthly data).

I want to test for a unit root to make sure my data is stationary before I look at the ACF, but the augmented DF and Phillips Perron give different conclusions and I'm not sure what is right for my purpose.

There typically is an increasing trend in addition to seasonality, and the increasing trend isn't a problem as long as it's not going to throw off the ACF results.

Thoughts?  This is something I'm automating (I want to be able to flag all customers as seasonal or not seasonal)

And I'd like to do it in Python, but will gladly do it in R if what I need isn't available in Python.

Please help!",1,1522491203.0,88fnw0,False,"For customers with more than 2 years tenure, I want to see if their product usage behavior is seasonal.

What else do I need to determine on top of a ACF with a stat sig lag(12)? (It's monthly data).

I want to test for a unit root to make sure my data is stationary before I look at the ACF, but the augmented DF and Phillips Perron give different conclusions and I'm not sure what is right for my purpose.

There typically is an increasing trend in addition to seasonality, and the increasing trend isn't a problem as long as it's not going to throw off the ACF results.

Thoughts?  This is something I'm automating (I want to be able to flag all customers as seasonal or not seasonal)

And I'd like to do it in Python, but will gladly do it in R if what I need isn't available in Python.

Please help!",0,"For customers with more than 2 years tenure, I want to see if their product usage behavior is seasonal.

What else do I need to determine on top of a ACF with a stat sig lag(12)? (It's monthly data).

I want to test for a unit root to make sure my data is stationary before I look at the ACF, but the augmented DF and Phillips Perron give different conclusions and I'm not sure what is right for my purpose.

There typically is an increasing trend in addition to seasonality, and the increasing trend isn't a problem as long as it's not going to throw off the ACF results.

Thoughts?  This is something I'm automating (I want to be able to flag all customers as seasonal or not seasonal)

And I'd like to do it in Python, but will gladly do it in R if what I need isn't available in Python.

Please help!",0,statistics,54935,,Finding seasonal customers,https://www.reddit.com/r/statistics/comments/88fnw0/finding_seasonal_customers/,all_ads,2018-03-31 06:13:23,63 days 19:22:11.036584000,
"So, as the sample size approaches infinity, the estimate of the mean of data approaches the true mean, and the deviation becomes tighter, or more accurate with the confidence intervals.

Given the above, is this the reason why predictive algorithms for sites like youtube, or music streaming sites have increasingly destroyed their usefulness for people who are unsure what they want to watch?

There is a limit to what videos I care to watch. I specifically like a particular content creator, not necessarily an entire game. However, youtube has recently been restricting results that I see in suggestions to things either directly related to the last video I watched, or directly related to videos I watch normally. It has become less useful. There used to be a fun game where you click on suggestions until you got to some really random video, and the comments were always filled with 'I've found the dark place on youtube again' kind of comments, or 'How the hell did I end up here?'

Nowadays, the suggestions are the same 200 videos, ordered based upon the last video you watched it feels like. Is this due to machine learning creating these very tight confidence intervals of what content I want to view?

I can't find new things on youtube anymore without specifically wanting to find them, and it is sad. There was an old algorithm that did magical things with suggestions where the level of vagueness in the relation to a previous video was much more entertaining to sift through.

Not sure if this is a good place to ask this question, but I figured it was something that could be discussed from a statistical perspective.",5,1522465124.0,88csl5,False,"So, as the sample size approaches infinity, the estimate of the mean of data approaches the true mean, and the deviation becomes tighter, or more accurate with the confidence intervals.

Given the above, is this the reason why predictive algorithms for sites like youtube, or music streaming sites have increasingly destroyed their usefulness for people who are unsure what they want to watch?

There is a limit to what videos I care to watch. I specifically like a particular content creator, not necessarily an entire game. However, youtube has recently been restricting results that I see in suggestions to things either directly related to the last video I watched, or directly related to videos I watch normally. It has become less useful. There used to be a fun game where you click on suggestions until you got to some really random video, and the comments were always filled with 'I've found the dark place on youtube again' kind of comments, or 'How the hell did I end up here?'

Nowadays, the suggestions are the same 200 videos, ordered based upon the last video you watched it feels like. Is this due to machine learning creating these very tight confidence intervals of what content I want to view?

I can't find new things on youtube anymore without specifically wanting to find them, and it is sad. There was an old algorithm that did magical things with suggestions where the level of vagueness in the relation to a previous video was much more entertaining to sift through.

Not sure if this is a good place to ask this question, but I figured it was something that could be discussed from a statistical perspective.",0,"So, as the sample size approaches infinity, the estimate of the mean of data approaches the true mean, and the deviation becomes tighter, or more accurate with the confidence intervals.

Given the above, is this the reason why predictive algorithms for sites like youtube, or music streaming sites have increasingly destroyed their usefulness for people who are unsure what they want to watch?

There is a limit to what videos I care to watch. I specifically like a particular content creator, not necessarily an entire game. However, youtube has recently been restricting results that I see in suggestions to things either directly related to the last video I watched, or directly related to videos I watch normally. It has become less useful. There used to be a fun game where you click on suggestions until you got to some really random video, and the comments were always filled with 'I've found the dark place on youtube again' kind of comments, or 'How the hell did I end up here?'

Nowadays, the suggestions are the same 200 videos, ordered based upon the last video you watched it feels like. Is this due to machine learning creating these very tight confidence intervals of what content I want to view?

I can't find new things on youtube anymore without specifically wanting to find them, and it is sad. There was an old algorithm that did magical things with suggestions where the level of vagueness in the relation to a previous video was much more entertaining to sift through.

Not sure if this is a good place to ask this question, but I figured it was something that could be discussed from a statistical perspective.",2,statistics,54935,,Question about the sample size of data limiting the usefulness of sites like Youtube and music streaming sites,https://www.reddit.com/r/statistics/comments/88csl5/question_about_the_sample_size_of_data_limiting/,all_ads,2018-03-30 22:58:44,64 days 02:36:50.036584000,
"In my classes there is always some mention of AIC/BIC. Certainly model complexity is an important thing to penalize for in the sense that you don't want to include variables that are unnecessary. 

Is there any research into just how much you should penalize complexity, is it ever a good thing? Do domain-specific fields in Genetics or Neuroscience treat model complexity differently, for example? 

Have there been really complex models that - counter to our intuition - have been accepted by the academic community? ",7,1522406458.0,8876mk,False,"In my classes there is always some mention of AIC/BIC. Certainly model complexity is an important thing to penalize for in the sense that you don't want to include variables that are unnecessary. 

Is there any research into just how much you should penalize complexity, is it ever a good thing? Do domain-specific fields in Genetics or Neuroscience treat model complexity differently, for example? 

Have there been really complex models that - counter to our intuition - have been accepted by the academic community? ",0,"In my classes there is always some mention of AIC/BIC. Certainly model complexity is an important thing to penalize for in the sense that you don't want to include variables that are unnecessary. 

Is there any research into just how much you should penalize complexity, is it ever a good thing? Do domain-specific fields in Genetics or Neuroscience treat model complexity differently, for example? 

Have there been really complex models that - counter to our intuition - have been accepted by the academic community? ",19,statistics,54935,,Are information criterions still an active area of research? Are parsimonious models always better?,https://www.reddit.com/r/statistics/comments/8876mk/are_information_criterions_still_an_active_area/,all_ads,2018-03-30 06:40:58,64 days 18:54:36.036584000,
" Originally, I had planned on running an ANOVA. However, after a test run of my test and reading about ANOVA versus t-test, I thought that I should be using a t-test because my independent variable has only two groups. I just finished a meeting with a member of my thesis committee to finalize my data plan before I start running tests on SPSS. He first seemed to think I should use an ANOVA. I explained why I thought I should use a t-test and he repeated that I should use an ANOVA because of the three groups. I told him that I didn't understand and we talked through it at which point he told me to use a t-test. 

Can I get some feedback from you all? I am currently reading a textbook about ANOVA and t-tests because I am concerned that there is something I don't understand about why he first told me to use an ANOVA.

I am looking at test scores from 2nd to 10th grade in three subject areas. The independent variable is whether students were enrolled in a specific program that lasted from K - 4th grade. So the two groups are Enrolled and Not Enrolled. I am then looking at their test scores to see if enrollment in the program is correlated with higher, equal, or lower scores. I am NOT comparing the scores between grades or between subject areas.",6,1522447996.0,88amh4,False," Originally, I had planned on running an ANOVA. However, after a test run of my test and reading about ANOVA versus t-test, I thought that I should be using a t-test because my independent variable has only two groups. I just finished a meeting with a member of my thesis committee to finalize my data plan before I start running tests on SPSS. He first seemed to think I should use an ANOVA. I explained why I thought I should use a t-test and he repeated that I should use an ANOVA because of the three groups. I told him that I didn't understand and we talked through it at which point he told me to use a t-test. 

Can I get some feedback from you all? I am currently reading a textbook about ANOVA and t-tests because I am concerned that there is something I don't understand about why he first told me to use an ANOVA.

I am looking at test scores from 2nd to 10th grade in three subject areas. The independent variable is whether students were enrolled in a specific program that lasted from K - 4th grade. So the two groups are Enrolled and Not Enrolled. I am then looking at their test scores to see if enrollment in the program is correlated with higher, equal, or lower scores. I am NOT comparing the scores between grades or between subject areas.",0," Originally, I had planned on running an ANOVA. However, after a test run of my test and reading about ANOVA versus t-test, I thought that I should be using a t-test because my independent variable has only two groups. I just finished a meeting with a member of my thesis committee to finalize my data plan before I start running tests on SPSS. He first seemed to think I should use an ANOVA. I explained why I thought I should use a t-test and he repeated that I should use an ANOVA because of the three groups. I told him that I didn't understand and we talked through it at which point he told me to use a t-test. 

Can I get some feedback from you all? I am currently reading a textbook about ANOVA and t-tests because I am concerned that there is something I don't understand about why he first told me to use an ANOVA.

I am looking at test scores from 2nd to 10th grade in three subject areas. The independent variable is whether students were enrolled in a specific program that lasted from K - 4th grade. So the two groups are Enrolled and Not Enrolled. I am then looking at their test scores to see if enrollment in the program is correlated with higher, equal, or lower scores. I am NOT comparing the scores between grades or between subject areas.",1,statistics,54935,,ANOVA or t-test?,https://www.reddit.com/r/statistics/comments/88amh4/anova_or_ttest/,all_ads,2018-03-30 18:13:16,64 days 07:22:18.036584000,
"Hi!

I'm doing my master's thesis and I'm really not that good in statistics.

I have surveyed two groups from, one from each country, asking them what's important to them when buying certain products. They needed to rate different factors (price, quality, etc.) from 1 to 5. My hypothesis is that the marketing approach of the company needs to be different for each group.

How do I go about testing this? The mean values don't help that much, as I need to find out which factors are most important to the certain group and then somehow statistically prove, that they differ (or not).

Thanks!",6,1522444124.0,88a791,False,"Hi!

I'm doing my master's thesis and I'm really not that good in statistics.

I have surveyed two groups from, one from each country, asking them what's important to them when buying certain products. They needed to rate different factors (price, quality, etc.) from 1 to 5. My hypothesis is that the marketing approach of the company needs to be different for each group.

How do I go about testing this? The mean values don't help that much, as I need to find out which factors are most important to the certain group and then somehow statistically prove, that they differ (or not).

Thanks!",0,"Hi!

I'm doing my master's thesis and I'm really not that good in statistics.

I have surveyed two groups from, one from each country, asking them what's important to them when buying certain products. They needed to rate different factors (price, quality, etc.) from 1 to 5. My hypothesis is that the marketing approach of the company needs to be different for each group.

How do I go about testing this? The mean values don't help that much, as I need to find out which factors are most important to the certain group and then somehow statistically prove, that they differ (or not).

Thanks!",1,statistics,54935,,Comparing two groups' answers to same question,https://www.reddit.com/r/statistics/comments/88a791/comparing_two_groups_answers_to_same_question/,all_ads,2018-03-30 17:08:44,64 days 08:26:50.036584000,
"I am researching the effect of an innovation type on firm performance. In my quantile regression model, the relationship between innovation and firm performance is non-significant (p=,7). However, the moderator firm size is highly significant (P<,001). 
Is this possible? And how should this be interpreted? I don't get how firm size should influence the relationship of innovation and firm performance, if there initially is no relationship... Thanks!",5,1522457575.0,88btaw,False,"I am researching the effect of an innovation type on firm performance. In my quantile regression model, the relationship between innovation and firm performance is non-significant (p=,7). However, the moderator firm size is highly significant (P<,001). 
Is this possible? And how should this be interpreted? I don't get how firm size should influence the relationship of innovation and firm performance, if there initially is no relationship... Thanks!",0,"I am researching the effect of an innovation type on firm performance. In my quantile regression model, the relationship between innovation and firm performance is non-significant (p=,7). However, the moderator firm size is highly significant (P<,001). 
Is this possible? And how should this be interpreted? I don't get how firm size should influence the relationship of innovation and firm performance, if there initially is no relationship... Thanks!",0,statistics,54935,,Regression: IV-DV non.sig. but moderation highly sig.,https://www.reddit.com/r/statistics/comments/88btaw/regression_ivdv_nonsig_but_moderation_highly_sig/,all_ads,2018-03-30 20:52:55,64 days 04:42:39.036584000,
"Hey guys, 
I am doing a math and stats double major at my uni. I am a 2nd year and eager to learn about the whys and the hows and all the best applications of it. I would one day love to look at data and make predictions based on it or maybe make sense of it and interpt and give advice. It theres alot of room for freedom with these fields. That being said, anyone got any recommendations on any excellent books for some beginners? I have taken all the calculus sequences and taking linear algebra this summer.  Thanks on advance.",13,1522400361.0,886juz,False,"Hey guys, 
I am doing a math and stats double major at my uni. I am a 2nd year and eager to learn about the whys and the hows and all the best applications of it. I would one day love to look at data and make predictions based on it or maybe make sense of it and interpt and give advice. It theres alot of room for freedom with these fields. That being said, anyone got any recommendations on any excellent books for some beginners? I have taken all the calculus sequences and taking linear algebra this summer.  Thanks on advance.",0,"Hey guys, 
I am doing a math and stats double major at my uni. I am a 2nd year and eager to learn about the whys and the hows and all the best applications of it. I would one day love to look at data and make predictions based on it or maybe make sense of it and interpt and give advice. It theres alot of room for freedom with these fields. That being said, anyone got any recommendations on any excellent books for some beginners? I have taken all the calculus sequences and taking linear algebra this summer.  Thanks on advance.",7,statistics,54935,,Double major in math and stats,https://www.reddit.com/r/statistics/comments/886juz/double_major_in_math_and_stats/,all_ads,2018-03-30 04:59:21,64 days 20:36:13.036584000,
"This seems to be the new ""it"" thing in data. I have been told by the higher-ups that this is an amazing thing that chooses ""the best model"". I am of course concerned about overfitting, but I'm not even sure what these softwares are even doing. Is it just a regression? Are they just choosing variables based on the p-value? Does it consider the ""why""? I do think that it can be useful for data management, but I'm skeptical about it replacing statisticians.",14,1522397010.0,8866o9,False,"This seems to be the new ""it"" thing in data. I have been told by the higher-ups that this is an amazing thing that chooses ""the best model"". I am of course concerned about overfitting, but I'm not even sure what these softwares are even doing. Is it just a regression? Are they just choosing variables based on the p-value? Does it consider the ""why""? I do think that it can be useful for data management, but I'm skeptical about it replacing statisticians.",0,"This seems to be the new ""it"" thing in data. I have been told by the higher-ups that this is an amazing thing that chooses ""the best model"". I am of course concerned about overfitting, but I'm not even sure what these softwares are even doing. Is it just a regression? Are they just choosing variables based on the p-value? Does it consider the ""why""? I do think that it can be useful for data management, but I'm skeptical about it replacing statisticians.",8,statistics,54935,,What is your opinion on automation or machine learning software?,https://www.reddit.com/r/statistics/comments/8866o9/what_is_your_opinion_on_automation_or_machine/,all_ads,2018-03-30 04:03:30,64 days 21:32:04.036584000,
"I'm not sure if there is a test but I feel like there should be: Say I have a survey with 4 categorical options for favourite food group (meat, milk, veggies, bread). I survey 1000 people and the number of responses come back as follows:

* Meat 480
* Veggies 450
* Milk 40
* Bread 30

How do I test the hypothesis that the preference for meat is actually higher than veggies in the population?",3,1522455805.0,88bl0s,False,"I'm not sure if there is a test but I feel like there should be: Say I have a survey with 4 categorical options for favourite food group (meat, milk, veggies, bread). I survey 1000 people and the number of responses come back as follows:

* Meat 480
* Veggies 450
* Milk 40
* Bread 30

How do I test the hypothesis that the preference for meat is actually higher than veggies in the population?",0,"I'm not sure if there is a test but I feel like there should be: Say I have a survey with 4 categorical options for favourite food group (meat, milk, veggies, bread). I survey 1000 people and the number of responses come back as follows:

* Meat 480
* Veggies 450
* Milk 40
* Bread 30

How do I test the hypothesis that the preference for meat is actually higher than veggies in the population?",0,statistics,54935,,What statistical test am I thinking of?,https://www.reddit.com/r/statistics/comments/88bl0s/what_statistical_test_am_i_thinking_of/,all_ads,2018-03-30 20:23:25,64 days 05:12:09.036584000,
I am currently a freshman in college and am majoring in mathematics and minoring in statistics. Is it difficult to get a job with just a bachelor's?,17,1522394243.0,885vjn,False,I am currently a freshman in college and am majoring in mathematics and minoring in statistics. Is it difficult to get a job with just a bachelor's?,0,I am currently a freshman in college and am majoring in mathematics and minoring in statistics. Is it difficult to get a job with just a bachelor's?,7,statistics,54935,,Is it hard to get a job with just a bachelor's degree in math and a minor in statistics?,https://www.reddit.com/r/statistics/comments/885vjn/is_it_hard_to_get_a_job_with_just_a_bachelors/,all_ads,2018-03-30 03:17:23,64 days 22:18:11.036584000,
"I fit the following model:

    Y ~ a + b*age + c*treatment + d*(age*treatment)

Treatment and age were quantitative variables that were centered to sample means prior to fitting model. So in this model, ""c"" is effect of treatment at average age in the sample. If d>0, then treatment effect is higher at older ages than younger ages. And if d<0, then treatment effect is higher at younger ages than older ages. 

My hypothesis was that the treatment had an effect on Y and/or the treatment effect was heterogeneous with age. So I tested the null hypothesis c=d=0, which means there is no treatment effect any age (using F-test with 2 and n-4 degrees of freedom).

This F-test was significant. The estimates of c and d were each positive, which would seem to suggest that the treatment has a positive effect on Y at average age in the sample, and this effect is higher at older ages than younger ages.

Then, I also performed tests of c and d separately. Neither test was significant. So this means:

1. I failed to reject the null hypothesis that the treatment has no effect at the average age in the sample (H0: c=0)

2. I also failed to reject the null hypothesis that the treatment effect is the same across all age levels (H0: d=0).

3. But I rejected the null hypothesis that the treatment has no effect age zero AND treatment effect is the same across all age levels.

So, this result is tricky to interpret because data provides evidence of treatment effect. What can I say about this kind of result? I can claim that the data provides evidence for treatment had either a main effect or it was heterogeneous with age, but not point to specific one.


I plotted the total estimated treatment effect, along with 95% confidence bands, function of age. It is the line c+d*age on vertical axis  and age on horizontal axis (with variable age=0 correspondsing to sample mean). Its slope d and y-intercept c are both positive:

1. Among younger subjects, the confidence bands intersect with the x-axis implying the treatment was not significantly associated with outcome at those age levels. 
2. Among olderr subjects, the confidence bands did not intersect with the x-axis implying the treatment **was** significantly associated with outcome at those age levels. 

Any thoughts?",4,1522410323.0,887k73,False,"I fit the following model:

    Y ~ a + b*age + c*treatment + d*(age*treatment)

Treatment and age were quantitative variables that were centered to sample means prior to fitting model. So in this model, ""c"" is effect of treatment at average age in the sample. If d>0, then treatment effect is higher at older ages than younger ages. And if d<0, then treatment effect is higher at younger ages than older ages. 

My hypothesis was that the treatment had an effect on Y and/or the treatment effect was heterogeneous with age. So I tested the null hypothesis c=d=0, which means there is no treatment effect any age (using F-test with 2 and n-4 degrees of freedom).

This F-test was significant. The estimates of c and d were each positive, which would seem to suggest that the treatment has a positive effect on Y at average age in the sample, and this effect is higher at older ages than younger ages.

Then, I also performed tests of c and d separately. Neither test was significant. So this means:

1. I failed to reject the null hypothesis that the treatment has no effect at the average age in the sample (H0: c=0)

2. I also failed to reject the null hypothesis that the treatment effect is the same across all age levels (H0: d=0).

3. But I rejected the null hypothesis that the treatment has no effect age zero AND treatment effect is the same across all age levels.

So, this result is tricky to interpret because data provides evidence of treatment effect. What can I say about this kind of result? I can claim that the data provides evidence for treatment had either a main effect or it was heterogeneous with age, but not point to specific one.


I plotted the total estimated treatment effect, along with 95% confidence bands, function of age. It is the line c+d*age on vertical axis  and age on horizontal axis (with variable age=0 correspondsing to sample mean). Its slope d and y-intercept c are both positive:

1. Among younger subjects, the confidence bands intersect with the x-axis implying the treatment was not significantly associated with outcome at those age levels. 
2. Among olderr subjects, the confidence bands did not intersect with the x-axis implying the treatment **was** significantly associated with outcome at those age levels. 

Any thoughts?",0,"I fit the following model:

    Y ~ a + b*age + c*treatment + d*(age*treatment)

Treatment and age were quantitative variables that were centered to sample means prior to fitting model. So in this model, ""c"" is effect of treatment at average age in the sample. If d>0, then treatment effect is higher at older ages than younger ages. And if d<0, then treatment effect is higher at younger ages than older ages. 

My hypothesis was that the treatment had an effect on Y and/or the treatment effect was heterogeneous with age. So I tested the null hypothesis c=d=0, which means there is no treatment effect any age (using F-test with 2 and n-4 degrees of freedom).

This F-test was significant. The estimates of c and d were each positive, which would seem to suggest that the treatment has a positive effect on Y at average age in the sample, and this effect is higher at older ages than younger ages.

Then, I also performed tests of c and d separately. Neither test was significant. So this means:

1. I failed to reject the null hypothesis that the treatment has no effect at the average age in the sample (H0: c=0)

2. I also failed to reject the null hypothesis that the treatment effect is the same across all age levels (H0: d=0).

3. But I rejected the null hypothesis that the treatment has no effect age zero AND treatment effect is the same across all age levels.

So, this result is tricky to interpret because data provides evidence of treatment effect. What can I say about this kind of result? I can claim that the data provides evidence for treatment had either a main effect or it was heterogeneous with age, but not point to specific one.


I plotted the total estimated treatment effect, along with 95% confidence bands, function of age. It is the line c+d*age on vertical axis  and age on horizontal axis (with variable age=0 correspondsing to sample mean). Its slope d and y-intercept c are both positive:

1. Among younger subjects, the confidence bands intersect with the x-axis implying the treatment was not significantly associated with outcome at those age levels. 
2. Among olderr subjects, the confidence bands did not intersect with the x-axis implying the treatment **was** significantly associated with outcome at those age levels. 

Any thoughts?",3,statistics,54935,,How to explain an overall effect in interaction model?,https://www.reddit.com/r/statistics/comments/887k73/how_to_explain_an_overall_effect_in_interaction/,all_ads,2018-03-30 07:45:23,64 days 17:50:11.036584000,
"Hi All

I need some help:

I'm trying to forecast sales for some electronics products, and I have the following information:

1.Historical sales
2. Historical prices
3. Historical prices for other products in the category

I can provide the future prices of the product as well as future prices of the other products in the category too.

Can you please suggest what the easiest way is for me to forecast for this? I only have excel as a tool. I know there are other tools such as R and the like but I am short on time.

It will most likely require some kind of multiple regression but considering my lack of command on stats, I am struggling to implement this.

You would see that I have mentioned prices of other products in the category, and that is because if a normally expensive product is slightly reduced in price, it will impact the sales of the current product as the value proposition is not the same for the customer, so it is important to look at all products prices as well.

Your help on this matter is very much appreciated.

If you reckon it would be impossible in Excel, what is the next best alternative that is easiest to implement?

Thank you for reading.",2,1522429812.0,8891n1,False,"Hi All

I need some help:

I'm trying to forecast sales for some electronics products, and I have the following information:

1.Historical sales
2. Historical prices
3. Historical prices for other products in the category

I can provide the future prices of the product as well as future prices of the other products in the category too.

Can you please suggest what the easiest way is for me to forecast for this? I only have excel as a tool. I know there are other tools such as R and the like but I am short on time.

It will most likely require some kind of multiple regression but considering my lack of command on stats, I am struggling to implement this.

You would see that I have mentioned prices of other products in the category, and that is because if a normally expensive product is slightly reduced in price, it will impact the sales of the current product as the value proposition is not the same for the customer, so it is important to look at all products prices as well.

Your help on this matter is very much appreciated.

If you reckon it would be impossible in Excel, what is the next best alternative that is easiest to implement?

Thank you for reading.",0,"Hi All

I need some help:

I'm trying to forecast sales for some electronics products, and I have the following information:

1.Historical sales
2. Historical prices
3. Historical prices for other products in the category

I can provide the future prices of the product as well as future prices of the other products in the category too.

Can you please suggest what the easiest way is for me to forecast for this? I only have excel as a tool. I know there are other tools such as R and the like but I am short on time.

It will most likely require some kind of multiple regression but considering my lack of command on stats, I am struggling to implement this.

You would see that I have mentioned prices of other products in the category, and that is because if a normally expensive product is slightly reduced in price, it will impact the sales of the current product as the value proposition is not the same for the customer, so it is important to look at all products prices as well.

Your help on this matter is very much appreciated.

If you reckon it would be impossible in Excel, what is the next best alternative that is easiest to implement?

Thank you for reading.",0,statistics,54935,,forecasting using price,https://www.reddit.com/r/statistics/comments/8891n1/forecasting_using_price/,all_ads,2018-03-30 13:10:12,64 days 12:25:22.036584000,
"As an initial disclaimer, I have very little background in statistics. 

I was helping an engineering student with his homework the other day, and he was being asked to compute the p-value of an observation about average cholesterol. The context was testing to see if the average cholesterol of a certain group was higher than average. 

My understanding of p-values is: you fix a null hypothesis H, make an observation X, then:

p = P(X | H)

When using a two tailed test on a gaussian distributed population with mean mu and standard deviation sigma, the p-value would then be:

p = P(|X-mu|/sigma> c | X ~ N(mu, sigma) )

Where c is your normalized observed value.

So when these students are then asked to do a one tailed test, they just take the two-tailed p-value and double it, i.e. they remove the absolute value bars and compute:

p = P((X-mu)/sigma> c | X ~ N(mu, sigma) )

But this seems very wrong to me, because in the one tailed case, shouldn't the null hypothesis change from:

X ~ N(mu, sigma)

To 

X ~ N(lambda, sigma), where lambda <=mu

Because now your null hypothesis is that the true mean of your sample population is <=mu, so that rejecting it would mean you can conclude that it is bigger.

This second null hypothesis seems like it is the correct one in this case, but maybe slightly harder for students to compute, and it also seems like it should result in stronger p-values. Am I missing something, or is the philosophy that the simpler approach results in weaker conclusions, and is therefore harmless to adopt?",15,1522371845.0,882z76,False,"As an initial disclaimer, I have very little background in statistics. 

I was helping an engineering student with his homework the other day, and he was being asked to compute the p-value of an observation about average cholesterol. The context was testing to see if the average cholesterol of a certain group was higher than average. 

My understanding of p-values is: you fix a null hypothesis H, make an observation X, then:

p = P(X | H)

When using a two tailed test on a gaussian distributed population with mean mu and standard deviation sigma, the p-value would then be:

p = P(|X-mu|/sigma> c | X ~ N(mu, sigma) )

Where c is your normalized observed value.

So when these students are then asked to do a one tailed test, they just take the two-tailed p-value and double it, i.e. they remove the absolute value bars and compute:

p = P((X-mu)/sigma> c | X ~ N(mu, sigma) )

But this seems very wrong to me, because in the one tailed case, shouldn't the null hypothesis change from:

X ~ N(mu, sigma)

To 

X ~ N(lambda, sigma), where lambda <=mu

Because now your null hypothesis is that the true mean of your sample population is <=mu, so that rejecting it would mean you can conclude that it is bigger.

This second null hypothesis seems like it is the correct one in this case, but maybe slightly harder for students to compute, and it also seems like it should result in stronger p-values. Am I missing something, or is the philosophy that the simpler approach results in weaker conclusions, and is therefore harmless to adopt?",0,"As an initial disclaimer, I have very little background in statistics. 

I was helping an engineering student with his homework the other day, and he was being asked to compute the p-value of an observation about average cholesterol. The context was testing to see if the average cholesterol of a certain group was higher than average. 

My understanding of p-values is: you fix a null hypothesis H, make an observation X, then:

p = P(X | H)

When using a two tailed test on a gaussian distributed population with mean mu and standard deviation sigma, the p-value would then be:

p = P(|X-mu|/sigma> c | X ~ N(mu, sigma) )

Where c is your normalized observed value.

So when these students are then asked to do a one tailed test, they just take the two-tailed p-value and double it, i.e. they remove the absolute value bars and compute:

p = P((X-mu)/sigma> c | X ~ N(mu, sigma) )

But this seems very wrong to me, because in the one tailed case, shouldn't the null hypothesis change from:

X ~ N(mu, sigma)

To 

X ~ N(lambda, sigma), where lambda <=mu

Because now your null hypothesis is that the true mean of your sample population is <=mu, so that rejecting it would mean you can conclude that it is bigger.

This second null hypothesis seems like it is the correct one in this case, but maybe slightly harder for students to compute, and it also seems like it should result in stronger p-values. Am I missing something, or is the philosophy that the simpler approach results in weaker conclusions, and is therefore harmless to adopt?",6,statistics,54935,,A question about one tailed p-values,https://www.reddit.com/r/statistics/comments/882z76/a_question_about_one_tailed_pvalues/,all_ads,2018-03-29 21:04:05,65 days 04:31:29.036584000,
,5,1522406457.0,8876mg,False,,0,,0,statistics,54935,,How do you decide what K to use in a K-fold?,https://www.reddit.com/r/statistics/comments/8876mg/how_do_you_decide_what_k_to_use_in_a_kfold/,all_ads,2018-03-30 06:40:57,64 days 18:54:37.036584000,
"I recently came across a problem in some work I was doing and I was wondering how incorrect the following would be:

I ran 2 models with 1 DV and 1000 IVs. the IVs were exactly the same and only the DV changed.

Now for the vector of IV's, the first 500 IVs came from one source of data and the last 500 came from another data source.

Now, the researchers wanted to know if for each of the dependent variables they received more ""influence"" or had more of a preference for data source 1 than data source 2. They had a theory that the DV for model 1 would be more impacted by the data from datasource 1 and the DV for model2 would be more impacted by datasource 2. All IV data was dichotomous (coded 0/1) and the DV data was on the same scale.

One suggestion put forward that I did not have a answer for was to stack the coefficients from the two models, create a dummy code for model they came from (1 or 2), a dummy code for which data source the coefficient comes from (source 1 or source 2) and an interaction between the two. The interaction would then be able to say if coefficients from one source would increase coefficients more for one model than the other.

What would be the problems for such a method?

Here is a more in depth explanation of the experiment just so someone doesn't spend too long responding without all the information.

The study is a classic machine learning experiment of ""will the machine beat the judge"" but in an attempt to get machine learning more in the world of academia.  Participants submitted a resume and then experts rated them on how well they believed they would do at a certain task based on their resume. The participants completed the task and got a score. A random forest model was then used to predict how well they would do on the task based on the same resume. No surprise to anyone the random forest model did better than the human rater at predicting participant performance.  A second random forest was then used to predict judge scores based on judge scores (essentially an automatic rating system). Now we have 2 models, one that models the actual outcome and one the models judge scores.

Now, you can derive feature importance from a random forest [here](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/) is a reasonable summary. Now we have feature importance coming from the judge based model (showing where judges put their importance) and the feature importance from the outcome model (showing optimal performance based on the random forest). Now comes the comparison... we want to figure out where the judges went wrong and how they could change how they evaluate the information. We can see they didn't put the same weight exactly as the outcome model but it is pretty hard to interpret feature importance but what we can do is categorize where the features came from. For example, did the features come from a an easily digestible demographic information or did it come from a more complicated open response question. In general we hypothesized that humans raters would focus more on the easily digestible demographical information than the open response questions which will receive more focus by the outcome model. However, the only way we could think about testing it was doing what is described above: Stack the feature importances and then use the following model

Feature importance = Model + easily_digestible_info + model*easily_digestible_info.

Any other suggestions would be greatly appreciated.",18,1522309492.0,87wpos,False,"I recently came across a problem in some work I was doing and I was wondering how incorrect the following would be:

I ran 2 models with 1 DV and 1000 IVs. the IVs were exactly the same and only the DV changed.

Now for the vector of IV's, the first 500 IVs came from one source of data and the last 500 came from another data source.

Now, the researchers wanted to know if for each of the dependent variables they received more ""influence"" or had more of a preference for data source 1 than data source 2. They had a theory that the DV for model 1 would be more impacted by the data from datasource 1 and the DV for model2 would be more impacted by datasource 2. All IV data was dichotomous (coded 0/1) and the DV data was on the same scale.

One suggestion put forward that I did not have a answer for was to stack the coefficients from the two models, create a dummy code for model they came from (1 or 2), a dummy code for which data source the coefficient comes from (source 1 or source 2) and an interaction between the two. The interaction would then be able to say if coefficients from one source would increase coefficients more for one model than the other.

What would be the problems for such a method?

Here is a more in depth explanation of the experiment just so someone doesn't spend too long responding without all the information.

The study is a classic machine learning experiment of ""will the machine beat the judge"" but in an attempt to get machine learning more in the world of academia.  Participants submitted a resume and then experts rated them on how well they believed they would do at a certain task based on their resume. The participants completed the task and got a score. A random forest model was then used to predict how well they would do on the task based on the same resume. No surprise to anyone the random forest model did better than the human rater at predicting participant performance.  A second random forest was then used to predict judge scores based on judge scores (essentially an automatic rating system). Now we have 2 models, one that models the actual outcome and one the models judge scores.

Now, you can derive feature importance from a random forest [here](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/) is a reasonable summary. Now we have feature importance coming from the judge based model (showing where judges put their importance) and the feature importance from the outcome model (showing optimal performance based on the random forest). Now comes the comparison... we want to figure out where the judges went wrong and how they could change how they evaluate the information. We can see they didn't put the same weight exactly as the outcome model but it is pretty hard to interpret feature importance but what we can do is categorize where the features came from. For example, did the features come from a an easily digestible demographic information or did it come from a more complicated open response question. In general we hypothesized that humans raters would focus more on the easily digestible demographical information than the open response questions which will receive more focus by the outcome model. However, the only way we could think about testing it was doing what is described above: Stack the feature importances and then use the following model

Feature importance = Model + easily_digestible_info + model*easily_digestible_info.

Any other suggestions would be greatly appreciated.",0,"I recently came across a problem in some work I was doing and I was wondering how incorrect the following would be:

I ran 2 models with 1 DV and 1000 IVs. the IVs were exactly the same and only the DV changed.

Now for the vector of IV's, the first 500 IVs came from one source of data and the last 500 came from another data source.

Now, the researchers wanted to know if for each of the dependent variables they received more ""influence"" or had more of a preference for data source 1 than data source 2. They had a theory that the DV for model 1 would be more impacted by the data from datasource 1 and the DV for model2 would be more impacted by datasource 2. All IV data was dichotomous (coded 0/1) and the DV data was on the same scale.

One suggestion put forward that I did not have a answer for was to stack the coefficients from the two models, create a dummy code for model they came from (1 or 2), a dummy code for which data source the coefficient comes from (source 1 or source 2) and an interaction between the two. The interaction would then be able to say if coefficients from one source would increase coefficients more for one model than the other.

What would be the problems for such a method?

Here is a more in depth explanation of the experiment just so someone doesn't spend too long responding without all the information.

The study is a classic machine learning experiment of ""will the machine beat the judge"" but in an attempt to get machine learning more in the world of academia.  Participants submitted a resume and then experts rated them on how well they believed they would do at a certain task based on their resume. The participants completed the task and got a score. A random forest model was then used to predict how well they would do on the task based on the same resume. No surprise to anyone the random forest model did better than the human rater at predicting participant performance.  A second random forest was then used to predict judge scores based on judge scores (essentially an automatic rating system). Now we have 2 models, one that models the actual outcome and one the models judge scores.

Now, you can derive feature importance from a random forest [here](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/) is a reasonable summary. Now we have feature importance coming from the judge based model (showing where judges put their importance) and the feature importance from the outcome model (showing optimal performance based on the random forest). Now comes the comparison... we want to figure out where the judges went wrong and how they could change how they evaluate the information. We can see they didn't put the same weight exactly as the outcome model but it is pretty hard to interpret feature importance but what we can do is categorize where the features came from. For example, did the features come from a an easily digestible demographic information or did it come from a more complicated open response question. In general we hypothesized that humans raters would focus more on the easily digestible demographical information than the open response questions which will receive more focus by the outcome model. However, the only way we could think about testing it was doing what is described above: Stack the feature importances and then use the following model

Feature importance = Model + easily_digestible_info + model*easily_digestible_info.

Any other suggestions would be greatly appreciated.",18,statistics,54935,,Regression Coefficients as independent variables in second model,https://www.reddit.com/r/statistics/comments/87wpos/regression_coefficients_as_independent_variables/,all_ads,2018-03-29 03:44:52,65 days 21:50:42.036584000,
"So I ran a moderated multiple regression to test whether personality domains moderate the relationship between video game content and quality of life (QoL). Results came up not significant. I ran an ANOVA/Levene's test to determine if there was a difference between participants that played violent video games and those that did not play violent video games. Results came up nonsignificant, meaning there's no significant difference between groups in terms of QoL scores (I think I interpreted that correctly). My question is: Does this mean I shouldn't expect there to be any differences between these groups when I run a moderated multiple regression? Thanks!",1,1522365898.0,8825jn,False,"So I ran a moderated multiple regression to test whether personality domains moderate the relationship between video game content and quality of life (QoL). Results came up not significant. I ran an ANOVA/Levene's test to determine if there was a difference between participants that played violent video games and those that did not play violent video games. Results came up nonsignificant, meaning there's no significant difference between groups in terms of QoL scores (I think I interpreted that correctly). My question is: Does this mean I shouldn't expect there to be any differences between these groups when I run a moderated multiple regression? Thanks!",0,"So I ran a moderated multiple regression to test whether personality domains moderate the relationship between video game content and quality of life (QoL). Results came up not significant. I ran an ANOVA/Levene's test to determine if there was a difference between participants that played violent video games and those that did not play violent video games. Results came up nonsignificant, meaning there's no significant difference between groups in terms of QoL scores (I think I interpreted that correctly). My question is: Does this mean I shouldn't expect there to be any differences between these groups when I run a moderated multiple regression? Thanks!",0,statistics,54935,,Levene's test,https://www.reddit.com/r/statistics/comments/8825jn/levenes_test/,all_ads,2018-03-29 19:24:58,65 days 06:10:36.036584000,
What do you think of this new article?: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188299,2,1522323685.0,87y7uv,False,What do you think of this new article?: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188299,0,What do you think of this new article?: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188299,6,statistics,54935,,"[Discussion] Second-generation p-values: Improved rigor, reproducibility, & transparency in statistical analyses.",https://www.reddit.com/r/statistics/comments/87y7uv/discussion_secondgeneration_pvalues_improved/,all_ads,2018-03-29 07:41:25,65 days 17:54:09.036584000,
"Hello, this is my first time here on the sub so excuse me if this post is strange or something.
I’m an adolescent who is finishing up high school and starting college in the fall.  I haven’t declared a major yet, but I’m interested in Statistics because I’ve heard there is a demand and it’s one of the classes I enjoyed most in high school (also possibly minor in CS for added employability).
The thing I’m REALLY passionate about, however, is film.  I’m absolutely in love with cinema and it’s a huge part of who I am, so becoming involved in the industry would be amazing.  I know it’s a high risk industry and I’m not amazingly creative, which is why I’m not studying film in college.
My question is, to what extent are statisticians involved in the film industry?  I’m thinking there’s potential there for box office projections and data analysis mostly relating to finances.  I would love to try and set myself up for a practical career in statistics while still keeping the possibility of involvement in the film industry open.  ",4,1522327636.0,87yl19,False,"Hello, this is my first time here on the sub so excuse me if this post is strange or something.
I’m an adolescent who is finishing up high school and starting college in the fall.  I haven’t declared a major yet, but I’m interested in Statistics because I’ve heard there is a demand and it’s one of the classes I enjoyed most in high school (also possibly minor in CS for added employability).
The thing I’m REALLY passionate about, however, is film.  I’m absolutely in love with cinema and it’s a huge part of who I am, so becoming involved in the industry would be amazing.  I know it’s a high risk industry and I’m not amazingly creative, which is why I’m not studying film in college.
My question is, to what extent are statisticians involved in the film industry?  I’m thinking there’s potential there for box office projections and data analysis mostly relating to finances.  I would love to try and set myself up for a practical career in statistics while still keeping the possibility of involvement in the film industry open.  ",0,"Hello, this is my first time here on the sub so excuse me if this post is strange or something.
I’m an adolescent who is finishing up high school and starting college in the fall.  I haven’t declared a major yet, but I’m interested in Statistics because I’ve heard there is a demand and it’s one of the classes I enjoyed most in high school (also possibly minor in CS for added employability).
The thing I’m REALLY passionate about, however, is film.  I’m absolutely in love with cinema and it’s a huge part of who I am, so becoming involved in the industry would be amazing.  I know it’s a high risk industry and I’m not amazingly creative, which is why I’m not studying film in college.
My question is, to what extent are statisticians involved in the film industry?  I’m thinking there’s potential there for box office projections and data analysis mostly relating to finances.  I would love to try and set myself up for a practical career in statistics while still keeping the possibility of involvement in the film industry open.  ",5,statistics,54935,,Statistics in the Film Industry,https://www.reddit.com/r/statistics/comments/87yl19/statistics_in_the_film_industry/,all_ads,2018-03-29 08:47:16,65 days 16:48:18.036584000,
"If I have 5 independent variables and only one of them is strongly them is strongly correlated to Y with the rest being only mildly correlated... is it possible for this model to ever be as ""good"", whatever your measurement, as a model where all variables are highly correlated? My gut answer would be no, of course not. But I didn't know if that was always true.

Thanks",7,1522355995.0,880x9i,False,"If I have 5 independent variables and only one of them is strongly them is strongly correlated to Y with the rest being only mildly correlated... is it possible for this model to ever be as ""good"", whatever your measurement, as a model where all variables are highly correlated? My gut answer would be no, of course not. But I didn't know if that was always true.

Thanks",0,"If I have 5 independent variables and only one of them is strongly them is strongly correlated to Y with the rest being only mildly correlated... is it possible for this model to ever be as ""good"", whatever your measurement, as a model where all variables are highly correlated? My gut answer would be no, of course not. But I didn't know if that was always true.

Thanks",0,statistics,54935,,Do all variables need to have strong linear relationship to Y in multiple regression?,https://www.reddit.com/r/statistics/comments/880x9i/do_all_variables_need_to_have_strong_linear/,all_ads,2018-03-29 16:39:55,65 days 08:55:39.036584000,
"I am testing a moderated regression for an interaction term of two covariants, HFSS and NSLP on 4 outcome variables: TCHEXT, TCHINT, TCHPER, CLSNSS. I'm almost done but have a question I'm hoping to find some help with (my advisor is at a conference)

1. My results show that with the interaction term added, HFSS is no longer significant. NSLP continues to have moderate significance. However, HFSS*NSLP is also not significant (p=.748). Can someone help me interpret this? Does this mean the initial impacts of FSS is due to NSLP, but not the interaction of NSLP and FSS?

Thank you!",3,1522372717.0,8833hr,False,"I am testing a moderated regression for an interaction term of two covariants, HFSS and NSLP on 4 outcome variables: TCHEXT, TCHINT, TCHPER, CLSNSS. I'm almost done but have a question I'm hoping to find some help with (my advisor is at a conference)

1. My results show that with the interaction term added, HFSS is no longer significant. NSLP continues to have moderate significance. However, HFSS*NSLP is also not significant (p=.748). Can someone help me interpret this? Does this mean the initial impacts of FSS is due to NSLP, but not the interaction of NSLP and FSS?

Thank you!",0,"I am testing a moderated regression for an interaction term of two covariants, HFSS and NSLP on 4 outcome variables: TCHEXT, TCHINT, TCHPER, CLSNSS. I'm almost done but have a question I'm hoping to find some help with (my advisor is at a conference)

1. My results show that with the interaction term added, HFSS is no longer significant. NSLP continues to have moderate significance. However, HFSS*NSLP is also not significant (p=.748). Can someone help me interpret this? Does this mean the initial impacts of FSS is due to NSLP, but not the interaction of NSLP and FSS?

Thank you!",0,statistics,54935,,Moderated Linear Regression Significance,https://www.reddit.com/r/statistics/comments/8833hr/moderated_linear_regression_significance/,all_ads,2018-03-29 21:18:37,65 days 04:16:57.036584000,
"How would the hypothesis test be written for determining if the 40 yard dash time is related to the amount of rushing yards. This is for a project for school and I cannot figure it out. Is it an independent hypothesis test?

Thank You!",0,1522376806.0,883o0e,False,"How would the hypothesis test be written for determining if the 40 yard dash time is related to the amount of rushing yards. This is for a project for school and I cannot figure it out. Is it an independent hypothesis test?

Thank You!",0,"How would the hypothesis test be written for determining if the 40 yard dash time is related to the amount of rushing yards. This is for a project for school and I cannot figure it out. Is it an independent hypothesis test?

Thank You!",0,statistics,54935,,Hypothesis Tests,https://www.reddit.com/r/statistics/comments/883o0e/hypothesis_tests/,all_ads,2018-03-29 22:26:46,65 days 03:08:48.036584000,
"Title says all. I'm trying to solve for cohen's d, but all I have is the adjusted mean difference with the SD included. [This](http://ajcc.aacnjournals.org/content/16/6/575.long) is the link to the paper I'm using.

I'm using G* power, but without cohen's d it's pretty much useless to me. Is there any way to solve with just this info alone? 

Thanks

Edit: For those wondering, I'm trying to find cohen's D for the Epinephrine and Anxiety scores from [this](http://ajcc.aacnjournals.org/content/16/6/575/T3.large.jpg) table. Again, the issues is that I only have the adjusted mean difference, not the actual means between the dog and volunteer groups. I just want to be clear: I don't want the answer, I just want to know if it's possible to calculate this, and if so, how.",6,1522348062.0,8806vn,False,"Title says all. I'm trying to solve for cohen's d, but all I have is the adjusted mean difference with the SD included. [This](http://ajcc.aacnjournals.org/content/16/6/575.long) is the link to the paper I'm using.

I'm using G* power, but without cohen's d it's pretty much useless to me. Is there any way to solve with just this info alone? 

Thanks

Edit: For those wondering, I'm trying to find cohen's D for the Epinephrine and Anxiety scores from [this](http://ajcc.aacnjournals.org/content/16/6/575/T3.large.jpg) table. Again, the issues is that I only have the adjusted mean difference, not the actual means between the dog and volunteer groups. I just want to be clear: I don't want the answer, I just want to know if it's possible to calculate this, and if so, how.",0,"Title says all. I'm trying to solve for cohen's d, but all I have is the adjusted mean difference with the SD included. [This](http://ajcc.aacnjournals.org/content/16/6/575.long) is the link to the paper I'm using.

I'm using G* power, but without cohen's d it's pretty much useless to me. Is there any way to solve with just this info alone? 

Thanks

Edit: For those wondering, I'm trying to find cohen's D for the Epinephrine and Anxiety scores from [this](http://ajcc.aacnjournals.org/content/16/6/575/T3.large.jpg) table. Again, the issues is that I only have the adjusted mean difference, not the actual means between the dog and volunteer groups. I just want to be clear: I don't want the answer, I just want to know if it's possible to calculate this, and if so, how.",1,statistics,54935,,Is it possible to calculate effect size from an adjusted mean difference with a SD included?,https://www.reddit.com/r/statistics/comments/8806vn/is_it_possible_to_calculate_effect_size_from_an/,all_ads,2018-03-29 14:27:42,65 days 11:07:52.036584000,
"Hey /r/statistics!

I have a question to ask you. Today I was performing a statistical analysis for a client and I need to calculate stat. differences with standardized residuals. 

Link to the xtabs table from SPSS: https://imgur.com/pC1eJD0

The question is: How can I interpret these results? In the age group 50+ years significantly more (5% error) said ""Yes"" and sig. less said ""No"", but what about age groups 36-40, 41-45, 46-50? Significantly more (5%) said ""No"" while not significantly less said ""Yes""?

I would very much appreciate any help I can get :)
",0,1522336977.0,87zcja,False,"Hey /r/statistics!

I have a question to ask you. Today I was performing a statistical analysis for a client and I need to calculate stat. differences with standardized residuals. 

Link to the xtabs table from SPSS: https://imgur.com/pC1eJD0

The question is: How can I interpret these results? In the age group 50+ years significantly more (5% error) said ""Yes"" and sig. less said ""No"", but what about age groups 36-40, 41-45, 46-50? Significantly more (5%) said ""No"" while not significantly less said ""Yes""?

I would very much appreciate any help I can get :)
",0,"Hey /r/statistics!

I have a question to ask you. Today I was performing a statistical analysis for a client and I need to calculate stat. differences with standardized residuals. 

Link to the xtabs table from SPSS: https://imgur.com/pC1eJD0

The question is: How can I interpret these results? In the age group 50+ years significantly more (5% error) said ""Yes"" and sig. less said ""No"", but what about age groups 36-40, 41-45, 46-50? Significantly more (5%) said ""No"" while not significantly less said ""Yes""?

I would very much appreciate any help I can get :)
",0,statistics,54935,,Standardized residuals with dichotomous variables,https://www.reddit.com/r/statistics/comments/87zcja/standardized_residuals_with_dichotomous_variables/,all_ads,2018-03-29 11:22:57,65 days 14:12:37.036584000,
My teacher told me that in a confrence so I don’t have a valid source with an explanation.,73,1522216847.0,87ml77,False,My teacher told me that in a confrence so I don’t have a valid source with an explanation.,0,My teacher told me that in a confrence so I don’t have a valid source with an explanation.,174,statistics,54935,,"TIL you should almost never use the pie chart because the human eye has a hard time determining angles that are close, whereas it‘s easy to tell if a bar is longer/shorter even in close differences.",https://www.reddit.com/r/statistics/comments/87ml77/til_you_should_almost_never_use_the_pie_chart/,all_ads,2018-03-28 02:00:47,66 days 23:34:47.036584000,
